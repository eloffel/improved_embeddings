foundations and trends r(cid:1) in
information retrieval
vol. 4, no. 3 (2010) 175   246
c(cid:1) 2010 c. olston and m. najork
doi: 10.1561/1500000017

web crawling

by christopher olston and marc najork

contents

1 introduction

1.1 challenges
1.2 outline

2 crawler architecture

2.1 chronology
2.2 architecture overview
2.3 key design points

3 crawl ordering problem

3.1 model
3.2 web characteristics
3.3 taxonomy of crawl ordering policies

4 batch crawl ordering

4.1 comprehensive crawling
4.2 scoped crawling
4.3 e   cient large-scale implementation

176

178
179

180

180
184
185

194

195
197
202

203

204
208
213

5 incremental crawl ordering

5.1 maximizing freshness
5.2 capturing updates
5.3 e   cient large-scale implementation

6 avoiding problematic and undesirable content

6.1 redundant content
6.2 crawler traps
6.3 web spam
6.4 cloaked content

7 deep web crawling

7.1 types of deep web sites
7.2 problem overview
7.3 content extraction

8 future directions

references

215

217
222
223

225

225
226
227
228

230

230
232
232

236

239

foundations and trends r(cid:1) in
information retrieval
vol. 4, no. 3 (2010) 175   246
c(cid:1) 2010 c. olston and m. najork
doi: 10.1561/1500000017

web crawling

christopher olston1 and marc najork2

1 yahoo! research, 701 first avenue, sunnyvale, ca, 94089, usa

2 microsoft research, 1065 la avenida, mountain view, ca, 94043, usa

olston@yahoo-inc.com

najork@microsoft.com

abstract

this is a survey of the science and practice of web crawling. while at
   rst glance web crawling may appear to be merely an application of
breadth-   rst-search, the truth is that there are many challenges ranging
from systems concerns such as managing very large data structures
to theoretical questions such as how often to revisit evolving content
sources. this survey outlines the fundamental challenges and describes
the state-of-the-art models and solutions. it also highlights avenues for
future work.

1

introduction

a web crawler (also known as a robot or a spider) is a system for the
bulk downloading of web pages. web crawlers are used for a variety of
purposes. most prominently, they are one of the main components of
web search engines, systems that assemble a corpus of web pages, index
them, and allow users to issue queries against the index and    nd the web
pages that match the queries. a related use is web archiving (a service
provided by e.g., the internet archive [77]), where large sets of web pages
are periodically collected and archived for posterity. a third use is web
data mining, where web pages are analyzed for statistical properties,
or where data analytics is performed on them (an example would be
attributor [7], a company that monitors the web for copyright and
trademark infringements). finally, web monitoring services allow their
clients to submit standing queries, or triggers, and they continuously
crawl the web and notify clients of pages that match those queries (an
example would be gigaalert [64]).

the raison d     etre for web crawlers lies in the fact that the web is
not a centrally managed repository of information, but rather consists

176

177

of hundreds of millions of independent web content providers, each one
providing their own services, and many competing with one another.
in other words, the web can be viewed as a federated information repos-
itory, held together by a set of agreed-upon protocols and data formats,
such as the transmission control protocol (tcp), the domain name
service (dns), the hypertext transfer protocol (http), the hyper-
text markup language (html) and the robots exclusion protocol. so,
content aggregators (such as search engines or web data miners) have
two choices: they can either adopt a pull model where they will proac-
tively scour the web for new or updated information, or they could
try to establish a convention and a set of protocols enabling content
providers to push content of interest to the aggregators. indeed, the
harvest system [24], one of the earliest search services, adopted such
a push model. however, this approach did not succeed, and virtually
all content aggregators adopted the pull approach, with a few pro-
visos to allow content providers to exclude all or part of their content
from being crawled (the robots exclusion protocol) and to provide hints
about their content, its importance and its rate of change (the sitemaps
protocol [110]).

there are several reasons why the push model did not become the
primary means of acquiring content for search engines and other content
aggregators: the fact that web servers are highly autonomous means
that the barrier of entry to becoming a content provider is quite low,
and the fact that the web protocols were at least initially extremely
simple lowered the barrier even further     in fact, this simplicity is
viewed by many as the reason why the web succeeded where earlier
hypertext systems had failed. adding push protocols would have com-
plicated the set of web protocols and thus raised the barrier of entry for
content providers, while the pull model does not require any extra pro-
tocols. by the same token, the pull model lowers the barrier of entry for
content aggregators as well: launching a crawler does not require any
a priori buy-in from content providers, and indeed there are over 1,500
operating crawlers [47], extending far beyond the systems employed by
the big search engines. finally, the push model requires a trust relation-
ship between content provider and content aggregator, something that
is not given on the web at large     indeed, the relationship between

178

introduction

content providers and search engines is characterized by both mutual
dependence and adversarial dynamics (see section 6).

1.1 challenges

the basic web crawling algorithm is simple: given a set of seed uni-
form resource locators (urls), a crawler downloads all the web pages
addressed by the urls, extracts the hyperlinks contained in the pages,
and iteratively downloads the web pages addressed by these hyperlinks.
despite the apparent simplicity of this basic algorithm, web crawling
has many inherent challenges:

    scale. the web is very large and continually evolving.
crawlers that seek broad coverage and good freshness must
achieve extremely high throughput, which poses many di   -
cult engineering problems. modern search engine companies
employ thousands of computers and dozens of high-speed
network links.
    content selection tradeo   s. even the highest-throughput
crawlers do not purport to crawl the whole web, or keep up
with all the changes. instead, crawling is performed selec-
tively and in a carefully controlled order. the goals are to
acquire high-value content quickly, ensure eventual coverage
of all reasonable content, and bypass low-quality, irrelevant,
redundant, and malicious content. the crawler must balance
competing objectives such as coverage and freshness, while
obeying constraints such as per-site rate limitations. a bal-
ance must also be struck between exploration of potentially
useful content, and exploitation of content already known to
be useful.
    social obligations. crawlers should be    good citizens    of
the web, i.e., not impose too much of a burden on the web
sites they crawl. in fact, without the right safety mecha-
nisms a high-throughput crawler can inadvertently carry out
a denial-of-service attack.
    adversaries. some content providers seek to inject use-
less or misleading content into the corpus assembled by

1.2 outline

179

the crawler. such behavior is often motivated by    nancial
incentives, for example (mis)directing tra   c to commercial
web sites.

1.2 outline

web crawling is a many-faceted topic, and as with most interesting
topics it cannot be split into fully orthogonal subtopics. bearing that
in mind, we structure the survey according to    ve relatively distinct
lines of work that occur in the literature:

    building an e   cient, robust and scalable crawler (section 2).
    selecting a traversal order of the web graph, assuming
content is well-behaved and is interconnected via html
hyperlinks (section 4).
    scheduling revisitation of previously crawled content (sec-
tion 5).
    avoiding problematic and undesirable content (section 6).
    crawling so-called    deep web    content, which must be
accessed via html forms rather than hyperlinks (section 7).

section 3 introduces the theoretical crawl ordering problem studied
in sections 4 and 5, and describes structural and evolutionary proper-
ties of the web that in   uence crawl ordering. section 8 gives a list of
open problems.

2

crawler architecture

this section    rst presents a chronology of web crawler development,
and then describes the general architecture and key design points of
modern scalable crawlers.

2.1 chronology

web crawlers are almost as old as the web itself. in the spring of 1993,
shortly after the launch of ncsa mosaic, matthew gray implemented
the world wide web wanderer [67]. the wanderer was written in perl
and ran on a single machine. it was used until 1996 to collect statistics
about the evolution of the web. moreover, the pages crawled by the
wanderer were compiled into an index (the    wandex   ), thus giving
rise to the    rst search engine on the web. in december 1993, three
more crawler-based internet search engines became available: jump-
station (implemented by jonathan fletcher; the design has not been
written up), the world wide web worm [90], and the rbse spider [57].
webcrawler [108] joined the    eld in april 1994, and momspider [61]
was described the same year. this    rst generation of crawlers identi   ed
some of the de   ning issues in web crawler design. for example, mom-

180

2.1 chronology

181

spider considered politeness policies: it limited the rate of requests
to each site, it allowed web sites to exclude themselves from purview
through the nascent robots exclusion protocol [83], and it provided a
   black-list    mechanism that allowed the crawl operator to exclude sites.
webcrawler supported parallel downloading of web pages by structur-
ing the system into a central crawl manager and 15 separate download-
ing processes. however, the design of these early crawlers did not focus
on scalability, and several of them (rbse spider and webcrawler) used
general-purpose database management systems to store the state of the
crawl. even the original lycos crawler [89] ran on a single machine, was
written in perl, and used perl   s associative arrays (spilt onto disk using
the dbm database manager) to maintain the set of urls to crawl.

the following few years saw the arrival of several commercial search
engines (lycos, infoseek, excite, altavista, and hotbot), all of which
used crawlers to index tens of millions of pages; however, the design of
these crawlers remains undocumented.

mike burner   s description of the internet archive crawler [29] was
the    rst paper that focused on the challenges caused by the scale of the
web. the internet archive crawling system was designed to crawl on
the order of 100 million urls. at this scale, it is no longer possible to
maintain all the required data in main memory. the solution proposed
by the ia paper was to crawl on a site-by-site basis, and to parti-
tion the data structures accordingly. the list of urls to be crawled
was implemented as a disk-based queue per web site. to avoid adding
multiple instances of the same url to the queue, the ia crawler main-
tained an in-memory bloom    lter [20] of all the site   s urls discovered
so far. the crawl progressed by dequeuing a url, downloading the
associated page, extracting all links, enqueuing freshly discovered on-
site links, writing all o   -site links to disk, and iterating. each crawling
process crawled 64 sites in parallel, using non-blocking input/output
(i/o) and a single thread of control. occasionally, a batch process
would integrate the o   -site link information into the various queues.
the ia design made it very easy to throttle requests to a given host,
thereby addressing politeness concerns, and dns and robot exclusion
lookups for a given web site were amortized over all the site   s urls
crawled in a single round. however, it is not clear whether the batch

182 crawler architecture

process of integrating o   -site links into the per-site queues would scale
to substantially larger web crawls.

brin and page   s 1998 paper outlining the architecture of the    rst-
generation google [25] system contains a short description of their
crawler. the original google crawling system consisted of a single
urlserver process that maintained the state of the crawl, and around
four crawling processes that downloaded pages. both urlserver and
crawlers were implemented in python. the crawling process used asyn-
chronous i/o and would typically perform about 300 downloads in par-
allel. the peak download rate was about 100 pages per second, with an
average size of 6 kb per page. brin and page identi   ed social aspects
of crawling (e.g., dealing with web masters    complaints) as a major
challenge in operating a crawling system.

with the mercator web crawler, heydon and najork presented a
   blueprint design    for web crawlers [75, 94]. mercator was written
in java, highly scalable, and easily extensible. the    rst version [75]
was non-distributed; a later distributed version [94] partitioned the
url space over the crawlers according to host name, and avoided the
potential bottleneck of a centralized url server. the second mercator
paper gave statistics of a 17-day, four-machine crawl that covered
891 million pages. mercator was used in a number of web mining
projects [27, 60, 71, 72, 95], and in 2001 replaced the    rst-generation
altavista crawler.

shkapenyuk and suel   s polybot web crawler [111] represents another
   blueprint design.    polybot is a distributed system, consisting of a
crawl manager process, multiple downloader processes, and a dns
resolver process. the paper describes scalable data structures for the
url frontier and the    seen-url    set used to avoid crawling the same
url multiple times; it also discusses techniques for ensuring polite-
ness without slowing down the crawl. polybot was able to download
120 million pages over 18 days using four machines.

the ibm webfountain crawler [56] represented another industrial-
strength design. the webfountain crawler was fully distributed.
the three major components were multi-threaded crawling processes
(   ants   ), duplicate detection processes responsible for identifying
downloaded pages with near-duplicate content, and a central controller

2.1 chronology

183

process responsible for assigning work to the ants and for monitoring
the overall state of the system. webfountain featured a very    exible
crawl scheduling mechanism that allowed urls to be prioritized, main-
tained a politeness policy, and even allowed the policy to be changed
on the    y. it was designed from the ground up to support incremental
crawling, i.e., the process of recrawling pages regularly based on their
historical change rate. the webfountain crawler was written in c++
and used mpi (the message passing interface) to facilitate communi-
cation between the various processes. it was reportedly deployed on a
cluster of 48 crawling machines [68].

ubicrawler [21] is another scalable distributed web crawler. it uses
consistent hashing to partition urls according to their host component
across crawling machines, leading to graceful performance degradation
in the event of the failure of a crawling machine. ubicrawler was able to
download about 10 million pages per day using    ve crawling machines.
ubicrawler has been used for studies of properties of the african
web [22] and to compile several reference collections of web pages [118].
recently, yan et al. described irlbot [84], a single-process web
crawler that is able to scale to extremely large web collections without
performance degradation. irlbot features a    seen-url    data struc-
ture that uses only a    xed amount of main memory, and whose perfor-
mance does not degrade as it grows. the paper describes a crawl that
ran over two months and downloaded about 6.4 billion web pages. in
addition, the authors address the issue of crawler traps (web sites with
a large, possibly in   nite number of low-utility pages, see section 6.2),
and propose ways to ameliorate the impact of such sites on the crawling
process.

finally, there are a number of open-source crawlers, two of which
deserve special mention. heritrix [78, 93] is the crawler used by the
internet archive. it is written in java and highly componentized,
and its design is quite similar to that of mercator. heritrix is multi-
threaded, but not distributed, and as such suitable for conducting mod-
erately sized crawls. the nutch crawler [62, 81] is written in java as
well. it supports distributed operation and should therefore be suitable
for very large crawls; but as of the writing of [81] it has not been scaled
beyond 100 million pages.

184 crawler architecture

2.2 architecture overview

figure 2.1 shows the high-level architecture of a prototypical dis-
tributed web crawler. the crawler consists of multiple processes run-
ning on di   erent machines connected by a high-speed network. each
crawling process consists of multiple worker threads, and each worker
thread performs repeated work cycles.

at the beginning of each work cycle, a worker obtains a url from
the frontier data structure, which dispenses urls according to their
priority and to politeness policies. the worker thread then invokes the
http fetcher. the fetcher    rst calls a dns sub-module to resolve the
host component of the url into the ip address of the corresponding
web server (using cached results of prior resolutions if possible), and
then connects to the web server, checks for any robots exclusion rules
(which typically are cached as well), and attempts to download the web
page.

if the download succeeds, the web page may or may not be stored
in a repository of harvested web pages (not shown). in either case, the
page is passed to the link extractor, which parses the page   s html
content and extracts hyperlinks contained therein. the corresponding
urls are then passed to a url distributor, which assigns each url
to a crawling process. this assignment is typically made by hashing
the urls host component, its domain, or its ip address (the latter
requires additional dns resolutions). since most hyperlinks refer to
pages on the same web site, assignment to the local crawling process is
the common case.

next, the url passes through the custom url    lter (e.g., to
exclude urls belonging to    black-listed    sites, or urls with particu-
lar    le extensions that are not of interest) and into the duplicate url
eliminator, which maintains the set of all urls discovered so far and
passes on only never-before-seen urls. finally, the url prioritizer
selects a position for the url in the frontier, based on factors such as
estimated page importance or rate of change.1

1 change rates play a role in incremental crawlers (section 2.3.5), which route fetched urls
back to the prioritizer and frontier.

2.3 key design points

185

crawling process 1

crawling process 2

dns servers

dns resolver &

cache

dns resolver &

cache

dns servers

host names

ip addresses

host names

ip addresses

web servers

http fetcher

http fetcher

web servers

html pages

html pages

link extractor

link extractor

urls

urls

url distributor

url distributor

urls

urls

urls

urls

custom url filter

custom url filter

urls

urls

urls

urls

duplicate url

eliminator

duplicate url

eliminator

urls

urls

url prioritizer

url prioritizer

urls

urls

frontier

frontier

fig. 2.1 basic crawler architecture.

2.3 key design points

web crawlers download web pages by starting from one or more
seed urls, downloading each of the associated pages, extracting the

186 crawler architecture

hyperlink urls contained therein, and recursively downloading those
pages. therefore, any web crawler needs to keep track both of the
urls that are to be downloaded, as well as those that have already
been downloaded (to avoid unintentionally downloading the same page
repeatedly). the required state is a set of urls, each associated with
a    ag indicating whether the page has been downloaded. the oper-
ations that must be supported are: adding a new url, retrieving a
url, marking a url as downloaded, and testing whether the set con-
tains a url. there are many alternative in-memory data structures
(e.g., trees or sorted lists) that support these operations. however, such
an implementation does not scale to web corpus sizes that exceed the
amount of memory available on a single machine.

to scale beyond this limitation, one could either maintain the data
structure (e.g., the tree or sorted list) on disk, or use an o   -the-shelf
database management system. either solution allows maintaining set
sizes that exceed main memory; however, the cost of accessing items in
the set (particularly for the purpose of set membership test) typically
involves a disk seek, making it a fairly expensive operation. to achieve
high performance, a more specialized approach is needed.

virtually every modern web crawler splits the crawl state into two
major data structures: one data structure for maintaining the set of
urls that have been discovered (whether downloaded or not), and
a second data structure for maintaining the set of urls that have
yet to be downloaded. the    rst data structure (sometimes called the
   url-seen test    or the    duplicated url eliminator   ) must support set
addition and set membership testing, while the second data structure
(usually called the frontier) must support adding urls, and selecting
a url to fetch next.

2.3.1 frontier data structure and politeness

a straightforward implementation of the frontier data structure is a
first-in-first-out (fifo) queue. such an implementation results in a
breadth-   rst traversal of the web graph. however, this simple approach
has drawbacks: most hyperlinks on the web are    relative    (i.e., refer
to another page on the same web server). therefore, a frontier realized

2.3 key design points

187

as a fifo queue contains long runs of urls referring to pages on
the same web server, resulting in the crawler issuing many consecutive
http requests to that server. a barrage of requests in short order
is considered    impolite,    and may be construed as a denial-of-service
attack on the web server. on the other hand, it would be wasteful for
the web crawler to space out requests to the same server without doing
other useful work in the meantime. this problem is compounded in a
multithreaded or distributed crawler that issues many http requests
in parallel.

most web crawlers obey a policy of not issuing multiple overlapping
requests to the same server. an easy way to realize this is to maintain a
mapping from web servers to crawling threads, e.g., by hashing the host
component of each url.2 in this design, each crawling thread has a sep-
arate fifo queue, and downloads only urls obtained from that queue.
a more conservative politeness policy is to space out requests to
each web server according to that server   s capabilities. for example, a
crawler may have a policy to delay subsequent requests to a server by a
multiple (say 10  ) of the time it took to download the last page from
that server. this policy ensures that the crawler consumes a bounded
fraction of the web server   s resources. it also means that in a given time
interval, fewer pages will be downloaded from slow or poorly connected
web servers than from fast, responsive web servers. in other words,
this crawling policy is biased toward well-provisioned web sites. such a
policy is well-suited to the objectives of search engines, since large and
popular web sites tend also to be well-provisioned.

the mercator web crawler implemented such an adaptive politeness
policy. it divided the frontier into two parts, a    front end    and a    back
end.    the front end consisted of a single queue q, and urls were
added to the frontier by enqueuing them into that queue. the back

2 to amortize hardware cost, many web servers use virtual hosting, meaning that multiple
symbolic host names resolve to the same ip address. simply hashing the host component
of each url to govern politeness has the potential to overload such web servers. a better
scheme is to resolve the urls symbolic host name to an ip address and use a hash of that
address to assign urls to a queue. the drawback of that approach is that the latency
of dns resolution can be high (see section 2.3.3), but fortunately there tends to be a
high amount of locality in the stream of discovered host names, thereby making caching
e   ective.

188 crawler architecture

end consisted of many separate queues; typically three times as many
queues as crawling threads. each queue contained urls belonging to a
single web server; a table t on the side maintained a mapping from web
servers to back-end queues. in addition, associated with each back-end
queue q was a time t at which the next url from q may be processed.
these (q, t) pairs were organized into an in-memory priority queue, with
the pair with lowest t having the highest priority. each crawling thread
obtained a url to download by removing the highest-priority entry
(q, t) from the priority queue, waiting if necessary until time t had been
reached, dequeuing the next url u from q, downloading it, and    nally
reinserting the pair (q, tnow + k    x) into the priority queue, where tnow
is the current time, x is the amount of time it took to download u, and k
is a    politeness parameter   ; typically 10. if dequeuing u from q left q
empty, the crawling thread would remove the mapping from host(u)
to q from t , repeatedly dequeue a url u(cid:2) from q and enqueue u(cid:2) into
the back-end queue identi   ed by t (host(u(cid:2))), until it found a u(cid:2) such
that host(u(cid:2)) was not contained in t . at this point, it would enqueue
u(cid:2) in q and update t to map host(u(cid:2)) to q.

in addition to obeying politeness policies that govern the rate at
which pages are downloaded from a given web site, web crawlers may
also want to prioritize the urls in the frontier. for example, it may
be desirable to prioritize pages according to their estimated usefulness
(based for example on their id95 [101], the tra   c they receive,
the reputation of the web site, or the rate at which the page has
been updated in the past). the page ordering question is discussed in
section 4.

assuming a mechanism for assigning crawl priorities to web pages, a
crawler can structure the frontier (or in the mercator design described
above, the front-end queue) as a disk-based priority queue ordered by
usefulness. the standard implementation of a priority queue is a heap,
and insertions into a heap of n elements require log(n) element accesses,
each access potentially causing a disk seek, which would limit the data
structure to a few hundred insertions per second     far less than the
url ingress required for high-performance crawling.

an alternative solution is to    discretize    priorities into a    xed num-
ber of priority levels (say 10 to 100 levels), and maintain a separate url

2.3 key design points

189

fifo queue for each level. a url is assigned a discrete priority level,
and inserted into the corresponding queue. to dequeue a url, either
the highest-priority nonempty queue is chosen, or a randomized policy
biased toward higher-priority queues is employed.

2.3.2 url seen test

as outlined above, the second major data structure in any modern
crawler keeps track of the set of urls that have been previously dis-
covered and added to frontier. the purpose of this data structure is
to avoid adding multiple instances of the same url to the frontier;
for this reason, it is sometimes called the url-seen test (ust) or the
duplicate url eliminator (due). in a simple batch crawling setting
in which pages are downloaded only once, the ust needs to support
insertion and set membership testing; in a continuous crawling setting
in which pages are periodically re-downloaded (see section 2.3.5), it
must also support deletion, in order to cope with urls that no longer
point to a valid page.

there are multiple straightforward in-memory implementations of
a ust, e.g., a hash table or bloom    lter [20]. as mentioned above, in-
memory implementations do not scale to arbitrarily large web corpora;
however, they scale much further than in-memory implementations of
the frontier, since each url can be compressed to a much smaller
token (e.g., a 10-byte hash value). commercial search engines employ
distributed crawlers (section 2.3.4), and a hash table realizing the ust
can be partitioned across the machines in the crawling cluster, further
increasing the limit of how far such an in-memory implementation can
be scaled out.

if memory is at a premium, the state of the ust must reside on
disk. in a disk-based hash table, each lookup requires a disk seek,
severely limiting the throughput. caching popular urls can increase
the throughput by about an order of magnitude [27] to a few thousand
lookups per second, but given that the average web page contains on
the order of a hundred links and that each link needs to be tested for
novelty, the crawling rate would still be limited to tens of pages per
second under such an implementation.

190 crawler architecture

while the latency of disk seeks is poor (a few hundred seeks per
second), the bandwidth of disk reads and writes is quite high (on the
order of 50   100 mb per second in modern disks). so, implementations
performing random    le accesses perform poorly, but those that perform
streaming sequential reads or writes can achieve reasonable through-
put. the mercator crawler leveraged this observation by aggregating
many set lookup and insertion operations into a single large batch, and
processing this batch by sequentially reading a set of sorted url hashes
from disk and writing them (plus the hashes of previously undiscovered
urls) out to a new    le [94].

this approach implies that the set membership is delayed: we only
know whether a url is new after the batch containing the url has
been merged with the disk    le. therefore, we cannot decide whether
to add the url to the frontier until the merge occurs, i.e., we need
to retain all the urls in a batch, not just their hashes. however, it is
possible to store these urls temporarily on disk and read them back
at the conclusion of the merge (again using purely sequential i/o),
once it is known that they had not previously been encountered and
should thus be added to the frontier. adding urls to the frontier in
a delayed fashion also means that there is a lower bound on how soon
they can be crawled; however, given that the frontier is usually far
larger than a due batch, this delay is imperceptible except for the
most high-priority urls.

the irlbot crawler [84] uses a re   nement of the mercator scheme,
where the batch of urls arriving at the due is also written to disk,
distributed over multiple    les keyed by the pre   x of each hash. once
the size of the largest    le exceeds a certain threshold, the    les that
together hold the batch are read back into memory one by one and
merge-sorted into the main url hash    le on disk. at the conclusion
of the merge, urls are forwarded to the frontier as in the mercator
scheme. because irlbot stores the batch on disk, the size of a single
batch can be much larger than mercator   s in-memory batches, so the
cost of the merge-sort with the main url hash    le is amortized over
a much larger set of urls.

in the mercator scheme and its irlbot variant, merging a batch of
urls into the disk-based hash    le involves reading the entire old hash

2.3 key design points

191

   le and writing out an updated version. hence, the time requirement
is proportional to the number of discovered urls. a modi   cation of
this design is to store the url hashes on disk in sorted order as before,
but sparsely packed rather than densely packed. the k highest-order
bits of a hash determine the disk block where this hash resides (if it is
present). merging a batch into the disk    le is done in place, by reading
a block for which there are hashes in the batch, checking which hashes
are not present in that block, and writing the updated block back to
disk. thus, the time requirement for merging a batch is proportional to
the size of the batch, not the number of discovered urls (albeit with
high constant due to disk seeks resulting from skipping disk blocks).
once any block in the    le    lls up completely, the disk    le is rewritten
to be twice as large, and each block contains hashes that now share
their k + 1 highest-order bits.

2.3.3 auxiliary data structures

in addition to the two main data structures discussed in sections 2.3.1
and 2.3.2     the frontier and the ust/due     web crawlers maintain
various auxiliary data structures. we discuss two: the robots exclusion
rule cache and the dns cache.

web crawlers are supposed to adhere to the robots exclusion pro-
tocol [83], a convention that allows a web site administrator to bar web
crawlers from crawling their site, or some pages within the site. this is
done by providing a    le at url /robots.txt containing rules that spec-
ify which pages the crawler is allowed to download. before attempt-
ing to crawl a site, a crawler should check whether the site supplies
a /robots.txt    le, and if so, adhere to its rules. of course, download-
ing this    le constitutes crawling activity in itself. to avoid repeatedly
requesting /robots.txt, crawlers typically cache the results of previous
requests of that    le. to bound the size of that cache, entries must
be discarded through some cache eviction policy (e.g., least-recently
used); additionally, web servers can specify an expiration time for their
/robots.txt    le (via the http expires header), and cache entries should
be discarded accordingly.

urls contain a host component (e.g., www.yahoo.com), which is
   resolved    using the domain name service (dns), a protocol that

192 crawler architecture

exposes a globally distributed mapping from symbolic host names to ip
addresses. dns requests can take quite a long time due to the request-
forwarding nature of the protocol. therefore, crawlers often maintain
their own dns caches. as with the robots exclusion rule cache, entries
are expired according to both a standard eviction policy (such as least-
recently used), and to expiration directives.

2.3.4 distributed crawling

web crawlers can be distributed over multiple machines to increase
their throughput. this is done by partitioning the url space, such
that each crawler machine or node is responsible for a subset of the
urls on the web. the url space is best partitioned across web site
boundaries [40] (where a    web site    may refer to all urls with the same
symbolic host name, same domain, or same ip address). partitioning
the url space across site boundaries makes it easy to obey politeness
policies, since each crawling process can schedule downloads without
having to communicate with other crawler nodes. moreover, all the
major data structures can easily be partitioned across site boundaries,
i.e., the frontier, the due, and the dns and robots exclusion caches
of each node contain url, robots exclusion rules, and name-to-address
mappings associated with the sites assigned to that node, and nothing
else.

crawling processes download web pages and extract urls, and
thanks to the prevalence of relative links on the web, they will be them-
selves responsible for the large majority of extracted urls. when a
process extracts a url u that falls under the responsibility of another
crawler node, it forwards u to that node. forwarding of urls can
be done through peer-to-peer tcp connections [94], a shared    le sys-
tem [70], or a central coordination process [25, 111]. the amount of
communication with other crawler nodes can be reduced by maintain-
ing a cache of popular urls, used to avoid repeat forwardings [27].

finally, a variant of distributed web crawling is peer-to-peer crawl-
ing [10, 87, 100, 112, 121], which spreads crawling over a loosely col-
laborating set of crawler nodes. peer-to-peer crawlers typically employ
some form of distributed hash table scheme to assign urls to crawler

2.3 key design points

193

nodes, enabling them to cope with sporadic arrival and departure of
crawling nodes.

2.3.5 incremental crawling

web crawlers can be used to assemble one or more static snapshots of
a web corpus (batch crawling), or to perform incremental or continu-
ous crawling, where the resources of the crawler are divided between
downloading newly discovered pages and re-downloading previously
crawled pages. e   cient incremental crawling requires a few changes
to the major data structures of the crawler. first, as mentioned in
section 2.3.2, the due should support the deletion of urls that are
no longer valid (e.g., that result in a 404 http return code). second,
urls are retrieved from the frontier and downloaded as in batch crawl-
ing, but they are subsequently reentered into the frontier. if the frontier
allows urls to be prioritized, the priority of a previously downloaded
url should be dependent on a model of the page   s temporal behavior
based on past observations (see section 5). this functionality is best
facilitated by augmenting urls in the frontier with additional infor-
mation, in particular previous priorities and compact sketches of their
previous content. this extra information allows the crawler to compare
the sketch of the just-downloaded page to that of the previous version,
for example raising the priority if the page has changed and lowering
it if it has not. in addition to content evolution, other factors such as
page quality are also often taken into account; indeed there are many
fast-changing    spam    web pages.

3

crawl ordering problem

aside from the intra-site politeness considerations discussed in sec-
tion 2, a crawler is free to visit urls in any order. the crawl order
is extremely signi   cant, because for the purpose of id190
can be considered in   nite     due to the growth rate of new content,
and especially due to dynamically generated content [8]. indeed, despite
their impressive capacity, modern commercial search engines only index
(and likely only crawl) a fraction of discoverable web pages [11]. the
crawler ordering question is even more crucial for the countless smaller-
scale crawlers that perform scoped crawling of targeted subsets of the
web.

sections 3   5 survey work on selecting a good crawler order, with a

focus on two basic considerations:

    coverage. the fraction of desired pages that the crawler
acquires successfully.
    freshness. the degree to which the acquired page snapshots
remain up-to-date, relative to the current    live    web copies.

issues related to redundant, malicious or misleading content are covered
in section 6. generally speaking, techniques to avoid unwanted content

194

3.1 model

195

can be incorporated into the basic crawl ordering approaches without
much di   culty.

3.1 model

most work on crawl ordering abstracts away the architectural details
of a crawler (section 2), and assumes that urls in the frontier
data structure can be reordered freely. the resulting simpli   ed crawl
ordering model is depicted in figure 3.1. at a given point in time,
some historical crawl order has already been executed (p1, p2, p3, p4, p5
in the diagram), and some future crawl order has been planned
(p6, p7, p4, p8, . . .).1

in the model, all pages require the same amount of time to down-
load; the (constant) rate of page downloading is called the crawl rate,
typically measured in pages/second. (section 2 discussed how to max-
imize the crawl rate; here it is assumed to be    xed.) the crawl rate
is not relevant to batch crawl ordering methods, but it is a key factor
when scheduling page revisitations in incremental crawling.

fig. 3.1 crawl ordering model.

1 some approaches treat the crawl ordering problem hierarchically, e.g., select a visitation
order for web sites, and within each site select a page visitation order. this approach
helps mitigate the complexity of managing a crawl ordering policy, and is well aligned
with policies that rely primarily on site-level metrics such as site-level id95 to drive
crawl ordering decisions. many of the insights about page-level crawl ordering also apply
at the site level.

196 crawl ordering problem

pages downloaded by the crawler are stored in a repository. the
future crawl order is determined, at least in part, by analyzing the
repository. for example, one simple policy mentioned earlier, breadth-
   rst search, extracts hyperlinks from pages entering the repository,
identi   es linked-to pages that are not already part of the (historical
or planned) crawl order, and adds them to the end of the planned
crawl order.

the content of a web page is subject to change over time, and it
is sometimes desirable to re-download a page that has already been
downloaded, to obtain a more recent snapshot of its content. as men-
tioned in section 2.3.5, two approaches exist for managing repeated
downloads:

    batch crawling. the crawl order does not contain duplicate
occurrences of any page, but the entire crawling process is
periodically halted and restarted as a way to obtain more
recent snapshots of previously crawled pages. information
gleaned from previous crawl iterations (e.g., page importance
score estimates) may be fed to subsequent ones.
    incremental crawling. pages may appear multiple times
in the crawl order, and crawling is a continuous process that
conceptually never terminates.

it is believed that most modern commercial crawlers perform incremen-
tal crawling, which is more powerful because it allows re-visitation of
pages at di   erent rates. (a detailed comparison between incremental
and batch crawling is made by cho and garc    a-molina [39].)

3.1.1 limitations

this model has led to a good deal of research with practical implica-
tions. however, as with all models, it simpli   es reality. for one thing, as
discussed in section 2, a large-scale crawler maintains its frontier data
structure on disk, which limits opportunities for reordering. generally
speaking, the approach of maintaining a prioritized ensemble of fifo
queues (see section 2.3.1) can be used to approximate a desired crawl
order. we revisit this issue in sections 4.3 and 5.3.

3.2 web characteristics

197

other real-world considerations that fall outside the model include:
    some pages (or even versions of a page) take longer to down-
load than others, due to di   erences in size and network
latency.
    crawlers take special care to space out downloads of pages
from the same server, to obey politeness constraints, see sec-
tion 2.3.1. crawl ordering policies that assume a single crawl
rate constraint can, at least in principle, be applied on a per-
server basis, i.e., run n independent copies of the policy for
n servers.
    as described in section 2, modern commercial crawlers uti-
lize many simultaneous page downloader threads, running
on many independent machines. hence rather than a single
totally ordered list of pages to download, it is more accurate
to think of a set of parallel lists, encoding a partial order.
    special care must be taken to avoid crawling redundant and
malicious content; we treat these issues in section 6.
    if the page repository runs out of space, and expanding it
is not considered worthwhile, is becomes necessary to retire
some of the pages stored there (although it may make sense to
retain some metadata about the page, to avoid recrawling it).
we are not aware of any scholarly work on how to select pages
for retirement.

3.2 web characteristics

before proceeding, we describe some structural and evolutionary prop-
erties of the web that are relevant to the crawl ordering question. the
   ndings presented here are drawn from studies that used data sets of
widely varying size and scope, taken at di   erent dates over the span of
a decade, and analyzed via a wide array of methods. hence, caution is
warranted in their interpretation.

3.2.1 static characteristics

several studies of the structure of the web graph,
in which pages
are encoded as vertices and hyperlinks as directed edges, have been

198 crawl ordering problem

conducted. one notable study is by broder et al. [26], which uncovered
a    bowtie    structure consisting of a central strongly connected com-
ponent (the core), a component that can reach the core but cannot
be reached from the core, and a component that can be reached from
the core but cannot reach the core. (in addition to these three main
components there are a number of small, irregular structures such as
disconnected components and long    tendrils.   )

hence there exist many ordered pairs of pages (p1, p2) such that
there is no way to reach p2 by starting at p1 and repeatedly following
hyperlinks. even in cases where p2 is reachable from p1, the distance
can vary greatly, and in many cases hundreds of links must be traversed.
the implications for crawling are: (1) one cannot simply crawl to depth
n, for a reasonable value of n like n = 20, and be assured of covering
the entire web graph; (2) crawling    seeds    (the pages at which a crawler
commences) should be selected carefully, and multiple seeds may be
necessary to ensure good coverage.

in an earlier study, broder et al. [28] showed that there is an abun-
dance of near-duplicate content of the web. using a corpus of 30 million
web pages collected by the altavista crawler, they used the shingling
algorithm to cluster the corpus into groups of similar pages, and found
that 29% of the pages were more than 50% similar to other pages in
the corpus, and 11% of the pages were exact duplicates of other pages.
sources of near-duplication include mirroring of sites (or portions of
sites) and url synonymy, see section 6.1.

chang et al. [35] studied the    deep web,    i.e., web sites whose con-
tent is not reachable via hyperlinks and instead can only be retrieved by
submitting html forms. the    ndings include: (1) there are over one
million deep web sites; (2) more deep web sites have structured (multi-
   eld) query interfaces than unstructured (single-   eld) ones; and (3)
most query interfaces are located within a few links of the root of a web
site, and are thus easy to    nd by shallow crawling from the root page.

3.2.2 temporal characteristics

one of the objectives of crawling is to maintain freshness of the
crawled corpus. hence it is important to understand the temporal

3.2 web characteristics

199

characteristics of the web, both in terms of site-level evolution (the
appearance and disappearance of pages on a site) and page-level evo-
lution (changing content within a page).

3.2.2.1 site-level evolution

dasgupta et al. [48] and ntoulas et al. [96] studied creation and retire-
ment of pages and links inside a number of web sites, and found the
following characteristics (these represent averages across many sites):

    new pages are created at a rate of 8% per week.
    pages are retired at a rapid pace, such that during the course
of one year 80% of pages disappear.
    new links are created at the rate of 25% per week, which is
signi   cantly faster than the rate of new page creation.
    links are retired at about the same pace as pages, with 80%
disappearing in the span of a year.
    it is possible to discover 90% of new pages by monitoring
links spawned from a small, well-chosen set of old pages (for
most sites,    ve or fewer pages su   ce, although for some sites
hundreds of pages must be monitored for this purpose). how-
ever, discovering the remaining 10% requires substantially
more e   ort.

3.2.2.2 page-level evolution

some key    ndings about the frequency with which an individual web
page undergoes a change are:

    page change events are governed by a poisson process, which
means that changes occur randomly and independently, at
least in the case of pages that change less frequently than
once a day [39].2
    page change frequencies span multiple orders of magnitude
(sub-hourly, hourly, daily, weekly, monthly, annually), and
each order of magnitude includes a substantial fraction of

2 a poisson change model was originally postulated by co   man et al. [46].

200 crawl ordering problem

pages on the web [2, 39]. this    nding motivates the study of
non-uniform page revisitation schedules.
    change frequency is correlated with visitation frequency,
url depth, domain and topic [2], as well as page length [60].
    a page   s change frequency tends to remain stationary over
time, such that past change frequency is a fairly good pre-
dictor of future change frequency [60].

unfortunately,

it appears that there is no simple relationship
between the frequency with which a page changes and the cumulative
amount of content that changes over time. as one would expect, pages
with moderate change frequency tend to exhibit a higher cumulative
amount of changed content than pages with a low change frequency.
however, pages with high change frequency tend to exhibit less cumula-
tive change than pages with moderate change frequency. on the encour-
aging side, the amount of content that changed on a page in the past
is a fairly good predictor of the amount of content that will change in
the future (although the degree of predictability varies from web site
to web site) [60, 96].

many changes are con   ned to a small, contiguous region of a web
page [60, 85], and/or only a   ect transient words that do not character-
ize the core, time-invariant theme of the page [2]. much of the    new   
content added to web pages is actually taken from other pages [96].

the temporal behavior of (regions of) web pages can be divided into
three categories: static (no changes), churn (new content supplants old
content, e.g., quote of the day), and scroll (new content is appended to
old content, e.g., blog entries). simple generative models for the three
categories collectively explain nearly all observed temporal web page
behavior [99].

most web pages include at least some static content, resulting in
an upper bound on the divergence between an old snapshot of a page
and the live copy. the shape of the curve leading to the upper bound
depends on the mixture of churn and scroll content, and the rates
of churning and scrolling. one simple way to characterize a page is
with a pair of numbers: (1) the divergence upper bound (i.e., the
amount of non-static content), under some divergence measure such

3.2 web characteristics

201

d
e
r
e
d

i
s
n
o
c

s
r
o
t
c
a
f

s
e
v
i
t
c
e
j
b
o

e
u
q
i

n
h
c
e
t

   

   

   

   

   

   

   

y
t
i
c
i
m
a
n
y
d

e
c
n
a
v
e
l
e
r

e
c
n
a
t
r
o
p
m

i

s
s
e
n
h
s
e
r
f

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

e
g
a
r
   
e
v
o
c

   

   

   

   

   

   

]
8
0
1

,
5
9

,
3
4
[

h
c
r
a
e
s

t
s
r
   
-
h
t
d
a
e
r
b

]
5
4

,
3
4
[

k
n
a
r
e
g
a
p
y
b

]
3
4
[

e
e
r
g
e
d
n

i

y
b

]
4
0
1
[

t
c
a
p
m

i

h
c
r
a
e
s

]
8
4
[

e
t
a
r

g
n
i
n
w
a
p
s

]
9
[

e
z
i
s

e
t
i
s

y
b

y
b

y
b

e
z
i
t
i
r
o
i
r
p

e
z
i
t
i
r
o
i
r
p

e
z
i
t
i
r
o
i
r
p

e
z
i
t
i
r
o
i
r
p

e
z
i
t
i
r
o
i
r
p

)
2
.
4

n
o
i
t
c
e
s
(

g
n

i
l

w
a
r
c

d
e
p
o
c
s

]
6
4

,
1
4
[

e
c
n
e
c
s
e
l
o
s
b
o

]
1
4
[

e
g
a

]
9
9
[

t
n
e
t
n
o
c

t
c
e
r
r
o
c
n

i

]
5
1
1
[

t
n
e
m

s
s
a
r
r
a
b
m
e

e
z
i
m
n
m

i

i

e
z
i
m
n
m

i

i

e
z
i
m
n
m

i

i

e
z
i
m
n
m

i

i

]
3
0
1
[

t
c
a
p
m

i

h
c
r
a
e
s

e
z
i
m
i
x
a
m

)
2
.
5

n
o
i
t
c
e
s
(

e
r
u
t
p
a
c

e
t
a
d
p
u

]
6
5
[

n
i
a
t
n
u
o
f
b
e
w

]
1
[

c

i
p
o

.
s
e
u
q
i

n
h
c
e
t

g
n

i
r
e
d
r
o

l

w
a
r
c

f
o

y
m
o
n
o
x
a
t

2
.
3

.
g
i
f

202 crawl ordering problem

as shingle [28] or word di   erence; and (2) the amount of time it takes
to reach the upper bound (i.e., the time taken for all non-static content
to change) [2].

3.3 taxonomy of crawl ordering policies

figure 3.2 presents a high-level taxonomy of published crawl ordering
techniques. the    rst group of techniques focuses exclusively on order-
ing pages for    rst-time downloading, which a   ects coverage. these can
be applied either in the batch crawling scenario, or in the incremen-
tal crawling scenario in conjunction with a separate policy governing
re-downloading of pages to maintain freshness, which is the focus of
the second group of techniques. techniques in the third group con-
sider the combined problem of interleaving    rst-time downloads with
re-downloads, to balance coverage and freshness.

as re   ected in figure 3.2, crawl ordering decisions tend to be based
on some combination of the following factors: (1) importance of a page
or site, relative to others; (2) relevance of a page or site to the pur-
pose served by the crawl; and (3) dynamicity, or how the content of a
page/site tends to change over time.

some crawl ordering techniques are broader than others in terms
of which factors they consider and which objectives they target. ones
that focus narrowly on a speci   c aspect of crawling typically aim for a
   better    solution with respect to that aspect, compared with broader
   all-in-one    techniques. on the other hand, to be usable they may need
to be extended or combined with other techniques. in some cases a
straightforward extension exists (e.g., add importance weights to an
importance-agnostic formula for scheduling revisitations), but often
not. there is no published work on the best way to combine multiple
specialized techniques into a comprehensive crawl ordering approach
that does well across the board.

the next two chapters describe the techniques summarized in fig-
ure 3.2, starting with ones geared toward batch crawling (section 4),
and then moving to incremental crawl ordering techniques (section 5).

4

batch crawl ordering

a batch crawler traverses links outward from an initial seed set of urls.
the seed set may be selected algorithmically, or by hand, based on
criteria such as importance, outdegree, or other structural features of
the web graph [120]. a common, simple approach is to use the root
page of a web directory site such as opendirectory, which links to
many important sites across a broad range of topics. after the seed set
has been visited and links have been extracted from the seed set pages,
the crawl ordering policy takes over.

the goal of the crawl ordering policy is to maximize the weighted
coverage (wc) achieved over time, given a    xed crawl rate. wc is
de   ned as:

(cid:1)

p   c(t)

wc(t) =

w(p),

where t denotes the time elapsed since the crawl began, c(t) denotes the
set of pages crawled up to time t (under the    xed crawl rate assumption,
|c(t)|     t), and w(p) denotes a numeric weight associated with page p.
the weight function w(p) is chosen to re   ect the purpose of the crawl.
for example, if the purpose is to crawl pages about helicopters, one sets
w(p) = 1 for pages about helicopters, and w(p) = 0 for all other pages.

203

204 batch crawl ordering

fig. 4.1 weighted coverage (wc) as a function of time elapsed (t) since the beginning of a
batch crawl.

figure 4.1 shows some hypothetical wc curves. typically, w(p)     0,
and hence wc(t) is monotonic in t. under a random crawl ordering pol-
icy, wc(t) is roughly linear in t; this line serves as a baseline upon which
other policies strive to improve. an omniscient policy, which downloads
pages in descending order of w(p) yields a theoretical upper-bound
curve. (for the helicopter example, the omniscient policy downloads
all helicopter pages    rst, thereby achieving maximal wc before the
end of the crawl.) policies a and b fall in-between the random and
omniscient cases, with a performing better in the early stages of the
crawl, but b performing better toward the end. the choice between a
and b depends on how long the crawl is allowed to run before being
stopped (and possibly re-started to bring in a fresh batch of pages),
and to what use, if any, pages obtained early in the crawl are put while
the crawl is still in    ight.

the above framework can be applied to comprehensive batch crawl-
ing, in which the goal is to achieve broad coverage of all types of con-
tent, as well as to scoped batch crawling, where the crawler restricts its
attention to a relatively narrow slice of the web (e.g., pages about heli-
copters). this chapter examines the two scenarios in turn, focusing ini-
tially on crawl order e   ectiveness, with implementation and e   ciency
questions covered at the end.

4.1 comprehensive crawling

when the goal
is to cover high-quality content of all varieties, a
popular choice of weight function is w(p) = pr(p), where pr(p) is

4.1 comprehensive crawling

205

p   s importance score as measured by id95 [101].1 variations on
id95 used in the crawling context include: only counting external
links, i.e., ones that go between two web sites (najork and wiener [95]
discuss some tradeo   s involved in counting all links versus only exter-
nal links); biasing the id95 random jumps to go to a trusted set of
pages believed not to engage in spamming (see cho and schonfeld [45]
for discussion); or omitting random jumps entirely, as done by abite-
boul et al. [1].

in view of maximizing coverage weighted by id95 or some
variant, three main types of crawl ordering policies have been examined
in the literature. in increasing order of complexity, they are:

    breadth-   rst search [108]. pages are downloaded in the
order in which they are    rst discovered, where discovery
occurs via extracting all links from each page immediately
after it is downloaded. breadth-   rst crawling is appealing
due to its simplicity, and it also a   ords an interesting cov-
erage guarantee: in the case of id95 that is biased to
a small trusted page set t , a breadth-   rst crawl of depth d
using t as its seed set achieves wc     1       d+1 [45], where   
is the id95 damping parameter.
    prioritize by indegree [43]. the page with the highest
number of incoming hyperlinks from previously downloaded
pages, is downloaded next. indegree is sometimes used as a
low-complexity stand-in for id95, and is hence a nat-
ural candidate for crawl ordering under a id95-based
objective.
    prioritize by id95 (variant/estimate) [1, 43, 45].
pages are downloaded in descending order of id95
(or some variant), as estimated based on the pages and links
acquired so far by the crawler. straightforward application

1 although this cumulative id95 measure has been used extensively in the literature,
boldi et al. [23] caution that absolute id95 scores may not be especially meaningful,
and contend that id95 should be viewed as a way to establish relative page order-
ings. moreover, they show that biasing a crawl toward pages with high id95 in the
crawled subgraph leads to subgraphs whose id95 ordering di   ers substantially from
the id95-induced ordering of the same pages in the full graph.

206 batch crawl ordering

of this method involves recomputing id95 scores after
each download, or updating the id95 scores incremen-
tally [38]. another option is to recompute id95 scores
only periodically, and rely on an approximation scheme
between recomputations. lastly, abiteboul et al. [1] gave an
e   cient online method of estimating a variant of id95
that does not include random jumps, designed for use in con-
junction with a crawler.

the three published empirical studies that evaluate the above poli-
cies over real web data are listed in figure 4.2 (najork and wiener [95]
evaluated only breadth-   rst search). under the objective of crawling
high-id95 pages early (w(p) = pr(p)), the main    ndings from
these studies are the following:

    starting from high-id95 seeds, breadth-   rst crawling
performs well early in the crawl (low t), but not as well as
the other policies later in the crawl (medium to high t).
    perhaps unsurprisingly, prioritization by id95 performs
well throughout the entire crawl. the shortcut of only recom-
puting id95 periodically leads to poor performance, but
the online approximation scheme by abiteboul et al. [1] per-
forms well. furthermore, in the context of repeated batch
crawls, it is bene   cial to use id95 values from previous
iterations to drive the current iteration.
    there is no consensus on prioritization by indegree: one
study (cho et al. [43]) found that it worked fairly well (almost
as well as prioritization by id95), whereas another
study (baeza-yates et al. [9]) found that it performed very

study

cho et al. [43]

najork and wiener [95]
baeza-yates et al. [9]

data set
stanford web
general web

chile and greece

data size pub. year

105
108
106

1998
2001
2005

fig. 4.2 empirical studies of batch crawl ordering policies.

4.1 comprehensive crawling

207

poorly. the reason given by baeza-yates et al. [9] for poor
performance is that it is overly greedy in going after high-
indegree pages, and therefore it takes a long time to    nd
pages that have high indegree and id95 yet are only
discoverable via low-indegree pages. the two studies are over
di   erent web collections that di   er in size by an order of mag-
nitude, and are seven years apart in time.

in addition to the aforementioned results, baeza-yates et al. [9]
proposed a crawl policy that gives priority to sites containing a large
number of discovered but uncrawled urls. according to their empirical
study, which imposed per-site politeness constraints, toward the end of
the crawl (high t) the proposed policy outperforms policies based on
breadth-   rst search, indegree, and id95. the reason is that it
avoids the problem of being left with a few very large sites at the end,
which can cause a politeness bottleneck.

baeza-yates and castillo [8] observed that although the web graph
is e   ectively in   nite, most user browsing activity is concentrated within
a small distance of the root page of each web site. arguably, a crawler
should concentrate its activities there, and avoid exploring too deeply
into any one site.

4.1.1 search relevance as the crawling objective

fetterly et al. [58] and pandey and olston [104] argued that when the
purpose of crawling is to supply content to a search engine, id95-
weighted coverage may not be the most appropriate objective. accord-
ing to the argument, it instead makes sense to crawl pages that would
be viewed or clicked by search engine users, if present in the search
index. for example, one may set out to crawl all pages that, if indexed,
would appear in the top ten results of likely queries. even if id95
is one of the factors used to rank query results, the top result for query
q1 may have lower id95 than the eleventh result for some other
query q2, especially if q2 pertains to a more established topic.

fetterly et al. [58] evaluated four crawl ordering policies (breadth-
   rst; prioritize by indegree; prioritize by trans-domain indegree;

208 batch crawl ordering

prioritize by id95) under two relevance metrics:

    maxndcg: the total normalized distributed cumulative
gain (ndcg) [79] score of a set of queries evaluated over the
crawled pages, assuming optimal ranking.
    click count: the total number of clicks the crawled pages
attracted via a commercial search engine in some time period.

the main    ndings were that prioritization by id95 is the most
reliable and e   ective method on these metrics, and that imposing per-
domain page limits boosts e   ectiveness.

pandey and olston [104] proposed a technique for explicitly ordering
pages by expected relevance impact, under the objective of maximizing
coverage weighted by the number of times a page appears among the
top n results of a user query. the relatively high computational over-
head of the technique is mitigated by concentrating on queries whose
results are likely to be improved by crawling additional pages (deemed
needy queries). relevance of frontier pages to needy queries is estimated
from cues found in urls and referring anchortext, as    rst proposed in
the context of scoped crawling [43, 74, 108], discussed next.

4.2 scoped crawling

a scoped crawler strives to limit crawling activities to pages that fall
within a particular category or scope, thereby acquiring in-scope con-
tent much faster and more cheaply than via a comprehensive crawl.
scope may be de   ned according to topic (e.g., pages about aviation),
geography (e.g., pages about locations in and around oldenburg, ger-
many [6]), format (e.g., images and multimedia), genre (e.g., course
syllabi [51]), language (e.g., pages in portuguese [65]), or other aspects.
(broadly speaking, page importance, which is the primary crawl order-
ing criterion discussed in section 4.1, can also be thought of as a form
of scope.)

usage scenarios for scoped crawling include mining tasks that call
for crawling a particular type of content (e.g., images of animals), per-
sonalized search engines that focus on topics of interest to a partic-
ular user (e.g., aviation and gardening), and search engines for the

4.2 scoped crawling

209

   deep web    that use a surface-web crawler to locate gateways to deep-
web content (e.g., html form interfaces). in another scenario, one sets
out to crawl the full web by deploying many small-scale crawlers, each
of which is responsible for a di   erent slice of the web     this approach
permits specialization to di   erent types of content, and also facilitates
loosely coupled distributed crawling (section 2.3.4).

(cid:2)

as with comprehensive crawling (section 4.1), the mathematical
objective typically associated with scoped crawling is maximization of
p   c(t) w(p). in scoped crawling, the role
weighted coverage wc(t) =
of the weight function w(p) is to re   ect the degree to which page p falls
within the intended scope. in the simplest case, w(p)     {0,1}, where 0
denotes that p is outside the scope and 1 denotes that p is in-scope.
hence weighted coverage measures the fraction of crawled pages that
are in-scope, analogous to the precision metric used in information
retrieval.

typically the in-scope pages form a    nite set (whereas the full web is
often treated as in   nite, as mentioned above). hence it makes sense to
measure recall in addition to precision. two recall-oriented evaluation
techniques have been proposed: (1) designate a few representative in-
scope pages by hand, and measure what fraction of them are discovered
by the crawler [92]; (2) measure the overlap among independent crawls
initiated from di   erent seeds, to see whether they converge on the same
set of pages [34].

topical crawling (also known as    focused crawling   ), in which in-
scope pages are ones that are relevant to a particular topic or set of
topics, is by far the most extensively studied form of scoped crawling.
work on other forms of scope     e.g., pages with form interfaces [14],
and pages within a geographical scope [6, 63]     tends to use similar
methods to the ones used for topical crawling. hence we primarily
discuss topical crawling from this point forward.

4.2.1 topical crawling

the basic observation exploited by topical crawlers is that relevant
pages tend to link to other relevant pages, either directly or via short
chains of links. (this feature of the web has been veri   ed empirically

210 batch crawl ordering

in many studies, including chakrabarti et al. [34] and cho et al. [43].)
the    rst crawl ordering technique to exploit this observation was    sh
search [53]. the    sh search crawler categorized each crawled page p as
either relevant or irrelevant (a binary categorization), and explored the
neighborhood of each relevant page up to depth d looking for additional
relevant pages.

a second generation of topical crawlers [43, 74, 108] explored the
neighborhoods of relevant pages in a non-uniform fashion, opting to
traverse the most promising links    rst. the link traversal order was
governed by individual relevance estimates assigned to each linked-to
page (a continuous relevance metric is used, rather than a binary one).
if a crawled page p links to an uncrawled page q, the relevance estimate
for q is computed via analysis of the text surrounding p   s link to q (i.e.,
the anchortext and text near the anchortext2), as well as q   s url.
in one variant, relevance estimates are smoothed by associating some
portion of p   s relevance score (and perhaps also the relevance scores
of pages linking to p, and so on in a recursive fashion), with q. the
motivation for smoothing the relevance scores is to permit discovery of
pages that are relevant yet lack indicative anchortext or urls.

a third-generation approach based on machine learning and link
structure analysis was introduced by chakrabarti et al. [33, 34]. the
approach leverages pre-existing topic taxonomies such as the open
directory and yahoo!   s web directory, which supply examples of web
pages matching each topic. these example pages are used to train a
classi   er3 to map newly encountered pages into the topic taxonomy.
the user selects a subset of taxonomy nodes (topics) of interest to crawl,
and the crawler preferentially follows links from pages that the classi-
   er deems most relevant to the topics of interest. links from pages that
match    parent topics    are also followed (e.g., if the user indicated an
interest in bicycling, the crawler follows links from pages about sports
in general). in addition, an attempt is made to identify hub pages    
pages with a collection of links to topical pages     using the hits link

2 this aspect was studied in detail by pant and srinivasan [107].
3 pant and srinivasan [106] o   er a detailed study of classi   er choices for topical crawlers.

4.2 scoped crawling

211

analysis algorithm [82]. links from hub pages are followed with higher
priority than other links.

the empirical    ndings of chakrabarti et al. [34] established topical

crawling as a viable and e   ective paradigm:

    a general web crawler seeded with topical pages quickly
becomes mired in irrelevant regions of the web, yielding very
poor weighted coverage. in contrast, a topical crawler suc-
cessfully stays within scope, and explores a steadily growing
population of topical pages over time.
    two topical crawler instances, started from disparate seeds,
converge on substantially overlapping sets of pages.

beyond the basics of topical crawling discussed above, there are two

key considerations [92]: greediness and adaptivity.

4.2.1.1 greediness

paths between pairs of relevant pages sometimes pass through one or
more irrelevant pages. a topical crawler that is too greedy will stop
when it reaches an irrelevant page, and never discovers subsequent rel-
evant page(s). on the other extreme, a crawler that ignores relevance
considerations altogether degenerates into a non-topical crawler, and
achieves very poor weighted coverage, as we have discussed. the ques-
tion of how greedily to crawl is an instance of the explore versus exploit
tradeo    observed in many contexts. in this context, the question is: how
should the crawler balance exploitation of direct links to (apparently)
relevant pages, with exploration of other links that may, eventually,
lead to relevant pages?

in the approach of hersovici et al. [74], a page p inherits some of the
relevance of the pages that link to p, and so on in a recursive fashion.
this passing along of relevance forces the crawler to traverse irrelevant
pages that have relevant ancestors. a decay factor parameter controls
how rapidly relevance decays as more links are traversed. eventually, if
no new relevant pages are encountered, relevance approaches zero and
the crawler ceases exploration on that path.

212 batch crawl ordering

later work by diligenti et al. [54] proposed to classify pages accord-
ing to their distance from relevant pages. each uncrawled page p is
assigned a distance estimate d(p)     [0,   ) that represents the crawler   s
best guess as to how many links lie between p and the nearest rele-
vant page.4 pages are ordered for crawling according to d(  ). as long
as one or more uncrawled pages having d(p) = 0 are available, the
crawler downloads those pages; if not, the crawler resorts to down-
loading d(p) = 1 pages, and so on. the threshold used to separate    rel-
evant    pages from    irrelevant    ones controls greediness: if the threshold
is strict (i.e., only pages with strong relevance indications are classi   ed
as    relevant   ), then the crawler will favor long paths to strongly rele-
vant pages over short paths to weakly relevant pages, and vice versa.
a simple meta-heuristic to control the greediness of a crawler was
proposed by menczer et al. [92]: rather than continuously adjusting
the crawl order as new pages and new links are discovered, commit
to crawling n pages from the current crawl order before reordering.
this heuristic has the attractive property that it can be applied in
conjunction with any existing crawl ordering policy. menczer et al. [92]
demonstrated empirically that this heuristic successfully controls the
level of greediness, and that there is bene   t in not being too greedy,
in terms of improved weighted coverage in the long run. the study
done by diligenti et al. [54] also showed improved long-term weighted
coverage by not being overly greedy. we are not aware of any attempts
to characterize the optimal level of greediness.

4.2.1.2 adaptivity

in most topical crawling approaches, once the crawler is unleashed, the
page ordering strategy is    xed for the duration of the crawl. some have
studied ways for a crawler to adapt its strategy over time, in response to
observations made while the crawl is in    ight. for example, aggarwal
et al. [5] proposed a method to learn on the    y how best to combine

4 the d(  ) function can be trained in the course of the crawl as relevant and irrelevant pages
are encountered at various distances from one another. as an optional enhancement to
accelerate the training process, ancestors of page p are located using a full-web search
engine that services    links-to    queries, and added to the training data.

4.3 e   cient large-scale implementation

213

relevance signals found in the content of pages linking to p, content
of p   s    siblings    (pages linked to by the same    parent    page), and
p   s url, into a single relevance estimate on which to base the crawl
order.

evolutionary algorithms (e.g., id107) have been
explored as a means to adapt crawl behavior over time [37, 80, 91].
for example, the infospiders approach [91] employs many indepen-
dent crawling agents, each with its own relevance classi   er that adapts
independently over time. agents reproduce and die according to an
evolutionary process: agents that succeed in locating relevant content
multiply and mutate, whereas unsuccessful ones die o   . the idea is to
improve relevance estimates over time, and also to achieve specializa-
tion whereby di   erent agents become adept at locating di   erent pockets
of relevant content.

4.3 e   cient large-scale implementation

as discussed in section 2.3.1, breadth-   rst crawl ordering can use sim-
ple disk-based fifo queues. basic scoped crawling methods also a   ord
a fairly simple and e   cient implementation: the process of assess-
ing page relevance and assigning priorities to extracted urls can
occur in the main page processing pipeline,5 and a disk-based prior-
ity queue may be used to maintain the crawling frontier (also discussed
in section 2.3.1).

most of the comprehensive (non-scoped) approaches also oper-
ate according to numerical page priorities, but the priority values of
enqueued pages are subject to change over time as new information is
uncovered (e.g., new links to a page). the more sophisticated scoped
crawling approaches also leverage global information (e.g., chakrabarti
et al. [34] used link analysis to identify topical hub pages), and there-
fore fall into this category as well. with time-varying priorities, one
approach is to recompute priority values periodically     either from

5 in cases where relevance is assessed via a trained classi   er, training (and optional periodic
retraining) can occur o   ine and out of the way of the main crawling pipeline.

214 batch crawl ordering

scratch or incrementally6     using distributed disk-based methods sim-
ilar to those employed in database and map-reduce environments.

aside from facilitating scalable implementation, delaying the propa-
gation of new signals to the crawl order has a side-e   ect of introducing
an exploration component into an otherwise exploitation-dominated
approach, which is of particular signi   cance in the scoped crawling
case (see section 4.2.1.1). on the other hand, some time-critical crawl-
ing opportunities (e.g., a new entry of a popular blog) might be com-
promised. one way to mitigate this problem is to assign initial priority
estimates that are not based on global analysis, e.g., using site-level
features (e.g., site-level id95), url features (e.g., number of char-
acters or slashes in the url string), or features of the page from which
a url has been extracted (e.g., that page   s id95).

the opic approach [1] propagates numerical    cash    values to urls
extracted from crawled pages, including urls already in the frontier.
the intention is to maintain a running approximation of id95,
without the high overhead of the full id95 computation. if enough
memory is (collectively) available on the crawling machines, cash coun-
ters can be held in memory and incremented in near-real-time (cash
   ows across machine boundaries can utilize mpi or another message-
passing protocol). alternatively, following the deferred-updating rubric
mentioned above, cash increments can be logged to a side    le, with peri-
odic summing of cash values using a disk-based sort-merge algorithm.

6 basic computations like counting links can be maintained incrementally using e   cient
disk-based view maintenance algorithms; more elaborate computations like id95 tend
to be more di   cult but o   er some opportunities for incremental maintenance [38].

5

incremental crawl ordering

in contrast to a batch crawler, a continuous or incremental crawler
never    starts over.    instead, it continues running forever (conceptually
speaking). to maintain freshness of old crawled content, an incremental
crawler interleaves revisitation of previously crawled pages with    rst-
time visitation of new pages. the aim is to achieve good freshness and
coverage simultaneously.

coverage is measured according to the same weighted coverage
metric applied to batch crawlers (section 4). an analogous weighted
freshness metric is as follows:

wf(t) =

w(p)    f(p, t),

(cid:1)

p   c(t)

where f(p, t) is page p   s freshness level at time t, measured in one of
several possible ways (see below).1 one is typically interested in the

1 pages that have been removed from the web (i.e., their url is no longer valid) but whose
removal has not yet been detected by the crawler, are assigned a special freshness level,
e.g., the minimum value on the freshness scale in use.

215

216

incremental crawl ordering

steady-state average of wf:

wf = lim
t      

(cid:3) t

0

1
t

wf(t) dt.

at each step, an incremental crawler faces a choice between two

basic actions:

(1) download a new page. consequences include:

(a) may improve coverage.
(b) may supply new links, which can lead to discovery
of new pages.2 (new links also contribute to the
crawler   s estimates of page importance, relevance,
and other aspects like likelihood of being spam; cf.
section 6.)

(2) re-download an old page. consequences include:

(a) may improve freshness.
(b) may supply new links or reveal the removal of links,

with similar rami   cations as 1(b) above.

in the presence of dynamic pages and    nite crawling resources, there
is a tradeo    between coverage and freshness. there is no consensus
about the best way to balance the two. some contend that coverage
and freshness are like apples and oranges, and balancing the two objec-
tives should be left as a business decision, i.e., do we prefer broad
coverage of content that may be somewhat out-of-date, or narrower
coverage with fresher content? others have proposed speci   c schemes
for combining the two objectives into a single framework: the approach
taken in webfountain [56] focuses on the freshness problem, and folds
in coverage by treating uncrawled pages as having a freshness value of
zero. the opic approach [1] focuses on ensuring coverage of important
pages, and in the process periodically revisits old important pages.

aside from the two approaches just mentioned, most published
work on crawling focuses either uniquely on coverage or uniquely on

2 pages can also be discovered via    out-of-band    channels, e.g., e-mail messages, rss feeds,
user browsing sessions.

5.1 maximizing freshness

217

freshness. we have already surveyed coverage-oriented techniques in
section 4, in the context of batch crawling. in incremental crawling,
coverage can be expanded not only by following links from newly
crawled pages, but also by monitoring old pages to detect any new
links that might be added over time. this situation was studied by
dasgupta et al. [48], who used a set-cover formulation to identify small
sets of old pages that collectively permit discovery of most new pages.
the remainder of this chapter is dedicated to techniques that revisit
old pages to acquire a fresh version of their content (not just links to
new pages). section 5.1 focuses on maximizing the average freshness
of crawled content, whereas section 5.2 studies the subtly di   erent
problem of capturing the history of content updates. following these
conceptual discussions, section 5.3 considers practical implementation
strategies.

5.1 maximizing freshness

here the goal is to maximize time-averaged weighted freshness, wf, as
de   ned above. to simplify the study of this problem, it is standard prac-
tice to assume that the set of crawled pages is    xed (i.e., c(t) is static,
so we drop the dependence on t), and that each page p     c exhibits a
stationary stochastic pattern of content changes over time. freshness
maximization divides into three relatively distinct sub-problems:

    model estimation. construct a model for the temporal
behavior of each page p     c.
    resource allocation. given a maximum crawl rate r,
assign to each page p     c a revisitation frequency r(p) such
that
    scheduling. produce a crawl order that adheres to the tar-
get revisitation frequencies as closely as possible.

p   c r(p) = r.

(cid:2)

with model estimation, the idea is to estimate the temporal behav-
ior of p, given samples of the content of p or pages related to p. cho
and garc    a-molina [42] focused on how to deal with samples of p that
are not evenly spaced in time, which may be the case if the samples
have been gathered by the crawler itself in the past, while operating

218

incremental crawl ordering

under a non-uniform scheduling regime. barbosa et al. [15] considered
how to use content-based features of a single past sample of p to infer
something about its temporal behavior. cho and ntoulas [44] and tan
et al. [113] focused on how to infer the behavior of p from the behavior
of related pages     pages on the same web site, or pages with similar
content, link structure, or other features.

we now turn to scheduling. co   man et al. [46] pursued random-
ized scheduling policies, where at each step page p is selected with
id203 r(p)/r, independent of the past schedule. wolf et al. [115]
formulated the crawl scheduling problem in terms of network    ow, for
which prior algorithms exist. cho and garc    a-molina [41] studied a
special case of the scheduling problem in which pages have the same
target revisitation frequency r(p). all three works concluded that it is
best to space apart downloads of each page p uniformly in time, or as
close to uniformly as possible.

resource allocation is generally viewed as the central aspect of fresh-
ness maximization. we divide work on resource allocation into two cat-
egories, according to the freshness model adopted:

5.1.1 binary freshness model
in the binary freshness model, also known as obsolescence, f(p, t)    
{0,1}. speci   cally,

(cid:4)

f(p, t) =

1 if the cached copy of p is identical3 to the live copy
0 otherwise

.

under the binary freshness model, if f(p, t) = 1 then p is said to
be    fresh,    otherwise it is termed    stale.    although simplistic, a great
deal of useful intuition has been derived via this model.

the    rst to study the freshness maximization problem were co   man
et al. [46], who postulated a poisson model of web page change. specif-
ically, a page undergoes discrete change events, which cause the copy

3 it is common to replace the stipulation    identical    with    near-identical,    and ignore minor
changes like counters and timestamps. some of the techniques surveyed in section 6.1 can
be used to classify near-identical web page snapshots.

5.1 maximizing freshness

219

cached by the crawler to become stale. for each page p, the occurrence
of change events is governed by a poisson process with rate parameter
  (p), which means that changes occur randomly and independently,
with an average rate of   (p) changes per time unit.

a key observation by co   man et al. [46] was that in the case of
uniform page weights (i.e., all w(p) values are equal), the appealing
idea of setting revisitation frequencies in proportion to page change
rates, i.e., r(p)       (p) (called proportional resource allocation), can be
suboptimal. co   man et al. [46] also provided a closed-form optimal
solution for the case in which page weights are proportional to change
rates (i.e., w(p)       (p)), along with a hint for how one might approach
the general case.

cho and garc    a-molina [41] continued the work of co   man
et al.
[46], and derived a famously counterintuitive result: in the
uniform weights case, a uniform resource allocation policy, in which
r(p) = r/|c| for all p, achieves higher average binary freshness than
proportional allocation. the superiority of the uniform policy to the
proportional one holds under any distribution of change rates (  (p)
values).

the optimal resource allocation policy for binary freshness, also
given by cho and garc    a-molina [41], exhibits the following intrigu-
ing property: pages with a very fast rate of change (i.e.,   (p) very
high relative to r/|c|) ought never to be revised by the crawler, i.e.,
r(p) = 0. the reason is as follows: a page p1 that changes once per
second, and is revisited once per second by the crawler, is on aver-
age only half synchronized (f(p1) = 0.5). on the other hand, a page
p2 that changes once per day, and is revisited once per hour by the
crawler, has much better average freshness (f(p2) = 24/25 under ran-
domized scheduling, according to the formula given by cho and garc    a-
molina [41]). the crawling resources required to keep one fast-changing
page like p1 weakly synchronized can be put to better use keeping sev-
eral slow-changing pages like p2 tightly synchronized, assuming equal
page weights. hence, in terms of average binary freshness, it is best for
the crawler to    give up on    fast-changing pages, and put its energy into
synchronizing moderate- and slow-changing ones. this resource alloca-
tion tactic is analogous to advanced triage in the    eld of medicine [3].

220

incremental crawl ordering

the discussion so far has focused on a poisson page change model
in which the times at which page changes occur are statistically inde-
pendent. under such a model, the crawler cannot time its visits to
coincide with page change events. the following approaches relax the
independence assumption.

wolf et al.

[115] studied incremental crawling under a quasi-
deterministic page change model, in which page change events are
non-uniform in time, and the distribution of likely change times is
known a priori. (this work also introduced a search-centric page
weighting scheme, under the terminology embarrassment level. the
embarrassment-based scheme sets w(p)     c(p), where c(p) denotes the
id203 that a user will click on p after issuing a search query, as
estimated from historical search engine usage logs. the aim is to revisit
frequently clicked pages preferentially, thereby minimizing    embarrass-
ing    incidents in which a search result contains a stale page.)

the webfountain technique by edwards et al. [56] does not assume
any particular page evolution model a priori. instead, it categorizes
pages adaptively into one of n change rate buckets based on recently
observed change rates (this procedure replaces explicit model estima-
tion). bucket membership yields a working estimate of a page   s present
change rate, which is in turn used to perform resource allocation and
scheduling.

5.1.2 continuous freshness models

in a real crawling scenario, some pages may be    fresher    than others.
while there is no consensus about the best way to measure freshness,
several non-binary freshness models have been proposed.
in which f(p, t)        age(p, t), where

cho and garc    a-molina [41] introduced a temporal freshness metric,

(cid:4)

age(p, t) =

0
a otherwise

if the cached copy of p is identical to the live copy

,

where a denotes the amount of time the copies have di   ered. the
rationale for this metric is that the longer a cached page remains unsyn-
chronized with the live copy, the more their content tends to drift apart.

5.1 maximizing freshness

221

the optimal resource allocation policy under this age-based fresh-
ness metric, assuming a poisson model of page change, is given by cho
and garc    a-molina [41]. unlike in the binary freshness case, there is
no    advanced triage    e   ect     the revisitation frequency r(p) increases
monotonically with the page change rate   (p). since age increases with-
out bound, the crawler cannot a   ord to    give up on    any page.

olston and pandey [99] introduced an approach in which, rather
than relying on time as a proxy for degree of change, the idea is
to measure changes in page content directly. a content-based fresh-
ness framework is proposed, which constitutes a generalization of
binary freshness: a page is divided into a set of content fragments4
f1, f2, . . . , fn, each with a corresponding weight w(fi) that captures
the fragment   s importance and/or relevance. freshness is measured as
the (weighted) fraction of fragments in common between the cached
and live page snapshots, using the well-known jaccard set similarity
measure.

under a content-based freshness model, the goal is to minimize the
amount of incorrect content in the crawler   s cache, averaged over time.
to succeed in this goal, olston and pandey [99] argued that in addi-
tion to characterizing the frequency with which pages change, it is
necessary to characterize the longevity of newly updated page content.
long-lived content (e.g., today   s blog entry, which will remain in the
blog inde   nitely) is more valuable to crawl than ephemeral content
(e.g., today   s    quote of the day,    which will be overwritten tomorrow),
because it stays fresh in the cache for a longer period of time. the opti-
mal resource allocation policy for content-based freshness derived by
olston and pandey [99] di   erentiates between long-lived and ephemeral
content, in addition to di   erentiating between frequently and infre-
quently changing pages.

in separate work, pandey and olston [103] proposed a search-centric
method of assigning weights to individual content changes, based on
the degree to which a change is expected to impact search ranking.
the rationale is that even if a page undergoes periodic changes, and

4 fragments can be determined in a number of ways, e.g., using logical or visual document
structure, or using shingles [28]).

222

incremental crawl ordering

the new content supplied by the changes is long-lived, if the search
engine   s treatment of the page is una   ected by these changes, there is
no need for the crawler to revisit it.

5.2 capturing updates

for some crawling applications, maximizing average freshness of cached
pages is not the right objective. instead, the aim is to capture as many
individual content updates as possible. applications that need to cap-
ture updates include historical archival and temporal data mining, e.g.,
time-series analysis of stock prices.

the two scenarios (maximizing freshness versus capturing updates)
lead to very di   erent page revisitation policies. as we saw in section 5.1,
in the freshness maximization scenario, when resources are scarce the
crawler should ignore pages that frequently replace their content, and
concentrate on maintaining synchronization with pages that supply
more persistent content. in contrast, in the update capture scenario,
pages that frequently replace their content o   er the highest density of
events to be captured, and also the highest urgency to capture them
before they are lost.

early update capture systems, e.g., conquer [86], focused on
user-facing query languages, algorithms to di   erence page snapshots
and extract relevant tidbits (e.g., updated stock price), and query
processing techniques. later work considered the problem of when to
revisit each page to check for updates.

in work by pandey et al. [105], the objective was to maximize the
(weighted) number of updates captured. a suitable resource allocation
algorithm was given, which was shown empirically to outperform both
uniform and proportional resource allocation.

in subsequent work, pandey et al.

[102] added a timeliness
dimension, to represent the sensitivity of the application to delays
in capturing new information from the web. (for example, historical
archival has a low timeliness requirement compared with real-time stock
market analysis.) the key insight was that revisitation of pages whose
updates do not replace old content can be postponed to a degree permit-
ted by the application   s timeliness requirement, yielding a higher total

5.3 e   cient large-scale implementation

223

amount of information captured. a timeliness-sensitive resource alloca-
tion algorithm was given, along with a formal performance guarantee.

5.3 e   cient large-scale implementation

one key aspect of page revisitation policy is change model estimation.
for certain pages (e.g., newly discovered pages on important sites),
model estimation is time-critical; but for the vast majority of pages
(e.g., unimportant, well-understood, or infrequently crawled ones) it
can be performed lazily, in periodic o   ine batches. since most change
models deal with each page in isolation, this process is trivially paral-
lelizable. for the time-critical pages, the relevant data tend to be small
and amenable to caching: most techniques require a compact signature
(e.g., a few shingle hashes) for two to    ve of the most recent page snap-
shots. the search-centric approach of pandey and olston [103] requires
more detailed information from pages, and incorporates model estima-
tion into the process of re-building the search index.

after model estimation, the other two aspects of revisitation policy
are resource allocation and scheduling. while there is quite a bit of work
on using global optimization algorithms to assign revisitation schedules
to individual pages, it is not immediately clear how to incorporate such
algorithms into a large-scale crawler. one scalable approach is to group
pages into buckets according to desired revisitation frequency (as deter-
mined by resource allocation, see below), and cycle through each bucket
such that the time to complete one cycle matches the bucket   s revisi-
tation frequency target. this approach ensures roughly equal spacing
of revisitations of a given page, which has been found to be a good
rule of thumb for scheduling, as mentioned in section 5.1. it also yields
an obvious disk-based implementation: each bucket is a fifo queue,
where urls removed from the head are automatically appended to the
tail; crawler machines pull from the queues according to a weighted ran-
domized policy constructed to achieve the target revisitation rates in
expectation.

in the bucket-oriented approach, pages that are identical (or near-
identical) from the point of view of the adopted model are placed into
the same bucket. for example, in the basic freshness maximization

224

incremental crawl ordering

formulation under the binary freshness model (section 5.1.1), pages
can be bucketized by change frequency. assignment of a revisitation
frequency target to each bucket is done by the resource allocation pro-
cedure. the webfountain crawler by edwards et al. [56] uses change
frequency buckets and computes bucket revisitation frequency targets
periodically using a nonlinear problem (nlp) solver. since the nlp is
formulated at the bucket granularity (not over individual pages), the
computational complexity is kept in check. most other resource allo-
cation strategies can also be cast into the bucket-oriented nlp frame-
work. to take a simple example, an appealing variant of the uniform
resource allocation is to direct more resources toward important pages;
in this case, one can bucketize pages by importance, and apply a sim-
ple nlp over bucket sizes and importance weights to determine the
revisitation rate of each bucket.

6

avoiding problematic and undesirable content

this section discusses detection and avoidance of content that is redun-
dant, wasteful or misleading.

6.1 redundant content

as discussed in section 3.2.1, there is a prevalence of duplicate and
near-duplicate content on the web. shingling [28] is a standard way to
identify near-duplicate pages, but shingling is performed on web page
content, and thus requires these pages to have been crawled. as such,
it does not help to reduce the load on the crawler; however, it can be
used to limit and diversify the set of search results presented to a user.
some duplication stems from the fact that many web sites allow
multiple urls to refer to the same content, or content that is identical
modulo ever-changing elements such as rotating banner ads, evolving
comments by readers, and timestamps. schonfeld et al. proposed the
   duplicate url with similar text    (dust) algorithm [12] to detect
this form of aliasing, and to infer rules for normalizing urls into a
canonical form. dasgupta et al. [49] generalized dust by introduc-
ing a learning algorithm that can generate rules containing regular

225

226 avoiding problematic and undesirable content

expressions, experimentally tripling the number of duplicate urls that
can be detected. agarwal et al. attempted to bound the computational
complexity of learning rules using sampling [4]. rules inferred using
these algorithms can be used by a web crawler to normalize urls
after extracting them from downloaded pages and before passing them
through the duplicate url eliminator (section 2.3.2) and into the
frontier.

another source of duplication is mirroring [18, 19, 52]: providing
all or parts of the same web site on di   erent hosts. mirrored web sites
in turn can be divided into two groups: sites that are mirrored by the
same organization (for example by having one web server serving mul-
tiple domains with the same content, or having multiple web servers
provide synchronized content), and content that is mirrored by mul-
tiple organizations (for example, schools providing unix man pages on
the web, or web sites republishing wikipedia content, often somewhat
reformatted). detecting mirrored content di   ers from detecting dust
in two ways: on the one hand, with mirroring the duplication occurs
across multiple sites, so mirror detection algorithms have to consider
the entire corpus. on the other hand, entire trees of urls are mir-
rored, so detection algorithms can use url trees (suitably compacted
e.g., through hashing) as a feature to detect mirror candidates, and then
compare the content of candidate subtrees (for example via shingling).

6.2 crawler traps

content duplication in   ates the web corpus without adding much infor-
mation. another phenomenon that in   ates the corpus without adding
utility is crawler traps: web sites that populate a large, possibly in   -
nite url space on that site with mechanically generated content.
some crawler traps are non-malicious, for example web-based calen-
daring tools that generate a page for every month of every year, with a
hyperlink from each month to the next (and previous) month, thereby
forming an unbounded chain of dynamically generated pages. other
crawler traps are malicious, often set up by    spammers    to inject large
amounts of their content into a search engine, in the hope of hav-
ing their content show up high in search result pages or providing

6.3 web spam 227

many hyperlinks to their    landing page,    thus biasing link-based rank-
ing algorithms such as id95. there are many known heuristics
for identifying and avoiding spam pages or sites, see section 6.3. not
much research has been published on algorithms or heuristics for detect-
ing crawler traps directly. the irlbot crawler [84] utilizes a heuristic
called    budget enforcement with anti-spam tactics    (beast), which
assigns a budget to each web site and prioritizes urls from each web
site based on the site   s remaining budget combined with the domain   s
reputation.

6.3 web spam

web spam may be de   ned as    web pages that are crafted for the sole
purpose of increasing the ranking of these or some a   liated pages, with-
out improving the utility to the viewer    [97]. web spam is motivated
by the monetary value of achieving a prominent position in search-
engine result pages. there is a multi-billion dollar industry devoted to
search engine optimization (seo), most of it being legitimate but some
of it misleading. web spam can be broadly classi   ed into three cate-
gories [69]: keyword stu   ng, populating pages with highly searched or
highly monetizable terms; link spam, creating cliques of tightly inter-
linked web pages with the goal of biasing link-based ranking algorithms
such as id95 [101]; and cloaking, serving substantially di   erent
content to web crawlers than to human visitors (to get search referrals
for queries on a topic not covered by the page).

over the past few years, many heuristics have been proposed to
identify spam web pages and sites, see for example the series of airweb
workshops [76]. the problem of identifying web spam can be framed
as a classi   cation problem, and there are many well-known classi   -
cation approaches (e.g., id90, bayesian classi   ers, support
vector machines). the main challenge is to identify features that are
predictive of web spam and can thus be used as inputs to the classi-
   er. many such features have been proposed, including hyperlink fea-
tures [16, 17, 50, 116], term and phrase frequency [97], dns lookup
statistics [59], and html markup structure [114]. combined, these
features tend to be quite e   ective, although web spam detection is a

228 avoiding problematic and undesirable content

constant arms race, with both spammers and search engines evolving
their techniques in response to each other   s actions.

spam detection heuristics are used during the ranking phase of
search, but they can also be used during the corpus selection phase
(when deciding which pages to index) and crawling phase (when decid-
ing what crawl priority to assign to web pages). naturally, it is easier to
avoid crawling spam content in a continuous or iterative crawling set-
ting, where historical information about domains, sites, and individual
pages is available.1

6.4 cloaked content

cloaking refers to the practice of serving di   erent content to web
crawlers than to human viewers of a site [73]. not all cloaking is mali-
cious: for example, many web sites with interactive content rely heavily
on javascript, but most web crawlers do not execute javascript, so it
is reasonable for such a site to deliver alternative, script-free versions
of its pages to a search engine   s crawler to enable the engine to index
and expose the content.

web sites distinguish mechanical crawlers from human visitors
either based on their user-agent    eld (an http header that is used
to distinguish di   erent web browsers, and by convention is used by
crawlers to identify themselves), or by the crawler   s ip address (the
seo community maintains lists of the user-agent    elds and ip addresses
of major crawlers). one way for search engines to detect that a web
server employs cloaking is by supplying a di   erent user-agent    eld [117].
another approach is to probe the server from ip addresses not known
to the seo community (for example by enlisting the search engine   s
user base).

a variant of cloaking is called redirection spam. a web server utiliz-
ing redirection spam serves the same content both to crawlers and to
human-facing browser software (and hence, the aforementioned detec-
tion techniques will not detect it); however, the content will cause a

1 there is of course the possibility of spammers acquiring a site with a good history and
converting it to spam, but historical reputation-based approaches at least    raise the bar   
for spamming.

6.4 cloaked content

229

browser to immediately load a new page presenting di   erent content.
redirection spam is facilitated either through the html meta refresh
tag (whose presence is easy to detect), or via javascript, which most
browsers execute but most crawlers do not. chellapilla and maykov [36]
conducted a study of pages employing javascript redirection spam, and
found that about half of these pages used javascript   s eval statement
to obfuscate the urls to which they redirect, or even parts of the script
itself. this practice makes static detection of the redirection target (or
even the fact that redirection is occurring) very di   cult. chellapilla
and maykov argued for the use of lightweight javascript parsers and
execution engines in the crawling/indexing pipeline to evaluate scripts
(in a time-bounded fashion, since scripts may not terminate) to deter-
mine whether redirection occurs.

7

deep web crawling

some content is accessible only by    lling in html forms, and cannot be
reached via conventional crawlers that just follow hyperlinks.1 crawlers
that automatically    ll in forms to reach the content behind them are
called hidden web or deep web crawlers.

the deep web crawling problem is closely related to the problem
known as federated search or distributed information retrieval [30], in
which a mediator forwards user queries to multiple searchable collec-
tions, and combines the results before presenting them to the user. the
crawling approach can be thought of as an eager alternative, in which
content is collected in advance and organized in a uni   ed index prior to
retrieval. also, deep web crawling considers structured query interfaces
in addition to unstructured    search boxes,    as we shall see.

7.1 types of deep web sites

figure 7.1 presents a simple taxonomy of deep web sites. content
is either unstructured (e.g., free-form text) or structured (e.g., data

1 not all content behind form interfaces is unreachable via hyperlinks     some content is
reachable in both ways [35].

230

7.1 types of deep web sites

231

unstructured

content

news archive
(simple search)
news archive

structured
content

product review site

online bookstore

unstructured
query interface

structured

query interface

(advanced search)

fig. 7.1 deep web taxonomy.

records with typed    elds). similarly, the form interface used to query
the content is either unstructured (i.e., a single query box that accepts
a free-form query string) or structured (i.e., multiple query boxes that
pertain to di   erent aspects of the content).2

a news archive contains content that is primarily unstructured (of
course, some structure is present, e.g., title, date, author). in con-
junction with a simple textual search interface, a news archive consti-
tutes an example of an unstructured-content/unstructured-query deep
web site. a more advanced query interface might permit structured
restrictions on attributes that are extractable from the unstructured
content, such as language, geographical references, and media type,
yielding an unstructured-content/structured-query instance.

a product review site has relatively structured content (product
names, numerical reviews, reviewer reputation, and prices, in addition
to free-form textual comments), but for ease of use typically o   ers an
unstructured search interface. lastly, an online bookstore o   ers struc-
tured content (title, author, genre, publisher, price) coupled with a
structured query interface (typically a subset of the content attributes,
e.g., title, author and genre).

for simplicity most work focuses on either the upper-left quadrant
(which we henceforth call the unstructured case), or the lower-right
quadrant (structured case).

2 the unstructured versus structured dichotomy is really a continuum, but for simplicity
we present it as a binary property.

232 deep web crawling

7.2 problem overview

deep web crawling has three steps:

(1) locate deep web content sources. a human or crawler
must identify web sites containing form interfaces that lead
to deep web content. barbosa and freire [14] discussed the
design of a scoped crawler for this purpose.

(2) select relevant sources. for a scoped deep web crawling
task (e.g., crawling medical articles), one must select a rele-
vant subset of the available content sources. in the unstruc-
tured case this problem is known as database or resource
selection [32, 66]. the    rst step in resource selection is to
model the content available at a particular deep web site,
e.g., using query-based sampling [31].

(3) extract underlying content. finally, a crawler must
extract the content lying behind the form interfaces of the
selected content sources.

for major search engines, step 1 is almost trivial, since they already
possess a comprehensive crawl of the surface web, which is likely to
include a plethora of deep web query pages. steps 2 and 3 pose signi   -
cant challenges. step 2 (source selection) has been studied extensively
in the distributed information retrieval context [30], and little has been
done that speci   cally pertains to crawling. step 3 (content extraction)
is the core problem in deep web crawling; the rest of this chapter covers
the (little) work that has been done on this topic.

7.3 content extraction

the main approach to extracting content from a deep web site proceeds
in four steps (the    rst two steps apply only to the structured case):

(1) select a subset of form elements to populate,3 or perhaps
multiple such subsets. this is largely an open problem,
where the goals are to: (a) avoid form elements that merely

3 the remaining elements can remain blank, or be populated with a wildcard expression
when applicable.

7.3 content extraction

233

a   ect the presentation of results (e.g., sorting by price ver-
sus popularity); and (b) avoid including correlated elements,
which arti   cially increase the dimensionality of the search
space [88].

(2) if possible, decipher the role of each of the targeted form
elements (e.g., book author versus publication date), or at
least understand their domains (proper nouns versus dates).
raghavan and garc    a-molina [109] and several subsequent
papers studied this di   cult problem.

(3) create an initial database of valid data values (e.g.,    ernest
hemingway    and 1940 in the structured case; english words
in the unstructured case). some sources of this information
include [109]: (a) a human administrator; (b) non-deep-
web online content, e.g., a dictionary (for unstructured key-
words) or someone   s list of favorite authors; (c) drop-down
menus for populating form elements (e.g., a drop-down list
of publishers).

(4) use the database to issue queries to the deep web site
(e.g., publisher =    scribner   ), parse the result and extract
new data values to insert into the database (e.g., author =
   ernest hemingway   ), and repeat.

we elaborate on step 4, which has been studied under (variations

of) the following model of deep web content and queries [98, 117]:

a deep web site contains one or more content items, which are either
unstructured documents or structured data records. a content item
contains individual data values, which are text terms in the unstruc-
tured case, or data record elements like author names and dates in
the structured case. data values and content values are related via a
bipartite graph, depicted in figures 7.2 (unstructured case) and 7.3
(structured case).

a query consists of a single data value4 v submitted to the form
interface, which retrieves the set of content items directly connected

4 it is assumed that any data value can form the basis of a query, even though this is not
always the case in practice (e.g., a bookstore may not permit querying by publisher). also,
multi-value queries are not considered.

234 deep web crawling

fig. 7.2 deep web content model (unstructured content).

fig. 7.3 deep web content model (structured content).

to v via edges in the graph, called v    s result set. each query incurs
some cost to the crawler, typically dominated by the overhead of
downloading and processing each member of the result set, and hence
modeled as being linearly proportional to result cardinality.

under this model, the deep web crawling problem can be cast as
a weighted set-cover problem: select a minimum-cost subset of data
values that cover all content items. unfortunately, unlike in the usual
set-cover scenario, in our case the graph is only partially known at the
outset, and must be uncovered progressively during the course of the
crawl. hence, adaptive graph traversal strategies are required.

a simple greedy traversal strategy was proposed by barbosa and
freire [13] for the unstructured case: at each step the crawler issues as
a query the highest-frequency keyword that has not yet been issued,

7.3 content extraction

235

where keyword frequency is estimated by counting occurrences in docu-
ments retrieved so far. in the bipartite graph formulation, this strategy
is equivalent to selecting the data value vertex of highest degree, accord-
ing to the set of edges uncovered so far.

a similar strategy was proposed by wu et al. [117] for the structured
case, along with a re   nement in which the crawler bypasses data values
that are highly correlated with ones that have already been selected, in
the sense that they connect to highly overlapping sets of content items.
ntoulas et al. [98] proposed statistical models for estimating the
number of previously unseen content items that a particular data value
is likely to cover, focusing on the unstructured case.

google   s deep web crawler [88] uses techniques similar to the ones
described above, but adapted to extract a small amount of content
from a large number (millions) of sites, rather than aiming for extensive
coverage of a handful of sites.

8

future directions

as this survey indicates, crawling is a well-studied problem. however,
there are at least as many open questions as there are resolved ones.
even in the material we have covered, the reader has likely noticed
many open issues, including:

    parameter tuning. many of the crawl ordering policies
rely on carefully tuned parameters, with little insight or sci-
ence into how best to choose the parameters. for example,
what is the optimal level of greediness for a scoped crawler
(section 4.2)?
    retiring unwanted pages. given    nite storage capacity,
in practice crawlers discard or retire low-quality and spam
pages from their collections, to make room for superior pages.
however, we are not aware of any literature that explicitly
studies retirement policies. there is also the issue of how
much metadata to retain about retired pages, to avoid acci-
dentally rediscovering them, and to help assess the quality of
related pages (e.g., pages on the same site, or pages linking
to or linked from the retired page).

236

237

    holistic crawl ordering. whereas much attention has been
paid to various crawl ordering sub-problems (e.g., prioritizing
the crawl order of new pages, refreshing content from old
pages, revisiting pages to discover new links), there is little
work on how to integrate the disparate approaches into a
uni   ed strategy.
    integration of theory and systems work. there is also
little work that reconciles the theoretical work on crawl
ordering (sections 3   5) with the pragmatic work on large-
scale crawling architectures (section 2). for example, disk-
based url queues make it infeasible to re-order urls
frequently, which may impede exact implementation of some
of the crawl ordering policies. while we have hypothesized
scalable implementations of some of the policies (sections 4.3
and 5.3), a more comprehensive and empirical study of this
topic is needed.
    deep web. clearly, the science and practice of deep web
crawling (section 7) is in its infancy.

there are also several nearly untouched directions:
    vertical crawling. some sites may be considered impor-
tant enough to merit crawler specialization, e.g., ebay   s auc-
tion listings or amazon   s product listings. also, as the web
matures, certain content dissemination structures become
relatively standardized, e.g., news, blogs, tutorials, and dis-
cussion forums. in these settings the unit of content is not
always a web page; instead, multiple content units are some-
times appended to the same page, or a single content unit
may span multiple pages (connected via    next page    and
   previous page    links). also, many links lead to redundant
content (e.g., shortcut to featured story, or items for sale
by a particular user). a crawler that understands these for-
mats can crawl them more e   ciently and e   ectively than a
general-purpose crawler. there has been some work on crawl-
ing discussion forums [119], but little else. vertical crawl-
ing is not the same as scoped crawling (section 4.2): scoped

238 future directions

crawling focuses on collecting semantically coherent content
from many sites, whereas vertical crawling exploits syntactic
patterns on particular sites.
    crawling scripts. increasingly, web sites employ scripts
(e.g., javascript, ajax) to generate content and links on
the    y. almost no attention has been paid to whether or
how to crawl these sites. we are aware of only one piece of
preliminary work, by duda et al. [55].
    personalized content. web sites often customize their con-
tent to individual users, e.g., amazon gives personalized
recommendations based on a user   s browsing and purchas-
ing patterns. it is not clear how crawlers should treat such
sites, e.g., emulating a generic user versus attempting to spe-
cialize the crawl based on di   erent user pro   les. a search
engine that aims to personalize search results may wish to
push some degree of personalization into the crawler.
    collaboration between content providers
and
crawlers. as discussed in section 1, crawling is a pull
mechanism for discovering and acquiring content. many
sites now publish sitemaps and rss feeds, which enable
a push-oriented approach. modern commercial crawlers
employ a hybrid of push and pull, but there is little
academic study of this practice and the issues involved.
schonfeld and shivakumar [110] examined tradeo   s between
reliance on sitemaps for content discovery and the classical
pull approach.

references

[1] s. abiteboul, m. preda, and g. cobena,    adaptive on-line page importance
computation,    in proceedings of the 12th international world wide web con-
ference, 2003.

[2] e. adar, j. teevan, s. t. dumais, and j. l. elsas,    the web changes every-
thing: understanding the dynamics of web content,    in proceedings of the 2nd
international conference on web search and data mining, 2009.

[3] advanced triage (medical

term), http://en.wikipedia.org/wiki/triage#

advanced triage.

[4] a. agarwal, h. s. koppula, k. p. leela, k. p. chitrapura, s. garg, p. k. gm,
c. haty, a. roy, and a. sasturkar,    url id172 for de-duplication
of web pages,    in proceedings of the 18th conference on information and
knowledge management, 2009.

[5] c. c. aggarwal, f. al-garawi, and p. s. yu,    intelligent crawling on the world
wide web with arbitrary predicates,    in proceedings of the 10th international
world wide web conference, 2001.

[6] d. ahlers and s. boll,    adaptive geospatially focused crawling,    in proceedings

of the 18th conference on information and knowledge management, 2009.

[7] attributor. http://www.attributor.com.
[8] r. baeza-yates and c. castillo,    crawling the in   nite web,    journal of web

engineering, vol. 6, no. 1, pp. 49   72, 2007.

[9] r. baeza-yates, c. castillo, m. marin, and a. rodriguez,    crawling a country:
better strategies than breadth-   rst for web page ordering,    in proceedings of
the 14th international world wide web conference, 2005.

[10] b. bamba, l. liu, j. caverlee, v. padliya, m. srivatsa, t. bansal, m. palekar,
j. patrao, s. li, and a. singh,    dsphere: a source-centric approach to

239

240 references

crawling, indexing and searching the world wide web,    in proceedings of the
23rd international conference on data engineering, 2007.

[11] z. bar-yossef and m. gurevich,    random sampling from a search engine   s
index,    in proceedings of the 15th international world wide web conference,
2006.

[12] z. bar-yossef, i. keidar, and u. schonfeld,    do not crawl in the dust: dif-
ferent urls with similar text,    in proceedings of the 16th international world
wide web conference, 2007.

[13] l. barbosa and j. freire,    siphoning hidden-web data through keyword-based
interfaces,    in proceedings of the 19th brazilian symposium on databases
sbbd, 2004.

[14] l. barbosa and j. freire,    an adaptive crawler for locating hidden-web entry
points,    in proceedings of the 16th international world wide web conference,
2007.

[15] l. barbosa, a. c. salgado, f. de carvalho, j. robin, and j. freire,    looking
at both the present and the past to e   ciently update replicas of web content,   
in proceedings of the acm international workshop on web information and
data management, 2005.

[16] l. becchetti, c. castillo, d. donato, s. leonardi, and r. baeza-yates,    link-
based characterization and detection of web spam,    in proceedings of the 2nd
international workshop on adversarial information retrieval on the web,
2006.

[17] a. bencz  ur, k. csalog  any, t. sarl  os, and m. uher,    spamrank     fully auto-
matic link spam detection,    in proceedings of the 1st international workshop
on adversarial information retrieval on the web, 2005.

[18] k. bharat and a. broder,    mirror, mirror on the web: a study of host pairs
with replicated content,    in proceedings of the 8th international world wide
web conference, 1999.

[19] k. bharat, a. broder, j. dean, and m. henzinger,    a comparison of tech-
niques to    nd mirrored hosts on the www,    journal of the american society
for information science, vol. 51, no. 12, pp. 1114   1122, 2000.

[20] b. h. bloom,    space/time trade-o   s in hash coding with allowable errors,   

communications of the acm, vol. 13, no. 7, pp. 422   426, 1970.

[21] p. boldi, b. codenotti, , m. santini, and s. vigna,    ubicrawler: a scalable
fully distributed web crawler,    software     practice & experience, vol. 34,
no. 8, pp. 711   726, 2004.

[22] p. boldi, b. codenotti, m. santini, and s. vigna,    structural properties of
the african web,    in poster proceedings of the 11th international world wide
web conference, 2002.

[23] p. boldi, m. santini, and s. vigna,    paradoxical e   ects in id95 incremen-

tal computations,    internet mathematics, vol. 2, no. 3, pp. 387   404, 2005.

[24] c. m. bowman, p. b. danzig, d. r. hardy, u. manber, and m. f. schwartz,
   the harvest information discovery and access system,    in proceedings of the
2nd international world wide web conference, 1994.

[25] s. brin and l. page,    the anatomy of a large-scale hypertextual web search
engine,    in proceedings of the 7th international world wide web conference,
1998.

references

241

[26] a. broder, r. kumar, f. maghoul, p. raghavan, s. rajagopalan, r. stata,
a. tomkins, and j. wiener,    graph structure in the web,    in proceedings of
the 9th international world wide web conference, 2000.

[27] a. broder, m. najork, and j. wiener,    e   cient url caching for world wide
web crawling,    in proceedings of the 12th international world wide web
conference, 2003.

[28] a. z. broder, s. c. glassman, and m. s. manasse,    syntactic id91 of the
web,    in proceedings of the 6th international world wide web conference,
1997.

[29] m. burner,    crawling towards eternity: building an archive of the world wide

web,    web techniques magazine, vol. 2, no. 5, pp. 37   40, 1997.

[30] j. callan,    distributed information retrieval,    in advances in information
retrieval, (w. b. croft, ed.), pp. 127   150, kluwer academic publishers,
2000.

[31] j. callan and m. connell,    query-based sampling of text databases,    acm

transactions on information systems, vol. 19, no. 2, pp. 97   130, 2001.

[32] j. p. callan, z. lu, and w. b. croft,    searching distributed collections with
id136 networks,    in proceedings of the 18th annual international acm
sigir conference on research and development in information retrieval,
1995.

[33] s. chakrabarti, b. dom, p. raghavan, s. rajagopalan, d. gibson, and
j. kleinberg,    automatic resource compilation by analyzing hyperlink struc-
ture and associated text,    in proceedings of the 7th international world wide
web conference, 1998.

[34] s. chakrabarti, m. van den berg, and b. dom,    focused crawling: a new
approach to topic-speci   c web resource discovery,    in proceedings of the 8th
international world wide web conference, 1999.

[35] k. c.-c. chang, b. he, c. li, m. patel, and z. zhang,    structured databases
on the web: observations and implications,    acm sigmod record, vol. 33,
no. 3, pp. 61   70, 2004.

[36] k. chellapilla and a. maykov,    a taxonomy of javascript redirection spam,   
in proceedings of the 16th international world wide web conference, 2007.
[37] h. chen, m. ramsey, and c. yang,    a smart itsy bitsy spider for the web,   
journal of the american society for information science, vol. 49, no. 7,
pp. 604   618, 1998.

[38] s. chien, c. dwork, r. kumar, d. r. simon, and d. sivakumar,    link evolu-
tion: analysis and algorithms,    internet mathematics, vol. 1, no. 3, pp. 277   
304, 2003.

[39] j. cho and h. garc    a-molina,    the evolution of the web and implications for
an incremental crawler,    in proceedings of the 26th international conference
on very large data bases, 2000.

[40] j. cho and h. garc    a-molina,    parallel crawlers,    in proceedings of the 11th

international world wide web conference, 2002.

[41] j. cho and h. garc    a-molina,    e   ective page refresh policies for web
crawlers,    acm transactions on database systems, vol. 28, no. 4, pp. 390   426,
2003.

242 references

[42] j. cho and h. garc    a-molina,    estimating frequency of change,    acm trans-

actions on internet technology, vol. 3, no. 3, pp. 256   290, 2003.

[43] j. cho, j. garc    a-molina, and l. page,    e   cient crawling through url order-
ing,    in proceedings of the 7th international world wide web conference,
1998.

[44] j. cho and a. ntoulas,    e   ective change detection using sampling,    in pro-
ceedings of the 28th international conference on very large data bases, 2002.
[45] j. cho and u. schonfeld,    rankmass crawler: a crawler with high personal-
ized id95 coverage guarantee,    in proceedings of the 33rd international
conference on very large data bases, 2007.

[46] e. g. co   man, z. liu, and r. r. weber,    optimal robot scheduling for web

search engines,    journal of scheduling, vol. 1, no. 1, 1998.

[47] crawltrack,    list of spiders and crawlers,    http://www.crawltrack.net/

crawlerlist.php.

[48] a. dasgupta, a. ghosh, r. kumar, c. olston, s. pandey, and a. tomkins,
   the discoverability of the web,    in proceedings of the 16th international
world wide web conference, 2007.

[49] a. dasgupta, r. kumar, and a. sasturkar,    de-duping urls via rewrite
rules,    in proceedings of the 14th acm sigkdd international conference on
knowledge discovery and data mining, 2008.

[50] b. davison,    recognizing nepotistic links on the web,    in proceedings of the

aaai-2000 workshop on arti   cial intelligence for web search, 2000.

[51] g. t. de assis, a. h. f. laender, m. a. gon  calves, and a. s. da silva,
   a genre-aware approach to focused crawling,    world wide web, vol. 12,
no. 3, pp. 285   319, 2009.

[52] j. dean and m. henzinger,    finding related pages in the world wide web,    in

proceedings of the 8th international world wide web conference, 1999.

[53] p. debra and r. post,    information retrieval in the world wide web: making
client-based searching feasible,    in proceedings of the 1st international world
wide web conference, 1994.

[54] m. diligenti, f. m. coetzee, s. lawrence, c. l. giles, and m. gori,    focused
crawling using context graphs,    in proceedings of the 26th international con-
ference on very large data bases, 2000.

[55] c. duda, g. frey, d. kossmann, and c. zhou,    ajaxsearch: crawling,
indexing and searching web 2.0 applications,    in proceedings of the 34th inter-
national conference on very large data bases, 2008.

[56] j. edwards, k. s. mccurley, and j. a. tomlin,    an adaptive model for opti-
mizing performance of an incremental web crawler,    in proceedings of the 10th
international world wide web conference, 2001.

[57] d. eichmann,    the rbse spider     balancing e   ective search against web
load,    in proceedings of the 1st international world wide web conference,
1994.

[58] d. fetterly, n. craswell, and v. vinay,    the impact of crawl policy on web
search e   ectiveness,    in proceedings of the 32nd annual international acm
sigir conference on research and development in information retrieval,
2009.

references

243

[59] d. fetterly, m. manasse, and m. najork,    spam, damn spam, and statistics:
using statistical analysis to locate spam web pages,    in proceedings of the 7th
international workshop on the web and databases, 2004.

[60] d. fetterly, m. manasse, m. najork, and j. l. wiener,    a large-scale study
of the evolution of web pages,    in proceedings of the 12th international world
wide web conference, 2003.

[61] r. fielding,    maintaining distributed hypertext infostructures: welcome to
momspider   s web,    in proceedings of the 1st international world wide web
conference, 1994.

[62] a. s. foundation,    welcome to nutch!,    http://lucene.apache.org/nutch/.
[63] w. gao, h. c. lee, and y. miao,    geographically focused collaborative crawl-
ing,    in proceedings of the 15th international world wide web conference,
2006.

[64] gigaalert, http://www.gigaalert.com.
[65] d. gomes and m. j. silva,    characterizing a national community web,    acm

transactions on internet technology, vol. 5, no. 3, pp. 508   531, 2005.

[66] l. gravano, h. garc    a-molina, and a. tomasic,    the e   ectiveness of gloss
for the text database discovery problem,    in proceedings of the 1994 acm
sigmod international conference on management of data, 1994.

[67] m. gray,    internet growth and statistics: credits and background,   

http://www.mit.edu/people/mkgray/net/background.html.

[68] d. gruhl, l. chavet, d. gibson, j. meyer, p. pattanayak, a. tomkins, and
j. zien,    how to build a webfountain: an architecture for very large-scale
text analytics,    ibm systems journal, vol. 43, no. 1, pp. 64   77, 2004.

[69] z. gy  ongyi and h. garc    a-molina,    web spam taxonomy,    in proceedings of
the 1st international workshop on adversarial information retrieval, 2005.
[70] y. hafri and c. djeraba,    high performance crawling system,    in proceedings
of the 6th acm sigmm international workshop on multimedia information
retrieval, 2004.

[71] m. henzinger, a. heydon, m. mitzenmacher, and m. najork,    measuring
index quality using id93 on the web,    in proceedings of the 8th inter-
national world wide web conference, 1999.

[72] m. henzinger, a. heydon, m. mitzenmacher, and m. najork,    on near-
uniform url sampling,    in proceedings of the 9th international world wide
web conference, 2000.

[73] m. r. henzinger, r. motwani, and c. silverstein,    challenges in web search

engines,    sigir forum, vol. 36, no. 2, pp. 11   22, 2002.

[74] m. hersovici, m. jacovi, y. s. maarek, d. pelleg, m. shtalhaim, and s. ur,
   the shark-search algorithm     an application: tailored web site mapping,   
in proceedings of the 7th international world wide web conference, 1998.

[75] a. heydon and m. najork,    mercator: a scalable, extensible web crawler,   

world wide web, vol. 2, no. 4, pp. 219   229, 1999.

[76] international workshop series on adversarial information retrieval on the

web, 2005   .

[77] internet archive, http://archive.org/.
[78] internet archive,    heritrix home page,    http://crawler.archive.org/.

244 references

[79] k. j  arvelin and j. kek  al  ainen,    cumulated gain-based evaluation of ir
techniques,    acm transactions on information systems, vol. 20, no. 4,
pp. 422   446, 2002.

[80] j. johnson, k. tsioutsiouliklis, and c. l. giles,    evolving strategies for
focused web crawling,    in proceedings of the 20th international conference
on machine learning, 2003.

[81] r. khare, d. cutting, k. sitakar, and a. rifkin,    nutch: a    exible and scal-
able open-source web search engine,    technical report, commercenet labs,
2004.

[82] j. kleinberg,    authoritative sources in a hyperlinked environment,    journal

of the acm, vol. 46, no. 5, pp. 604   632, 1999.

[83] m. koster,    a standard for robot exclusion,    http://www.robotstxt.org/

orig.html, 1994.

[84] h.-t. lee, d. leonard, x. wang, and d. loguinov,    irlbot: scaling to 6 bil-
lion pages and beyond,    in proceedings of the 17th international world wide
web conference, 2008.

[85] l. lim, m. wang, s. padmanabhan, j. s. vitter, and r. c. agarwal,    charac-
terizing web document change,    in proceedings of the international conference
on advances in web-age information management, 2001.

[86] l. liu, c. pu, w. tang, and w. han,    conquer: a continual query system
for update monitoring in the www,    international journal of computer
systems, science and engineering, vol. 14, no. 2, 1999.

[87] b. t. loo, o. cooper, and s. krishnamurthy,    distributed web crawling over

dhts,    uc berkeley technical report csd-04-1305, 2004.

[88] j. madhavan, d. ko, l. kot, v. ganapathy, a. rasmussen, and a. halevy,
   google   s deep-web crawl,    in proceedings of the 34th international confer-
ence on very large data bases, 2008.

[89] m. mauldin,    lycos: design choices in an internet search service,    ieee

expert, vol. 12, no. 1, pp. 8   11, 1997.

[90] o. a. mcbryan,    genvl and wwww: tools for taming the web,    in pro-

ceedings of the 1st international world wide web conference, 1994.

[91] f. menczer and r. k. belew,    adaptive retrieval agents: internalizing local
context and scaling up to the web,    machine learning, vol. 39, pp. 203   242,
2000.

[92] f. menczer, g. pant, and p. srinivasan,    topical web crawlers: evaluating
adaptive algorithms,    acm transactions on internet technology, vol. 4, no. 4,
pp. 378   419, 2004.

[93] g. mohr, m. stack, i. ranitovic, d. avery, and m. kimpton,    an introduction
to heritrix, an open source archival quality web crawler,    in proceedings of
the 4th international web archiving workshop, 2004.

[94] m. najork and a. heydon,    high-performance web crawling,    technical

report, compaq src research report 173, 2001.

[95] m. najork and j. l. wiener,    breadth-   rst search crawling yields high-quality
pages,    in proceedings of the 10th international world wide web conference,
2001.

references

245

[96] a. ntoulas, j. cho, and c. olston,    what   s new on the web? the evolu-
tion of the web from a search engine perspective,    in proceedings of the 13th
international world wide web conference, 2004.

[97] a. ntoulas, m. najork, m. manasse, and d. fetterly,    detecting spam web
pages through content analysis,    in proceedings of the 15th international
world wide web conference, 2006.

[98] a. ntoulas, p. zerfos, and j. cho,    downloading textual hidden web content
through keyword queries,    in proceedings of the acm/ieee joint conference
on digital libraries, 2005.

[99] c. olston and s. pandey,    recrawl scheduling based on information
longevity,    in proceedings of the 17th international world wide web con-
ference, 2008.

[100] v. j. padliya and l. liu,    peercrawl: a decentralized peer-to-peer architecture
for crawling the world wide web,    georgia institute of technology technical
report, 2006.

[101] l. page, s. brin, r. motwani, and t. winograd,    the id95 citation
ranking: bringing order to the web,    technical report, stanford university,
1998.

[102] s. pandey, k. dhamdhere, and c. olston,    wic: a general-purpose algorithm
for monitoring web information sources,    in proceedings of the 30th interna-
tional conference on very large data bases, 2004.

[103] s. pandey and c. olston,    user-centric web crawling,    in proceedings of the

14th international world wide web conference, 2005.

[104] s. pandey and c. olston,    crawl ordering by search impact,    in proceedings

of the 1st international conference on web search and data mining, 2008.

[105] s. pandey, k. ramamritham, and s. chakrabarti,    monitoring the dynamic
web to respond to continuous queries,    in proceedings of the 12th international
world wide web conference, 2003.

[106] g. pant and p. srinivasan,    learning to crawl: comparing classi   cation
schemes,    acm transactions on information systems, vol. 23, no. 4, pp. 430   
462, 2005.

[107] g. pant and p. srinivasan,    link contexts in classi   er-guided topical
crawlers,    ieee transactions on knowledge and data engineering, vol. 18,
no. 1, pp. 107   122, 2006.

[108] b. pinkerton,    finding what people want: experiences with the webcrawler,   

in proceedings of the 2nd international world wide web conference, 1994.

[109] s. raghavan and h. garc    a-molina,    crawling the hidden web,    in proceedings

of the 27th international conference on very large data bases, 2001.

[110] u. schonfeld and n. shivakumar,    sitemaps: above and beyond the crawl of
duty,    in proceedings of the 18th international world wide web conference,
2009.

[111] v. shkapenyuk and t. suel,    design and implementation of a high-
performance distributed web crawler,    in proceedings of the 18th international
conference on data engineering, 2002.

[112] a. singh, m. srivatsa, l. liu, and t. miller,    apoidea: a decentralized peer-
to-peer architecture for crawling the world wide web,    in sigir workshop on
distributed information retrieval, 2003.

246 references

[113] q. tan, z. zhuang, p. mitra, and c. l. giles,    a id91-based sampling
approach for refreshing search engine   s database,    in proceedings of the 10th
international workshop on the web and databases, 2007.

[114] t. urvoy, t. lavergne, and p. filoche,    tracking web spam with hidden style
similarity,    in proceedings of the 2nd international workshop on adversarial
information retrieval on the web, 2006.

[115] j. l. wolf, m. s. squillante, p. s. yu, j. sethuraman, and l. ozsen,    opti-
mal crawling strategies for web search engines,    in proceedings of the 11th
international world wide web conference, 2002.

[116] b. wu and b. davison,    identifying link farm spam pages,    in proceedings of

the 14th international world wide web conference, 2005.

[117] p. wu, j.-r. wen, h. liu, and w.-y. ma,    query selection techniques for
e   cient crawling of structured web sources,    in proceedings of the 22nd inter-
national conference on data engineering, 2006.

[118] yahoo! research barcelona,

   datasets

for web

spam detection,   

http://www.yr-bcn.es/webspam/datasets.

[119] j.-m. yang, r. cai, c. wang, h. huang, l. zhang, and w.-y. ma,    incorpo-
rating site-level knowledge for incremental crawling of web forums: a list-wise
strategy,    in proceedings of the 15th acm sigkdd international conference
on knowledge discovery and data mining, 2009.

[120] s. zheng, p. dmitriev, and c. l. giles,    graph-based seed selection for web-
scale crawlers,    in proceedings of the 18th conference on information and
knowledge management, 2009.

[121] k. zhu, z. xu, x. wang, and y. zhao,    a full distributed web crawler based

on structured network,    in asia information retrieval symposium, 2008.

