nice  to  meet  you

integer  linear  programming  

in  
nlp

constrained  conditional  models

ming   wei  chang,  nick  rizzolo,  dan  roth
department  of  computer  science
university  of  illinois  at  urbana   champaign

june 2010 
naacl 

page 1

0: 2

ilp  &  constraints  conditional  models  (ccms)

constraints  driven  learning  and  decision  making

    making  global  decisions  in  which  several  local  interdependent  decisions  play  a  

role.

    informally:

    everything  that  has  to  do  with  constraints  (and  learning  models)

issues  to  attend  to:
    while  we  formulate  the  problem  as  an  ilp  problem,  id136  

    formally:  

    we  typically  make  decisions  based  on  models  such  as:

can  be  done  multiple  ways  
    search;  sampling;  dynamic  programming;  sat;  ilp  

argmaxy wt   (x,y)

    ccms (specifically,  ilp  formulations)  make  decisions  based  on  models  such  as:

    the  focus  is  on  joint  global  id136
    learning  may  or  may  not  be  joint.  

argmaxy wt   (x,y)  +      c     c   c d(y,  1c)

    we  do  not  define  the  learning  method,  but  we   ll  discuss  it  and  make  suggestions

    decomposing  models  is  often  beneficial

    ccms make  predictions  in  the  presence  of  /guided  by  constraints

    why  constraints?

    the  goal:  building  a  good  nlp  systems  easily
    we  have  prior  knowledge  at  our  hand

    how  can  we  use  it?  
    we  suggest  that  knowledge  can  often  be  injected  directly

    can  use  it  to  guide  learning
    can  use  it  to  improve  decision  making  
    can  use  it  to  simplify  the  models  we  need  to  learn

    how  useful  are  constraints?
    useful  for  supervised  learning  
    useful  for  semi   supervised  &  other  label   lean  learning  paradigms
    sometimes  more  efficient  than  labeling  data  directly

0: 3

0: 4

1

id136

comprehension

a process that maintains and 
updates a collection of propositions 
about the state of affairs. 

(england,  june,  1989)  - christopher  robin  is  alive  and  well.    he  lives  in 
england.  he is the same person that you read about in the book, winnie the 
pooh. as a boy, chris lived in a pretty home called cotchfield farm.  when 
chris was three years old, his father wrote a poem about him.  the poem was 
printed in a magazine for others to read.  mr. robin then wrote a book.  he 
made up a fairy tale land where chris lived.  his friends were animals.  there 
was a bear called winnie the pooh.  there was also an owl and a young pig, 
called a piglet.  all the animals were stuffed toys that chris owned.  mr. robin 
made them come to life with his words.  the places in the story were all near 
cotchfield farm. winnie the pooh was written in 1925.  children still love to 
read about christopher robin and his animal friends.  most people don't know 
he is a real person who is grown now.  he has written two books of his own.  
they tell what it is like to be famous.

1. christopher robin was born in england.      2.  winnie the pooh is a title of a book.  
3. christopher robin   s dad was a magician.     4. christopher robin must be at least 65 now.

this is an id136 problem

0: 5

0: 6

this  tutorial:  ilp  &  constrained  conditional  models

this  tutorial:  ilp  &  constrained  conditional  models

    part  1:  introduction  to  constrained  conditional  models (30min)

    part  2:  how  to  pose  the  id136  problem (45  minutes)

    examples:  

    ne  +  relations  
    information  extraction      correcting  models  with  ccms

    first  summary:  why  are  ccm  important
    problem  setting

    features  and  constraints;  some  hints  about  training  issues

    introduction  to  ilp  
    posing  nlp  problems  as  ilp  problems

    1.  sequence  tagging                    (id48/crf  +  global  constraints)
    2.  srl                                                                        (independent  classifiers  +  global  constraints)  
    3.  sentence  compression  (language  model  +  global  constraints)

    less  detailed  examples  

    1.  co   reference  
    2.  a  bunch  more  ...

    part  3:  id136  algorithms  (ilp  &  search) (15  minutes)

    compiling  knowledge  to  linear  inequalities
    other  algorithms  like  search

break

0: 7

0: 8

2

this  tutorial:  ilp  &  constrained  conditional  models  (part  ii)

this  tutorial:  ilp  &  constrained  conditional  models  (part  ii)

    part  4:  training  issues  (80  min)

    learning  models

    independently  of  constraints  (l+i);  jointly  with  constraints  (ibt)
    decomposed  to  simpler  models

    learning  constraints    penalties

    independently  of  learning  the  model  
    jointly,  along  with  learning  the  model  

    dealing  with  lack  of  supervision

    constraints  driven  semi   supervised  learning  (codl)
    indirect  supervision  

    learning  constrained  latent  representations

    part  5:  conclusion  (&  discussion)    (10  min)

    building  ccms;    features  and  constraints.  mixed  models  vs.  joint  models;  
    where  is  knowledge  coming  from

0: 9

0: 10

the end

this  tutorial:  ilp  &  constrained  conditional  models

this  tutorial:  ilp  &  constrained  conditional  models

    part  1:  introduction  to  constrained  conditional  models (30min)

    part  2:  how  to  pose  the  id136  problem (45  minutes)

    examples:  

    ne  +  relations  
    information  extraction      correcting  models  with  ccms

    first  summary:  why  are  ccm  important
    problem  setting

    features  and  constraints;  some  hints  about  training  issues

    introduction  to  ilp  
    posing  nlp  problems  as  ilp  problems

    1.  sequence  tagging                    (id48/crf  +  global  constraints)
    2.  srl                                                                        (independent  classifiers  +  global  constraints)  
    3.  sentence  compression  (language  model  +  global  constraints)

    less  detailed  examples  

    1.  co   reference  
    2.  a  bunch  more  ...

    part  3:  id136  algorithms  (ilp  &  search) (15  minutes)

    compiling  knowledge  to  linear  inequalities
    other  algorithms  like  search

break

0: 11

0: 12

3

this  tutorial:  ilp  &  constrained  conditional  models  (part  ii)

this  tutorial:  ilp  &  constrained  conditional  models  (part  ii)

    part  4:  training  issues  (80  min)

    learning  models

    independently  of  constraints  (l+i);  jointly  with  constraints  (ibt)
    decomposed  to  simpler  models

    learning  constraints    penalties

    independently  of  learning  the  model  
    jointly,  along  with  learning  the  model  

    dealing  with  lack  of  supervision

    constraints  driven  semi   supervised  learning  (codl)
    indirect  supervision  

    learning  constrained  latent  representations

    part  5:  conclusion  (&  discussion)    (10  min)

    building  ccms;    features  and  constraints.  mixed  models  vs.  joint  models;  
    where  is  knowledge  coming  from

0: 13

0: 14

the end

learning  and  id136  

    global  decisions  in  which  several  local  decisions  play  a  role    but  

there  are  mutual  dependencies  on  their  outcome.
    e.g.  structured  output  problems      multiple  dependent  output  variables

    (learned)  models/classifiers  for  different  sub   problems

    in  some  cases,  not  all  local  models  can  be  learned  simultaneously
    key  examples  in  nlp  are  textual  entailment  and  qa  
    in  these  cases,  constraints  may  appear  only  at  evaluation  time

    incorporate  models    information,  along  with  prior  

knowledge/constraints,  in  making  coherent  decisions  
    decisions  that  respect  the  local  models  as  well  as  domain  &  context  

specific  knowledge/constraints.

training constraints  conditional  models  

decompose model

   

   

   

   

decompose model from constraints

learning  model
    independently  of  the  constraints  (l+i)
    jointly,  in  the  presence  of  the  constraints  (ibt)
    decomposed  to  simpler  models
learning  constraints    penalties
    independently  of  learning  the  model  
    jointly,  along  with  learning  the  model  
dealing  with  lack  of  supervision
    constraints  driven  semi   supervised  learning  (codl)
    indirect  supervision  
learning  constrained  latent  representations

0: 15
page 15

0: 16

4

this  tutorial:  ilp  &  constrained  conditional  models

    part  1:  introduction  to  constrained  conditional  models (30min)

pipeline

raw data

    most  problems  are  not  single  classification  problems

    examples:  

    ne  +  relations  
    information  extraction      correcting  models  with  ccms

    first  summary:  why  are  ccm  important
    problem  setting

    features  and  constraints;  some  hints  about  training  issues

id52

phrases

semantic entities 

relations

parsing

wsd

id14

    conceptually,  pipelining  is  a  crude  approximation

    interactions  occur  across  levels  and  down  stream  decisions  often interact  

with  previous  decisions.

    leads  to  propagation  of  errors
    occasionally,  later  stages  are  easier  but  cannot  correct  earlier errors.

    but,  there  are  good  reasons  to  use  pipelines  
    putting  everything  in  one  basket  may  not  be  right  
    how  about  choosing  some  stages  and  think  about  them  jointly?

1: 1

1: 2

id136  with  general  constraint  structure  [roth&yih   04,07]
recognizing  entities  and  relations

improvement over 
motivation i
no id136: 2-5%

other
other

0.05
0.05

other
other

0.10
0.10

per
per

0.85
0.85

per
per

0.60
0.60

y = argmax    y score(y=v) [[y=v]] = 

0.10
0.10

0.30
0.30

loc
loc

loc
loc

other
other
other

per
per
per

loc
loc
loc

0.05
0.05
0.05

0.50
0.50
0.50

0.45
0.45
0.45

= argmax score(e1 = per)   [[e1 = per]] + score(e1 = loc)   [[e
= loc]] +   
key question:
1
how to guide the global 
id136?
why not learn jointly?

dole    s wife, elizabeth , is a native of n.c.
score(r
e1
e3
1

= s-of)   [[r
e2
1

= s-of]] +   .. 

subject to constraints
r12

r23

irrelevant
irrelevant
irrelevant

spouse_of
spouse_of
spouse_of

born_in
born_in
born_in

0.05
0.05
0.05

0.45
0.45
0.45

0.50
0.50
0.50

irrelevant
irrelevant

spouse_of
spouse_of

born_in
born_in

0.10
0.10

0.05
0.05

0.85
0.85

note:
non sequential 
model

models could be learned separately; constraints may come up only at decision time.

task  of  interests:  structured  output

    for  each  instance,  assign  values  to  a  set  of  variables
    output  variables  depend on  each  other

    common tasks  in

    natural  language  processing  

    parsing;  semantic  parsing;  summarization;  id68;  co   reference  

resolution,  textual  entailment   

    information  extraction
    entities,  relations,   

    many  pure machine  learning  approaches  exist

    hidden  markov  models  (id48s) ;  crfs
    structured  id88s and  id166s   

    however,     

1: 3

1: 4

1

information extraction via id48

motivation ii

lars ole andersen . program analysis and specialization for the 

c programming language.  phd thesis. diku , 

university of copenhagen, may 1994 .

[author]
[title]
[editor]
[booktitle]
[tech-report]
[institution]
[date]

prediction result of a trained id48

lars ole andersen . program analysis and
specialization for the 
c 
programming language
.  phd thesis .
diku , university of copenhagen , may
1994 .

unsatisfactory results !

strategies  for  improving  the  results

    (pure)  machine  learning  approaches

    higher  order  id48/crf?
    increasing  the  window  size?
    adding  a  lot  of  new  features  

increasing the model complexity

    requires  a  lot  of  labeled  examples

    what  if  we  only  have  a  few  labeled  examples?

    any  other  options?  

can we keep the learned model 
simple and still make expressive 
decisions? 

    humans  can  immediately detect  bad  outputs  
    the  output  does  not  make  sense

1: 5

1: 6

information extraction without prior knowledge

examples  of  constraints

lars ole andersen . program analysis and specialization for the 

c programming language.  phd thesis. diku , 

university of copenhagen, may 1994 .

[author]
[title]
[editor]
[booktitle]
[tech-report]
[institution]
[date]

prediction result of a trained id48

lars ole andersen . program analysis and
specialization for the 
c 
programming language
.  phd thesis .
diku , university of copenhagen , may
1994 .

violates lots of natural
constraints!

1: 7

    each  field  must  be  a consecutive  list  of  words  and  can  appear  

at  most once in  a  citation.  

    state  transitions  must  occur  on punctuation  marks.

    the  citation  can  only  start  with author or editor.  

    the  words pp.,  pages correspond  to page.
    four  digits  starting  with 20xx  and  19xx are date.
    quotations can  appear  only  in title
          .

easy to express pieces of    knowledge   
non propositional; may use quantifiers

1: 8

2

information extraction with constraints

    adding constraints, we get correct results!

    without changing the model

    [author]

[title]

lars ole andersen . 
program analysis and specialization for the 
c programming language .

diku , university of copenhagen , 
may, 1994 .

[tech-report] phd thesis .
[institution]
[date]
constrained  conditional  models  allow:
    learning  a  simple  model  
    make  decisions  with  a  more  complex  model
    accomplished  by  directly  incorporating  constraints  to  bias/re   

ranks  decisions  made  by  the  simpler  model

problem  setting

    random  variables y:

y1

c(y1,y4)

y2

c(y2,y3,y6,y7,y8)

y4

y5

y6

y7

y3

y8

observations

    conditional  distributions p (learned  by  models/classifiers)  
    constraints c    any  boolean  function  

defined  over  partial  assignments    (possibly:    +  weights  w )

    goal: find  the     best    assignment

    the  assignment  that  achieves  the  highest  global  performance.

    this  is  an  integer  programming  problem

y*=argmaxy p   y                subject to constraints c

(+ w   c)

1: 9

1: 10

constrained  conditional  models  (aka  ilp  id136)

penalty for violating
the constraint.

(soft) constraints 
component

weight vector for 
   local    models

features, classifiers; log-
linear models  (id48, 
crf) or a combination

how far y is from 
a    legal    assignment

ccms can  be  viewed  as  a  general  interface  to  easily  combine  
domain  knowledge  with  data  driven  statistical  models

how  to  solve?

how  to  train?

this  is  an  integer  linear  program
solving  using  ilp  packages  gives  an    
exact  solution.  
search  techniques  are  also  possible

training is  learning  the  objective  
function.
how  to  exploit  the  structure  to                
minimize  supervision?

1: 11

features  versus  constraints

     i :  x     y      r;                          ci :  x     y      {0,1};                  d:  x     y      r;  
    in  principle,  constraints  and  features  can  encode  the  same  propeties
    in  practice,  they  are  very  different

    features  

    local  ,  short  distance  properties      to  allow  tractable  id136  
    propositional  (grounded):            
    e.g.  true  if:                         the    followed  by  a  noun occurs  in  the  sentence   

    constraints

    global  properties
    quantified,  first  order  logic  expressions  
    e.g.true if:                 all  yis in  the  sequence  y are  assigned  different  values.   

indeed, used differently

1: 12

3

encoding  prior  knowledge

constrained  conditional  models      1st summary

    consider  encoding  the  knowledge  that:  

    entities  of  type  a  and  b  cannot  occur  simultaneously  in  a  sentence  

    the     feature    way              

need more training data

    results  in  higher  order  id48,  crf
    may  require  designing  a  model  tailored  to  knowledge/constraints
    large  number  of  new  features:  might  require  more  labeled  data  
    wastes  parameters  to  learn  indirectly  knowledge we  have.

    the    constraints    way

a form of supervision

    keeps  the  model  simple;    add  expressive  constraints  directly
    a  small    set  of  constraints
    allows  for  decision  time  incorporation  of  constraints

    everything  that  has  to  do  with  constraints  and  learning  models
    in  both  examples,  we  first  learned  models

    either  for  components  of  the  problem

    classifiers  for  relations  and  entities  

    or  the  whole  problem  

    citations

    we  then  included  constraints  on  the  output

    as  a  way  to     correct    the  output  of  the  model  

    in  both  cases  this  allows  us  to  

    learn  simpler  models  than  we  would  otherwise

    as  presented,  global  constraints  did  not  take  part  in  training

    global  constraints  were  used  only  at  the  output.

    a  simple  (and  very  effective)  training  paradigm  (l+i);  we   ll  discuss  others  

1: 13

1: 14

constrained  conditional  models      1st part

    introduced  ccms as  a  formalisms  that  allows  us  to

    learn  simpler  models  than  we  would  otherwise
    make  decisions  with  expressive  models,  augmented  by  declarative  

constraints

    focused  on  modeling      posing  nlp  problems  as  ilp  problems

    1.  sequence  tagging                    (id48/crf  +  global  constraints)
    2.  srl                                                                        (independent  classifiers  +  global  constraints)  
    3.  sentence  compression  (language  model  +  global  constraints)

    described  id136

    from  declarative  constraints  to  ilp;  solving  ilp,  exactly  &  approximately

    next  half      learning

    supervised  setting,  and  supervision   lean  settings

1: 15

4

this  tutorial:  ilp  &  constrained  conditional  models

ccms are  optimization  problems

    part  2:  how  to  pose  the  id136  problem (45  minutes)

    introduction  to  ilp  
    posing  nlp  problems  as  ilp  problems

    1.  sequence  tagging                    (id48/crf  +  global  constraints)
    2.  srl                                                                        (independent  classifiers  +  global  constraints)  
    3.  sentence  compression  (language  model  +  global  constraints)

    less  detailed  examples  

    1.  co   reference  
    2.  a  bunch  more  ...

    part  3:  id136  algorithms  (ilp  &  search) (15  minutes)

    compiling  knowledge  to  linear  inequalities
    other  algorithms  like  search

break

    we  pose  id136  as  an  optimization  problem

    integer  linear  programming  (ilp)

    advantages:

    keep  model  small;  easy  to  learn
    still  allowing  expressive,  long   range  constraints
    mathematical  optimization  is  well  studied
    exact  solution  to  the  id136  problem  is  possible
    powerful  off   the   shelf  solvers  exist

    disadvantage:

    the  id136  problem  could  be  np   hard

linear  programming:  example

linear  programming:  example

    telfa co.  produces  tables  and  chairs

    each  table  makes  $8  profit,  each  chair  makes  $5  profit.

    we  want  to  maximize  the  profit.  

    telfa co.  produces  tables  and  chairs

    each  table  makes  $8  profit,  each  chair  makes  $5  profit.
    a  table  requires  1  hour  of  labor  and  9  sq.  feet  of  wood.
    a  chair  requires  1  hour  of  labor  and  5  sq.  feet  of  wood.
    we  have  only  6  hours  of  work  and  45sq.  feet  of  wood.

    we  want  to  maximize  the  profit.  

z = ~c    ~x

a~x     ~b

2:3

2:2

2:4

solving  linear  programming  problems

solving  linear  programming  problems

   8
5   
   82 + 52

cost (profit) vector

solving  linear  programming  problems

2:5

2:7

solving  linear  programming  problems

2:6

2:8

solving  linear  programming  problems

solving  linear  programming  problems

2:9

2:10

integer  linear  programming  has  integer  solutions

integer  linear  programming

    in  nlp,  we  are  dealing  with  discrete  outputs,  therefore  we   re  

almost  always  interested  in  integer  solutions.

    ilp  is  np   complete,  but  often  efficient  for  large  nlp  problems.
    in  some  cases,  the  solutions  to  lp  are  integral  (e.g totally  unimodular

constraint  matrix).

    nlp  problems  are  sparse!

    not  many  constraints  are  active
    not  many  variables  are  involved  in  each  constraint

2:11

2:12

posing  your  problem

penalty for violating
the constraint.

(soft) constraints 
component

weight vector for 
   local    models

a collection of classifiers; 
id148  (id48, 
crf) or a combination

how far y is from 
a    legal    assignment

    how do we write our models in this form?

    what goes in an objective function?
    how to design constraints?

ccm  examples

    many  works  in  nlp  make  use  of  constrained  conditional  

models,  implicitly  or  explicitly.

    next  we  describe  three  examples  in  detail.

    example  1:  sequence  tagging

    adding  long  range  constraints  to  a  simple  model

    example  2:  semantic  role  labeling

    the  use  of  id136  with  constraints  to  improve  semantic  parsing

    example  3: sentence  compression

    simple  language  model  with  constraints  outperforms  complex  models

2:13

2:14

example  1:  sequence  tagging

id48 / crf:

y    = argmax

y   y

p (y0)p (x0|y0)

n   1yi=1

p (yi|yi   1)p (xi|yi)

here, y   s are variables; x   s are fixed.

our objective function must 
include all entries of the cpts.

p (y0)

p (y1|y0) p (y2|y1) p (y3|y2) p (y4|y3)

p (x0|y0) p (x1|y1) p (x2|y2) p (x3|y3) p (x4|y4)

every edge is a boolean variable 
that selects a transition cpt entry.

example:

they are related: if we choose 
y0 = d then we must choose an edge
y0 = d    y1 = ? .

the
d

n

a

v

man
d

n

a

v

saw
d

n

a

v

the
d

n

a

v

dog
d

n

a

v

every assignment to the y   s is a path.

example  1:  sequence  tagging

id48 / crf:

example:

y    = argmax

y   y

p (y0)p (x0|y0)

n   1yi=1

p (yi|yi   1)p (xi|yi)

the
d

n

a

v

man
d

n

a

v

saw
d

n

a

v

the
d

n

a

v

dog
d

n

a

v

as an ilp:

maximize xy   y

subject to

  0,y1{y0=y} +

n   1xi=1xy   y xy0   y

  i,y,y0 1{yi=y     yi   1=y0}

  0,y = log(p (y)) + log(p (x0|y))
  i,y,y0 = log(p (y|y0)) + log(p (xi|y))

2:15

2:16

example  1:  sequence  tagging

id48 / crf:

example:

y    = argmax

y   y

p (y0)p (x0|y0)

n   1yi=1

p (yi|yi   1)p (xi|yi)

as an ilp:

maximize xy   y

subject to

  0,y1{y0=y} +

the
d

n

a

v

man
d

n

a

v

saw
d

n

a

v

the
d

n

a

v

dog
d

n

a

v

n   1xi=1xy   y xy0   y
xy   y

  i,y,y0 1{yi=y     yi   1=y0}

  0,y = log(p (y)) + log(p (x0|y))
  i,y,y0 = log(p (y|y0)) + log(p (xi|y))

1{y0=y} = 1

discrete predictions

1{y0=   nn   } = 1
1{y0=   vb   } = 1
1{y0=   jj   } = 1

example  1:  sequence  tagging

id48 / crf:

example:

y    = argmax

y   y

p (y0)p (x0|y0)

n   1yi=1

p (yi|yi   1)p (xi|yi)

as an ilp:

maximize xy   y

subject to

1{y0=y} = 1

n   1xi=1xy   y xy0   y
xy   y
   y, 1{y0=y} = xy0   y
1{yi   1=y0     yi=y} = xy00   y

   y, i > 1 xy0   y
n   1xi=1xy   y

1{y0=   v   } +

1{yi   1=y     yi=   v   }     1

  0,y1{y0=y} +

  i,y,y0 1{yi=y     yi   1=y0}

  0,y = log(p (y)) + log(p (x0|y))
  i,y,y0 = log(p (y|y0)) + log(p (xi|y))

the
d

n

a

v

man
d

n

a

v

saw
d

n

a

v

the
d

n

a

v

dog
d

n

a

v

discrete predictions

1{y0=y     y1=y0}

1{yi=y     yi+1=y00}

feature consistency

there must be a verb!

example  1:  sequence  tagging

id48 / crf:

example:

y    = argmax

y   y

p (y0)p (x0|y0)

n   1yi=1

p (yi|yi   1)p (xi|yi)

as an ilp:

maximize xy   y

subject to

1{y0=y} = 1

n   1xi=1xy   y xy0   y
xy   y
   y, 1{y0=y} = xy0   y
1{yi   1=y0     yi=y} = xy00   y

   y, i > 1 xy0   y

the
d

n

a

v

man
d

n

a

v

saw
d

n

a

v

the
d

n

a

v

dog
d

n

a

v

discrete predictions

1{y0=y     y1=y0}

1{yi=y     yi+1=y00}

feature consistency

  0,y1{y0=y} +

  i,y,y0 1{yi=y     yi   1=y0}

  0,y = log(p (y)) + log(p (x0|y))
  i,y,y0 = log(p (y|y0)) + log(p (xi|y))

1{y0=   nn   } = 1
1{y0=   dt        y1=   jj   } = 1

1{y0=   dt        y1=   jj   } = 1
1{y1=   nn        y2=   vb   } = 1

2:17

2:18

ccm  examples:  (add  constraints;  solve  as  ilp)

    many  works  in  nlp  make  use  of  constrained  conditional  

models,  implicitly  or  explicitly.

    next  we  describe  three  examples  in  detail.  

    example  1:  sequence  tagging

    adding  long  range  constraints  to  a  simple  model

    example  2:  semantic  role  labeling

    the  use  of  id136  with  constraints  to  improve  semantic  parsing

    example  3: sentence  compression

    simple  language  model  with  constraints  outperforms  complex  models

2:19

2:20

example  2:  semantic  role  labeling

who did what to whom, when, where, why,   

demo:http://l2r.cs.uiuc.edu/~cogcomp

simple  sentence:

i left my pearls to my daughter in my will .
[i]a0 left [my pearls]a1 [to my daughter]a2 [in my will]am-loc .

approach :
1) reveals  several relations. 

2)  produces a very good 
semantic parser. f1~90%
3) easy and fast: ~7 sent/sec
(using xpress-mp)

top ranked system in conll   05 

shared task

key difference is the id136

    a0
    a1
    a2
    am-loc location

leaver
things left
benefactor

i left my pearls to my daughter in my will .

2:21

2:22

algorithmic  approach

candidate arguments

i left my nice pearls to her

    identify argument  candidates

    pruning    [xue&palmer,  emnlp   04]
    argument  identifier  
    binary  classification

    classify argument  candidates

    argument  classifier  

    multi   class  classification

    id136

    use  the  estimated  id203  distribution  

given  by  the  argument  classifier

    use  structural  and  linguistic  constraints
    infer  the  optimal  global  output

i left my nice pearls to her
i left my nice pearls to her
[ [    [       [      [
]    ]  ]            ]     ]

i left my nice pearls to her

2:23

semantic  role  labeling  (srl)

i left my pearls to my daughter in my will .

0.5
0.15
0.15
0.1
0.1

0.15
0.6
0.05
0.05
0.05

0.05
0.05
0.7
0.05
0.15

0.05
0.1
0.2
0.6
0.05

0.3
0.2
0.2
0.1
0.2

page 24

semantic  role  labeling  (srl)

semantic  role  labeling  (srl)

i left my pearls to my daughter in my will .

i left my pearls to my daughter in my will .

0.5
0.15
0.15
0.1
0.1

0.15
0.6
0.05
0.05
0.05

0.05
0.05
0.7
0.05
0.15

0.05
0.1
0.2
0.6
0.05

0.3
0.2
0.2
0.1
0.2

page 25

0.05
0.05
0.7
0.05
0.15

0.05
0.1
0.2
0.6
0.05

0.3
0.2
0.2
0.1
0.2

page 26

0.5
0.15
0.15
0.1
0.1

0.15
0.6
0.05
0.05
0.05

one id136 
problem for each 
verb predicate. 

srl:  posing  the  problem

any boolean rule can be encoded as 
a set of linear inequalities.

if there is an r-ax phrase, there is an ax

1{yi=   ax   }

maximize

where
subject to

constraints

    no  duplicate  argument  classes

   y     y,

    r   ax

1{yi=y}     1

   y     yr,

1{yi=y=   r-ax   }    

n   1xi=0
n   1xi=0

    c   ax

n   1xi=0
jxi=0

if there is an c-x phrase, there is an ax before it

   j, y     yc, 1{yj =y=   c-ax   }    
many  other  possible  constraints:

1{yi=   ax   }

universally quantified 

rules

lbj: allows a developer to encode 
constraints in fol; these are 
compiled into linear inequalities 
automatically. 

    unique  labels
    no  overlapping  or  embedding
    relations  between  number  of  arguments;  order  constraints
    if  verb  is  of  type  a,  no  argument  of  type    b
joint id136 can be used also to combine different srl systems.

  xi,y1{yi=y}

  x,y =       f (x, y) =   y    f (x)

1{yi=y} = 1

1{yi=y}     1

n   1xi=0xy   y
   i, xy   y
n   1xi=0

   y     y,

n   1xi=0

   y     yr,

1{yi=y=   r-ax   }    

   j, y     yc , 1{yj =y=   c-ax   }    

n   1xi=0
jxi=0

1{yi=   ax   }

1{yi=   ax   }

2:27

2:28

example  3:  sentence  compression  (clarke  &  lapata)

ccm  examples:  (add  constraints;  solve  as  ilp)

    many  works  in  nlp  make  use  of  constrained  conditional  

models,  implicitly  or  explicitly.

    next  we  describe  three  examples  in  detail.  

    example  1:  sequence  tagging

    adding  long  range  constraints  to  a  simple  model

    example  2:  semantic  role  labeling

    the  use  of  id136  with  constraints  to  improve  semantic  parsing

    example  3: sentence  compression

    simple  language  model  with  constraints  outperforms  complex  models

example

example:

0
big
big

1
fish
fish

2
eat

3
small

4
fish

5
in
in

0

   

   

   
               
   
8
1
         
   

5
   

   

6

1

568

156

015

2:29

2:30

language  model   based  compression

7
small

8
pond
pond

6
a
a
1

   

2:31

2:32

example:  summarization

trigram  model  in  action

this  formulation  requires  some  additional  constraints

big fish  eat small  fish  in  a  small pond

no  selection  of  decision  variables  can  make  these  trigrams  appear  
consecutively  in  output.

we  skip  these  constraints  here.  

2:33

2:34

modifier  constraints

example

2:35

2:36

example

sentential  constraints

2:37

2:38

example

example

2:39

2:40

more  constraints

sentence  compression:  posing  the  problem

n   2xi=0

n   1xj=i+1

nxk=j+1

maximize

subject to

  k,i,j   i,j,k

if the id136 variable is on, the three 

if the three corresponding auxiliary variables are on, 
corresponding auxiliary variables must also be on.

the id136 variable must be on.

   i, j, k, 0     i < j < k     n,

3  i,j,k       i +   j +   k
2 +   i,j,k       i +   j +   k

(k     i     2)  i,j,k +

  s     k     i     2

j   1xs=i+1

  s +

k   1xs=j+1

if the id136 variable is on, no intermediate 

auxiliary variables may be on.

2:41

2:42

other  ccm  examples:  coref (denis  &  baldridge)

other  ccm  examples:  coref (denis  &  baldridge)

two types of entities: 

   base entities   
   anaphors    (pointers)

error analysis: 

1)   base entities    that    point    to anaphors.
2)anaphors that don   t    point    to anything.

2:43

2:44

other  ccm  examples:  coref (denis  &  baldridge)

other  ccm  examples:  opinion  recognition  

    y.  choi,  e.  breck,  and  c.  cardie.  joint  extraction  of  entities  and  

relations  for  opinion  recognition  emnlp   2006

    semantic  parsing  variation:

    agent=entity
    relation=opinion

    constraints:

    an  agent  can  have  at  most  two  opinions.
    an  opinion  should  be  linked  to  only  one  agent.
    the  usual  non   overlap  constraints.

2:45

2:46

other  ccm  examples:  temporal  ordering

other  ccm  examples:  temporal  ordering

    n.  chambers  and  d.  jurafsky.  jointly  combining  implicit  
constraints  improves  temporal  ordering.  emnlp   2008.

    n.  chambers  and  d.  jurafsky.  jointly  combining  implicit  
constraints  improves  temporal  ordering.  emnlp   2008.

three  types  of  edges:

1)annotation  relations  before/after
2)transitive  closure  constraints
3)time  id172  constraints

2:47

2:48

related  work:  language  generation.

mt  &  alignment  

    regina  barzilay and  mirella  lapata.  aggregation  via  set  

partitioning  for  natural  language  generation.hlt   naacl   
2006.

    ulrich  germann,  mike  jahr,  kevin  knight,  daniel  marcu,  and  

kenji  yamada.  fast  decoding  and  optimal  decoding  for  
machine  translation.  acl  2001.

    john  denero and  dan  klein.  the  complexity  of  phrase  

alignment  problems.  acl   hlt   2008.

    constraints:

    transitivity:  if  (ei,ej)were aggregated,  and  (ei,ejk)  were  too,  then  (ei,ek)  

get  aggregated.

    max  number  of  facts  aggregated,  max  sentence  length.  

2:49

2:50

summary  of  examples

solvers  

    we  have  shown  several  different  nlp  solution  that  make  use  

of  ccms.

    all  applications  presented  so  far  used  ilp  for  id136.  
    people  used  different  solvers

    examples  vary  in  the  way  models  are  learned.

    in  all  cases,  constraints  can  be  expressed  in  a  high  level  
language,  and  then  transformed  into  linear  inequalities.  

    learning  based  java  (lbj)  [rizzolo&roth    07,     10] describe  an  
automatic  way  to  compile  high  level  description  of  constraint  
into  linear  inequalities.  

    xpress   mp
    glpk
    lpsolve
    r
    mosek
    cplex

2:51

2:52

this  tutorial:  ilp  &  constrained  conditional  models

learning  based  java:  translating  to  ilp

    part  2:  how  to  pose  the  id136  problem (45  minutes)

    introduction  to  ilp  
    posing  nlp  problems  as  ilp  problems

    1.  sequence  tagging                    (id48/crf  +  global  constraints)
    2.  srl                                                                        (independent  classifiers  +  global  constraints)  
    3.  sentence  compression  (language  model  +  global  constraints)

    less  detailed  examples  

    1.  co   reference  
    2.  a  bunch  more  ...

    part  3:  id136  algorithms  (ilp  &  search) (15  minutes)

    compiling  knowledge  to  linear  inequalities
    other  algorithms  like  search

break

    constraint  syntax  based  on  first  order  logic

    declarative;  interspersed  within  pure  java
    grounded  in  the  program   s  java  objects

    automatic  run   time  translation  to  linear  inequalities

    creates  auxiliary  variables
    resulting  ilp  size  is  linear  in  size  of  propositionalization

3:1

3:2

ilp:  speed  can  be  an  issue

example  1:  search  based  id136  for  srl

    id136  problems  in  nlp

    sometimes  large  problems  are  actually  easy  for  ilp

    e.g.  entities   relations
    many  of  them  are  not     difficult   

    when  ilp  isn   t  fast  enough,  and  one  needs  to  resort  

to  approximate  solutions.  

    the  problem:  general  solvers  vs.  specific  solvers

    ilp  is  a  very  general  solver
    but,  sometimes  the  structure  of  the  problem  allows  for  simpler  

id136  algorithms.  

    next  we  give  examples  for  both  cases.  

    the  objective  function

          

         

max

       
ij x
c
ij

i

,

j

classification confidence

    constraints

maximize summation of the scores 
subject to linguistic constraints

indicator variable
assigns the j-th class for the i-th token

    unique  labels
    no  overlapping  or  embedding
    if  verb  is  of  type  a,  no  argument  of  type    b
       

    intuition:  check  constraints    violations  on  partial  assignments

3:3

3:4

id136  using  beam  search

heuristic  id136

shape:  argument

color:  label

beam  size  =  2,
constraint:
only  one  red

    for  each  step,  discard  partial  assignments  that  violate  

constraints!

rank  them  according  to  
classification  confidence!  

example  2:  exploiting  structure  in  id136:  id68

    how  to  get    a  score  for  the  pair?
    previous  approaches:

    extract  features  for  each  source  and  target  entity  pair

    the  ccm  approach:

    introduce  an  internal  structure  (characters)
    constrain  character  mappings  to     make  sense   .

3:5

3:7

    problems  of  heuristic  id136

    problem  1:  possibly,  sub   optimal  solution
    problem  2:  may  not  find  a  feasible  solution

    drop  some  constraints,  solve  it    again

    using  search  on  srl  gives  comparable  results  to  using  ilp,  

but  is  much  faster.    

3:6

id68  discovery  with  ccm

assume  the  weights  
are  given.  
more  on  this  later.

score = sum of the mappings   
weight
s. t. mapping satisfies constraints

score = sum of the 
mappings    weight

    natural  constraints

    pronunciation  constraints
    one   to   one  
    non   crossing
      

a weight is assigned to each edge.

include it or not? a binary decision.

    the  problem  now:  id136

    how  to  find  the  best  mapping  that  satisfies  the  constraints?

3:8

finding  the  best  character  mappings

finding  the  best  character  mappings

    an  integer  linear  

programming  problem

maximize the mapping score

pronunciation constraint

one-to-one constraint

non-crossing constraint

    is  this  the  best  id136  

algorithm?

max

xc
ij
ij

   
tjsi
,
      
x
x
0
,1
   
   
ij
ij
i
j
xb
),(
,
   
   
ij
   
i
x
,
,1
   
   
ij
j
imkj
,
,
x
ij

   
   

,
   

x

km

i
,
   
     
...

z
   
,0
   

   

jmk
,
1

    a  dynamic  programming  

algorithm

maximize the mapping score

restricted mapping constraints

one-to-one constraint

non-crossing constraint

    exact  and  fast!  

,

we can 
decompose 
the id136 
problem into 
two parts

take home message:
although ilp can solve 
most problems, the 
fastest id136 
algorithm depends on 
the constraints and can 
be simpler 

3:10

3:9

other  id136  options

    constraint  relaxation  strategies

    try  linear  programming

    [roth  and  yih,  icml  2005]

    cutting  plane  algorithms      do  not  use  all  constraints  at  first

    dependency  parsing:  exponential  number  of  constraints  
    [riedel  and  clarke,  emnlp  2006]

    other  search  algorithms

    a   star,  hill  climbing   
    gibbs  sampling  id136  [finkel et.  al,  acl  2005]

    named  entity  recognition:  enforce  long  distance  constraints
    can  be  considered  as  :  learning  +  id136
    one  type  of  constraints  only

id136  methods      summary

    why  ilp?    a  powerful  way  to  formalize the  problems  

    however,  not  necessarily  the  best  algorithmic solution

    heuristic  id136  algorithms  are  useful  sometimes!

    beam  search
    other  approaches:  annealing     

    sometimes,  a  specific  id136  algorithm  can  be  designed

    according  to  your  constraints

3:11

3:12

constrained  conditional  models      1st part

extra  slides

    introduced  ccms as  a  formalisms  that  allows  us  to

    learn  simpler  models  than  we  would  otherwise
    make  decisions  with  expressive  models,  augmented  by  declarative  

constraints

    focused  on  modeling      posing  nlp  problems  as  ilp  problems

    1.  sequence  tagging                    (id48/crf  +  global  constraints)
    2.  srl                                                                        (independent  classifiers  +  global  constraints)  
    3.  sentence  compression  (language  model  +  global  constraints)

    described  id136

    from  declarative  constraints  to  ilp;  solving  ilp,  exactly  &  approximately

    next  half      learning

    supervised  setting,  and  supervision   lean  settings

3:14

learning  based  java:  translating  to  ilp  (1/2)

learning  based  java:  translating  to  ilp  (2/2)

    modeling  language  for  use  with  java
    classifiers  use  other  classifiers  as  feature  extractors
    constraints  written  in  fol  over  java  objects

    automatically  translated  to  linear  inequalities  at  run   time

    convert  to  conjunctive  normal  form  (cnf);  (np   hard)

    create  temporary  variables

    normalize

    redistribute

    create  indicator  variables

(   i, yi =    r-a0   )     (   j, yj =    a0   )
yj =    a0   

yi 6=    r-a0   !    
   n^i=1
n_j=1
      yi 6=    r-a0       
yj =    a0         
n_j=1
n^i=1
nxj=1
   i,   1     1{yi=   r-a0   }   +

1{yj =   a0   }     1

(   i, yi =    r-a0   )     (   j, yj =    a0   )

   i, 1t1 +

1{yj =   a0   }     1

yi 6=    r-a0   !    

n_j=1

yj =    a0   

t1    

yj =    a0   , where t1    

n^i=1

yi 6=    r-a0   

   n^i=1
n_j=1

nxj=1
nxi=1
nxi=1

n    

1    

1{yi=   r-a0   }     n1t1

1{yi=   r-a0   }     1t1

    every  temporary  variable  is  defined  by  exactly  2  inequalities

3:15

3:16

where  are  we  ?

constrained  conditional  model  :  id136

    we  hope  we  have  already  convinced  you  that

    using  constraints is  a  good  idea  for  addressing  nlp  problems
    constrained  conditional  models provide  a  good  platform

    we  were  talking  about  using  expressive  constraints

    to  improve  existing  models
    learning  +  id136
    the  problem:  id136  

    a  powerful  id136  tool:  integer  linear  programming

    srl,  co   ref,  summarization,  entity   and   relation   
    easy  to  inject  domain  knowledge

constraint violation penalty

(soft) constraints 
component

weight vector for 
   local    models

a collection of classifiers; 
id148  (id48, 
crf) or a combination

how far y is from 
a    legal    assignment

how  to  solve?

how  to  train?

this  is  an  integer  linear  program

solving  using  ilp  packages  gives  an    
exact  solution.  

search  techniques  are  also  possible  

how  to  decompose  the  global  
objective  function?

should  we  incorporate  constraints  in  
the  learning  process?  

3:17

3:18

advantages  of  ilp  solvers:  review

    ilp  is  expressive:  we  can  solve  many  id136  problems  

    converting  id136  problems  into  ilp  is  easy

    ilp  is  easy  to  use: many  available  packages

    (open  source  packages):  lpsolve,  glpk,     
    (commercial  packages):  xpressmp,  cplex
    no  need  to  write  optimization  code!

    why  should  we  consider  other  id136  options?

3:19

this  tutorial:  ilp  &  constrained  conditional  models  (part  ii)

training constrained  conditional  models  

decompose model

    part  4:  training  issues  (80  min)

    learning  models

    independently  of  constraints  (l+i);  jointly  with  constraints  (ibt)
    decomposed  to  simpler  models

    learning  constraints    penalties

    independently  of  learning  the  model  
    jointly,  along  with  learning  the  model  

    dealing  with  lack  of  supervision

    constraints  driven  semi   supervised  learning  (codl)
    indirect  supervision  

    learning  constrained  latent  representations

4: 1

where  are  we?  

    modeling  &  algorithms  for  incorporating  constraints  

    showed  that  ccms allow  for  formalizing  many  problems  
    showed  several  ways  to  incorporate  global  constraints  in  the  decision.  

   

training:  coupling  vs.  decoupling  training  and  id136.  
    incorporating  global  constraints  is  important  but
    should  it  be  done  only  at  evaluation  time or  also  at  training  time?
    how  to  decompose  the  objective  function  and  train  in  parts?
    issues  related  to:

    modularity,  efficiency  and  performance,  availability  of  training data

   

problem  specific  considerations

   

   

   

   

   

   

   

decompose model from constraints

learning  model
    independently  of  the  constraints  (l+i)
    jointly,  in  the  presence  of  the  constraints  (ibt)
    decomposed  to  simpler  models
learning  constraints    penalties
    independently  of  learning  the  model  
    jointly,  along  with  learning  the  model  
dealing  with  lack  of  supervision
    constraints  driven  semi   supervised  learning  (codl)
    indirect  supervision  
learning  constrained  latent  representations

4: 2

training constrained  conditional  models  

learning  model
    independently  of  the  constraints  (l+i)
    jointly,  in  the  presence  of  the  constraints  (ibt)

decompose model from constraints

first  term: learning  from  data    (could  be  further  decomposed)
second  term: guiding  the  model  by  constraints
    can  choose  if  constraints    weights  trained,  when  and  how,  or  taken  into  

account  only  in  evaluation.

    at  this  point      the  case  of  hard  constraints

4: 3

4: 4

comparing  training  methods

training  methods

    option  1:  learning  +  id136 (with  constraints)

    ignore  constraints  during  training

    option  2:    id136  (with  constraints)  based  training  

    consider  constraints  during  training

    in  both  cases:  global  decision  making  with  constraints

    question:  isn   t  option  2  always  better?  

    not  so  simple   

    next,    the     local  model    story   

learning  +  id136    (l+i)
learn  models  independently

id136  based  training  (ibt)
learn  all  models  together!

intuition
intuition
learning  with  
learning  with  
constraints  may  make  
constraints  may  make  
learning  more  difficult  
learning  more  difficult  

f1(x)

f2(x)

f3(x)

y2

y3

y1

y4

y5

y

f4(x)

f5(x)

x1

x2

x3

x4

x5

x
x7

x6

4: 5

4: 6

training  with  constraints
example:  id88   based  global  learning

true  global  labeling

local predictions
apply  constraints:

y

y   
y   

-1 1

-1

1-1

-1 1
-1 1

1
1
1-1

1
1

f2(x)

f3(x)

x

x1

f1(x)

x3

x4

x5

x7

x2

x6

y

f4(x)

f5(x)

which  one  is  better?    
when  and  why?

4: 7

l+i  &  ibt:  general  view      structured  id88

    graphics  for  the  case:    f(x,y)  =  f(x)
    graphics  for  the  case:    f(x,y)  =  f(x)

for  each  iteration

for  each  (x, ygold )  in  the  training  data

ypred=
if    ypred !=  ygold

   =     +  f(x, ygold )      f(x, ypred)  

endif

endfor

the  difference  between  
the  difference  between  

l+i  and  ibt
l+i  and  ibt

4: 8

claims  [punyakanok  et.  al  ,  ijcai  2005]

bound  prediction

    theory  applies  to  the  case  of  local  model  (no  y  in  the  features)

    when  the  local  modes  are     easy    to  learn,  l+i  outperforms  ibt.

    in  many  applications,  the  components  are  identifiable and  easy  to  learn  (e.g.,  

argument,  open   close,  per).

    only  when  the  local  problems  become  difficult  to  solve  in  isolation,  ibt  

outperforms  l+i,  but  needs  a  larger  number  of  training  examples.

l+i: cheaper computationally; modular
ibt  is better in the limit, and other extreme cases. 

    other  training  paradigms  are  possible
    pipeline   like  sequential  models:  [roth, small, titov: ai&stat   09]

    identify  a  preferred  ordering  among  components
    learn  k   th model  jointly  with  previously  learned  models

l+i vs.  ibt:  the  more  identifiable  
individual  problems  are, the  better  
overall  performance  is  with  l+i

           opt + ( ( d log m + log 1/    ) / m )1/2
        0 + ( ( cd log m + c2d +  log 1/    ) / m )1/2

bounds

simulated data

    local 

    global
indication for 
hardness of 

problem

   opt=0.1
   opt=0

   opt=0.2

4: 9

4: 10

relative  merits:  srl

in  some  cases  problems  are  hard  due  
to  lack  of  training  data.  

semi   supervised  learning

training constrained  conditional  models  (ii)

decompose model

l+i is better.

when  the problem 
is artificially made 
harder, the tradeoff 
is clearer. 

   

   

   

   

hard

difficulty of the learning problem

(# features)

easy

decompose model from constraints

learning  model
    independently  of  the  constraints  (l+i)
    jointly,  in  the  presence  of  the  constraints  (ibt)
    decomposed  to  simpler  models
local  models  (trained  independently)   vs. structured  models
    in  many  cases,    structured  models  might  be  better  due  to  expressivity
but,  what  if  we  use  constraints?
local  models +  constraints vs.  structured  models  +  constraints
    hard  to  tell:  constraints  are  expressive  
    for  tractability  reasons,  structured  models  have  less  expressivity  than  the  use  

of  constraints;  local  can  be  better,  because  local  models  are  easier  to  learn

4: 11

4: 12

recall:  example  1:  sequence  tagging  (id48/crf)

example:  crfs are  ccms

but, you can do better

id48 / crf:

example:

y    = argmax

y   y

p (y0)p (x0|y0)

n   1yi=1

p (yi|yi   1)p (xi|yi)

the
d

n

a

v

man
d

n

a

v

saw
d

n

a

v

the
d

n

a

v

dog
d

n

a

v

  0,y1{y0=y} +

  i,y,y0 1{yi=y     yi   1=y0}

  0,y = log(p (y)) + log(p (x0|y))
  i,y,y0 = log(p (y|y0)) + log(p (xi|y))

as an ilp:

maximize xy   y

subject to

1{y0=y} = 1

n   1xi=1xy   y xy0   y
xy   y
   y, 1{y0=y} = xy0   y
1{yi   1=y0     yi=y} = xy00   y

   y, i > 1 xy0   y
n   1xi=1xy   y

1{y0=   v   } +

1{yi   1=y     yi=   v   }     1

discrete predictions

1{y0=y     y1=y0}

1{yi=y     yi+1=y00}

feature consistency

there must be a verb!

    id136  in  this  model  is  done  via                                

    consider  a  common  model  for  sequential  id136:  id48/crf
y5
x5

the    viterbi algorithm.  

y4
x4

y2
x2

y3
x3

y1
y
x x1

s

a
b
c

a
b
c

a
b
c

a
b
c

a
b
c

t

    viterbi is  a  special  case  of  the  linear  programming  based  

id136.
    it  is  a  shortest  path  problem,  which  is  a  lp,  with  a  canonical  matrix  that  is  

totally  unimodular.  therefore,  you  get  integrality  constraints  for  free.  

    one  can  now  incorporate  non   sequential/expressive/declarative

constraints  by  modifying  this  canonical  matrix

    no  value  can  appear  twice;  a  specific  value  must  appear  at  least once;  a   b

    and,  run  the  id136  as  an  ilp  id136.

learn a rather simple model; make decisions with a more expressive model

4: 13

4: 14

example:  semantic  role  labeling  revisited

which  model  is  better?  semantic  role  labeling

s

a
b
c

a
b
c

a
b
c

a
b
c

a
b
c

t

    sequential  models

    local  models

    conditional  random  field  
    global  id88

    training: sentence  based
    testing: find  best  global  

assignment  (shortest  path)
    +  with  constraints

    logistic  regression
    avg.  id88

    training: token  based.  
    testing:  find  best  assignment  

locally
    +  with  constraints  (global)

    experiments  on  srl:  [roth and yih, icml 2005]

    story: inject expressive constraints into conditional random field

sequential models
sequential models
l+il+i

ibtibt

crf   d

crf   ibt

69.14

local
local
l+il+i

avg.  p
58.15

model
baseline

crf
66.46

71.94

+  constraints
training  time

local models are now better than sequential models! 
local models are now better than sequential models! 

sequential  models  are  better  than  local  models  !  
sequential  models  are  better  than  local  models  !  

48

38

73.91

69.82
145

74.49
0.8

(with constraints) 
(with constraints) 

(no  constraints)  
(no  constraints)  

4: 15

4: 16

summary:  training  methods      supervised  case

training constrained  conditional  models  

    many  choices  for  training  a  ccm

    learning  +  id136 (training  w/o    constraints;  add  constraints  later)
    id136  based  learning  (training  with  constraints)

    based  on  this,  what  kind  of  models should  you    use?

    decomposing  models  can  be  better  that  structured  models

    advantages  of  l+i

    require    fewer  training  examples
    more  efficient;  most  of  the  time,  better  performance
    modularity;  easier  to  incorporate  already  learned  models.

    next:  soft  constraints;  supervision   lean  models

   

   

   

   

learning  model
    independently  of  the  constraints  (l+i)
    jointly,  in  the  presence  of  the  constraints  (ibt)
    decomposed  to  simpler  models
learning  constraints    penalties
    independently  of  learning  the  model  
    jointly,  along  with  learning  the  model  
dealing  with  lack  of  supervision
    constraints  driven  semi   supervised  learning  (codl)
    indirect  supervision  
learning  constrained  latent  representations

soft  constraints

i=1   kd(y, 1ci(x))

   pk

    hard  versus  soft  constraints
    hard  constraints:    fixed  penalty
    soft  constraints:      need  to  set  the  penalty

  i =    

    why  soft  constraints?

    constraints  might  be  violated  by  gold  data
    some  constraint  violations  are  more  serious
    an  example  can  violate  a  constraint  multiple  times!
    degree  of  violation  is  only  meaningful  when  constraints  are  soft!

4: 17

4: 18

example: information extraction

lars  ole  andersen  .  program  analysis  and  specialization  for  the  
c  programming  language.    phd  thesis.  diku  ,  university  

of  copenhagen,  may  1994  .

[author]
[title]
[editor]
[booktitle]
[tech   report]
[institution]

[date]

prediction  result  of  a  trained  id48

lars  ole  andersen  .  program  analysis  and
specialization  for  the  
c  
programming  language
.    phd  thesis  .
diku  ,  university  of  copenhagen  ,  may
1994  .

4: 19
19

violates lots of natural
constraints!

4: 2020

examples  of  constraints

degree  of  violations

    each  field  must  be  a consecutive  list  of  words  and  can  appear  

at  most once in  a  citation.  

one  way:  count  how  many  times  the  assignment  y  violated  the  constraint  

d(y, 1c(x)) =pt

j=1   c(yj)

1      if  assigning  yi to  xi violates  the  constraint  c  
with  respect to  assignment  (x1,..,xi   1;y1,   ,yi   1)

    state  transitions  must  occur  on punctuation  marks.

  c(yj) =

    the  citation  can  only  start  with author or editor.  

    the  words pp.,  pages correspond  to page.
    four  digits  starting  with 20xx  and  19xx are date.
    quotations can  appear  only  in title
          .

4: 21
21

0        otherwise

state  transition  must  occur  on  
punctuations.

   

i y
,
i

1
   

       

y
i

x
i

1
   

 is a punctuation

lars
lars
auth
editor
editor
auth
  c(y1)=0   c(y2)=1   c(y3)=1   c(y4)=0
  c(y1)=0   c(y2)=0   c(y3)=1   c(y4)=0

andersen
andersen
editor
editor

ole
ole
book
auth

.
.

     c(yi)  =1
     c(yj)  =2

4: 22
22

reason  for  using  degree  of  violation

learning  the  penalty  weights

    an  assignment  might  violate  a  constraint  multiple  times
    allow  us  to  chose  a  solution  with  fewer  constraint  violations

lars
auth
editor
  c(y1)=0   c(y2)=0   c(y3)=1   c(y4)=0

andersen
editor

ole
auth

.

lars
auth
editor
  c(y1)=0   c(y2)=1   c(y3)=1   c(y4)=0

andersen
editor

ole
book

.

      f (x, y)    pk

i=1   kd(y, 1ci(x))

    strategy  1:  independently  of  learning  the  model  
    handle  the  learning  parameters     and  the  penalty     separately
    learn  a  feature  model  and  a  constraint  model
    similar  to  l+i,  but  also  learn  the  penalty  weights
    keep  the  model  simple

the first one is 
better because of 
d(y,1c(x))!

    strategy  2:  jointly,  along  with  learning  the  model  
    handle  the  learning  parameters       and  the  penalty     together
    treat  soft  constraints  as  high  order  features
    similar  to  ibt,  but  also  learn  the  penalty  weights

4: 23
23

4: 24
24

strategy  1:  independently  of  learning  the  model  

strategy  1:  independently  of  learning  the  model  (cont.)

    model:  (first  order)  hidden  markov  model

p  (x, y)

    constraints:  long  distance  constraints

    the  i   th the  constraint:
    the  id203  that  the  i   th constraint  is  violated  

ci

p (ci = 1)

    the  learning  problem

    given  labeled  data,  estimate
    for  one  labeled  example,

   and p (ci = 1)

score(x, y) = id48 id203    constraint violation score

    training:  maximize  the  score  of  all  labeled  examples!

score(x, y) = id48 id203    constraint violation score

    the  new  score  function  is  a  ccm!

    setting  
    new  score:

  i =     log p (ci=1)

p (ci=0)

log score(x, y) =       f (x, y)    pk

    maximize  this  new  scoring  function  on  labeled  data

i=1   id(y, 1ci(x)) + c

    learn  a  id48  separately
    estimate                                                  separately  by  counting  how  many times  the  

p (ci = 1)

constraint  is  violated  by  the  training  data!

    a  formal  justification  for  optimizing  the  model  and  the  

penalty  weights  separately!

4: 25
25

4: 26

strategy  2:  jointly,  along  with  learning  the  model  

learning  constraint  penalty  with  crf

    review:  structured  learning  algorithms

    structured  id88,  structured  id166
    need  to  supply  the  id136  algorithm:
    for  example,  structured  id166  

maxy wt   (x, y)

minw kwk2

2 + cpl

i=1 ls(xi, yi, w),

    the  function                                                    measures  the  distance  between  gold  label  

ls(x, y, w)

and  the  id136  result  of  this  example!

    simple  solution  for  joint  learning

    add  constraints  directly  into  the  id136  problem
   

contains  both  features  and  constraint  violations      

w =          ,   (x, y)

    conditional  random  field

minw

    the  id203  :

p (y|x, w) = exp(wt   (x,y))

    testing:  solve  the  same     max    id136  problem
    training:  need  to  solve  the     sum    problem

1

2kwk2    pi log p (yi|xi, w)
p   y

exp(wt   (x,  y))

    using  crf  with  constraints

    easy  constraints:  dynamic  programming  for  both  sum  and  max  

problems

    difficult  constraints:  dynamic  programming  is  not  feasible

    the  max  problem  can  still  be  solved  by  ilp
    the  sum  problem  needs  to  be  solved  by  a  special   

designed/approximated  solution

4: 27
page 27

4: 28
page 28

summary:  learning  constraints    penalty  weights

    learning  the  penalty  for  soft  constraints  is  important

    constraints  can  be  violated  by  gold  data
    degree  of  violation
    some  constraints  are  more  important

    learning  constraints    penalty  weights

    learning  penalty  weights  is  a  learning  problem
    independent  approach:  fix  the  model

    generative  models  +  constraints

    joint  approach

    treat  constraints  as  long  distance  features
    max  is  generally  easier  than  the  sum  problem

training constrained  conditional  models  

   

   

   

   

learning  model
    independently  of  the  constraints  (l+i)
    jointly,  in  the  presence  of  the  constraints  (ibt)
    decomposed  to  simpler  models
learning  constraints    penalties
    independently  of  learning  the  model  
    jointly,  along  with  learning  the  model  
dealing  with  lack  of  supervision
    constraints  driven  semi   supervised  learning  (codl)
    indirect  supervision  
learning  constrained  latent  representations

4: 29

4: 30

dealing  with  lack  of  supervision

constraints  as  a  way  to  encode  prior  knowledge

    goal  of  this  tutorial:  learning  structured  models

    learning  structured  models  requires  annotating structures.

    very  expensive  process

    idea1:  can  we  use  constraints  as  a  supervision  resource?

    setting:  semi   supervised  learning

    idea2:  can  we  use  binary  labeled  data  to  learn  a  structured  

model?
    setting:  indirect  supervision  (will  explain  latter)

    consider  encoding  the  knowledge  that:  

    entities  of  type  a  and  b  cannot  occur  simultaneously  in  a  sentence  

    the     feature    way              
    requires  larger  models

    the    constraints    way

need more training data

a effective way to inject 
knowledge

    keeps  the  model  simple;    add  expressive  constraints  directly
    a  small    set  of  constraints
    allows  for  decision  time  incorporation  of  constraints

we can use constraints as a way to replace training data

4: 31

4: 32

constraint  driven  semi/un  supervised  learning

codl
use  constraints  to  generate  better  training  
samples  in  semi/unsupervised  leaning.

in  traditional  semi/unsupervised  learning,  models  
can  drift  away  from  correct    model

resource     
resource     
seed  examples     
seed  examples     

model
better  model
better  model

feedback
better  feedback
better  feedback
learn  from  labeled  data
learn from labeled data
learn  from  labeled  data

prediction
prediction  +  constraints
prediction  +  constraints
label unlabeled data
label  unlabeled  data
label  unlabeled  data

unlabeled data

constraints  driven  learning  (codl)

[chang, ratinov, roth, acl   07;icml   08,long   10]

(w0,  0)=learn(l) 
for n iterations do

t=   

supervised  learning  algorithm  parameterized  by
(w,  ).  learning  can  be  justified  as  an  optimization
procedure  for  an  objective  function

id136  with  constraints:
augment  the  training  set  

for each x in unlabeled dataset

h     argmaxy wt   (x,y) -       k dc(x,y)
t=t     {(x, h)}

(w,  ) =     (w0,  0) + (1-    ) learn(t)

learn  from  new  training  data
weigh  supervised  &  
unsupervised  models.

excellent  experimental  results showing  the  advantages  of  using  constraints,  
especially  with  small  amounts  on  labeled  data  [chang  et.  al,  others]

33

4: 33

4: 34

value  of  constraints  in  semi   supervised  learning

train  and  test  with  constraints!

objective function: 

learning w/o constraints: 300 examples.

learning w 10 constraints

constraints are used to 
bootstrap a semi-
supervised learner
poor model + constraints 
used to annotate 
unlabeled data, which in 
turn is used to keep 
training the model. 

# of available labeled examples

300

key :

we do not modify the 
id48 at all!

constraints can be 
used to train the 
model!

4: 35

4: 36

exciting  recent  research

word  alignment  via  constraints

    generalized  expectation  criteria

    the  idea:  instead  of  labeling  examples,  label  constraint  features!
    g. mann and a. mccallum. jmlr, 2009

    posterior  id173

    reshape  the  posterior  distribution  with  constraints
    instead  of  doing  the     hard   em    way,  do  the  soft   em  way!
    k. ganchev, j. gra  a, j. gillenwater and b. taskar, jmlr, 2010

    different  learning  algorithms,    the  same  idea;

    use  constraints  and  unlabeled  data  as  a  form  of  supervision!

    to  train  a  generative/discriminative  model

    word  alignment,  information  extraction,  document  classification   

    posterior  id173

k. ganchev, j. gra  a, j. gillenwater and b. taskar, jmlr, 2010

    goal:  find  the  word  alignment  between  an  english  sentence  

and  a  french  sentence

    learning  without  using  constraints
    train  a  e   >  f  model  (via  em),  train  a  f   >  e  model  (via  em)
    enforce  the  constraints  at  the  end!  one   to   one  mapping,  consistency

    learning  with  constraints
    enforce  the  constraints  during  training
    use  constraints  to  guide  the  learning  procedure
    running  (soft)  em  with  constraints!

4: 37

4: 38

id203  interpretation  of  ccm

    with  a  probabilistic  model

theoretical  support

    in  k. ganchev, j. gra  a, j. gillenwater and b. taskar, jmlr, 2010

maxy log p (x, y)    pm

k=1   id(y, 1ck(x))

    implication

   

new distribution     p (x, y) exp   p   id(y,1ck (x))

    constraint  driven  learning  with  full  distribution

    step  1:  find  the  best  distribution  that  satisfy  the     constraints   
    step  2:  update  the  model  according  to  the  distribution

given any distribution p(x,y), the closest distribution that 
   satisfies the constraints    is in the form of ccm!

new distribution     p (x, y) exp   p   id(y,1ck (x))

4: 39

4: 40

training constrained  conditional  models  

   

   

   

   

learning  model
    independently  of  the  constraints  (l+i)
    jointly,  in  the  presence  of  the  constraints  (ibt)
    decomposed  to  simpler  models
learning  constraints    penalties
    independently  of  learning  the  model  
    jointly,  along  with  learning  the  model  
dealing  with  lack  of  supervision
    constraints  driven  semi   supervised  learning  (codl)
    indirect  supervision  
learning  constrained  latent  representations

different  types  of  structured  learning  tasks

    type  1:  structured  output  prediction

    dependencies  between  different  output  decisions
    we  can  add  constraints  on  the  output  variables
    examples:  parsing,  pos  tagging,     .

    type  2:  binary  output  tasks  with  latent  structures

    output:  binary,  but  requires  an  intermediate  representation  

(structure)

    the  intermediate  representation  is  hidden
    examples:  paraphrase  identification,  te,     

4: 41

4: 42

structured  output  learning

structure  output  problem:

dependencies  between  
different  outputs

x1

x2

x3

x4

x5

x7

x

x6

y2

y3

y1

y4

y5

y

standard  binary  classification  problem

single  output  problem:

only  one  output

y1

y

x1

x2

x3

x4

x5

x7

x

x6

4: 43

4: 44

binary  classification  problem  with  latent  representation

textual  entailment

binary  output  problem

with  latent  variables

f2

f1

f3

f4

f5

x

x1

x2

x3

x4

x5

x7

x6

y1

y

former military specialist carpenter took the helm at fictitiouscom inc. 
after five years as press official at the united states embassy in the 
united kingdom.

    entailment requires an intermediate representation 
    alignment based features 
    given the intermediate features     learn a decision
    entail/ does not entail 

x1

x2

x3

x4

x5

x7

x6

x3

x4

x1

x2

but only positive entailments are expected to have 
a meaningful intermediate representation

jim carpenter worked for the us government.

4: 45

4: 46

paraphrase  identification

given an input x     x
learn a model f : x     {-1, 1}

    consider  the  following  sentences:  

    s1:                      druce will  face  murder  charges,  conte  said.

    s2:                      conte  said  druce will  be  charged  with  murder  .

we need latent variables that explain: 

why this is a positive example.

    are  s1 and  s2 a  paraphrase  of  each  other?
    there  is  a  need  for  an  intermediate  representation to  justify  

this  decision

given an input x     x
learn a model f : x      h     {-1, 1}

algorithms:  two  conceptual  approaches  

    two  stage  approach  (typically  used  for te  and  paraphrase  identification)

    learn  hidden  variables;  fix  it

    need  supervision  for  the  hidden  layer  (or  heuristics)

    for  each  example,  extract  features  over  x and  (the  fixed)  h.
    learn  a  binary  classier

    proposed  approach:  joint  learning  

    drive  the  learning  of  h from  the  binary  labels
    find  the  best  h(x)
    an  intermediate  structure  representation  is  good  to  the  extent  is  

supports  better  final  prediction.

    algorithm?

4: 47

4: 48

learning  with  constrained  latent  representation  (lclr):  intuition

learning  with  constrained  latent  representation  (lclr):  framework

    if  x  is  positive

    there  must  exist  a  good  explanation  (intermediate  representation)
        h,  wt   (x,h)      0
    or,  maxh wt   (x,h)      0

    if    x  is  negative

    no  explanation  is  good  enough  to  support  the  answer  
        h,  wt   (x,h)      0
    or,  maxh wt   (x,h)      0

    decision  function:  maxh wt   (x,h)  :

    see  if  the  latent  structure  is  good  enough  to  support  the  labels!
    an  ilp  formulation:  ccm  on  the  latent  structure!

    lclr  provides  a  general  id136  formulation  that  allows  

that  use  of  expressive  constraints
    flexibly  adapted  for  many  tasks  that  require  latent  representations.  

lclr model

declarative model

    id141:  model  input  as  graphs,  v(g1,2),  e(g1,2)

    four  hidden  variables:  

    hv1,v2     possible  vertex  mappings;  he1,e2     possible  edge  mappings  

    constraints:

    each  vertex  in  g1 can  be  mapped  to  a  single  vertex  in  g2 or  to  null
    each  edge  in  g1 can  be  mapped  to  a  single  edge  in  g2 or  to  null
    edge  mapping  is  active  iff the  corresponding  node  mappings  are  active

4: 49

4: 50

lclr:  the  learning  framework

    altogether,  this  can  be  combined  into  an  objective  function:

new feature vector for the final decision. 
chosen h selects a representation.

iterative  objective  function  learning

generate features

id136

for h subj. to c

prediction
with inferred h

min

w

1
2kwk2 +

lxi=1

`(   yi max
h   c

wt xs     (x)

hs  s(x))

id136: best h subject to constraints c

    id136  procedure  inside  the  minimization  procedure
    why  does  id136  help?
    similar  to  what  we  mentioned  with    s=  
    focus:  the  binary  classification  task  

update weight 
vector

initial objective 

function 

training
w/r to binary 
decision label

4: 51

4: 52

    formalized  as  structured  id166  +  constrained  hidden  structure
    lcrl:  learning  constrained  latent  representation

optimization

    non  convex,  due  to  the  maximization  term  inside  the  global  

minimization  problem

    in  each  iteration:

    find  the  best  feature  representation  h* for  all  positive  examples  (off   

the  shelf  ilp  solver)

    having  fixed  the  representation  for  the  positive  examples,  update  w  

solving  the  convex  optimization  problem:

    not  the  standard  id166/lr:  need  id136

    asymmetry: only  positive  examples  require  a  good  

intermediate  representation  that  justifies  the  positive  label.  
    consequently,  the  objective  function  decreases  monotonically  

experimental  results

id68:

recognizing id123:

paraphrase identification:*

4: 53

4: 54

summary

training constrained  conditional  models  

    many  important  nlp  problems  require  latent  structures

    lclr:

    an  algorithm  that  applies  ccm  on  latent  structures  with  ilp  id136
    suitable  for  many  different  nlp  tasks
    easy  to  inject  linguistic  constraints  on  latent  structures
    a  general  learning  framework  that  is  good  for  many  loss  functions

    take  home  message:

    it  is  possible  to  apply  constraints  on  many  important  problems  with  

latent  variables!

   

   

   

   

learning  model
    independently  of  the  constraints  (l+i)
    jointly,  in  the  presence  of  the  constraints  (ibt)
    decomposed  to  simpler  models
learning  constraints    penalties
    independently  of  learning  the  model  
    jointly,  along  with  learning  the  model  
dealing  with  lack  of  supervision
    constraints  driven  semi   supervised  learning  (codl)
    indirect  supervision  
learning  constrained  latent  representations

4: 55

4: 56

indirect  supervision  for  structured  prediction

predicting  phonetic  alignment  (for  id68)

    can  we  use  other     weaker    supervision  resources?

    it  is  possible  to  use  binary  labeled  data  for  structured  output  

prediction  tasks!

    invent  a  companion  binary  decision  problem!

    parse  citations:  lars  ole  andersen  .  program  analysis  and  

specialization  for  the  c  programming  language.    phd  thesis.  diku ,  
university  of  copenhagen,  may  1994  .

    companion: given  a  citation;  does  it  have  a  legitimate  parse?
    pos  tagging
    companion: given  a  word  sequence,  does  it  have  a  legitimate  pos  

tagging  sequence?

    the  binary  supervision  is  easier  to  get.  but  is  it  helpful?

i

target  task

ylat
     

  

companion  task

i      l      l    i    n  o    i    s
   
  

yes/no

  

  

  

  

  

  

  

   
    target  task

    input:    an  english  named  entity    and  its  hebrew  id68
    output: phonetic  alignment  (character  sequence  mapping)
    a  structured output  prediction  task  (many  constraints),  hard  to  label
why  it  is  a  companion  task?  

    companion  task

    input:  an  english  named  entity  and  an  hebrew  named  entity
    companion  output: do  they  form  a  id68  pair?
    a  binary  output    problem,  easy  to  label    
    negative  examples  are  free,  given  positive  examples

4: 57

4: 58

companion  task  binary  label  as  indirect  supervision

joint  learning  with  indirect  supervision  (j   lis)

    the  two  tasks  are  related just  like  the  binary  and  structured

tasks  discussed  earlier

id68   pairs  
positive
must   have      good    phonetic  
alignments

negative id68  pairs    
cannot    have     good    phonetic  
alignments

    all  positive  examples  must  have  a  good  structure
    negative  examples  cannot  have  a  good  structure
    we  are  in  the  same  setting  as  before

    binary  labeled  examples  are  easier to  obtain
    we  can  take  advantage  of  this  to  help  learning  a  structured  model  

    here:  combine  binary  learning  and  structured  learning

    joint  learning   : if  available,  make  use  of  both  supervision  types

target  task

i

   

ylat
     

  

companion  task

i      l      l    i    n  o    i    s
   
  

yes/no

  

  

  

  

  
  
loss  function:  lb,  as  before;  ls,    structural  learning
key: the  same  parameter  w for  both  components

  

1min
2

w

cww
t
1

   

   

si
   

cwyxl
s

   

)

(

;

,

i

i

wzxl
)
b

(

;

,

i

i

   

bi
   

2

4: 59

loss  on  target  task

loss  on  companion  task

4: 60

experimental  result

experimental  result

    very  little  direct  (structured)  supervision.  
    (almost  free)  large  amount  binary  indirect  supervision

    very  little  direct  (structured)  supervision.  
    (almost  free)  large  amount  binary  indirect  supervision

4: 61

4: 62

relations  to  other  frameworks

    b=  ,  l=(squared)  hinge  loss:  structural  id166  
    s=  ,  lclr

    related  to  structural  latent  id166  (yu  &  johachims)  and  to  

felzenszwalb.  

    if  s=  ,  conceptually  related  to  contrastive  estimation

    no     grouping    of  good  examples  and  bad  neighbors
    max  vs.  sum:  we  do  not  marginalize  over  the  hidden  structure  space

    allows  for  complex  domain  specific  constraints  

    related  to  some  

semi   supervised  approaches,  
but  can  use  negative  
examples  (sheffer et.  al)

dealing  with  lack  of  supervision

    constraint  driven  learning

    use  constraints  to  guide  semi   supervised  learning!

[chang, ratinov, roth, acl   07;icml   08,long   10]

    use  binary  labeled  data  to  help  structure  output  prediction
    training  structure  predictors  by  inventing  (easy  to  supervise) binary  

labels  [icml   10]

    driving  supervision  signal  from world   s  response  
    efficient  semantic  parsing =  ilp  base  id136  +  world   s  response

4: 63
page 63

4: 64

connecting  language  to  the  world

real  world  feedback

can  i  get  a  coffee  with  no  
sugar  and  just  a  bit  of  milk

great!

arggg

semantic parser

make(coffee,sugar=no,milk=little)

can  we  rely  on  this  interaction  to  provide  supervision?

traditional  approach:
learn  from  logical  forms  
and  gold  alignments

expensive!

x

y

r

nl
query

logical
query
query  
response:

   what  is  the  largest  state  that  borders  ny?"

largest(  state(  next_to(  const(ny))))

pennsylvania

interactive  computer  

system

semantic  parsing  is a    structured  prediction  problem:  
identify  mappings  from  text  to  a  meaning  representation

the  id136  problem:  a  ccm  formulation,  with  many  
constraints  

4: 65

4: 66

real  world  feedback

empirical  evaluation

supervision  =  expected  response

    key  question:  can  we  learn  from  this  type  of  supervision?

our  approach: use  
only the  responses

binary  
supervision

x

r

nl
query

query  
response:

   what  is  the  largest  state  that  borders  ny?"

pennsylvania

check  if  predicted  response  ==  expected  response

expected :  pennsylvania
predicted :  pennsylvania
positive  response

expected :  pennsylvania
predicted :  nyc

negative  response

train  a  structured  predictor  with  this  binary  supervision  !

algorithm

#  training  
structures

no  learning:  initial  objective  fn
0
binary  signal:  protocol  i                                                                                      
0
binary  signal:  protocol  ii
0
310
wm*2007      (fully  supervised      uses  
gold  structures)  

test  set  
accuracy
22.2%
69.2  %  
73.2  %
75  %

*[wm]      y.   w.  wong  and  r.  mooney.  2007.  learning  synchronous  grammars  for  
semantic  parsing  with  lambda  calculus.  acl.

4: 67

4: 68

summary

    constrained  conditional  models: computational  framework  
for  global  id136  and  a  vehicle  for  incorporating  knowledge

    direct  supervision  for  structured  nlp  tasks  is  expensive

    indirect  supervision  is  cheap  and  easy  to  obtain

    we  suggested  learning  protocols  for  indirect  supervision

    make  use  of  simple,  easy  to  get,  binary  supervision  
    showed  how  to  use  it  to  learn  structure
    done  in  the  context  of  constrained  conditional  models  

    id136  is  an  essential  part  of  propagating  the  simple  supervision

    learning  structures  from  real  world  feedback

    obtain  binary  supervision  from     real  world    interaction
    indirect  supervision  replaces  direct  supervision

summary:  training constrained  conditional  models  

   

   

   

   

learning  model
    independently  of  the  constraints  (l+i)
    jointly,  in  the  presence  of  the  constraints  (ibt)
    decomposed  to  simpler  models
learning  constraints    penalties
    independently  of  learning  the  model  
    jointly,  along  with  learning  the  model  
dealing  with  lack  of  supervision
    constraints  driven  semi   supervised  learning  (codl)
    indirect  supervision  
learning  constrained  latent  representations

4: 69

4: 70

this  tutorial:  ilp  &  constrained  conditional  models  (part  ii)

conclusion

    part  5:  conclusion  (&  discussion)    (10  min)

    building  ccms;    features  and  constraints.  mixed  models  vs.  joint  models;  
    where  is  knowledge  coming  from

    constrained  conditional  models combine

    learning  conditional  models with  using  declarative  expressive  constraints
    within  a  constrained  optimization  framework

    our  goal  was  to  describe:

    a  clean  way  of  incorporating  constraints  to  bias  and  improve  decisions  of  

learned  models

    a  clean  way  to  use  (declarative)  prior  knowledge  to  guide  semi   supervised  

learning

    ways  to  make  use  of  (declarative)  prior  knowledge  when  choosing

intermediate  (latent)  representations.    

    provide  examples  for  the  diverse  usage  ccms have  already  found  

in  nlp
    significant  success  on  several  nlp  and  ie  tasks (often,  with  ilp)  

the end

5: 1

5: 2

y3
y8

technical  conclusions

summary:  constrained  conditional  models

    presented  and  discussed  modeling  issues  

    how  to  improve  existing  models  using  declarative  information  
    incorporating  expressive  global  constraints  into  simpler  learned models  

    discussed  id136  issues

    often,  the  formulation  is  via  an  integer  linear  programming  formulation,  

but  algorithmic  solutions  can  employ  a  variety  of  algorithms.

    training  issues      training  protocols  matters

    training  with/without  constraints;  soft/hard  constraints;  
    performance,  modularity  and  ability  to  use  previously  learned  models.  
    supervision   lean  models

    we  did  not  attend  to  the  question  of     how  to  find  constraints   

    emphasis  on:  background  knowledge  is  important,  exists,  use  it.
    but,  it   s  clearly  possible  to  learn  constraints.

conditional markov random field

constraints network

y1

y4

y5

y2
y6

y7

y3
y8

y1

y4

y5

y2
y6

y7

y*  =  argmaxy     wi   (x;  y)

       i   i dc(x,y)

    linear  objective  functions  
    typically    (x,y)  will  be  local  

functions,  or    (x,y)  =    (x)

    expressive  constraints  over  output  

variables  

    soft,  weighted  constraints  
    specified  declaratively  as  fol  formulae

    clearly,  there  is  a  joint  id203  distribution  that  represents  

this  mixed model.  
    we  would  like  to:  

key difference from mlns which provide a concise 
definition of a model, but the whole joint one.

    learn a  simple  model  or  several  simple models
    make  decisions with  respect  to  a  complex model

5: 3

5: 4

1

designing  ccms

y1

y4

y5

y2
y6

y7

y3
y8

y1

y4

y5

y2
y6

y7

y3
y8

y*  =  argmaxy     wi   (x;  y)

       i   i dc(x,y)

    linear  objective  functions  
    typically    (x,y)  will  be  local  

functions,  or    (x,y)  =    (x)

    expressive  constraints  over  output  

variables  

    soft,  weighted  constraints  
    specified  declaratively  as  fol  formulae

questions?

    thank  you!

    decide  what  variables  are  of  interest      learn  model  (s)  
lbj (learning based java): http://l2r.cs.uiuc.edu/~cogcomp
    think  about  constraints  among  the  variables  of  interest
programming along with building learned models, high level specification of 
    design  your  objective  function  

a modeling language for constrained conditional models. supports

constraints and id136 with constraints

5: 5

5: 6

textual  entailment

id14
punyakanok et. al   05,08

phrasal verb id141 
[connor&roth   07]

learning  and  id136  

id136 for entailment
braz et. al   05, 07

entity matching [li et. al, 
aaai   04, naacl   04]

is it true that   ?
(id123)

eyeing the huge market 
potential, currently led by 
google, yahoo took over 
search company 
overture services inc. last 
year

   

yahoo acquired overture
overture is a search company
google is a search  company
google owns overture
         .

    global  decisions  in  which  several  local  decisions  play  a  role    but  

there  are  mutual  dependencies  on  their  outcome.
    e.g.  structured  output  problems      multiple  dependent  output  variables

    (learned)  models/classifiers  for  different  sub   problems

    in  some  cases,  not  all  local  models  can  be  learned  simultaneously
    key  examples  in  nlp  are  textual  entailment  and  qa  
    in  these  cases,  constraints  may  appear  only  at  evaluation  time

    incorporate  models    information,  along  with  prior  

knowledge/constraints,  in  making  coherent  decisions  
    decisions  that  respect  the  local  models  as  well  as  domain  &  context  

specific  knowledge/constraints.

5: 7

5: 8
page 8

2

training constraints  conditional  models  

decompose model

questions?

   

thank  you

   

   

   

   

decompose model from constraints

learning  model
    independently  of  the  constraints  (l+i)
    jointly,  in  the  presence  of  the  constraints  (ibt)
    decomposed  to  simpler  models
learning  constraints    penalties
    independently  of  learning  the  model  
    jointly,  along  with  learning  the  model  
dealing  with  lack  of  supervision
    constraints  driven  semi   supervised  learning  (codl)
    indirect  supervision  
learning  constrained  latent  representations

5: 9

5: 10

3

bibliography on constrained conditional models and using

integer id135 in nlp

june 1, 2010

references

althaus, e., n. karamanis, and a. koller (2004, july). computing locally coherent discourses. in
acl, barcelona, spain, pp. 399   406.

barzilay, r. and m. lapata (2006a). aggregation via set partitioning for natural language gener-
ation. in proc. of the human language technology conference of the north american chapter of
the association of computational linguistics (hlt-naacl).

barzilay, r. and m. lapata (2006b, june). aggregation via set partitioning for natural language
generation. in proceedings of the human language technology conference of the naacl, main
conference, new york city, usa, pp. 359   366. association for computational linguistics.

bellare, k., g. druck, and a. mccallum (2009). alternating projections for learning with expec-
tation constraints. in uai.

bramsen, p., p. deshpande, y. k. lee, and r. barzilay (2006, july). inducing temporal graphs.
in emnlp, sydney, australia, pp. 189   198. association for computational linguistics.

chambers, n. and d. jurafsky (2008). jointly combining implicit constraints improves tempo-
ral ordering. in proc. of the conference on empirical methods in natural language processing
(emnlp).

chang, m., d. goldwasser, d. roth, and v. srikumar (2010, jun). discriminative learning over
constrained latent representations. in naacl.

chang, m., l. ratinov, n. rizzolo, and d. roth (2008, july). learning and id136 with
constraints. in proceedings of the national conference on arti   cial intelligence (aaai).

chang, m., l. ratinov, and d. roth (2007, jun). guiding semi-supervision with constraint-driven
learning.
in proc. of the annual meeting of the acl, prague, czech republic, pp. 280   287.
association for computational linguistics.

chang, m., l. ratinov, and d. roth (2008, july). constraints as prior knowledge.
workshop on prior knowledge for text and language processing, pp. 32   39.

in icml

1

chang, m., v. srikumar, d. goldwasser, and d. roth (2010). structured output learning with
indirect supervision. in icml.

che, w., z. li, y. hu, y. li, b. qin, t. liu, and s. li (2008, august). a cascaded syntactic and
semantic id33 system. in conll, manchester, england, pp. 238   242. coling 2008
organizing committee.

choi, y., e. breck, and c. cardie (2006). joint extraction of entities and relations for opinion
recognition. in proc. of the 2006 conference on empirical methods in natural language processing
(emnlp).

clarke, j., d. goldwasser, m. chang, and d. roth (2010, july). driving id29 from the
world   s response. in proceedings of the fourteenth conference on computational natural language
learning (conll-2010).

clarke, j. and m. lapata (2006). constraint-based sentence compression: an integer programming
approach. in proc. of the coling/acl 2006 main conference poster sessions (acl).

clarke, j. and m. lapata (2007). modelling compression with discourse constraints.
in proc.
of the conference on empirical methods in natural language processing and on computational
natural language learning (emnlp-conll).

clarke, j. and m. lapata (2008). global id136 for sentence compression: an integer linear
programming approach. journal of arti   cial intelligence research (jair) 31, 399   429.

daum  e iii, h. (2008, october). cross-task knowledge-constrained self training. in proceedings
of the 2008 conference on empirical methods in natural language processing, honolulu, hawaii,
pp. 680   688. association for computational linguistics.

denero, j. and d. klein (2008, june). the complexity of phrase alignment problems. in proceed-
ings of acl-08: hlt, short papers, columbus, ohio, pp. 25   28. association for computational
linguistics.

denis, p. and j. baldridge (2007). joint determination of anaphoricity and coreference resolution
using integer programming. in proc. of the annual meeting of the north american chapter of the
association for computational linguistics - human language technology conference (naacl-
hlt).

deshpande, p., r. barzilay, and d. karger (2007, april). randomized decoding for selection-and-
ordering problems. in human language technologies 2007: the conference of the north american
chapter of the association for computational linguistics; proceedings of the main conference,
rochester, new york, pp. 444   451. association for computational linguistics.

filippova, k. and m. strube (2008a). dependency tree based sentence compression. in iid86.

filippova, k. and m. strube (2008b, october). sentence fusion via dependency graph compression.
in emnlp, honolulu, hawaii, pp. 177   185. association for computational linguistics.

finkel, j. r. and c. d. manning (2008). the importance of syntactic parsing and id136
in semantic rolelabeling. in proc. of the annual meeting of the association for computational
linguistics - human language technology conference, short papers (acl-hlt).

2

ganchev, k., j. gra  ca, j. gillenwater, and b. taskar (2010). posterior id173 for structured
latent variable models. jmlr.

germann, u., m. jahr, k. knight, d. marcu, and k. yamada (2001, july). fast decoding and
optimal decoding for machine translation. in proceedings of 39th annual meeting of the association
for computational linguistics, toulouse, france, pp. 228   235. association for computational
linguistics.

graca, j. v., k. ganchev, and b. taskar (2007). expectation maximization and posterior con-
straints. in nips, volume 20.

klenner, m. (2006). grammatical role labeling with integer id135. in eacl.

klenner, m. (2007a). enforcing consistency on coreference sets. in ranlp.

klenner, m. (2007b, june). shallow dependency labeling. in acl, prague, czech republic, pp.
201   204. association for computational linguistics.

koomen, p., v. punyakanok, d. roth, and w. yih (2005). generalized id136 with multiple
id14 systems (shared task paper). in i. dagan and d. gildea (eds.), proc. of the
annual conference on computational natural language learning (conll), pp. 181   184.

mann, g. and a. mccallum (2008). generalized expectation criteria for semi-supervised learning
of conditional random    elds. in acl, number 870 - 878.

martins, a., n. a. smith, and e. xing (2009a, august). concise integer id135
formulations for id33. in acl.

martins, a. f. t., n. a. smith, and e. p. xing (2009b). polyhedral outer approximations with
application to natural language parsing. in icml, new york, ny, usa, pp. 713   720. acm.

mcdonald, r. (2007). a study of global id136 algorithms in id57.
in ecir.

punyakanok, v., d. roth, w. yih, and d. zimak (2004, august). id14 via
integer id135 id136. in proc. the international conference on computational
linguistics (coling), geneva, switzerland, pp. 1346   1352.

punyakanok, v., d. roth, w. yih, and d. zimak (2005). learning and id136 over constrained
output. in proc. of the international joint conference on arti   cial intelligence (ijcai).

punyakanok, v., d. roth, w. yih, d. zimak, and y. tu (2004). id14 via
generalized id136 over classi   ers (shared task paper). in h. t. ng and e. rilo    (eds.), proc.
of the annual conference on computational natural language learning (conll), pp. 130   133.

riedel, s. and j. clarke (2006, july). incremental integer id135 for non-projective
id33. in emnlp, sydney, australia, pp. 129   137. association for computational
linguistics.

rizzolo, n. and d. roth (2007, september). modeling discriminative global id136. in proc.
of the first international conference on semantic computing (icsc), irvine, california, pp. 597   
604. ieee.

3

rizzolo, n. and d. roth (2010, may). learning based java for rapid development of nlp
systems. in proceedings of the international conference on language resources and evaluation
(lrec), valletta, malta.

roth, d. (2005). learning based programming.

roth, d. and w. yih (2005). integer id135 id136 for conditional random    elds.
in proc. of the international conference on machine learning (icml), pp. 737   744.

roth, d. and w. yih (2007). global id136 for entity and relation identi   cation via a linear
programming formulation. in l. getoor and b. taskar (eds.), introduction to statistical relational
learning. mit press.

sagae, k., y. miyao, and j. tsujii (2007, june). hpsg parsing with shallow dependency constraints.
in acl, prague, czech republic, pp. 624   631. association for computational linguistics.

tsai, t., c. wu, y. lin, and w. hsu (2005, june). exploiting full parsing information to label
semantic roles using an ensemble of me and id166 via integer id135. in conll,
ann arbor, michigan, pp. 233   236. association for computational linguistics.

4

