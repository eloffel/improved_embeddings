introduction to classification:
likelihoods, margins, features, and kernels

dan klein
uc berkeley

nlp.cs.berkeley.edu

acknowledgements

    many slides adapted from previous tutorials 

with christopher manning and ben taskar

    includes several diagrams used or adapted 

from ray mooney, andrew moore, mike collins

1

introduction

    much of nlp can be seen as making decisions 

    about structured analyses (sequences, trees, graphs)
    on the basis of multiple information sources (words, word classes, 

tree configurations, etc)

    widespread adoption of discriminative methods

    use of arbitrary features
    various formulations: maxent, id166, id88
    common use: local discriminative decisions, possibly chained
    newer: global methods which exploit model structure (crfs, max-

margin networks)

    this tutorial will cover the core ideas behind:

    part i: basic linear classification
    part ii: kernels and structure

    non-goals: not an overview of nlp applications, not at all 
complete coverage of the huge literature on classification!

outline

    part i: basic linear classification

    multiclass linear decision rules
    approaches: id88, maximum likelihood, 

maximum margin

    advantages / disadvantages / tradeoffs

    part ii: kernels and structure

    kernels and kernelization of classifiers
    basic structured classification

2

example: text classification

    we want to classify documents into categories

document

    win the election    
    win the game    
    see a movie    

category

politics
sports
other

    classically, do this on the basis of words in the document, but 

other information sources are potentially relevant:
    document length
    average word length
    document   s source
    document layout

some definitions

sometimes, we want y 

to depend on x

inputs

output 
space

outputs

true 
outputs

feature 
vectors

    win the election    

sports, politics, other

sports

politics

either x is implicit, 

or y contains x

sports        win   

politics        election   

politics        win   

3

block feature vectors

    sometimes, we think of the input as having features, 
which are multiplied by outputs to form the candidates

    win the election    

   win   

   election   

non-block feature vectors
    sometimes the features of candidates cannot be 

decomposed in this regular way

    example: a parse tree   s features may be the 

productions present in the tree

s

np
n    n

vp
v

s

np
vp
n v    n

    different candidates will thus often share features
    we   ll return to the non-block case later

s

np

vp

np
n    n

vp
v

np
n
vp
v    n

4

linear models: scoring

   

in a linear model, each feature gets a weight w

    we compare hypotheses on the basis of their linear scores:

linear models: prediction rule

    the linear prediction rule:

    we   ve said nothing about where weights come from!

5

binary decision rule

    heavily studied case: binary classification

    simplifed: only class    1    has features

    decision rule is a hyperplane
    one side will be class 1
    other side will be class 0

y
e
n
o
m

bias  : -3
free  :  4
money :  2
the   :  0 
...

ham

2

1

0

0

spam

1

free

multiclass decision rule

    if more than two 

classes:
    highest score wins
    boundaries are more 

complex

    harder to visualize

    there are other ways: e.g. reconcile pairwise decisions

6

learning classifier weights

    two broad approaches to learning weights

    generative: work with a probabilistic model of the data, 

weights are (log) local conditional probabilities
    advantages: learning weights is easy, smoothing is well-

understood, backed by understanding of modeling

    discriminative: set weights based on some error-

related criterion
    advantages: error-driven, often weights which are good for 
classification aren   t the ones which best describe the data

    we   ll mainly talk about the latter

example: stoplights

reality

lights working

lights broken

p(g,r,w) = 3/7

p(r,g,w) = 3/7

p(r,r,b) = 1/7

nb model

working?

ns

ew

nb factors:
    p(w) = 6/7 
    p(r|w) = 1/2 
    p(g|w) = 1/2

    p(b) = 1/7 
    p(r|b) = 1 
    p(g|b) = 0

7

example: stoplights

    what does the model say when both lights are red?

    p(b,r,r) = (1/7)(1)(1) 
    p(w,r,r) = (6/7)(1/2)(1/2) 
    p(w|r,r) = 6/10!

= 1/7 
= 6/28

= 4/28
= 6/28

    we   ll guess that (r,r) indicates lights are working

    imagine if p(b) were boosted higher, to 1/2:
= 4/8
= 1/8

    p(b,r,r) = (1/2)(1)(1) 
    p(w,r,r) = (1/2)(1/2)(1/2) 
    p(w|r,r) = 1/5!

= 1/2
= 1/8

    non-generative values can give better classification

linear models: na  ve-bayes

    (multinomial) na  ve-bayes is a linear model, where:

y

d2

d1

dn

8

how to pick weights?

    goal: choose    best    vector w given training data

    for now, we mean    best for classification   

    the ideal: the weights which have greatest test set 

accuracy / f1 / whatever
    but, don   t have the test set
    must compute weights from training set

    maybe we want weights which give best training set 

accuracy?
    hard discontinuous optimization problem
    may not (does not) generalize to test set
    easy to overfit

though, min-error 

training for mt 
does exactly this.

minimize training error?

    a id168 declares how costly each mistake is

    e.g. 0 loss for correct label, 1 loss for wrong label
    can weight mistakes differently (e.g. false positives worse 
than false negatives or hamming distance over structured 
labels)

    we could, in principle, minimize training loss:

    this is a hard, discontinuous optimization problem

9

linear models: id88

    the id88 algorithm

    iteratively processes the training set, reacting to training errors
    can be thought of as trying to drive down training error

    the (online) id88 algorithm:

    start with zero weights
    visit training instances one by one

    try to classify

    if correct, no change!
    if wrong: adjust weights

examples: id88

    separable case

10

examples: id88

    separable case

id88s and separability

    a data set is separable if some 
parameters classify it perfectly

    convergence: if training data 

separable, id88 will 
separate (binary case)

    mistake bound: the maximum 

number of mistakes (binary case) 
related to the margin or degree of 
separability

separable

non-separable

11

examples: id88

    non-separable case

examples: id88

    non-separable case

12

issues with id88s

    overtraining: test / held-out accuracy 

usually rises, then falls
    overtraining isn   t quite as bad as 

overfitting, but is similar

    id173: if the data isn   t 
separable, weights often thrash 
around
    averaging weight vectors over time 
    [freund  & schapire 99, collins 02]

can help (averaged id88)

    mediocre generalization: finds a 

   barely    separating solution

problems with id88s

    id88    goal   : separate the training data

1. this may be an entire 

feasible space

2. or it may be impossible

13

linear separators

    which of these linear separators is optimal? 

objective functions

    what do we want from our weights?

    depends!
    so far: minimize (training) errors:

    this is the    zero-one loss   

    discontinuous, minimizing is np-complete
    not really what we want anyway

    maximum id178 and id166s have other 

objectives related to zero-one loss

14

classification margin (binary)

    distance of xi to separator is its margin, mi
    examples closest to the hyperplane are support vectors
    margin     of the separator is the minimum m

   

m

classification margin

    for each example xi and possible mistaken candidate y, we 

avoid that mistake by a margin mi(y) (with zero-one loss)

    margin     of the entire separator is the minimum m

    it is also the largest     for which the following constraints 

hold

15

maximum margin

    separable id166s: find the max-margin w

    can stick this into matlab and (slowly) get an id166
    won   t work (well) if non-separable

why max margin?
    why do this?  various arguments:

    solution depends only on the boundary cases, or support vectors

(but remember how this diagram is broken!)

    solution robust to movement of support vectors
    sparse solutions (features not in support vectors get zero weight)
    generalization bound arguments
    works well in practice for many problems

support vectors

16

max margin / small norm

    reformulation: find the smallest w which separates data

remember this 

condition?

        scales linearly in w, so if ||w|| isn   t constrained, we can 

take any separating w and scale up our margin

    instead of fixing the scale of w, we can fix     = 1

gamma to w

17

soft margin classification  

    what if the training set is not linearly separable?
    slack variables   i can be added to allow misclassification of 
difficult or noisy examples, resulting in a soft margin classifier

  i

  i

maximum margin

note: exist other 
choices of how to 
penalize slacks!

    non-separable id166s

    add slack to the constraints
    make objective pay (linearly) for slack:

    c is called the capacity of the id166     the 

smoothing knob

    learning:

    can still stick this into matlab if you want
    constrained optimization is hard; better methods!
    we   ll come back to this later

18

maximum margin

linear models: maximum id178

    maximum id178 (id28)

    use the scores as probabilities:

really, we should all stop calling this maximum 
id178     it   s multiclass id28 or a 

maximum likelihood log-linear model   

    maximize the (log) conditional likelihood of training data

make positive
normalize

19

maximum id178 separators

maximum id178 ii

    motivation for maximum id178:

    connection to maximum id178 principle (sort of)
    might want to do a good job of being uncertain on 

noisy cases   

        in practice, though, posteriors are pretty peaked

    id173 (smoothing)

20

maximum id178 separators

maximum id178

21

log-loss

    if we view maxent as a minimization problem:

    this minimizes the    log loss    on each example

    one view: log loss is an upper bound on zero-one loss

unconstrained optimization

    the maxent objective is an unconstrained optimization problem

    basic idea: move uphill from current guess
    gradient ascent / descent follows the gradient incrementally
    at local optimum, derivative vector is zero
    will converge if step sizes are small enough, but not efficient
    all we need is to be able to evaluate the function and its derivative

22

derivative for maximum id178

big weights are bad

total count of feature n 
in correct candidates

expected count of 

feature n in predicted 

candidates

convexity

    the maxent objective is nicely behaved:

    differentiable (so many ways to optimize)
    convex (so no local optima)

convex

non-convex

convexity guarantees a single, global maximum value 

because any higher points are greedily reachable

23

unconstrained optimization

    once we have a function f, we can find a local optimum by 

iteratively following the gradient

    for convex functions, a local optimum will be global
    basic gradient ascent isn   t very efficient, but there are 
simple enhancements which take into account previous 
gradients: conjugate gradient, l-bfgs

    there are special-purpose optimization techniques for 

maxent, like iterative scaling, but they aren   t better

remember id166s   

    we had a constrained minimization

       but we can solve for    i

    giving

24

hinge loss

    consider the per-instance objective:

    this is called the    hinge loss   
    unlike maxent / log loss, you 

stop gaining objective once the 
true label wins by enough

    you can start from here and 

derive the id166 objective

plot really only right 

in binary case

max vs    soft-max    margin

    id166s:

    maxent:

you can make this zero

    but not this one

    very similar!  both try to make the true score 
better than a function of the other scores
    the id166 tries to beat the augmented runner-up
    the maxent classifier tries to beat the    soft-max   

25

id168s: comparison

    zero-one loss

    hinge

    log

separators: comparison

26

status check

    we   ve covered:

    basics
    what the id88 does and how to train it
    the max margin objective (but not how to optimize it)
    the maximum id178 objective and how to optimize it

    next:

       dual classification    with id88s
    dual optimization, how to optimize id166s
    kernel methods
    structured classification

part ii
    kernels

    dual algorithms
    kernels and kernelization

    structured classification

    structured inputs
    structured learning

27

nearest-neighbor classification

    nearest neighbor, e.g. for digits:
    take new example
    compare to all training examples
    assign based on closest example

    encoding: image is vector of intensities:

    similarity function:

    e.g. dot product of two images    vectors

non-parametric classification

    non-parametric: more examples means 

(potentially) more complex classifiers

    how about k-nearest neighbor?

    we can be a little more sophisticated, 

averaging several neighbors

    but, it   s still not really error-driven learning
    the magic is in the distance function

    overall: we can exploit rich similarity 

functions, but not objective-driven 
learning

28

a tale of two approaches   

    nearest neighbor-like approaches

    work with data through similarity functions
    no explicit    learning   

    linear approaches

    explicit training to reduce empirical error
    represent data through features

    kernelized linear models

    explicit training, but driven by similarity!
    flexible, powerful, very very slow

the id88, again
    start with zero weights
    visit training instances one by one

    try to classify

    if correct, no change!
    if wrong: adjust weights

mistake vectors

29

id88 weights

    what is the final value of w?

    can it be an arbitrary real vector?
    no!  it   s built by adding up feature vectors (mistake vectors).

mistake counts

    can reconstruct weight vectors (the primal representation) 

from update counts (the dual representation) for each i

dual id88

    track mistake counts rather than weights

    start with zero counts (   )
    for each instance i
    try to classify xi,

    if correct, no change!
    if wrong: raise the mistake count for this example and prediction

30

dual / kernelized id88

    how to classify an example x?

    if someone tells us the value of k for each pair of 
candidates, never need to build the weight vectors

issues with dual id88

    problem: to score each candidate, we may have to 

compare to all training candidates

    very, very slow compared to primal dot product!
    one bright spot: for id88, only need to consider 
    slightly better for id166s where the alphas are (in theory) 

candidates we made mistakes on during training

sparse

    this problem is serious: fully dual methods (including 
    of course, we can (so far) also accumulate our weights 

kernel methods) tend to be extraordinarily slow

as we go...

31

kernels: who cares?

    so far: a very strange way of doing a very 

simple calculation

       kernel trick   : we can substitute any* similarity 

function in place of the dot product

    lets us learn new kinds of hypotheses

* fine print: if your kernel doesn   t satisfy certain 
technical requirements, lots of proofs break.  
e.g. convergence, mistake bounds.  in practice, 
illegal kernels sometimes work (but not always).

some kernels

    kernels implicitly map original vectors to higher 

dimensional spaces, take the dot product there, and 
hand the result back

    linear kernel:

    quadratic kernel:

    rbf: infinite dimensional representation

    discrete kernels: e.g. string kernels, tree kernels

32

example: kernels

    quadratic kernels

non-linear separators

    another view: kernels map an original feature space to 

some higher-dimensional feature space where the 
training set is (more) separable

  :  y       (y)

33

some structured kernels

    pid18 tree kernels 
[collins and duffy 02]
    function of two trees
    measures the number of 

tree fragments in common 
(weighted by fragment 
size)

    computed by a dynamic 

program

    dependency tree 

kernels [culotta and 
sorensen 04]

    many more   

why kernels?

    can   t you just add these features on your own (e.g. 

add all pairs of features instead of using the quadratic 
kernel)?
    yes, in principle, just compute them
    no need to modify any algorithms
    but, number of features can get large (or infinite)
    some kernels not as usefully thought of in their expanded 

representation, e.g. rbf or data-defined kernels [henderson 
and titov 05]

    kernels let us compute with these features implicitly

    example: implicit dot product in quadratic kernel takes much 

less space and time per dot product

    of course, there   s the cost for using the pure dual algorithms   

34

kernels vs. similarity functions
    q: what does it take for a similarity 

function to be a kernel?

    a: it must satisfy some technical 

conditions:
    kernel matrix must be symmetric and 

positive semidefinite (e.g. self-similarity is 
high)

    note: making diagonal very large can 

sometimes suffice

dual formulation for id166s

    we want to optimize: (separable case for now)

    this is hard because of the constraints
    solution: method of lagrange multipliers
    the lagrangian representation of this problem is:

    all we   ve done is express the constraints as an adversary which 
leaves our objective alone if we obey the constraints but ruins our 
objective if we violate any of them

35

lagrange duality

    we start out with a constrained optimization problem:

    we form the lagrangian:

    this is useful because the constrained solution is a 

saddle point of     (this is a general property):

primal problem in w

dual problem in    

dual formulation ii

    duality tells us that

has the same value as

    this is useful because if we think of the       s as constants, we have an 

unconstrained min in w that we can solve analytically.

    then we end up with an optimization over     instead of w (easier).

36

dual formulation iii

    minimize the lagrangian for fixed       s:

    so we have the lagrangian as a function of only       s:

coordinate descent i

    despite all the mess, z is just a quadratic in each    i(y)
    coordinate descent: optimize one variable at a time

0

0

    if the unconstrained argmin on a coordinate is negative, 

just clip to zero   

37

coordinate descent ii

    ordinarily, treating coordinates independently is a bad idea, but here 

the update is very fast and simple

    so we visit each axis many times, but each visit is quick

    this approach works fine for the separable case
    for the non-separable case, we just gain a simplex constraint and 
so we need slightly more complex methods (smo, exponentiated
gradient)

what are the alphas?
    each candidate corresponds to a primal 

constraint

    in the solution, an    i(y) will be:
    zero if that constraint is inactive
    positive if that constrain is active
    i.e. positive on the support vectors

    support vectors contribute to weights:

support vectors

38

dual linear classifiers

    for id166s and id88s, we ended up with exactly 

the same rule:

    this form holds more generally (the representer

theorem gives conditions)
    basically, components of the weight vector perpendicular to all 

training examples increase the norm of w without impacting 
training example scores

    e.g. one can show that all weight vectors learned by maxent

have the same form

    so we could kernelize maxent as well   

id170

    so far: talked about candidates y as if from a fixed set of labels
    in principle, nothing at all changes if y   s have structure!
    in practice, big issues:

    id88: argmax is hard because |y| is big

    maxent: expectations are hard because |y| is big

    max margin: too many constraints / alphas because |y| is big

39

example: parse reranking

    if the candidate set |y| is small, then no problem:

    [collins 02] reranking with id88
    [charniak and johnson 05] reranking with maxent

    start with the n-best outputs of a good base parser

    define tons of features on a parses y
    no need to even make the feature local

    learn classifier using only these candidates

    everything works exactly as we   ve discussed!

example: structured id88
    id88 is nice: only need structured id136:

    can do this search with a dynamic program provided 

the features used decompose in appropriate local 
ways [collins 02]

    only need to be able to do viterbi search (and even 

approximate search can work).  see also [daume et al 
06])

    online margin methods like mira [crammer and 

singer 03] get some of the margin effect with similar 
requirements by updating minimally

40

example: crfs

    for maxent, we need feature expectations: 

    we can sometimes calculate these expectations with, 

e.g., dynamic programs

    for sequences, id49 (crfs) do 

exactly this computations [lafferty et al 01]

    trees work fine as well [johnson 01]
    much other crf work in the acl community!
    good property: crfs put probabilities over candidates

example: structured margin

    for maximum margin, things are harder:

    one option: only use constraints as you find you need 

them [tsochantaridis et al 05]

    a better option: the alphas often decompose along 

id145 structures [taskar et al 03, 05]

41

summary

    basic feature-driven classification

    id88
    maximum id178
    maximum margin

    kernels and structure

    much, much more on this topic!

a very few references

   

   

impossible to even start to list all the relevant work!

large list at: http://www.kernel-machines.org/books/

some texts:
   
    classic: tom mitchell    machine learning,    1997.
    christopher bishop,    pattern recognition and machine learning,    2007.

    work directly cited in the tutorial:

   

   

eugene charniak and mark johnson,    coarse-to-fine n-best parsing and maxent discriminative 
reranking,    acl, 2005.
koby crammer and yoram singer,    ultraconservative online algorithms for multiclass problems,    
journal of machine learning research, 2003.

with id88 algorithms,    emnlp, 2002. 

    michael collins,    discriminative training methods for id48: theory and experiments 
    michael collins and nigel duffy,    convolution kernels for natural language,    nips, 2001. 
   

james henderson and ivan titov,    data-defined kernels for parse reranking derived from probabilistic 
models,    acl, 2005 

    mark johnson,    joint and conditional estimation of tagging and parsing models,    acl 2001.
   

john lafferty, andrew mccallum, and fernando pereira,    id49: probabilistic models 
for segmenting and labeling sequence data,    icml, 2001.
ben taskar, dan klein, michael collins, daphne koller and chris manning,    max-margin parsing,    
emnlp, 2004. 
ben taskar, carlos guestrin and daphne koller    max-margin markov networks,    nips, 2003.
ioannis tsochantaridis, thorsten joachims, thomas hofmann, and yasemin altun,    large margin 
methods for structured and interdependent output variables,    jmlr, 2005. 

   

   
   

42

