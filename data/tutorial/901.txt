automated id53: review of the main approaches 

andrea andrenucci, eriks sneiders 

stockholm university/ royal institute of technology, sweden 

{andrea, eriks}@dsv.su.se 

 
 

 

 

 

abstract 

automated question- answering aims at delivering 
concise  information  that  contains  answers  to  user 
questions.  this  paper  reviews  and  compares  three 
main  question-answering  approaches  based  on 
natural  language  processing,  information  retrieval, 
and question templates, eliciting their differences and 
the context of application that best suits each of them. 
 
1. introduction 
 

it  was  mainly 

the  question-answering  (qa)  paradigm,  i.e.  the 
process  of  retrieving  precise  answers 
to  natural 
language (nl) questions, was introduced in late 1960-
ies  and  early  1970-ies  within  the  framework  of  nl 
understanding. 
in 
interfaces  to  problem  solving  systems  for  specific 
domains.  the  advent  of  www  has  reintroduced  the 
need for user-friendly querying techniques that reduce 
information overflow, and poses new challenges to the 
research  in  automated  qa.  important  qa  application 
areas  are  information  extraction  from  the  entire  web 
(   intelligent     search  engines),  online  databases,  and 
inquiries on individual websites.  

implemented 

three  main 

this  paper  discusses 

research 
in  automated  qa.  natural 
approaches  utilized 
language processing (nlp) maps user questions into a 
formal  world  model  and  ensures  the  most  reliable 
answers.  information  retrieval  (ir)  powered  qa, 
together with nlp, focus on fact extraction from large 
amounts  of  text.  template-based  qa  matches  user 
questions  to  a  number  of  templates  that  cover  often 
queried parts of the knowledge domain. the next three 
sections  review  the  approaches;  the  last  section 
compares them eliciting the context of application that 
best suits each of them. 
 
2. natural language processing 
 

information  extraction 

nlp techniques are used in applications that make 
queries  to  databases,  extract  information  from  text, 
retrieve relevant documents from a collection, translate 
from one language to another, generate text responses, 
or recognize spoken words converting them into text. 
from the qa point of view, nl interfaces to databases 
[1]  and 
[28]  are  most 
interesting. a common feature of nlp systems is that 
they  convert  text  input  into  formal  representation  of 
meaning such as logic (first order predicate calculus), 
semantic  networks,  conceptual  dependency  diagrams, 
or frame-based representations [16, p. 502]. since the 
early  days  of  nlp,  qa  systems  simulated  human 
intelligence  within  the  nl  understanding  research 
field. they worked as nl front-end to databases [48], 
dialogue systems [47] or story comprehension systems 
[21]. qa systems were limited to specific and narrow 
domains  such  as  algebra,  astronomy  and  natural 
science.  figure  1  on  the  next  page  shows  the 
architecture  of  a  typical  nlp  qa  system  (the  figure 
originally  from  [1]).  two  main  areas,  the  linguistic 
front-end  and  the  domain-dependent  knowledge,  are 
clearly identified in the architecture. 

translating 

in  nl  before 

the  linguistic  front-end  parses  and  analyses  the 
user 
into  an 
intermediate  logical  query.  the  logical  query  is  then 
converted  into  a  database  query  language,  supported 
by  the  underlying  database  management  system.  the 
domain  dependent  knowledge  contains  information 
specific  for  the  domain  of  interest:  a  lexicon  and  a 
world  model.  the 
lexicon  contains  admissible 
vocabulary  words  from  the  knowledge  domain,  often 
divided in word stems and word affixes (i.e. suffixes or 
prefixes), plus a representation of how they fit together 
[16, p. 66]. the world model describes the structure of 
the domain of interest, i.e. the hierarchy of classes of 
the  domain  objects,  plus  the  properties  and  the 
constraints  that  characterize  the  relationship  between 
them.  traditionally  the  world  model  is  expressed 
through  ontologies       explicit  specifications  of  the 
conceptualization of the knowledge base [14].  

input 

it 

 

514

nlp-based  qa  systems  may  utilize  machine 
learning  to  improve  their  syntax  rules  [25],  lexicon 
[43], semantic rules [50], or the world model [46]. 

for  a  detailed  overview  of  nl  front-ends  to 
databases  see  [1],  [8].  most  traditional  nl  interfaces 
that do thorough language processing remain research 
prototypes as their commercially added value does not 
cover the costs. some commercially available systems 
were  symantec   s  q&a,  ibm   s  language  access  [2] 
and bim   s loqui [30]. 

nl input
nl input

syntax rules
syntax rules

pars er
pars er

lexicon
lexicon

semantic
semantic

rules
rules

linguistic
linguistic
front-end
front-end

parse
parse

tree
tree

semantic
semantic
interpreter
interpreter

logic al   query
logic al   query

db query
db query
generator
generator

db   query
db   query

db
db

db management
db management

sys tem
sys tem

retrieved res ults
retrieved res ults

world model
world model

mapping to
mapping to

db info
db info

domain-
domain-
dependent
dependent
knowledge
knowledge

res ponse
res ponse
generator
generator

res ponse
res ponse

 

figure  1.  architecture  of  a  typical  nlp  qa 
system, from [1] 

 
information extraction (ie) from large amounts of 
unstructured  text  became  especially  important  along 
with the expansion of www. it has moved the interest 
of the qa community to a more shallow approach to 
nlp  [41,  p.8].  shallow  nlp  does  not  imply  text 
understanding,  i.e.  semantic  analysis  of  nl  input. 
instead, it focuses on extracting text chunks matching 
patterns or entities, thus providing the answer to user 
questions  [32].  for  instance  in  a  question  like     who 
won the 1998 nobel peace prize?   , the presence of the 
interrogative pronoun    who    implies the extraction of 
an  entity  of  type     person  name     associated  with  the 
keywords,     won   ,     1998   ,     nobel   ,     peace     and 
   prize     (see  [33]  about  ie  in  qa).  in  some  cases 
machine  learning  has  been  applied  in  order  to  learn 
patterns  to  extract  specific  phrases  [11].  the  shallow 
nlp  approach  is  more  domain-independent  than 
traditional  nlp,  but  requires 
to  be 
explicitly present in the text [44]. this lighter approach 
is implemented in information extraction systems such 
as  circus  [22]  and  lasie  [12],  which  recognize 
entities and their relationship in running text, with the 
purpose of storing them in a structured format for data 
mining or id53.  

the  answer 

 

3. information retrieval 
 

information retrieval deals with the representation, 
storage,  organization  of  and  access  to  information 
items. automated qa is related to ir as users submit 
queries in order to find answers to questions they have 
in  mind  [15].  ir  systems  are  traditionally  seen  as 
document  retrieval  systems,  i.e.  systems  that  return 
documents  that  are  relevant  to  the  user   s  information 
need, but that do not supply direct answers. a further 
step  towards  the  qa  paradigm  is  the  development  of 
document  retrieval  systems  into  passage  retrieval 
systems [35], which focus on retrieving text passages 
rather than entire documents. passage retrieval is now 
a  standard  component  of  modern  ir-based  qa 
applications (ir qa), such as [37], [31] and [7]. 

smart [34] is one of the most significant systems 
for  ir  research:  its  techniques,  such  as  the  vector-
space  model,  have  become  standards  in  the  ir 
community.  smart  is  still  used  in  modern  systems 
such as the ones described in [4] and [5]. faq finder 
[4] is among the first systems for retrieving frequently 
asked  questions  (faq)  from  text  files.  it  utilizes 
traditional  statistical  methods  such  as  term  frequency 
and 
inverted  document  frequency,  plus  semantic 
knowledge  of  english  words  provided  by  id138 
[26].  
 
4. information retrieval meets nlp 
 

information 

the  ir  and  nlp  research  communities  have 
common interests [15]. the joint interest towards qa 
has increased during the latest years as a rising number 
of technically inexperienced people have gained access 
to  online 
sources.  a  number  of 
conferences,  such  as  anlp-naacl  [23]  and  trec 
[44] have promoted this research. the text retrieval 
conferences  (trec)  aim  at  comparing  ir  systems 
implemented  by  academic  and  commercial  research 
groups.  the  systems  participating  at  various  trec 
tracks  run  pre-selected  queries  and  retrieve  text 
documents.  the  results  are  evaluated  manually.  the 
qa  track  was  introduced  in  trec-8  (1999)  with the 
purpose  of  testing  retrieval  of  short  answers  to  fact-
based  questions  instead  of  ranked  lists  of  documents. 
voorhees [45] summarizes the main steps taken by the 
trec qa systems as follows: 
     distinguishing  the  expected  answer  type  from  the 
user  question.  for  example  questions  starting  with 
   who     imply  the  search  for     person  names     as 
answers. 

 

515

     reducing  the  text  corpus  to  a  smaller  set  of 
relevant  documents  or  passages  that  most  likely 
contain the answer(s) to the question. 
     ranking the passages containing the answer(s) and 
extracting the answer(s) to be presented to the user. 

some trec participants use shallow nlp based on 
pattern matching ([3], [37], [49]) while others rely on 
heavier  nlp  techniques  ([36],  [27],  [9]).  the  best 
performing system within the two latest trec, power 
answer [27], had reached 83% accuracy in trec 02 
and  70%  in  trec  03.  accuracy  is  defined  as  the 
fraction of answers judged correct. starting from 2001, 
trec   s  qa  track  uses  the  web  as  the  primary 
information  source.  systems  such  as 
the  ones 
described in [3], [7] exploit the web data redundancy. 
the advantages of data redundancy have been listed in 
[3]  and  can  be  summarized  as  follows:  1)  the 
occurrence  of  multiple  linguistic  formulations  of 
similar statements increases the chances of finding an 
answer 
the  submitted  question  2) 
answers that occur more often on the web are usually 
more reliable. 

that  matches 

mulder  [20] is one of the first open domain web-
based  qa  systems.  mulder  is  similar  to  the  systems 
evaluated  at  trec  and  combines  both  ir  and  nlp 
techniques.  the  system  pre-process  user  questions  in 
nl  with  a  syntactic  parser  and  classify  them  into 
nominal, numerical and temporal categories. the result 
of  the  syntactic  parsing  is  translated  into  a  series  of 
keywords  to  be  submitted  to  google.  mulder  then 
parses  the  retrieved  web  pages  and  extracts  relevant 
snippets,  generating  a 
list  of  possible  candidate 
answers  from  these  snippets.  the  candidates  are  then 
scored  according 
relevant 
keywords, whose relevance is based on a common ir 
metric, inverted document frequency.  
 
5. template based id53 
 

the  proximity  of 

template-based  qa  extends  the  pattern  matching 
approach of nl interfaces to databases [1, p.14] where 
the  intelligence  of  the  system  is  embodied  in  a 
collection of manually created question templates. the 
start  system,  which  has  answered  more  than  a 
million questions since 1993, makes use of knowledge 
annotations  as     computer-analyzable  collections  of 
natural  language  sentences  and  phrases  that  describe 
the content of various information segments    [17]. an 
annotation  mimics  the  structure     subject-relationship-
object   .  start  matches  a  user  question  to  the 
annotation  entries  on  both  the  word  level  (using 
additional  lexical  information  about  synonyms,  is-a 
trees,  etc.)  and 
for 
paraphrased arguments of verbs, nominalization, etc.). 

the  structure 

(rules 

level 

to 

 

516

when  the  user  question  matches  an  annotation  entry, 
start follows the pointer to the information segment 
tagged by the annotation and returns it as the answer. 
omnibase  [18]  is  a  virtual  database  on  top  of  the 
start   s  knowledge  annotations  that  comprises  a 
number of online semi-structured data sources, such as 
cia  fact  book  (country-information-at  system, 
www.cia.gov/cia/publications/factbook/)  or 
internet 
movie database (www.imdb.com). omnibase adopts a 
stylized    object-property-value    data model where data 
sources have objects, which have properties. with the 
help of omnibase, start translates questions into a 
structured  request  for  the  values  of  these  properties. 
each  data  source  has  a  wrapper  that  translates 
omnibase   s  requests  into  local  queries.  sneiders  [39] 
presents  a  similar  approach  for  querying  databases. 
the  system  operates  question  templates,  which  have 
open concepts to be filled with data instances, mapped 
into  the  conceptual  model  of  the  database.  each 
question template has a representative set of keyword 
expressions  similar  to  regular  expressions  [38]  that 
match  relevant  user  questions,  and  it  is  paired  with  a 
query  template       an  unfilled  database  query.  a 
question  template  is  viewed  as  a  predicate  with 
variable and fixed parameters: 
    data1,   datan:q(fixed1,   fixedm,variable1,   variablen) 

during the process of matching a template to a user 
question,  the  fixed  parameters  (fixed1,   fixedm)  are 
bound to the user question. if there are database data 
the  variable 
instances 
parameters  (variable1,     variablen)  and  make  the 
statement  q  true,  then  these  data  instances  constitute 
the  answer.  this  is  an  n-dimensional  case  of  looking 
for the omnibase-kind of property values.  

(data1,   datan) 

the nl annotations of start and omnibase, and 
the  question  templates  share  the  same  feature       they 
are  labels,  attached  to  pieces  of  answer  information, 
that  mimic  the  structure  of  nl  queries  and  map  this 
structure  into  the  underlying  data  model.  the  answer 
can be static text, data returned by an sql query or a 
multimedia presentation.  

the semantic web is a promising source of answers 
[19].  sneiders  [40,  pp.  253-269]  proposes  a  generic 
model  of  template-based  qa  that  shows  the  relations 
between  a  knowledge  domain,  its  conceptual  model, 
structured  databases,  question 
templates,  user 
questions,  and  describes  about  24  constituents  of 
template-based qa.  

individual  question 

templates  are  autonomous 
although  they  may  profit  from  a  common  knowledge 
base.  synonymy  and  linguistic  ambiguities  may  be 
resolved locally within the context of a template [38, p. 
102]. question templates embody the major part of the 
   intelligence     of  the  system  and therefore are created 
manually; automatic generation of the templates would 

that 

fit 

require  another  manually  created  knowledge  base. 
machine learning may help, but we haven   t found any 
evidence  of  extensive  use  of  machine  learning  in 
creating  question  templates.  a  fine-tuned  system  can 
reach as high recall/precision levels as 81/98 and 98/93 
percent  [40,  p.154,  229].  lin  [24]  has  noted  that 
template-based qa is effective because the distribution 
of user queries follows zipf   s law     a small fraction of 
question types account for a large number of questions 
asked.  lin  presents  a  graph  that  plots  the  number  of 
question  types  into  the  number  of  questions  they 
match. 50 question types account for more than 45% of 
trec-2001  questions.  lin  [24]  proposes  also  the 
federated  and  distributed  approaches  to  qa  on  the 
web combined into a single system. according to the 
federated  approach,  a  number of online databases are 
covered  by  question 
is  an 
example of a federated-approach system. ir qa is the 
technique for the distributed approach.  

templates.  omnibase 

although there hasn   t been much academic research 
done in template-based qa, the approach has become 
commercially successful. ask jeeves (www.ask.com), 
which  licensed  its  technology  from  start,  became 
the first large scale question-answering system, until it 
moved  to  a  more  traditional  search  engine  business. 
kiwilogic  (www.kiwilogic.de)  offers  chat  robots  that 
organize question templates in a tree structure, which 
allows  maintaining  a  human-like  dialogue.  the  robot 
remembers  the  previously  retrieved  question  template 
and  its  subject,  which  enabled  so  called  elliptic 
questions  with 
subject.  quickask 
(www.askology.com)  is  a  collection  of  systems  for 
question  answering  on  individual  websites  using 
structured databases and faq lists. an installation on 
www.trygghansa.se  (december  2004)  combines  the 
answers  with  keyword  search  results,  which  puts 
forward most relevant information while rendering an 
overview  of  related  information  not  covered  by 
question templates.  
 
6. differences between the approaches 
 

implied 

to  our  knowledge,  no  formal  comparison  of  the 
three  qa  approaches  has  been  done  so  far.  the 
comparison  presented  here  is  based  on  judgments  on 
individual  publications  and  analysis  of  the  features 
described  in  the  previous  sections.  this  comparison 
can be seen as a starting point for further discussion on 
qa  approaches  and  better  development  of  future 
systems.  we  have  chosen  the  following  criteria: 
quality and source of the answers, size of the domain, 
domain and language portability.  

nlp-based  qa  provides  the  most  reliable  human-
computer dialogue and answers, which is achieved by 

the means of mapping text into a formal semantic real-
world  representation.  the  primary  focus  of  nlp  has 
been  natural  language  interfaces  for  databases  and 
expert  systems  in  a  narrow  domain.  molla  et  al.  [28] 
show  that  narrow  and  technical  domains  require 
thorough (deep) nlp as opposed to data intensive ir 
qa.  the  knowledge  base  of  an  nlp  system  poses 
portability difficulties. figure 1 shows six components 
(linguistic  front-end)  that  change  when  the  input 
language  changes,  and  three  components  (domain 
knowledge)  that  change  when  the  knowledge  domain 
changes. the changes require involvement of qualified 
personnel       programmers,  knowledge  engineers,  and 
database administrators [1, pp. 25-27]. winewater [46] 
increases the portability using two different lexicons, a 
domain dependent and a domain independent one. chu 
and  meng  [6]  improve  portability  with  the  help  of  a 
semantic  graph  that  represents  the  structure  of  the 
database,  keywords  manually  assigned  to  the  nodes 
and attributes in the graph. this is, in essence, a move 
towards the template-based approach. 

techniques  overcomes 

ir  qa,  enhanced  with  shallow  or  deep  nlp 
techniques, focuses on fact retrieval from a large text 
corpus. systems that exploit shallow techniques do not 
comprehend  either  user  questions  or  the  text.  data 
redundancy       a  number  of  similar  statements  that 
contain  the  answer       in  a  large  corpus  and  use  of 
shallow nlp techniques increase the chance of finding 
the right answer without any guarantee that the answer 
is  correct.  shallow  nlp  techniques  have  also  been 
largely used in the extraction of definition answers in 
trec 03 (e.g. [13] and [42]). ir enhanced with deep 
nlp 
the     understanding    
deficiency  of  the  shallow  approach.  for  instance  in 
power  answer  [27],  candidate  answers  and  user 
questions  are  semantically  represented  in  logic  form 
and  compared.  a  logic  proof  based  on  abductive 
justifications  is  then  performed  among  the  valid 
answers.  the  world  model  is  provided  both  by 
id138  ontologies  [26]  and  lexical  chains  [29],  i.e. 
groups of semantically related words that link together 
two  concepts.  the  commercial  version  of  this  system 
requires  however  a  special  ontological  module  that 
extends  the  id138  ontology  with  domain  specific 
concepts.  

ir  techniques  are  generally  considered  language 
and domain independent. nevertheless, the amount of 
nlp  applied  in  order  to  increase  the  quality  of  the 
answers  influences  the  portability  of  the  system. 
applying deep nlp techniques on open domains and 
large  corpus  of  data  is  both  compute  intensive  and 
time-consuming [10].    

template-based qa does not process text. like ir 
enhanced  with  shallow  nlp,  it  presents  relevant 
information  without  any  guarantee  that  the  answer  is 

 

517

a 

correct.  the  answer  can  be  anything  attached  to 
question  templates       static  text,  data  returned  by  an 
sql query, multimedia presentations. it is advisable to 
combine  a  template-based  qa  system  with  other 
search techniques to cover less frequent user questions 
not  foreseen  by  the  templates.  question  templates 
conform  user  questions  in  a  certain  language  and  a 
certain  knowledge  domain;  therefore  they  are  not 
language  or  domain  portable.  most  domains  overlap, 
therefore  individual  autonomous  templates  or  their 
parts can be reused in a new domain. it does not take 
highly  qualified  labour  to  create  and  maintain  a 
question  template  if  the  core  of  the  template  is 
keyword-based 
[38].  easy 
maintenance  of  the  templates  helps  kanisa  advocate 
the concept of pre-packaged knowledge (white-papers 
on  www.kanisa.com).  kanisa  sells  customizable 
template-based knowledge bases designed for different 
businesses.  

expressions 

regular 

formal 

semantic 

table 1 summarizes the three qa approaches from 
the application point of view. on a large domain such 
as  the  entire  web,  ir  qa  is  most  appropriate. 
thorough nlp is technically possible, but the costs of 
building 
representation  of 
information  on  the  entire  web  are  rather  high.  ask 
jeeves tried to cover the web with a large number of 
question 
coverage  gaps  were 
considerable, 
therefore  question  answering  was 
supported  by  keyword  search.  question  templates  on 
the web are most useful if linked to online databases 
(the federated approach, [24]).  

templates.  the 

small domains (e.g., a scientific discipline) fit nlp 
and template-based qa (e.g., 200 templates cover 50% 
of all queries). both techniques can form an interface 
of a structured database as well. fact extraction from 
text is done by nlp (small domains) and ir qa (large 
domains),  but  not  by  template-based  qa,  since  this 
approach does not process text. 
 
table 1. comparison of the main approaches 
thorough nlp  ir & nlp  templates
 
entire web 
no 
structured data  yes 
facts from text  yes 
advice-giving  yes 
reliability % 
small domains  yes 

yes 
no 
yes 
no 
accuracy > 70 recall > 80 
no 

yes 
yes 
no 
yes 

close to 100 

advice-giving  means  answering  questions  without 
focusing  on  facts,  e.g.     how  to  do          type  of 
questions.  faq  answering  usually  implies  advice-
giving.  nlp  and  template-based  qa  are  appropriate 
for that, while experiments within ir qa did not yield 
high quality [4]. 

in  conclusion  there  is  no  best  or  worst  qa 
its  own  niche, 

approach.  each  approach  has 

yes 

 

518

 

application environment, and tasks. if the quality of the 
answers  is  crucial,  nlp  should  be  applied.  if  facts 
need to be extracted from text, ir qa should be used. 
limited  domains,  step-by-step  growing  knowledge 
bases  (e.g.  ask  jeeves),  or  lack  of  highly  qualified 
labour  may  take  advantage  of  the  template-based 
approach.  future  research  trend  should  involve  the 
combination of the approaches into one single system 
and adapt the techniques to the application domain. 

7. references 
 
[1] i. androutsopoulos, g. d. ritchie, p. thanisch,    natural 
language interfaces to databases: an introduction   , journal 
of natural language engineering, 1 (1), 1995. 
[2]  j.l.  binot  et  al.,     natural  language  interfaces:  a  new 
philosophy   , sun expert magazine, 1991. 
[3] e. brill, j et al.,    data-intensive id53   , in 
d.  harman  and  e.  voorhees  (eds):  proc  of  trec  2001, 
nist, gaithersburg, usa, 2001. 
[4]  r.  burke  et  al.,     question  answering  from  frequently 
asked  question  files:  experiences  with  the  faq  finder 
system   , ai magazine, 18 (2), 1997, pp. 57-66. 
[5]  c.  cardie et al.,    examining the role of statistical and 
linguistic  knowledge  sources  in  a  general-knowledge 
question-answering  system   ,  proc.  of  anlp-2000,  acl 
press, usa, 2000, pp. 180   187. 
[6] w. chu, and f. meng, database query formation from 
natural  language  using  semantic  modeling  and  statistical 
keyword  meaning  disambiguation,  technical  report 
990003, ucla, usa. 
[7] c. clarke et al.,    web reinforced id53   , 
in  d.  harman  and  e.  voorhees  (eds): proc of trec 2001, 
nist, gaithersburg, usa, 2001. 
[8]  a.  copestake  and  k.  sparck  jones,     natural  language 
interfaces 
to  databases   ,  the  knowledge  engineering 
review, 5 (4), 1990, pp. 225-249. 
[9] a. r. diekema et al.,    cnlp at the trec-2002 question 
answering track   , in l.p. buckland and e. voorhees (eds): 
proc. of trec 2002, nist, gaithersburg, usa, 2002. 
[10] o. ferret et al.,    document selection refinement based 
on  linguistic  features  for  qalc,  a  question  answering 
system   , proc. of ranlp 2001, john benjamin, usa, 2001. 
[11]  d.  freitag,     toward  general-purpose  learning  for 
information extraction   , in proc. of acl/coling  98, acl 
press, usa, 1998. 
[12]  r.  gaizauskas  and  y.  wilks,     information  extraction: 
beyond document retrieval   , journal of documentation, 54 
(1), 1998, pp. 70-105. 
[13]  r.  gaizauskas  et  al.,     the  university  of  sheffield   s 
trec  2003  q&a  experiment   ,  in  l.p.  buckland  and  e. 
voorhees  (eds):  proc  of  trec  2003,  nist,  gaithersburg, 
usa, 2003. 
[14]  t.r.  gruber,     a  translation  approach  to  portable 
ontology  specifications   ,  knowledge  acquisition,  5  (2), 
1993. 
[15]  l.  hirschman  and  r.  gaizauskas,     natural  language 
id53: the view from here   , natural language 
engineering, 7 (4), 2001, pp. 275-300. 

issues 

review   ,  current 

[34] g. salton, the smart retrieval system     experiments 
in  automatic  document  processing,  prentice  hall  inc.,  nj, 
usa, 1971. 
[35]  g.  salton,  j.  allan  and  c.  buckley,     approaches  to 
passage retrieval in full text information systems   , in proc. of 
sigir  93, acm press, n y, usa, 1993. 
[36]  s.  scott  and  r.  gaizauskas,     university  of  sheffield 
trec-9  q  &  a  system   ,  in  d.  harman  and  e.  voorhees 
(eds): proc. of trec 2000, nist, gaithersburg, usa, 2000. 
[37] m. m. soubbotin and s.m. soubbotin,    use of patterns 
for detection of answer strings: a systematic approach   , in 
l.p. buckland and e. voorhees (eds): proc. of trec 2002, 
nist, gaithersburg, usa, 2002. 
[38]  e.  sneiders,     automated  faq  answering:  continued 
experience with shallow language understanding   , proc. of 
the 1999 aaai fall symp., aaai press, usa, 1999, pp. 97-
107.  
[39]  e.  sneiders,     automated  question  answering  using 
question templates that cover the conceptual model of the 
database   , proc. of nldb'2002, springer-verlag, germany. 
2002. 
[40] e. sneiders, automated id53: template-
based  approach,  phd  thesis,  stockholm  university /  kth 
press, sweden, 2002. 
[41]  k.  sparck  jones,     natural  language  processing:  a 
historical 
in  computational 
linguistics, 2001. 
[42] r. sutcliffe et al.,    id53 using the dlt 
system at trec 2003   , in l.p. buckland and e. voorhees 
(eds): proc. of trec 2003, nist, gaithersburg, usa, 2003. 
[43]  c.  a.  thompson,  and  r.  j.  mooney,     automatic 
construction  of  semantic  lexicons  for  learning  natural 
language 
the  1999  aaai  fall 
symposium, aaai press, usa, 1999. 
[44]  e.m.  voorhees,     the  trec  question  answering 
track   , natural language engineering, 7 (4), 2001, pp. 361-
378. 
[45] e. m. voorhees,    overview of the trec 2003 question 
answering track   , in l.p. buckland and e. voorhees (eds): 
proc. of trec 2003, nist, gaithersburg, usa, 2003. 
[46]  w.  winiwater,     an  adaptive  natural  language 
interface  architecture  to  access  faq  knowledge  bases   , 
proc.  of  the  4th  int.  conf.  on  applications  of  n  l  to 
information systems, 1999.  
[47]  t.  winograd,     understanding  natural  language   , 
cognitive psychology, 3 (1), 1972. 
[48]  w.a  woods,  r.m.  kaplan  and  b.  nash-webber,  the 
lunar  sciences  natural  language  information  system,  bbn 
rep.  2378,  bolt  beranek  and  newman,  cambridge,  mass., 
usa, 1977. 
[49] l. wu et al.,    fduqa on trec2003 qa task   , in l.p. 
buckland  and  e.  voorhees  (eds):  proc.  of  trec  2003, 
nist, gaithersburg, usa, 2003. 
[50]  j.  m  zelle  and  r.  j.  mooney,     learning  to  parse 
database queries using inductive logic programming   , proc. 
of 
1999.

interfaces   ,  proc.  of 

conf. 

13th 

nat. 

the 

ai 

on 

[16]  d.  jurafsky  and  j.h.  martin,  speech  and  language 
processing, prentice hall, nj, usa, 2000. 
[17]  b.  katz,     annotating  the  world  wide  web  using 
natural  language   ,  proc.  of  riao  97,  mcgill  university, 
canada, 1997. 
[18]  b.  katz  et  al.,     omnibase:  uniform  access 
to 
heterogeneous  data  for  question  answering   ,  proc.  of 
nldb'2002, springer-verlag, germany, 2002. 
[19]  b.  katz,  j.  lin,  and  d.  quan,     natural  language 
annotations  for  the  semantic  web   ,  proc.  of  odbase 
2002, springer-verlag, germany, 2002. 
[20]  c.  kwok,  o.  etzioni,  d.  weld,     scaling  question 
answering  to  the  web   ,  acm  trans.  on  information 
systems, 19 (3), 2001, pp. 242-262. 
[21]  w.  lehnert,     a  conceptual  theory  of  qa   ,  proc.  of 
ijcai-77, william kaufmann, usa, pp. 158-164. 
[22]  w.  lehnert  et  al.,     umass/hughes:  description  of  the 
circus  system  as  user  for  muc-5   ,  proc.  of  muc-5, 
morgan kaufman, usa, 1993, pp. 277-291. 
[23]  m.  light  et  al.,  proceedings  of  anlp-naacl  2000, 
acl press, usa, 2000. 
[24]  j.  lin,     the  web  as  a  resource  for  question 
answering:  perspective  and  challenges   ,  proc.  of  lrec 
2002. 
[25]  r.m.  losee,     learning  syntactic  rules  and  tags  with 
id107 for information retrieval and filtering   , 
information  processing  &  management,  32  (2),  1996,  pp. 
185-197. 
[26]  g.a.  miller,     id138:  a  lexical  database  for 
english   ,  comm.  of  the  acm,  38  (11),  acm  press,  usa, 
1995. 
[27]  d.  moldovan  et  al.,     lcc  tools  for  question 
answering   ,  in  l.p.  buckland  and  e.  voorhees (eds): proc 
of trec 2003, nist, gaithersburg, usa, 2003. 
[28]  d.  molla  et  al.,     nlp  for  answer  extraction  in 
technical  domains   ,  proc.  of  eacl  2003,  morgan 
kaufmann, usa, 2003. 
[29] j. morris and g. hirst,    lexical cohesion computed by 
thesaural relations as an indicator of the structure of text   , 
computational linguistics, 17 (1), 1991, pp. 21-48. 
[30]  n.  ott,     aspects  of  the  automatic  generation  of  sql 
statements 
in  a  natural  language  query 
interface   , 
information systems, 17 (2), 1992, pp. 147-159. 
[31] d. ravichandran and eh. hovy,    learning surface text 
patterns for a id53 system   , proc. of acl-
2002, acl press, usa, 2002. 
[32] e. riloff and w. lehnert,    information extraction as a 
basis 
for  high-precision  text  classification   ,  acm 
transactions on information systems 12 (3), 1994, pp. 296-
333. 
[33]  s.  rohini  and  l.  wei,     information  extraction 
supported  question  answering   ,  in  d.  harman  and  e. 
voorhees  (eds):  proc.  of  trec  1999,  nist,  gaithersburg, 
usa, 1999. 

 

 

519

