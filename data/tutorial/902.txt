natural language engineering 7 (4): 275{300.

c(cid:13) 2001 cambridge university press

275

doi: 10.1017/s1351324901002807 printed in the united kingdom

natural language id53:

the view from here

l. h i r s c h m a n

the mitre corporation, bedford, ma, usa

r. g a i z a u s k a s

department of computer science, university of she(cid:14)eld, she(cid:14)eld, uk

e-mail: r.gaizayskas@dcs.shef.ac.uk

1 introduction: what is id53?

as users struggle to navigate the wealth of on-line information now available, the
need for automated id53 systems becomes more urgent. we need
systems that allow a user to ask a question in everyday language and receive an
answer quickly and succinctly, with su(cid:14)cient context to validate the answer. current
search engines can return ranked lists of documents, but they do not deliver answers
to the user.

id53 systems address this problem. recent successes have been
reported in a series of question-answering evaluations that started in 1999 as part
of the text retrieval conference (trec). the best systems are now able to answer
more than two thirds of factual questions in this evaluation.

the combination of user demand and promising results have stimulated inter-
national interest and activity in id53. this special issue arises from
an invitation to the research community to discuss the performance, requirements,
uses, and challenges of id53 systems.

the papers in this special issue cover a small part of the emerging research in
id53. our introduction provides an overview of id53
as a research topic. the (cid:12)rst article, by ellen voorhees, describes the history of
the trec id53 evaluations, the results and the associated evaluation
methodology. the second paper, by buchholz and daelemans, explores the require-
ments for answering complex questions that have compound answers or multiple
correct answers. the third paper, by lin and pantel, describes a new algorithm
to capture paraphrases that allow a more accurate mapping from questions to
potential answers. the fourth paper, by light, mann, rilo(cid:11) and breck, describes
experiments that systematically factor and assess id53 into component
subproblems.

the current state of the (cid:12)eld is such that at best partial solutions can be provided
to the broad challenges of id53, and, given the limited space in this
issue, only some of these challenges can be considered here. nevertheless we believe

276

l. hirschman and r. gaizauskas

that the papers included in this issue are a signi(cid:12)cant re   ection of the current state
of achievement and the preoccupations of active researchers in the area. from the
other submissions we received for this issue, it is clear that the (cid:12)eld is in a phase
of active system building and creative experimentation, and not so much one of
re   ective, comparative or theoretical analysis. thus, while it might be desirable for
an issue such as this to o(cid:11)er a consolidating, synthetic overview of progress to date
and issues for the future, in reality all it can o(cid:11)er is a limited view from the ground
level of an exciting, dynamic research area {    the view from here   .

in the rest of this introduction we provide a brief discussion of the dimensions
of id53 as a research area (section 2), followed by a pocket sketch
of the history of natural language id53 (section 3), an overview of
current approaches (section 4), a discussion of resources and evaluation method-
ologies (section 5), and we conclude with re   ections on future directions for qa
research (section 6). we hope that this introduction will provide a useful general per-
spective on id53 research which complements the detailed technical
contributions of the other papers.

2 question-answering: dimensions of the problem

to answer a question, a system must analyse the question, perhaps in the context of
some ongoing interaction; it must (cid:12)nd one or more answers by consulting on-line
resources; and it must present the answer to the user in some appropriate form,
perhaps associated with justi(cid:12)cation or supporting materials.

several recent conferences and workshops have focused on aspects of the question
answering research area. starting in 1999, the text retrieval conference (trec)1
has sponsored a question-answering track which evaluates systems that answer
factual questions by consulting the documents of the trec corpus. a number
of systems in this evaluation have successfully combined information retrieval and
natural language processing techniques.

evaluation using reading comprehension tests provides a di(cid:11)erent approach to
id53, based on a system   s ability to answer questions about a speci(cid:12)c
reading passage. these are tests that are used to evaluate students    comprehension,
and, as a result, they provide a basis for comparing system performance to human
performance. this was the subject of a johns hopkins summer workshop2 and a
workshop on reading comprehension at the anlp-naacl joint conference in
seattle in 2000 (light, brill, charniak, harper, rilo(cid:11) and voorhees, 2000).

these conferences, workshops and evaluations are opening up the rich problem
domain associated with id53. this section provides an overview of
some dimensions of this research in terms of:

(cid:15) applications
(cid:15) users
(cid:15) question types

1 see http://trec.nist.gov.
2 see http://www.clsp.jhu.edu/ws2000/groups/reading/prj desc.shtml.

natural language id53: the view from here

277

(cid:15) answer types
(cid:15) evaluation
(cid:15) presentation.

2.1 applications

id53 has many applications (see section 3 for more discussion). we
can subdivide these applications based on the source of the answers: structured
data (databases), semi-structured data (for example, comment (cid:12)elds in databases)
or free text (the focus of the articles in this volume). we can further distinguish
among search over a (cid:12)xed set of collections, as used in trec (particularly useful
for evaluation); search over the web, as discussed in the buchholz and daelemans
paper; search over a collection or book, e.g. an encyclopedia (kupiec, 1993); or
search over a single text, as done for reading comprehension evaluations.

we can also distinguish between domain-independent id53 systems
and domain speci(cid:12)c systems, such as help systems. we can even imagine applying
id53 techniques to material in other modalities, such as annotated
images or speech data. overall, we would expect that as collections become larger
and more heterogeneous, (cid:12)nding answers for questions in such collections will
become harder { although the paper by light, mann, rilo(cid:11) and breck (this issue)
indicates that having multiple answer sources (answer redundancy) increases the
likelihood of (cid:12)nding an answer.

2.2 users

users can range from (cid:12)rst time or casual users to repeat or    power    users who
might use such a system routinely in the course of their work. clearly, these di(cid:11)erent
classes of users require di(cid:11)erent interfaces, ask di(cid:11)erent questions and want di(cid:11)erent
kinds of answers. the issue of di(cid:11)erent users is discussed at length in a recent
roadmap document for id53 research { see burger, cardie, chaudhri,
gaizauskas, harabagiu, israel, jacquemin, lin, maiorano, miller, moldovan, ogden,
prager, rilo(cid:11), singhal, shrihari, strzalkowski, voorhees and weishedel ( 2001). for
(cid:12)rst time users, it may be important to explain the limitations of the system, so that
the user can understand how to interpret the answers returned. for expert users, it
may be desirable to develop and update a model of the user, so that summaries can
emphasize novel information and omit information previously provided to the user.

2.3 questions

we do not yet understand how to predict what makes some questions harder than
others. this is an issue of importance to the educational testing community, where
testers must prepare and validate standardized tests such as reading comprehension
tests (kukich, 2000).

we can distinguish questions by answer type: factual answers vs. opinion vs.
summary. we focus here on questions with factual answers, although reading com-
prehension tests, for example, often include other kinds of questions (what is this

278

l. hirschman and r. gaizauskas

story about? or what is the author   s attitude towards the main character in this story?).
the id53 roadmap (burger et al., 2001) includes tackling increasingly
challenging kinds of questions in later years.

next we can distinguish di(cid:11)erent kinds of questions: yes/no questions, \wh"
questions (who was the (cid:12)rst president, how much does a killer whale weigh), indirect
requests (i would like you to list ...), and commands (name all the presidents...). all
of these should be treated as questions. however, systems that depend heavily on
the use of \wh" words for clues (who needs a person answer, when needs a time
answer) may have di(cid:14)culty processing such questions when phrased as name the
(cid:12)rst president as opposed to who was the (cid:12)rst president. the issue of detecting and
learning paraphrases is the focus of the lin and pantel paper.

we have evidence that some kinds of questions are harder than others. for
example, why and how questions tend to be more di(cid:14)cult, because they require
understanding causality or instrumental relations, and these are typically expressed
as clauses or separate sentences (hirschman et al., 1999). as the light, mann,
rilo(cid:11) and breck paper discusses, if a system does a good job of analyzing the type
of answer expected, this narrows the space of possible answers. certain kinds of
questions are harder to answer because of an insu(cid:14)ciently narrowed answer type; for
example, what questions are notoriously hard, because they provide little constraint
on the answer type (what happened vs. what did they see vs. what did they do).

2.4 answers

answers may be long or short, they may be lists or narrative. they may vary
with intended use and intended user. for example, if a user wants justi(cid:12)cation, this
requires a longer answer. but short answer reading comprehension tests require
short answers (phrases).

there are also di(cid:11)erent methodologies for constructing an answer: through ex-
traction { cutting and pasting snippets from the original document(s) containing the
answer { or via generation. where the answer is drawn from multiple sentences or
multiple documents, the coherence of an extracted answer may be reduced, requiring
generation to synthesize the pieces into a coherent whole.

in the limit, id53 and summarization may merge as research areas.
a generic summary answers the question: what is this story about? and a topic-
speci(cid:12)c summary provides information in a story about the requested topic { in
e(cid:11)ect, an answer. see mani et al. (to appear) for a discussion of intrinsic evaluation
of summarization structured around providing answers to questions.

2.5 evaluation

what makes an answer good? is a good answer long, containing su(cid:14)cient context
to justify its selection as an answer? context is useful if the system presents multiple
candidate answers, because it allows the user to (cid:12)nd a correct answer, even when that
answer is not the top ranked answer. however, in other cases, short answers may be
better. the experiences of the trec id53 evaluations (voorhees, this

natural language id53: the view from here

279

issue) show that it is easier to provide longer segments that contain an embedded
answer than shorter segments. in section 5, we discuss issues of evaluation and
criteria for question selection and answer correctness in greater detail.

2.6 presentation

finally, in real information seeking situations, there is a user who interacts with
a system in real time. the user often starts with a general (and underspeci(cid:12)ed)
question, and the system provides feedback directly { or indirectly by returning
too many documents. the user then narrows the search, thus engaging in a kind
of dialogue with the system. facilitating such dialogue interactions would likely
increase both usability and user satisfaction. in addition, if interfaces were able
to handle both speech input and dialogue, id53 systems could be
used to provide conversational access to web based information { an area of great
commercial interest, particularly to telecommunications and web content providers.
to date, there has been little work on interfaces for id53. there
have been few systematic evaluations of how to best present the information to
the user, how many answers to present to a user, how much context to provide, or
whether to provide complete answers vs. short answers with an attached summary
or pointers, etc. this is an area that will receive increased attention as commercial
id53 interfaces begin to be deployed.

3 a brief history of id53

there has been a dramatic surge in interest in natural language id53
since the introduction of the id53 track in the text retrieval confer-
ences, beginning with trec-8 in 1999 (voorhees and harman, 2000). however this
recent interest is by no means the (cid:12)rst time the topic has been addressed by natural
language processing (nlp) researchers. in fact, simmons (1965) begins a survey
article    answering english questions by computer    with the statement that his pa-
per reviews no fewer than (cid:12)fteen implemented english language question-answering
systems built over the preceding (cid:12)ve years. these systems include conversational
question answerers, front-ends to structured data repositories and systems which try
to (cid:12)nd answers to questions from text sources, such as encyclopedias.

3.1 natural language front ends to databases

the best-known early id53 program3 is baseball (green et al.,
1961), a program for answering questions about baseball games played in the
american league over one season. given a question such as who did the red sox
lose to on july 5? or how many games did the yankees play in july? or even on how
many days in july did eight teams play?, baseball analysed the question, using

3 de(cid:12)ned here as taking as input an unrestricted range of questions in natural language, and

attempting to supply an answer by searching stored data.

280

l. hirschman and r. gaizauskas

linguistic knowledge, into a canonical form which was then used to generate a query
against the structured database containing the baseball data.

while baseball was relatively sophisticated, even by current standards, in
how it dealt with the syntax and semantics of questions, it was limited in terms
of its domain { baseball only { and by the fact that it was intended primarily
as an interface to a structured database and not as an interface to a large text
collection. in this regard baseball was the (cid:12)rst of a series of programs designed
as    natural language front-ends to databases   . in this tradition, the assumption was
that computers would hold vast amounts of data in structured databases, the details
of which would be opaque to many users. rather than compel users { typically
construed as time-pressured, computationally-challenged executives { to learn the
structure of a database and a specialised language for querying it, the aim was to
allow users to communicate in their own language with an interface that knew about
questions and about the database structure and could negotiate the translation.

the most well-remembered other early work in this tradition is the lunar
system. lunar was designed \to enable a lunar geologist to conveniently access,
compare and evaluate the chemical analysis data on lunar rock and soil composition
that was accumulating as a result of the apollo moon mission" (woods, 1973). lu-
nar could answer questions such as what is the average concentration of aluminum
in high alkali rocks? or how many brescias contain olivine?. more than a toy, it was
demonstrated at a lunar science convention in 1971 and was able to answer 90% of
the in-domain questions posed by working geologists, without prior instructions as
to phrasing. again note the limitation to a narrow domain.

throughout the 1970s, further work continued in this tradition (see the articles on
the planes, ladder, and team systems in grosz, sparck jones and webber (
1986)). a good review of this work through to 1990 can be found in copestake and
sparck jones (1990).

from the perspective of the current research focus in id53, the key
limitation of this work is that it presumes the knowledge the system is using to
answer the question is a structured knowledge base in a limited domain, and not an
open-ended collection of unstructured texts, the processing of which is itself a major
part of the qa challenge. of relevance, however, is the valuable work done in this
area on the syntactic and semantic analysis of questions4 and on the pragmatics of
the interchange between user and system { see, e.g. webber (1986), for a discussion
of the useful distinction between    answers    (the information literally requested by a

4 one interesting di(cid:11)erence between the questions typically discussed in the literature on
natural language front ends to databases and those in the literature on qa against open
text collections is the role of quanti(cid:12)ers and logical connectives. in questions posed against
databases, quanti(cid:12)ers and connectives frequently play a signi(cid:12)cant role, e.g. who are all
the students in mat201 who also take mat216?. put otherwise, such questions tend to
ask about the extensions of complex sets de(cid:12)ned in terms of set theoretic operations on
simpler sets. questions against open text collections, on the other hand, tend to be about
(cid:12)nding properties or relations of entities known via a de(cid:12)nite description { where is the taj
mahal?, what year did the berlin wall come down?, which team won the fa cup in 1953?.
in such questions quanti(cid:12)ers and connectives do not play a major role. no doubt this
will change as open text collection qa gets more ambitious, bringing these two traditions
closer together.

natural language id53: the view from here

281

question) and    responses    (which may include an answer but also possibly helpful
information beyond what was requested, justi(cid:12)cation, clari(cid:12)cation of misconceptions
or mistaken presuppositions in the question, etc.) and arguments as to why answers
alone are not enough for usable systems.

3.2 dialogue interactive advisory systems

while natural language front-ends to databases is an application area for ques-
tion answering that attracted researchers early on, another area of initially purely
theoretical interest was id53 in human-machine dialogue. as is well
known, alan turing (1950) proposed conversational understanding as the test for
machine intelligence; he presented his challenge in the form of an interrogator who
poses questions to an unseen entity (person or machine) and is then asked to judge
which is which on the basis of their responses.

early dialogue systems such as shrdlu (winograd, 1972) and gus (bobrow
et al., 1977) were built as research systems to help researchers understand the issues
involved in modelling human dialogue. shrdlu was built for a toy domain of a
simulated robot moving objects in a blocks world; gus simulated a travel advisor
and had access to a restricted database of information about airline    ights. for both
these systems, sample dialogues reveal the serious challenges that must be overcome
in the building interactive advisory systems, particularly in dealing with anaphora
and ellipsis.

despite that fact that such early interactive id53 systems used
structured data as their knowledge source there is no requirement that they do
so { text collections could be used instead, though of course real-time response is
essential for such systems. this is now the focus of current international research
on conversational spoken language interfaces. for example, mit   s jupiter system
provides a telephone-based conversational interface for international weather infor-
mation (zue et al., 2000)5. it harvests on-line weather information from multiple web
sites, and responds to naturally phrased questions, such as what will the weather be
tomorrow in tokyo?. such systems point out the many user-centered and pragmatic
issues in id53 that are easy to overlook if one is focused solely on the
ability to express complex queries and get correct responses to them from complex
data sets.

3.3 id53 and story comprehension

an obvious way to test whether someone has understood a text is to ask them
questions about it: if they can answer correctly, they have understood; if not, they
have not. this technique is widely used for testing humans (e.g. for determining
reading levels of children or second language learners) and early on was recognised
as an appropriate way of testing the capabilities of natural language understanding
systems.

5 see also: http://www.sls.lcs.mit.edu/sls/whatwedo/applications/jupiter.html.

282

l. hirschman and r. gaizauskas

the most notable early work here is that of wendy lehnert. working within
schank   s framework of scripts and plans as devices for modelling human story
comprehension (schank and abelson, 1977), she devised a theory of question an-
swering and an implementation of that theory in a system called qualm (lehnert,
1977). her key concern in this work was to move away from the view that natural
language id53 should be seen merely as a front-end to a completely
separate data or information retrieval process. instead she viewed the process of
id53 as one in which both the understanding and answering of a
question relies on the context of the story and pragmatic notions of appropriateness
of answer. in her approach both question and story text are analysed into a con-
ceptual dependency representation. but id53 is not just a process of
matching these representations. the interpretation of a question also requires it to
be assigned one of thirteen conceptual categories, such as    veri(cid:12)cation   ,    request   ,
   causal antecedent   ,    enablement   ,    instrumental/procedural   , etc. classifying ques-
tions this way is necessary to avoid answering questions such as do you know the
time? with yes or how did john pass the exam? with a pen. further id136 may
need to be made on the basis of context. to answer a question such as who wasn   t
at the math lecture today? an exhaustive list of most of the world   s population is
not required. once the question is interpreted, answering may still require more than
simply matching against memory. expectations that stories may have aroused when
told, and then contradicted, may need to be recreated to answer a question. on
being told john ordered a hamburger we may assume he ate it. but if the story goes
on to say it was so burnt he left the restaurant, we cancel that assumption. however,
the literal representation of the story will not contain the fact that the hamburger
was not eaten. if the question is then posed why did john not eat the hamburger?
then failing to provide any answer is not a good response, and not one a human
would make. the appropriate response is that it was burnt. to make this response
may require recreating expectations at answer retrieval time and determining what
in the text violated them. the key point is that comprehension, as tested by this
kind of question, is a dynamic process that involves integrating world knowledge
and the information literally conveyed in the text.

further work has gone on in story comprehension, but much of it within the psy-
chology community (e.g. see kintsch (1998)) and work on developing computational
models of story understanding dwindled through the 1980s and 1990s. however,
there has been a recent revival of interest in the area, following the creation of a
reading comprehension evaluation task (hirschman et al., 1999). arguably, the area
was held back because there was no agreed way to evaluate systems and evaluation
has assumed a much more central role in the methodology of natural language
research over the past 15 years. but reading comprehension tests o(cid:11)er a solution to
this problem, a solution which has the additional merit of being inexpensive, in that
test materials are already available for humans and do not require special e(cid:11)orts to
produce them.

the story comprehension work shares with current open text collection qa the
characteristic that answers to questions must be derived from unstructured texts.
however, like natural language front ends to databases and advisory systems, the

natural language id53: the view from here

283

questions asked to a story comprehension qa system may follow on from each other
in a dialogue-like way, and may involve anaphora or ellipsis between questions
(who was us president in 1958? in 1960? ... which party did he lead?). further,
unlike open text collection qa, the text containing the answer is known in advance.
multiple questions about a single text force a deeper processing of that text, and
the problems of noise introduced by similar but irrelevant texts are avoided, as are
the computational issues surrounding the processing of massive numbers of texts.
however, story comprehension tests tend to provide less answer redundancy, which
increases the di(cid:14)culty of the answer location task, as discussed in the light, mann,
rilo(cid:11) and breck paper.

3.4 information retrieval, information extraction and id53

information retrieval (ir), which, following convention, we take to be the retrieval
of relevant documents in response to a user query, has been an active research
area since the mid-1950s (sparck jones and willett, 1997). it is related to question
answering in the sense that users form queries because they wish to (cid:12)nd answers
to questions. however, beyond this the similarity largely ends. ir systems return
documents, not answers, and users are left to extract answers from the documents
themselves. furthermore, the queries users put to ir systems need not be formed as
syntactically correct interrogatives, and in fact may su(cid:11)er for being so. and, subtle
syntactic di(cid:11)erences, as, for example, between the questions who killed lee harvey
oswald? and who did lee harvey oswald kill?, are completely lost on most ir
systems, which simply reduce a query to a bag of stemmed open class words.

ir is, however, relevant to id53 for two reasons. first, ir techniques
have been extended to return not just relevant documents, but relevant passages
within documents. the size of these passages can be steadily reduced, at least in
theory, so that in the limiting case, what is extracted is, e(cid:11)ectively, just the answer
to a question. thus, id53 can be thought of as passage retrieval in
the limit. second, the ir community has, over the years, developed an extremely
thorough methodology for evaluation, the most well-known current exemplars of
which are the annual text retrieval conferences, or trecs, run by the us
national institute of standards and technology. it is from this methodology and
community that the recent id53 evaluation developed, which in turn
has stimulated much of the current interest in id53 (voorhees, this
issue).

the other strand of research that has fed into the current trec question
answering track is information extraction (ie) or, as it was initially known, message
understanding. ie can be de(cid:12)ned as the activity of (cid:12)lling prede(cid:12)ned templates from
natural language texts, where the templates are designed to capture information
about key role players in stereotypical events (gaizauskas and wilks, 1998). for
example, a template is easily de(cid:12)ned to capture information about corporate take-
over events; such a template has slots for the acquiring company, the acquired
company, the date of the acquisition, the amount paid, etc. running an ie system
designed to (cid:12)ll this template over large volumes of text results in a structured

284

l. hirschman and r. gaizauskas

database of information about corporate take-overs. this database can then be used
for other purposes, e.g. database queries, data mining, summarisation. in the current
context, ie templates can be viewed as expressing a question and a (cid:12)lled template
as containing an answer. thus, ie may be viewed as a limited form of question
answering in which the questions (templates) are static and the data from which the
questions are to be answered are an arbitrarily large dynamic collection of texts.

the ie community devised its own evaluation exercise { the message understand-
ing conferences, or mucs6 { which ran between 1987 and 1998. the termination
of the muc exercises, coupled with the desire to continue to push language under-
standing technology in novel directions via open evaluation exercises, were enabling
conditions for the current trec id53 evaluation.

3.5 the logic of questions and answers

the preceding sections have presented a sketchy survey of work on automated
id53. there is also a body of work outside computer science on
questions and answers, some of which is of relevance and has in   uenced work on
automated id53.

in 1955 m. l and a. n. prior (1955) introduced the term    erotetic logic    to refer to
the study of questions as logical entities distinct from statements. while this study by
no means started with them (e.g. see hamblin (1967) for references to philosophical
work on questions in aristotle, medieval logic, and the 19th-century), their work is
an early expression of what became, over the next 20 years, a serious attempt to
apply formal logical techniques to the analysis of questions, i.e. to de(cid:12)ne a suitable
syntax and semantics for a formal language of questions and answers.

as with parallel e(cid:11)orts in the logic of assertion, the e(cid:11)orts of logicians working in
this area have not primarily been directed towards accounting for natural language
usage. rather, they have been concerned with providing a good formal notation and
set of conceptual distinctions for investigating questions and answers:

we hope thus to illuminate the question-answer situation in english in much the same way
as formal logic illuminates the id136 situation in english, in order to thereby contribute to
our understanding of the erotetic \deep structure" of natural language. (from the introduction
to belnap and steel (1976)).

a good review of work on the logic of questions and answers can be found
in harrah (1984). he covers in particular depth the work of belnap and steel
(1976) and of (cid:23)aqvist (1975). belnap and steel   s account provides a good example
of the general concerns of logicians working in this area. they begin by assuming a
clearly de(cid:12)ned assertoric language ((cid:12)rst order predicate calculus with functions and
identity). they then formally de(cid:12)ne the key basic notions of elementary questions {
either whether-questions or which-questions { and direct answer and argue that these
formal de(cid:12)nitions capture essential intuitions about basic questions and answers to

6 proceedings of the last muc are available on-line at: http://www.itl.nist.gov/iaui/-

894.02/related projects/muc/index.html.

natural language id53: the view from here

285

them. given this basis they go on to explore more complex forms of questions and
answers and notions of presupposition, e(cid:11)ectiveness and completeness. the result is
a very rich formal account which provides a set of analytical tools of considerable
potential utility for automated id53.

of course, logicians are not interested in how answers to questions are derived in
practice. there is a strand of work on computing answers to logical queries posed
against logic databases. this tradition starts with green   s work (green, 1969) on
using resolution theorem provers to capture the instance found in constructing a
proof of an existentially quanti(cid:12)ed formula, and leads on into logic programming
and deductive databases. however, this work captures a very small part of the
question-answer logic developed by logicians such as belnap and steel (1976). one
of the challenges facing researchers into natural language id53 is
how to bridge, or at least narrow, the gap between engineering experimentation and
theoretical understanding. both sides will bene(cid:12)t from work of the other.

4 overview of current approaches

the previous section has indicated the scope of work relevant to the general task of
automated id53. let us now look at the sorts of approaches which
are currently being employed to address this task.

as a framework for discussing actual systems, it is useful to have in mind a generic
architecture for the qa task. speci(cid:12)c systems can then be seen as instantiations of the
general architecture, with particular choices being made concerning representation
and processing for each component of the overall model.

figure 1 proposes such a general architecture for the qa task, conceived as that
of asking natural language questions to a system that has as its knowledge source
a large collection of natural language texts. not all qa systems will implement all
components in the model (in particular most current trec qa systems do not
utilise dialogue or user models); and, there may well be systems that implement
functionality not in the model, or which cannot be easily mapped into it. still,
having such a general model in mind is useful, and helps to guide and structure
discussion.

we brie   y describe each of the processing stages in the model, then return to each
stage in somewhat more detail, discussing issues to be faced when implementing that
stage and exemplifying choices by reference to actual systems { for the most part
systems developed to participate in the trec id53 track. however,
systems developed for, for example, the reading comprehension task can also be
described in these terms.

1. question analysis. the natural language question input by the user needs to
be analysed into whatever form or forms are needed by subsequent parts of
the system. the question may be interpreted in the context of an on-going
dialogue and in the light of a model which the system has of the user. the
user could be asked to clarify his or her question before proceeding.

286

l. hirschman and r. gaizauskas

user

question

response

clarification request

question
analysis

qrep1

candidate
document
selection

qrep2

candidate

documents

user
model

dialogue
context

response
generation

ranked
answers

candidate
document
analysis

analysed

documents

answer
extraction

document
collection

document
collection

preprocessing

preprocessed
documents

fig. 1. generic architecture for a id53 system

2. document collection preprocessing. assuming the system has access to a large
document collection as a knowledge resource for answering questions, this
collection may need to be processed before querying, in order to transform it
into a form which is appropriate for real-time id53.

3. candidate document selection. a subset of documents from the total document
collection (typically several orders of magnitude smaller) is selected, comprising
those documents deemed most likely to contain an answer to the question.

4. candidate document analysis. if the preprocessing stage has only super(cid:12)cially
analysed the documents in the document collection, then additional detailed
analysis of the candidates selected at the preceding stage may be carried out.
5. answer extraction. using the appropriate representation of the question and of
each candidate document, candidate answers are extracted from the documents
and ranked in terms of probable correctness.

6. response generation. a response is returned to the user. this may be a(cid:11)ected
by the dialogue context and user model, if present, and may in turn lead to
their being updated.

4.1 question analysis

the (cid:12)rst stage is question analysis. the input to this stage is, by assumption, a
natural language question, though this needs qualifying in several ways. first, there
may be constraints on the input language. for example, the user may be required
to use a subset of natural language, a    controlled language   , which is limited in
terms of vocabulary and syntax { most natural language front ends to databases

natural language id53: the view from here

287

(section 3.1) are limited in this way. it may even be that the user is constrained to
use a form-(cid:12)lling interface for expressing questions that signi(cid:12)cantly simpli(cid:12)es the
system   s task of interpreting the question, albeit limiting the expressivity available to
the user (see the buchholz and daelemans paper in this issue). secondly, in addition
to the explicit input of the question string there may be implicit input, in the form
of context, if the system supports an on-going dialogue (so, for instance, there may
be ellipsis or anaphora in the question which requires access to dialogue context to
be interpreted). other implicit input could be the system   s knowledge of the user
and his or her goals.

output from this stage is one or more representations of the question for use in
subsequent stages. for example, if the candidate document selection mechanism to
be used in the next stage is an ir system, then one question representation might
be a stemmed, weighted term vector for input to the search engine. however, this
representation is unlikely to be adequate to allow exact answer strings to be picked
out of the documents returned by the search engine. to do this, most systems resort
to more detailed analysis of the question which typically involves two steps:

1. identifying the semantic type of the entity sought by the question (a date, a

person, a company, and so on);

2. determining additional constraints on the answer entity by, for example:

(a) identifying key words in the question which will be used in matching

candidate answer-bearing sentences; or,

(b) identifying relations { syntactic or semantic { that ought to hold between
a candidate answer entity and other entities or events mentioned in the
question.

the (cid:12)rst step requires (cid:12)rst looking at the key question word { when seeks a
date or time; where a location; who a person. however, this is not enough, since
various english question words, such as which and what do not carry much semantic
typing information. the type of entity questions such as which company : : : ? or
what building : : : ? are seeking is also easy to determine. but for questions that
involve more syntactically complex constructions such as what was the beatles   
(cid:12)rst hit single? or how many (cid:12)rst class degrees in computer science were awarded
at cambridge last year? things become more di(cid:14)cult.

various systems have, therefore, built hierarchies of question types based on the
types of answer sought, and attempt to place the input question into the appropriate
category in the hierarchy. moldovan et al. (2000), for example, manually constructed
a question type hierarchy of about 25 types from the analysis of the trec-8 training
data. srihari and li (2000) base their question type hierarchy on an extension of the
muc named entity classes and use a shallow parser to identify the question type,
or what they call the asking point. hovy, gerber, hermjacob, junk and lin (2001)
constructed a qa typology of 47 categories based on an analysis of some 17,000    real   
questions, extending the analysis to look beyond the semantic type literally requested
so as to classify questions like who discovered america? as person, while classifying
questions such as who was christopher columbus? as why-famous. harabagiu,
moldovan, pas(cid:24)ca, mihalcea, surdeanu, bunescu, g^irji, rus and mor(cid:20)arescu (2001)

288

l. hirschman and r. gaizauskas

describe a manually crafted top-level answer type hierarchy which links into parts
of id138 to extend the set of possible answer types available to their system.

once the type of entity being sought has been identi(cid:12)ed, the remaining task of
question analysis is to identify additional constraints that entities matching the type
description must also meet. this process may be as simple as extracting keywords
from the rest of the question to be used in matching against candidate answer-
bearing sentences. this set of keywords may then be expanded, using synonyms
and/or morphological variants (srihari and li, 2000) or using full-blown query
expansion techniques by, for example, issuing a query based on the keywords against
an encyclopedia and using top ranked retrieved passages to expand the keyword set
(ittycheriah et al., 2001). or, the constraint identi(cid:12)cation process may involve parsing
the question with grammars of varying sophistication. harabagiu et al. (2001) use a
wide-coverage statistical parser which aims to produce full parses. the constituent
analysis of a question that it produces is transformed into a semantic representation
which captures dependencies between terms in the question. scott and gaizauskas
(2001) use a robust partial parser which aims to determine grammatical relations in
the question where it can (e.g. main verb plus logical subjects and objects). where
these relations link to the entity identi(cid:12)ed as the sought entity, they are passed on
as constraints to be taken into account during answer extraction.

4.2 document collection preprocessing

if questions are to be answered in real time against gigabytes, and soon, terabytes,
of text then o(cid:11)-line preprocessing of the text is necessary. so far most trec
qa systems appear to rely on conventional document indexing engines to do this.
however, there is certainly no need to limit preprocessing to this sort of term
indexing. even if the candidate document selection stage relies on a conventional
search engine to make its (cid:12)rst selection of documents, having prestored a more
extensive analysis of all texts in the text collection would render the candidate
document analysis stage of our generic model unnecessary. for example, if one   s
system relied on building a logical form meaning representation of a text before
attempting to extract answers from it, there is no reason in principle why this
processing cannot be done in advance for the whole text collection. one system that
adopts this approach is the extrans system (molla aliod et al., 1998) which derives
logical representations of the document collection in advance of any querying.
a system that does shallow linguistic processing of the document collection in
advance is the sri highlight information extraction system (milward and thomas,
2000). this system does tagging, id39 and chunking over large
document sets o(cid:11)-line and then stores the results as indexed constraints that can be
matched during real-time user interaction with the system. prager (2001) preprocesses
the document collection and annotates terms with one of 50 semantic tags, which are
indexed during the document indexing process in addition to the terms themselves.
katz (1997) extracts ternary relation expressions of the form <subject relation
object> from syntactic analyses of natural language sentences in web pages and

natural language id53: the view from here

289

builds an indexed database from them to support subsequent id53
against the web.

4.3 candidate answer document selection

as observed, most existing trec qa systems use some form of conventional ir
search engine to select an initial set of candidate answer-bearing documents from a
large text collection. choosing this approach to winnow down the overall collection
to a much smaller set of documents to be examined in detail is not the end of
the matter, however. first, one must decide whether one wants to use a boolean or
ranked answer search engine. despite the higher results of ranked answer engines in
standard ir evaluation, certain trec qa participants have argued that boolean
engines are more suitable for use in conjunction with a qa system (moldovan et
al., 2000). if a ranked answer engine is used, a decision must be made as to how
many retrieved documents will be used, i.e. how far down the ranking to consider; if
a boolean engine is used, the issue of restricting the number of returned documents
to examine still needs to be addressed. secondly, the search engine may allow
passage retrieval, and various parameters need setting here (passage length, passage
windowing interval). or, subsequent to retrieval, a topic-based text segmenter may
be used to identify coherent text segments shorter than a full document which
may then be re-ranked. see, for example, clarke, cormack, kisman and lynam
(2001) who present and evaluate an algorithm for passage selection speci(cid:12)cally for
id53 and hovy et al. (2001) for some experiments with how far down
a ranked segmentation list to proceed. prager (2001) investigates the question of
whether combining results from multiple search engines can improve performance,
and concludes that it can, at least to a limited extent.

4.4 candidate answer document analysis

once candidate answer-bearing documents or document passages/segments have
been selected, these text segments may then be further analysed. this will not be
necessary if the system has already fully preprocessed all documents (as discussed
above in section 4.2) or if it is not designed to perform any further analysis.

typically, however, systems now analyse the selected documents or document
portions using at the very least a named entity identi(cid:12)er, which recognises and
classi(cid:12)es multiword strings as names of companies, persons, locations, etc. the
classes of names which are identi(cid:12)ed tend minimally to be those de(cid:12)ned in the
message understanding conference named entity task7, but in many cases these
have been extended to include a variety of additional classes, such as products,
addresses and measures, or re(cid:12)ned to include subclasses, such as towns, cities,
provinces, and countries.

typical also at this stage are sentence splitting, part-of-speech tagging, and chunk
parsing (identifying noun groups, verb groups, some prepositional phrases, etc.).

7 the muc-7 named entity task de(cid:12)nition is available from: http://www.itl.nist.gov/-

iaui/894.02/related projects/muc/index.html.

290

l. hirschman and r. gaizauskas

ferret, grau, hurault-plantet, illouz and jacquemin (2001), for example, describe a
qa system which uses shallow syntactic analysis to identify multiword terms and
their variants in the selected documents and to reindex and re-rank the documents
before matching against the question representation. some systems go further and
do a fuller syntactic analysis followed by some sort of transduction of the derived
syntactic structure into a set of relational constraints expressed either in a logical
language or using relational labels between selected terms in the original sentence
(e.g. between chunk heads). so, as noted above in the discussion of question analysis,
harabagiu et al. (2001) employ a wide-coverage statistical parser trained on the
id32 to derive a dependency representation of sentences in the candidate
answer documents, and then map this dependency representation into a (cid:12)rst order
logical representation, as they have done with the question; other qa systems
that employ syntactic analysis to map candidate answer bearing documents into
a logical or quasi-logical form prior to answer extraction are described by molla
and hess (1998), scott and gaizauskas (2001), zajac (2001). hovy et al. (2001) also
use a parser trained on the id32 (see hermjacob (2001)), but in their
case, rather than deriving a syntactically-oriented phrase structure tree, and then
mapping this into a logical form representation, they instead derive a representation
of the sentence directly annotated with semantic role information; buchholz and
daelemans (this issue) describe a    grammatical relation (cid:12)nder   , again trained on the
id32, which adds relational labels such as subject and object between np
and vp chunks previously found by a chunker.

4.5 answer extraction

at this stage the representation of the question and the representation of the
candidate answer-bearing texts are matched against each other and a set of candidate
answers is produced, ranked according to likelihood of correctness.

typically, systems that have analysed the question into an expected answer type
plus, optionally, some set of additional constraints will also have analysed the
candidate documents, or document segments, at least as far as annotation with
semantic types drawn from the set of answer types. thus, the matching process
may require (cid:12)rst that a text unit from a candidate answer text (perhaps a sentence,
if sentence splitting has been carried out) contain a string whose semantic type
matches that of the expected answer. matching here can be type subsumption {
perhaps construed as hyponomy in a lexical resource such as id138 { and need
not be restricted to identity.

then, once a text unit containing an expected answer type has been found, other
constraints may be applied to the text unit. these constraints may be viewed as
absolute, so that failure to satisfy them rules out the candidate; or they may be
viewed as preferences, which can be used to assign a score to the candidate for use
in ranking the answer. considerable variation between systems exists in terms of
the types of constraints used at this stage, how id124 is carried out,
and how constraints are weighted.

for example, moldovan, harabagiu, pas(cid:24)ca, mihalcea, goodrum, g^irji and rus
(2000) follow this approach. once an expression of the correct answer type is found

natural language id53: the view from here

291

in a candidate answer-bearing paragraph, an answer window around the candidate
is established and various quantitative features such as word overlap between the
question and the answer window are used in a weighted numerical heuristic to
compute an overall score for the window. thus, for every candidate answer-bearing
paragraph which contains an expression of the correct answer type, a score is derived
for the answer-window containing the answer candidate and these scores are used
to compute an overall ranking for all answer candidates. harabagiu et al. (2001)
extends this approach by using a machine learning algorithm to optimise the weights
in the linear scoring function which combines the features characterising the answer
windows.

srihari and li (2000), reverse the order of this general procedure, (cid:12)rst applying
question constraints other than expected answer type to rank sentences in candidate
answer-bearing text segments and then using the expected answer type as a (cid:12)lter to
extract the appropriate portion (e.g. 50 bytes for trec) of the selected sentences.
to rank sentences they use features such as how many unique question keywords
are found in the sentence, the order of keywords in the sentence compared to their
order in the question, and the whether the key verb or a variant matches. ittycheriah,
franz, zhu and ratnaparkhi (2001) combine both expected answer type matching
and a variety of word-based comparison measures in a single scoring function which
they apply to three sentence windows which they move over candidate answer-
bearing documents. see light et al. (this issue) for a discussion of upper bounds on
word-based comparison approaches.

systems which derive richer document and question representations, i.e. logical
forms or text annotated with semantic or grammatical role information, can use the
additional constraints expressed in these representations to constrain the matching
process. for example, a system that can identify logical subjects and objects can
correctly identify jack ruby as the answer to the question who killed lee harvey
oswald? when presented with the sentences ruby killed oswald and oswald killed
kennedy { something that word overlap approaches have di(cid:14)culties with. however,
most systems which utilise such grammatical constraints have realised that for the
system to be robust the constraints must be treated as preferences only, and not
as mandatory. for while when they match they are likely to guarantee a correct
answer, to insist that they match is to demand too much { sacri(cid:12)cing too much recall
for precision. thus, systems that can use semantic or grammatical role information
typically fall through to less principled word overlap measures when their principled
constraints do not get instantiated. hovy et al. (2001), scott and gaizauskas (2001),
and buchholz and daelemans (this issue) are all examples of systems which exploit
richer id194s where possible, but have a fallback strategy when
the richer constraints are not applicable.

4.6 response generation

for the trec qa evaluations, the sole response that most systems generate is a
ranked list of the top (cid:12)ve answers, where each answer is a text string8 of up to n

8 answers may also be list of strings for the so-called    list    questions like name three waterfalls

over 100 meters introduced in trec-10.

292

l. hirschman and r. gaizauskas

bytes (where n = 50 or n = 250) which has been extracted from a text (or texts) in
the document collection.

this sort of response is likely to be inadequate in real applications for a variety of
reasons. first, n-byte extracts are unlikely to be grammatical, or make good reading.
minimally, they need links back to their source documents to provide linguistic
context (e.g. to resolve dangling anaphors). or they need to be rephrased so as
to make them comprehensible. secondly, users may want more or less evidence or
context for the answer. this will depend on the user and how much they trust
the system and how much they know already about the topic in question. thirdly,
answers may be more complex or extensive than users had anticipated, and the
system may need to make decisions to truncate its output, or to initiate a dialogue
with the user in order to decide how to proceed. see buchholz and daelemans
(this issue) for a discussion of complex and answers and a di(cid:11)erent approach to
presentation of search results.

5 resources and evaluation

this section addresses two critical issues for the development of id53
as a research area: resources and evaluation. these two issues are closely intertwined;
developers need resources into order to build systems and they need evaluation
methods (and training and test data) in order to evaluate the e(cid:11)ectiveness of their
systems. historically, the introduction of large-scale common evaluations, such as
trec, has created strong communities of interest and has accelerated research
progress. in this section, we look at some of the specialised resources needed for
id53. we then discuss evaluation, with a particular focus on methods
for automated evaluation.

5.1 resources

to create a id53 system, researchers need corpora of question-and-
answer sets. ideally, these sets would be naturally occurring questions, whose answers
are contained in (or derived from) some larger collection of documents.

kupiec (1993) used trivial pursuit questions as a source of question-answer pairs,
and an on-line encyclopedia as the    collection    in which to search for answers.
recently, the trec id53 track has been a valuable source of ques-
tion/answer pairs occurring in collections of news stories. for reading comprehen-
sion, there are now several corpora of short answer reading comprehension tests
available; see light, mann, rilo(cid:11) and breck, this volume, and also (hirschman et
al., 1999).

however, to use machine learning and statistical techniques e(cid:11)ectively, larger
corpora are needed. for example, questions have a syntax that is di(cid:11)erent from
assertions, and a large corpus of expository or narrative prose will typically contain
very few questions { so that the rules developed on the basis of general corpora will
not work well (without specialised tuning) for question analysis. accurate analysis
of questions thus requires a large corpus of questions and associated short answers

natural language id53: the view from here

293

to develop high performance components for id52, parsing and
question typing.

some recent work on question typing (associating questions with particular se-
mantic classes of answers) has explored    found    corpora. for example, mann (2001)
used two corpora of trivia questions with short answers.9 other researchers have
mined frequently asked questions (faqs) as sources of question-answer sets. these
are particularly useful in developing id53 systems in speci(cid:12)c domains,
to support on-line help systems or to partially automate help desks. in addition,
there are an increasing number of question-answer web sites, for example, sites that
provide tests for language learners, or news providers that host quizzes on cur-
rent events. additional work in mining or capturing such on-line collections would
accelerate progress in id53.

one obvious source of question-answer pairs is multiple choice questions. these
are widely used in standardised tests, such as reading comprehension tests and
testing of english as a foreign language (toefl) tests10. however, multiple
choice questions are not as natural as short answer questions, since they are primarily
designed for ease of grading. also, because standardised tests are expensive resources
to create, it can be di(cid:14)cult to obtain corpora of such materials for use in research
or evaluation.

5.2 questions

the above discussion on resources assumes that any collection of question-answer
pairs would be of interest; however, some question types are much more tractable
than others. research to date has focused mostly on the easier kinds of questions.
in the (cid:12)rst two trec evaluations, for example, questions were limited to simple
factual questions that had answers in the associated document collections. as a
result, the best strategy for the evaluation was always to produce a ranked list of
proposed correct answers, since no credit was given to the system for    knowing    that
it was not certain of the answer.

the next evaluation (trec-10, november 2001) will increase question complexity
in two dimensions: allowing questions that have no answers, and allowing questions
with    list    answers.

this additional complexity will require changes in answer evaluation and in
system construction. to handle questions that do not have answers in the underlying
collection (that is, where    no answer found    is a correct answer), systems will
need to measure their certainty about an answer.

questions requiring a list as an answer (e.g. list the countries bordering afghan-
istan) may need to cull the answers from multiple sentences or clauses in a single
document or may need to synthesize the answer from multiple documents. both of
these changes require extending the simple model beyond merely (cid:12)nding a sentence
or region of a document that best answers the question.

9 the speci(cid:12)c web sites were www.triviaspot.com and www.phishy.net/trivia.
10 see www.toefl.org for sample materials.

294

l. hirschman and r. gaizauskas

5.3 answer evaluation

the (cid:12)rst problem in evaluation is to decide on the criteria for judging an answer.
the following list captures some possible criteria for answer evaluation; see breck,
burger, ferro, hirschman, house, light and mani (2000) for a discussion of these
criteria:

tion.

(cid:15) relevance: the answer should be a response to the question.
(cid:15) correctness: the answer should be factually correct.
(cid:15) conciseness: the answer should not contain extraneous or irrelevant informa-
(cid:15) completeness: the answer should be complete, i.e. a partial answer should not
(cid:15) coherence: an answer should be coherent, so that the questioner can read it
(cid:15) justi(cid:12)cation: the answer should be supplied with su(cid:14)cient context to allow a

get full credit.

easily.

reader to determine why this was chosen as an answer to the question.

so far, evaluations have focused primarily on relevance, although the trec
id53 evaluation now requires that the answer be justi(cid:12)ed within the
document, and the byte limitation on answers goes a (cid:12)rst step towards addressing
the conciseness criterion. in some cases, optimizing along one criterion may reduce
   goodness    along another dimension { for example, answer justi(cid:12)cation may reduce
answer conciseness. therefore, the criteria of evaluation must be related to the
intended use, the intended users, and the interface.

once there is agreement on criteria for what constitutes a good answer, there need
to be repeatable evaluation procedures. the voorhees paper (this issue) describes
the trec process which uses human assessors to read and evaluate each answer.
experiments during trec-8 determined that consistency among the human asses-
sors was good enough to preserve the relative ranking of systems. as a result, it was
possible to have answers graded by only a single evaluator. this has signi(cid:12)cantly
decreased cost, but because a human is required, this method does not support
systematic iterative testing for hill climbing or machine learning.

it is always useful to provide human performance benchmarks for an evaluation
task. in this regard, reading comprehension tests are ideal, since they are designed
to evaluate people, e.g. children in school, or adult learners of a second language.
however, these tests also require human assessors for evaluation, unless multiple
choice tests are used.

5.4 automated evaluation techniques

there is ongoing research in automated evaluation methods. for grading short
answers, it is possible to automate comparison of system (or student) answers to
answer keys created by a human expert (breck et al., 2000; hirschman et al., 2000).
such comparisons, while not as accurate as those done by human assessors as in
trec, still provide reasonably good agreement (93{95%) with human assessors {
good enough for iterative training and machine learning.

natural language id53: the view from here

295

automated answer and essay grading is a topic of great signi(cid:12)cance to the ed-
ucational testing community. multiple choice tests are still widely used, despite
agreement that short answer tests and essays are better tests. this is because open
ended tests are felt to be too laborious or too subjective for use in large scale
standardized testing. however, recent work in automated essay grading (kukich,
2000; laudauer and laham, 2000) has demonstrated the feasibility of automated
evaluation for essay tests, sometimes in conjunction with a single human assessor.
these results provide the hope that automated answer grading systems could even-
tually approximate human graders. if we were able to construct systems that could
evaluate or grade answers with results consistent with human performance, new
possibilities open up: such systems could complement teachers in the class room
by grading student exercises or allowing students to do self paced learning. and
if a system could both answer questions accurately and evaluate the correctness
of answers, it could even teach people, or at least provide a    learning companion   
(goodman et al., 1998).

6 future directions

despite 40 years of activity, we are just beginning to explore id53
as a research area. the attraction of the id53 challenge is that it
is both tractable (witness the impressive performance evaluation results from the
trec id53 evaluation) and highly relevant: even the limited solutions
developed to date provide signi(cid:12)cant value added over coarse-grained document
retrieval. it is also stimulating cross-fertilisation of ideas between researchers in
natural language processing, information retrieval and arti(cid:12)cial intelligence.

recent research in this area has been focused primarily on the trec question
answering evaluation, although to a lesser extent on other tasks, such as reading
comprehension. open    common    evaluations, such as trec, are enormous drivers
of progress. they bring a rigorous empirical approach to research, providing a
challenge task, experimentation, empirical validation and comparison of results, and
replication.

however, formal evaluations are always an abstraction of the real problems. they
are primarily designed for the ease and replicability of evaluation. the current eval-
uations are only the (cid:12)rst stages of an ambitious plan to evaluate many dimensions
of id53.11 it is important to review the larger agenda of question
answering, which goes far beyond our current ability to build or evaluate such sys-
tems. as a research agenda, id53 poses long-term research challenges
in many critical areas of natural language processing:

(cid:15) applications: section 3 provided an overview of the use of id53
to access structured information (in databases) as well as free text. other

11 see the id53 roadmap document (burger et al., 2001) for incremen-
the id53 task. there is now also a signi(cid:12)cant re-
(aquaint) starting in the us focused on id53; see

tal expansion of
search e(cid:11)ort
www.ic-arda.org/solicitations/aquaint/.

296

l. hirschman and r. gaizauskas

applications include automated help, web content access (by both speech and
text), front-ends to knowledge sources such as on-line encyclopedias or to bib-
liographic resources (e.g. to medline for biomedical literature). important
applications also exist in the educational world for language teaching and for
companion learning systems. automated evaluation techniques will support
short answer grading, as well as automated essay grading. future question
answering systems should be able to access content in multiple languages and
across multiple media as well.
(cid:15) users: current id53 systems provide answers to isolated factual
questions. real users want real-time interactive question and answer capabil-
ities, with coherent succinct answers presented in context for easy inspection.
power users will need systems that have constantly updated user models, so
that the system presents only novel information. other sets of users will want
digests or background summaries of information, or information organized
by time or by location. users new to a domain { learners { may need in-
correct factual or conceptual presuppositions of their questions identi(cid:12)ed and
corrected; and they need responses tailored to their level of comprehension.
once id53 front-ends become a part of the user   s environment,
users will want support for collaborative id53, to allow teams of
people to research questions, share information and integrate partial answers.
this will require progress in areas such as real-time architectures for question
answering, user modeling, and collaborative work environments.
(cid:15) question types: question types will move from the factual to more complex
forms of question, including lists, summarization of contradictory information,
and explanations, including answers to how or why questions, and eventually,
what if questions. systems will have to recognize paraphrases of the same
underlying question, including cross-language id53, which would
allow the user to ask a question in their native language, access information
from documents in multiple languages and receive an answer in their native
language. this will require progress in cross language retrieval and machine
translation.
(cid:15) answer types: one of the greatest challenges may be in presenting appropri-
ate answers. answers must be correct, succinct, coherent and justi(cid:12)ed (either
within the answer or as a pointer to the source(s) of information). systems
will need to handle cases when multiple answers are found, when no answer
is found, or when contradictory answers are found. systems will have to go
beyond extraction of snippets of text to provide answer synthesis across sen-
tences and across documents. providing appropriate coherent answers will be
a major research area and will depend heavily on progress in text summariza-
tion. id53 systems could also go beyond text-based sources, to
include other media: spoken language, imagery, data from structured sources,
including databases and knowledge bases. and the ideal id53
system would be able to retrieve and merge these answers into the appropriate
(multimedia) form for the end user.
(cid:15) evaluation: with the introduction of each new set of features, the evaluation

natural language id53: the view from here

297

paradigm will have to be adapted to evaluate the enhanced capabilities of
the systems. there are two particularly important challenges here. first, better
automated evaluation is needed to support machine learning and statistical
methods. if successful, automated evaluation methods will enable new applica-
tions in the education and training (cid:12)elds. secondly, a user-centered evaluation
method needs to be developed, so that user concerns (speed of response, answer
display, support for interactivity and collaboration, usability of answer) can
be evaluated. without such user-centered evaluations, an important dimension
of research will be neglected.
(cid:15) presentation: ultimately, id53 is about providing information to
users. the success in meeting users    needs will provide the market    pull    that
drives this area forward. it is important to understand what users need and
develop user-centered evaluations to drive this work. further research is needed
that will draw heavily on work in interactive retrieval, answer presentation and
summarization, conversational interfaces and human-computer interaction in
general.

from this list, we see that the long-term view of id53 intersects
with many areas of natural language processing, id99, human-
computer interaction, multimedia processing, collaborative systems, and intelligent
tutoring systems. this issue represents a snapshot of this area at an early stage in
this ambitious research agenda:    the view from here   .

acknowledgements

the authors would like to thank the following for their invaluable help in reviewing
papers for this special
issue: giuseppe attardi, eric breck, ted briscoe, john
carroll, eugene charniak, robert dale, brigitte grau, sanda harabagiu, ed hovy,
christian jacquemin, guy lapalme, marc light, michael littman, paul martin,
gideon mann, hwee tou ng, john prager, ellen rilo(cid:11), tomas strzalkowski, john
tait, yorick wilks.

references

belnap, n. d. and steel, t. b. (1976) the logic of questions and answers. yale university

press, new haven and london.

bobrow, d., kaplan, r., kay, m., norman, d., thompson, h. and winograd, t. (1977) gus,

a frame driven dialog system. arti(cid:12)cial intelligence, 8: 155{173.

breck, e., burger, j., ferro, l., hirschman, l., house, d., light, m. and mani, i. (2000) how
to evaluate your id53 system every day : : : and still get real work done.
proceedings 2nd international conference on language resources and evaluation (lrec-
2000), pp. 1495{1500.

burger, j., cardie, c., chaudhri, v., gaizauskas, r., harabagiu, s., israel, d., jacquemin, c.,
lin, c.-y., maiorano, s., miller, g., moldovan, d., ogden, b., prager, j., rilo(cid:11), e., singhal,
a., shrihari, r., strzalkowski, t., voorhees, e. and weischedel, r. (2001) issues, tasks and
program structures to roadmap research in question & answering (q&a). available at:
http://www-nlpir.nist.gov/projects/duc/roadmapping.html.

298

l. hirschman and r. gaizauskas

clarke, c., cormack, g., kisman, d. and lynam, t.

(2001) question answer-
ing by passage selection (multitext experiments for trec-9. proceedings 9th text
retrieval conference (trec-9). nist special publication 500-249. available at:
http://trec.nist.gov/pubs.html.

copestake, a. and sparck jones, k. (1990) id139 to databases. the

knowledge engineering review, 5(4): 225{249.

ferret, o., grau, b., hurault-plantet, m., illouz, g. and jacquemin, c. (2001) terminological
variants for document selection and question/answer matching. proceedings association
for computational linguistics workshop on open-domain id53, pp. 46{53.

gaizauskas, r. and wilks, y. (1998) information extraction: beyond document retrieval.

journal of documentation, 54(1): 70{105.

goodman, b., soller, a., linton, f. and gaimari, r. (1998) encouraging student re   ection
and articulation using a learning companion. international journal of arti(cid:12)cial intelligence
in education, 9: 237{255.

green, b. f., wolf, a. k., chomsky, c. and laughery, k. (1961) baseball: an automatic

question answerer. proceedings western joint computer conference 19, pp. 219{224.

green, c. (1969) theorem proving by resolution as a basis for question-answering systems.

machine intelligence, 4: 183{205.

grosz, b. j., sparck jones, k. and webber, b. l., editors. (1986) readings in natural language

processing. morgan kaufmann, los altos, ca.

hamblin, c. (1967) questions. in: edwards, p., editor, the encyclopedia of philosophy, pp.

49{53. macmillan, new york.

harabagiu, s., moldovan, d., pas(cid:24)ca, m., mihalcea, r., surdeanu, m., bunescu, r., g^irji,
r., rus, v. and mor(cid:20)arescu, p. (2001) falcon: boosting knowledge for answer engines.
proceedings 9th text retrieval conference (trec-9). nist special publication 500-249.
(available at: http://trec.nist.gov/pubs.html.)

harrah, d. (1984) the logic of questions. in: gabbay, d. and guenthner, f., editors, handbook

of philosophical logic, vol. ii, pp. 715{764. reidel.

hermjacob, u. (2001) parsing and question classi(cid:12)cation for id53. proceedings
association for computational linguistics workshop on open-domain id53,
pp. 17{22.

hirschman, l., light, m., breck, e. and burger, j. (1999) deep read: a reading com-
prehension system. proceedings 37th annual meeting of the association for computational
linguistics, pp. 325{332.

hirschman, l., breck, e., light, m., burger, j. and ferro, l. (2000) automated grading
of short answer tests. institute for electrical and electronic engineers (ieee) intelligent
systems, 15(5): 31{34.

hovy, e., gerber, l., hermjacob, u., junk, m. and lin, c.-y. (2001) question answer-
ing in webclopedia. proceedings 9th text retrieval conference (trec-9). nist special
publication 500-249. (available at: http://trec.nist.gov/pubs.html.)

ittycheriah, a., franz, m., zhu, w.-j. and ratnaparkhi, a. (2001) ibm   s statistical question
answering system. proceedings 9th text retrieval conference (trec-9). nist special
publication 500-249. (available at: http://trec.nist.gov/pubs.html.)

katz, b. (1997) from sentence processing to information access on the world wide web.
proceedings american association for arti(cid:12)cial intelligence (aaai) spring symposium on
natural language processing for the world wide web, stanford university, stanford ca.
(available at: http://www.ai.mit.edu/people/boris/webaccess/.)

kintsch, w. (1998) comprehension: a paradigm for cognition. cambridge university press,

cambridge.

kukich, k. (2000) beyond id43. institute for electrical and electronic

engineers (ieee) intelligent systems, 15(5): 22{27.

kupiec, j. (1993) murax: a robust linguistic approach for id53 using an on-
line encyclopedia. proceedings assocition for computing machinery special interest group
on information retrieval (acm-sigir)    93, pp. 181{190. pittsburgh, pa.

natural language id53: the view from here

299

laudauer, t. and laham, d. (2000) the intelligent essay assessor. institute for electrical and

electronic engineers (ieee) intelligent systems, 15(5): 27{31.

lehnert, w. (1977) a conceptual theory of id53. proceedings 5th international

joint conference on arti(cid:12)cial intelligence, pp. 158{164.

light, m., brill, e., charniak, e., harper, m., rilo(cid:11), e. and voorhees, e., editors. (2000)
proceedings workshop on reading comprehension tests as evaluation for computer-based
language understanding systems, seattle. association for computational linguistics.

mani, i., firmin, t., house, d., klein, g., sundheim, b. and hirschman, l. (to appear)
the tipster summac text summarization evaluation. journal of natural language
engineering.

mann, g. s. (2001) a statistical method for short answer extraction. proceedings association

for computational linguistics workshop on open-domain id53, pp. 23{30.

milward, d. and thomas, j.

to information ex-
traction. proceedings assocition for computational linguistics (acl) workshop on re-
cent advances in natural language processing and information retrieval. (available at:
http://www.cam.sri.com/html/highlight.html.)

(2000) from information retrieval

moldovan, d., harabagiu, s., pas(cid:24)ca, m., mihalcea, r., goodrum, r., g^irji, r. and
for sur(cid:12)ng the answer net. proceedings 8th text
(available at:

rus, v.
retrieval conference (trec-8). nist special publication 500-246.
http://trec.nist.gov/pubs.html.)

(2000) lasso: a tool

molla aliod, d., berri, j. and hess, m. (1998) a real world implementation of answer extrac-
tion. proceedings 9th international conference on database and id109 applications
workshop    natural language and information systems    (nlis   98), pp. 143{148.

prager, j.

(2001) one search engine or two for id53. proceedings 9th
text retrieval conference (trec-9). nist special publication 500-249. (available at:
http://trec.nist.gov/pubs.html.)

prior, m. l. and prior, a. n. (1955) erotetic logic. the philosophical review, 64(1): 43{59.
(cid:23)aqvist, l. (1975) a new approach to the logical theory of interrogatives. tbl verlag gunter

narr, t  ubingen.

schank, r. c. and abelson, r. p. (1977) scripts, plans, goals, and understanding. lawrence

erlbaum, hillsdale, nj.

scott, s. and gaizauskas, r. (2001) university of she(cid:14)eld trec-9 q & a system. proceedings
9th text retrieval conference (trec-9). nist special publication 500-249. (available at:
http://trec.nist.gov/pubs.html.)

simmons, r. f. (1965) answering english questions by computer: a survey. communications

association for computing machinery (acm), 8(1): 53{70.

sparck jones, k. and willett, p., editors. (1997) readings in information retrieval. morgan

kaufmann, san francisco, ca.

srihari, r. and li, w. (2000) information extraction supported id53. proceedings
8th text retrieval conference (trec-8). nist special publication 500-246. (available at:
http://trec.nist.gov/pubs.html.)

turing, a. (1950) computing machinery and intelligence. mind, 59(236): 433{460.
voorhees, e. m. and harman, d. k.,

editors.

trieval conference (trec-8). nist special publication 500-246.
http://trec.nist.gov/pubs.html.)

(2000) proceedings 8th text re-
(available at:

webber, b. l. (1986) questions, answers and responses: interacting with knowledge-base
systems. in: brodie, m. l. and mylopoulos, j., editors, on knowledge base management
systems: integrating arti(cid:12)cial intelligence and database technologies, pp. 365{402. springer-
verlag.

winograd, t. (1972) understanding natural language. academic press, new york.
woods, w. (1973) progress in natural language understanding { an application to lunar
geology. american federation of information processing societies (afips) conference pro-
ceedings, 42: 441{450.

300

l. hirschman and r. gaizauskas

zajac, r. (2001) towards ontological id53. proceedings association for com-

putational linguistics workshop on open-domain id53, pp. 31{37.

zue, y., sene(cid:11), s., glass, j., polifroni, j., pao, c., hazen, t. j. and heatherington, l. (2000)
jupiter: a telephone-based conversational interface for weather information. institute
for electrical and electronic engineers (ieee) trans. speech and audio processing, 8(1):
100{112.

