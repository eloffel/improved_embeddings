foundations and trends r(cid:1) in
information retrieval
vol. 1, no. 2 (2006) 91   231
c(cid:1) 2007 j. prager
doi: 10.1561/1500000001

open-domain question   answering

john prager

ibm t.j. watson research center, 1s-d56, p.o. box 704,
yorktown heights, ny 10598, usa, jprager@us.ibm.com

abstract

the top-performing question   answering (qa) systems have been of
two types: consistent, solid, well-established and multi-faceted systems
that do well year after year, and ones that come out of nowhere employ-
ing totally innovative approaches and which out-perform almost every-
body else. this article examines both types of system in depth. we
establish what a    typical    qa-system looks like, and cover the com-
monly used approaches by the component modules. understanding this
will enable any pro   cient system developer to build his own qa-system.
fortunately there are many components available for free from their
developers to make this a reasonable expectation for a graduate-level
project. we also look at particular systems that have performed well
and which employ interesting and innovative approaches.

1

introduction

question   answering (qa) is a research activity which is di   cult to
de   ne precisely, but most practitioners know what it is when they see
it. loosely speaking, it is the    eld of study concerning the development
of automatic systems to generate answers to questions in natural lan-
guage. the source of the answers and the manner of generation are left
open, as are the kinds of questions. however, as a    rst approximation,
the    eld is currently mostly concerned with answering factual questions
(questions about agreed, or at least authoritatively reported facts) by
consulting one or more corpora of textual material.

this is not to say that such questions are exclusively about simple
properties of objects and events (the height of mt everest, the birth-
date of mozart, and so on). the    eld is also interested in de   nitions
(   nding important unspeci   ed characteristics of an entity); relation-
ships (how entities are interrelated); and even opinions (how people or
organizations have reacted to events). what is common between these
is that qa systems are currently extractive: they just report informa-
tion that is found in external resources such as newswire, without any
attempt to prove the authors correct, and also without any attempt to
construct answers that are only implicit in the sources.

92

93

the kind of qa that will be the subject of this article for the
most part will be that which is the subject of the annual trec (text
retrieval conference) evaluation at nist [76] beginning in 1999. the
majority of the qa systems that have been developed, both in academia
and industrial research labs, have been at least partly for participation
at trec, and the majority of technical papers on the subject have
used trec corpora, question sets and metrics for evaluation, so such
a focus is only natural. however, we will also address aspects of qa that
trec has avoided so far, and we will examine some of the de   ciencies
of the trec-style approach.

qa draws upon and informs many of the sub   elds of information
retrieval (ir) and natural language processing (nlp), but is quite
di   erent from them in certain ways. qa is a very practical activity    
more an engineering    eld than a science     and as such, at least today,
is more a collection of tools and techniques than formulas and theorems.
this is very understandable when one considers that at its heart, qa is
concerned with matching a natural language question with a snippet of
text (or in general, several snippets), an algorithmic solution to which
could be said to be nlp-complete.1

qa is heavily reliant on processes such as id39
(ner), parsing, search, indexing, classi   cation and various algorithms
from machine learning, but as we will see it seems to be surprisingly
insensitive to the particular choices made. in the author   s experience,
choosing to use a state-of-the-art component over a less advanced ver-
sion does not usually make much di   erence in the qa-system   s overall
performance. what makes a di   erence is how the components are orga-
nized relative to each other; in other words, it is what the system is
trying to do that is typically more important than how it does it. this
leads to the fascinating situation where contributions usually come from
the introduction of brand-new approaches, rather than the    ne-tuning
of parameters in, say, ranking algorithms. the top-performing systems
in trec have been of two types: consistent, solid, well-established, and
multi-faceted systems that do well year after year, and ones that come

1 a play on the notion of np-completeness from the    eld of computational complexity, and
ai-completeness, the less formal but still widely held belief that solution to any hard
arti   cial intelligence (the parent    eld of nlp) problem leads to the solution of any other.

94 introduction

out of nowhere employing totally innovative approaches and which out-
perform almost everybody else.

this article will examine both types of system in depth. we will
establish what a    typical    qa-system looks like, and cover the com-
monly used approaches by the component modules. understanding this
will enable any pro   cient system developer to build his own qa-system.
fortunately, there are many components available for free from their
developers to make this a reasonable expectation for a graduate-level
project. we will also look at particular systems that have performed
well and which employ interesting and innovative approaches, but we
will not examine every single system that has acquitted itself well.
we will not cover commercial systems that have not been forthcoming
about their internal workings.

1.1 a brief history of qa

the    eld of qa, as it is currently conceived, was inaugurated in 1999
when nist introduced a question   answering track for trec-8. how-
ever, it was not for this that the    rst question   answering systems were
developed. for those, we need to go back to the 60s and 70s and the
heyday of arti   cial intelligence facilities such as mit   s ai lab. in
those days and in those locations almost all programming was done in
lisp and prolog and derived languages such as planner and micro-
planner. these were the test-beds of pioneering ai systems, many of
which could now with hindsight be called qa systems, although they
were not at the time (see, e.g., shrdlu [80]).

for the most part, these systems were natural-language interfaces
to databases. a question, problem or action to be taken was input in
english. this was parsed into a semantic form     a semantic represen-
tation of the    meaning    of the information need. then either directly
or through a theorem-proving or other inferencing system, goals were
generated which could be directly translated into database queries or
robot commands. the repertoires, both in terms of actions taken or
inputs understood, were severely limited, and were either never used in
a practical system, or were only usable for the very narrow application

1.1 a brief history of qa 95

for which they were designed. such systems included lifer/ladder
[23], lunar [81], and chat-80 [79].

what these systems had in common was that they were toy sys-
tems. they were brittle, and did not scale. they used very complex
approaches (inferencing, subgoals, etc.) and did not degrade grace-
fully [41]. they su   ered from lack of general-purpose community
resources, which made them expensive to develop or extend. what
is ultimately damning is that there was no easily identi   able line of
evolution from those systems to the present day; they died out like
dinosaurs.

possibly the    rst system that can be recognized as what some might
call a modern qa system, in the sense that it was open-domain and
used unrestricted free text, was the murax system [30]. it processed
natural-language questions seeking noun-phrase answers from an on-
line encyclopedia; it used shallow linguistic processing and ir, but did
not use inferencing or other knowledge-based techniques.

the next milestone came shortly after the creation of the world
wide web: mit   s start system [27, 28] was the    rst web question   
answering system. this work has progressed to the present day, and
is still available.2 ask jeeves3 (now ask.com), founded in 1996, was
maybe the    rst widely-known web qa system, although since it returns
documents, not answers, one can debate if it is a true qa system.4
since that time, other qa systems have come online, both academic
and commercial; these include brainboost5 and answerbus,6 both of
which return single sentences. there is a strong argument that even
if a number or a noun-phrase, say, is the technically correct answer
to a question, a sentence attesting to the subject fact is even better.
especially when typical system   s accuracy is far from 100%, as much
transparency is desired as possible.

in 1992 nist, the u.s. national institute of standards and tech-
nology, inaugurated the annual text retrieval conference, commonly

2 http://www.start.csail.mit.edu.
3 http://www.ask.com.
4 although if the answer is embedded in the document abstract presented in the hit-list,
the end-user typically would not care.
5 http://www.brainboost.com.
6 http://www.answerbus.com/index.shtml.

96 introduction

called trec. every year, trec consists of a number of tracks (the
exact composition usually changes a little from year to year), each one
concerned with a di   erent aspect of ir. in each track, one or more
evaluations are run in which teams from around the world participate.
trec is generally now considered a hugely important factor in ir
research, since it provides relevance judgments7 and allows researchers
to compare methodologies and algorithms on a common testbed. many
more details about trec can be found on its o   cial website8 or in a
recently-published book from nist [76].

in 1999, nist added a qa track to trec. there was a feeling that
the question   answering problem could bene   t from bringing together
the nlp and ir communities. nlp techniques were, and still are, much
more precise than ir techniques, but considerably more computation-
ally expensive; nlp was typically used in closed-world domains, ir
typically in open-domain. thus using the power of ir to search many
megabytes or gigabytes of text in short times, together with the re   ne-
ment of nlp techniques to pinpoint an answer was expected to be
a worthwhile technological challenge. the track has continued to this
day, although it has changed in ways discussed later.

to emphasize the world-wide interest in qa that has arisen in
recent years, we will mention here some other venues for qa. in 2001,
ntcir,9 a series of evaluation workshops to promote research in ir and
nlp activities in asian languages, introduced a question   answering
task. in 2003, the cross-language evaluation forum (clef),10 an
european trec-like context for cross-language ir,
inaugurated a
multiple-language question   answering track. both of these are on-
going. recent workshops include: open-domain qa (acl 2001),
qa: strategy and resources (lrec 2002), workshop on multilin-
gual summarization and qa (coling 2002), information retrieval
for qa (sigir 2004), pragmatics of qa (htl/naacl 2004), qa
in restricted domains (acl 2004), qa in restricted domains (aaai
2005), id136 for textual qa (aaai 2005), multilingual qa (eacl

7 in some cases provided by nist assessors, in others by the research community.
8 http://trec.nist.gov/.
9 http://research.nii.ac.jp/ntcir/outline/prop-en.html.
10 http://www.clef-campaign.org/.

1.1 a brief history of qa 97

2006), interactive qa (hlt/naacl 2006) and task-focused summa-
rization and qa (coling/acl 2006).

1.1.1 trec minutiae

for those interested, table 3.2 in section 3.6 lists the teams/systems
that have placed in the top 10 in the main qa task since the trec8
qa in 1999. however, in the remainder of this article we will not for
the most part be reporting performance scores of teams since over time
these wane in signi   cance, and besides they can be discovered in the
teams    own writings and in the annual trec proceedings. we will
report, where known and of interest, how di   erent components con-
tribute to teams    overall scores.

as discussed in [53], the di   culty of questions cannot be assessed
independently of knowing the corpus or other resource in which the
answers must be found. up to the writing of this article, trec has
favored newswire text, which has the characteristics of being written by
educated english-speaking adults and of being edited, so there is a min-
imum of typological errors (as compared with mail and web documents,
and especially compared with ocr (id42) or
asr (automatic id103) documents). details of the trec
datasets are given in section 3.6.1.

1.1.2 aquaint

in 2001, the u.s. government, through a small agency called arda
(advanced research development activity) began a three-phase multi-
year activity called aquaint (advanced question   answering for
intelligence). in each phase, a couple of dozen (approximately) u.s.
teams, from academia primarily but also industry, were funded to per-
form research and development with the ultimate goal of producing
qa systems that could be used e   ectively by intelligence analysts in
their daily work. one of the goals of the program was to push research
away from factoid qa (see section 2.1.1) into questions about reasons,
plans, motivations, intentions, and other less tangible quantities. these
are very di   cult objectives and it remains to be seen to what extent
they can be achieved in the near future. one direct consequence of

98 introduction

this thrust, though, has been to support and in   uence the qa-track in
trec.

in particular, the aquaint program organized several pilot stud-
ies in di   erent dimensions of qa. the de   nition pilot explored a di   er-
ent approach to de   nition questions        nding a comprehensive set of
descriptive text fragments (called nuggets) rather than a single phrase
as required for factoid questions. when in 2003 trec started using
question series     groups of questions about a single target     the last
question in every series was    other,    meaning    return everything impor-
tant that has not been asked about yet   ; this was a direct outgrowth
of the de   nition pilot.

the relationship pilot, where a relationship was de   ned as one of
eight broad ways in which one entity could in   uence another (organiza-
tional, familial,    nancial etc.) became a subtask of [75, 76]. there have
also been opinion and knowledge-based question pilots, which have
prompted research reported elsewhere in the literature. more informa-
tion about these pilots can be found on the nist web site.11

1.2 article plan

this article is designed to give readers a good background in the    eld of
question   answering. the goal in writing it has been to cover the basic
principles of qa along with a selection of systems that have exhibited
interesting and signi   cant techniques, so it serves more as a tutorial
than as an exhaustive survey of the    eld. we will not cover (except
for occasional mentions in passing) opinion or relationship questions,
interactive qa or cross-language qa. further reading can be found in
two recent books [42, 72], as well as the proceedings of the qa tracks of
trec. finally, we should mention the review article by hirschman and
gaizauskas [25], which summarizes the state of question   answering as
of 2001. as of this writing, there is no web-site or umbrella publication
from aquaint.

the rest of this article is organized as follows. in chapter 2,
the theory and practice of

we provide a general overview of

11 http://trec.nist.gov/data/qa/add qaresources.html.

1.2 article plan

99

question   answering (mostly practice). we look at the typical archi-
tecture of a qa system, the typical components that comprise it, the
technical issues they tackle and some of the most common and suc-
cessful techniques used to address these problems. in chapter 3, we
look at the di   erent ways qa systems are evaluated. in chapter 4, we
look at some of the speci   c approaches that have been used by well-
performing systems. we will note that three themes seem to permeate
these approaches: testing of identity, use of analogy, and detection of
redundancy. because these are very high-level concepts, each of which
can be achieved in a number of di   erent ways, it should be of no sur-
prise that di   erent methodologies, namely linguistic, statistical, and
knowledge-based, are all found in qa systems. in chapter 5, we step
back and look at the more abstract concepts of user modeling and
question complexity; the issues here have not to date been tackled
seriously by the community, but it is asserted here that they are of
signi   cant importance, and dealing with them will be necessary for
future success. we conclude in chapter 6 with some comments about
challenges for qa.

2

overview of question   answering

in this chapter, we provide a fairly comprehensive analysis of the qa
problem and typical qa system architectures and components. we
start with a description of the di   erent question types, as commonly
used in the qa community. we then present a brief overview of the
basic building blocks of qa system architecture, to ground further dis-
cussions, and in section 2.3, we discuss terminology. in section 2.4, we
discuss the major components and processes in a qa system in some
detail. in section 2.5, we look at some of the general issues that cut
across the individual components.

2.1 classes of questions

everyone knows what a question is, but there is no formal de   nition
of what an appropriate question for a qa system is. clearly there is a
di   erence between

   who is the president of china?   

and

   what is 2 plus 2?   

100

2.1 classes of questions

101

at least as far as the required processing is concerned. one possible def-
inition, although it is probably more de facto than de jure, is that cur-
rent qa is entirely extractive: it    nds answers that are already known
by somebody and have been written in some resource, rather than gen-
erating new knowledge. an incidental consequence of this de   nition
is that a proper answer is an answer string paired with a pointer to
the document that it is extracted from, which a   ects evaluation (see
chapter 3). we will look into these issues in some detail later, but for
now we will rely on approximation and intuition.

2.1.1 factoid questions

the type of question that has been the center of attention in the
early trec-qa years is (unfortunately) termed the factoid question.1
figure 2.1 shows a selection of factoid questions from trecs 8 and 9.
here and throughout the rest of the article, a pre   xed integer indicates
it is a question from a former trec evaluation, and the number is the
trec question number.2

there is no formal de   nition of factoid question. intuitively, a fac-
toid is asking about a simple fact or relationship, and the answer is
easily expressed in just a few words, often a noun-phrase. trivia-based
board games and television shows deal in factoids. it might be thought
that how and why questions are excluded, but sometimes they can
be answered concisely, and have indeed shown up in trec. the only
constraints that trec imposes on factoids lay in the format of the
returned answer (a simple string of characters) and the manner of eval-
uation (see chapter 3), to distinguish them from list and de   nition
questions.

1 the term was borrowed from the cable news channels usage, but etymologically it means
the opposite of what it is now used to mean: the -oid ending in english means    similar but
not equal to,    as in spheroids and humanoids, so by extension a factoid is an assertion that
might seem to be true but really is not     like mars having canals, for instance. however,
we will adopt the common usage here.
2 questions 1   200 were from trec8, 201   893 from trec9, 894   1393 from trec2001, and
1394   1893 from trec2002. in trec2003, the numbering was reset to start again at 1
to count targets, with an in   xed numbering scheme indicating target.subquestion. targets
t1   65 were from trec2004, t66   140 from trec2005, and t141   215 from trec2006.

102 overview of question   answering

9: how far is yaroslavl from moscow?

15: when was london's docklands light railway constructed? 

22: when did the jurassic period end? 

29: what is the brightest star visible from earth? 

30: what are the valdez principles? 

73: where is the taj mahal? 

134: where is it planned to berth the merchant ship, lane victory, which merchant 
marine veterans are converting into a floating museum? 

197: what did richard feynman say upon hearing he would receive the nobel 
prize in physics? 

198: how did socrates die? 

200: how tall is the replica of the matterhorn at disneyland? 

227: where does dew come from? 

269: who was picasso? 

298: what is california's state tree? 

450: what does caliente mean (in english)? 

fig. 2.1 selection of questions from trecs 8 and 9.

factoid questions (and list questions too, see next section) can
also be asked in forms like    name a city where . . .     or    tell me an
actress who . . .     but these can be readily converted to their    what x    
equivalents by simple transformations, and then treated identically in
processing.

several points can be made regarding the questions in figure 2.1.
for the most part, the question itself determines the answer type, the
class of entities that a correct answer will belong to. for example, it is
clear from #9 that a distance is required. there are often issues of units
and granularity     does #15 seek a period or its ending point, and if the
latter, is a year good enough? a month? a day? some questions, such
as #30, are very open, and very little indication is available about the
answer type. most such questions can be answered by a noun or noun-
phrase, but it is not so clear whether that is true of #198 and #269.
how (and why) questions can sometimes be answered by a simple
phrase (in the case of #198,    suicide    or    hemlock   ), but in the general

2.1 classes of questions

103

case the answer requires an explanation that can take many sentences.
#269 is an example of what is nowadays called a de   nition question,
and although simple phrases can be found (   spanish painter   ), they
clearly do not do the question justice. we will be discussing de   nition
questions separately.

some questions have interesting internal structure     for example
   come from    questions such as #227. this particular one is asking
about the end-result of a physical process, so naming the process is
probably what is required. by contrast,

where does uranium come from?

might be asking about the ore or about the countries or regions on
earth where it is mined.

where does co   ee come from?

might be asking about the country of origin, or the plant, or the man-
ufacturing process, or a combination. we immediately see that deter-
mination of answer type is not just a simple matter of classifying the
question syntax.

the approach taken by nist in the trec evaluations is that any
reasonable answer attested in the corpus and approved by human asses-
sors3 is correct. in later evaluations this has been further re   ned to
   reasonable to an average english-speaking adult.    for the most part,
timeliness has been ignored, so that in response to

147: who is the prime minister of japan?

any of several past japanese prime ministers, as long as they were
current at the time of the article, would be correct. however, if the
article explicitly says that the person no longer holds the position (e.g.,
   former prime minister,       ex-governor   ), that person would not be
a correct answer. in trec2006, the rules were changed so that the
most recent qualifying answer was required, if no time-constraints were
explicitly given in the question.

3 nist uses retired analysts for assessing the qa track.

104 overview of question   answering

2.1.2 list questions

sometimes it is desired to ask for all entities that satisfy a given crite-
rion, for example

4.4: what movies did james dean appear in?

such questions are called list questions, and a response would be a
set of factoid responses (e.g., {answer, docid}). the fact that the set
of answers can be collected from a set of documents does not change
things much. as we shall see shortly, in the later stages of processing
the system has a set of candidate answers, and while for factoids a
single answer is required, for list questions the system must return
as many as are thought to be correct. this imposes a need for the
system to not just assign a numerical score to candidates for purposes
of ranking, but to have some idea of a con   dence threshold to know
when to truncate the list. finally, it should be mentioned that on rare
occasions the answers to an apparently list question can reasonably
be expected to be found together in a single phrase, and so could be
considered a factoid. this has been exploited in trec a few times, e.g.,

388: what were the names of the three ships used by columbus?

2.1.3 de   nition questions

questions such as

269: who is picasso?

or

1102: what does a de   brillator do?

really need a collection of facts or properties for a decent answer. this
was recognized by nist and the research community, so these de   ni-
tion questions were dropped in trec2002 since they did not cleanly
   t into the factoid mold. they were reintroduced for subsequent evalu-
ations under the name    other    (see below), where they were explicitly
identi   ed and evaluated by how good a list of properties or nuggets
were returned. de   nition questions are the subject of section 4.7.

2.1 classes of questions

105

3.1 factoid when was the comet discovered? 
3.2 factoid how often does it approach the earth? 
3.3 list    in what countries was the comet visible on its last return?
3.4 other   other 

fig. 2.2 question series from trec2004, for target    hale bopp comet.   

2.1.4 other questions

in this classi   cation, the word    other    has a technical meaning.
from trec2004 on, the evaluation replaced the previously-used
   at set of questions with a two-level hierarchy: participants were
given a set of subjects or targets, which could be people, organiza-
tions, etc., and for each target a number of questions (usually 4   6)
were asked     see figure 2.2. these groups of questions form what are
called question series. a series consists of a number of factoid and list
questions (often employing anaphoric reference to the target), plus a
single open-ended so-called    other    question. no explicit question was
asked here, but rather participants were expected to return nuggets of
relevant facts about the target that had not been covered in the explicit
factoid and list questions. in evaluation (see section 3.3) the nuggets
are graded as vital, ok, or not ok. the other category resem-
bles a cross between the de   nition questions of previous years and list
questions with no provided cardinality.

2.1.5 relationship questions

in trec2005, relationship questions were introduced for the    rst
time. the word relationship here takes its meaning from its use in the
aquaint relationship pilot, where analysts de   ned a relationship
loosely as a means by which one entity could in   uence another. eight
such types of in   uence were identi   ed, including    nancial connections,
communications links and    nancial ties. the questions would either
ask whether a certain relationship existed, or what evidence there was
for one. in both cases, responses were not the exact answers used for
factoid questions, but text snippets (e.g., clauses, sentences, even para-

106 overview of question   answering

graphs). as a consequence, the evaluation methodology for relationship
questions has been the same as for    other    questions.

2.1.6

interactive qa

in trec2006, nist introduced the ciqa (complex interactive qa)
task. the questions were relationship questions similar to those just
mentioned, except they were in two parts, and there was an inter-
active phase where the system could get more information from the
originator. the questions, called topics, had two components: one of
a small number of templates, which was a question with slots that
were populated with di   erent values from topic to topic, and a nar-
rative, which was a sentence or paragraph giving more information
about what the analyst wanted to know. sample topics are shown in
figure 2.3.

in the (optional) interactive phase, systems could present a web
page to the analyst for feedback. systems could ask for any kinds of
help, including guidance as to which query terms might be helpful or
whether presented answer passages were relevant. the interaction time
was limited to three minutes. as in the relationship task, the evaluation
was nugget-based.

[china]? 

states]? 

question 26: what evidence is there for transport of [smuggled vcds] from [hong kong] to 

narrative 26: the analyst is particularly interested in knowing the volume of smuggled vcds 

and also the ruses used by smugglers to hide their efforts. 

question 27: what evidence is there for transport of [drugs] from [mexico] to [the united 

narrative 27: the analyst would like to know of efforts to curtail the transport of drugs from 

mexico to the united states. specifically, the analyst would like to know of the 
success of the efforts by local or international authorities. 

question 32:  what [financial relationships] exist between [drug companies] and [universities]?
narrative 32:  the analyst is concerned about universities which do research on medical 

subjects slanting their findings, especially concerning drugs, toward drug 
companies which have provided money to the universities. 

fig. 2.3 sample questions from the trec2006 ciqa task.

2.2 a typical software architecture

107

2.2 a typical software architecture

the introduction of qa at trec was due to a number of forces coming
together: there was a widespread need in the public for answering simple
questions, as was evident from examining the query logs of web search
engines; there was a desire in the technical community for bringing
together the ir and nlp sub-communities; machines were thought
fast enough to tackle what was understood to be a di   cult problem;
and the state of the art in ir and nlp seemed appropriate to address
the problem competently.

so what were the technical reasons for bringing together ir and
nlp for question   answering? unlike with the toy systems of the past,
it was realized that for a general-purpose solution, the resource needed
to be an existing large corpus of textual information. one possibility
was the web itself, but a solution preferred by nist at the time was to
use their newswire corpora; these were of course smaller than the web,
but still large enough to present a real technical challenge, and tamer
than the web, in the sense that the document formats and language use
were more uniform and correct, and information presented was more
generally consistent and accurate, so the problem was more tractable.
more importantly, a    xed corpus allowed for reproducible experiments.
in order to do a good job of    nding answers, not just documents,
in response to a natural-language question, considerable nlp would
be required. nlp, as with most symbolic activities, tends to be very
computationally expensive. processing a million documents in response
to a question was out of the question. good ir systems, on the other
hand, were and are particularly adept at    nding just those few doc-
uments out of huge corpora that have the best participation of query
terms. thus it seemed like a natural marriage     use ir to narrow down
the search to a relatively small number of documents, and then process
them using nlp techniques to extract and rank the answers.

the traditional qa approach is to generate from the question a
keyword-based query for a search engine. these keywords will be the
signi   cant words in the question, after undergoing certain transforma-
tions that we will discuss shortly. ir systems typically work by    nd-
ing documents for which some ranking formula is maximized; these

108 overview of question   answering

documents are usually those which have as many occurrences of the
rarer query terms as possible.

however, consider a question beginning    when . . . ,    seeking a time
or date. putting the word    when    in the query will not generally
be helpful, since most answers, and most sentences containing those
answers, will not except by chance have that word in them. the same
goes for all the so-called wh-words.4 the wh-words, along with any
words they modify (   what color,       how much,    etc.) represent the
type of information desired, called the answer type. these are typically
left out of the query, although, as we will cover in section 4.3.1, the
technique of predictive annotation does put terms representing the
answer type in the query.

almost all qa systems have at their core the components and con-
   guration shown in figure 2.4. a typical system nowadays may have
20 or more modules in the block diagram, but the basic architecture as
shown here.

the question provided by the user is    rst processed by a question
analysis module. this module has two outputs: the keyword query

question 

question 
analysis 

keyword query

search 

web or 
corpus 

answer type

documents 
or passages

answer 
extraction

answer(s)

fig. 2.4 basic architecture of a question   answering system.

4    who,       what,       when,       where,       which,       why    and, despite the spelling,    how.    also
implicitly included are locutions such as    name the . . . .   

2.2 a typical software architecture

109

and the answer type. the search module will use the keyword query to
search either a pre-indexed corpus or the web     or both     and return
a set of documents or passages thought to contain the answer. these,
as well as the answer type generated by question analysis, are processed
by a module we will call answer extraction (although its name varies
from system to system), which generates a ranked list of candidate
answers, of the given answer type.

participants in trec are required to    nd the answer in a desig-
nated corpus5     about a million newswire documents     although the
internal processing is allowed to be done using any resource desired.
what gets returned by a system is the pair {answer, document-id},
and it is required that the document provided actually answer the ques-
tion. some developers actually try to    nd the answer itself in another
resource     typically the web, but possibly a structured resource such
as a gazetteer     and then re-locate that answer in the trec corpus;
that last stage is called projection. the trick to projection is not so
much to    nd an instance of the answer in the trec corpus, but to
   nd it in a document where it answers the given question. the block-
diagram of a generic system using projection is shown in figure 2.5.

2.2.1 overview of qa processing

in a nutshell, the operation of a typical qa system is as follows: given a
question,    nd a candidate answering passage, check that the semantics
of the passage and question match, and extract the answer. some of the
high-level issues associated with these steps are presented here. more
details are given in succeeding sections.

if the corpus to be searched is su   ciently small, each passage can
be processed without the need for a preliminary search, but we will dis-
regard this as uninteresting and practically unlikely. the primary issue
for search, then, is to trade-o    precision and recall, as is typical in ir. if
the search is too broad, not much will have been achieved, and down-
stream components such as answer extraction will be overburdened
both with quantity of processing and noise. if the search is too nar-

5 this is an arti   cial restriction, introduced both to make assessment feasible, and to make
the experiments reproducible.

110 overview of question   answering

question 

question 
analysis 

keyword query

search 

web 
and/or 
corpus 

answer type

documents 
or passages

designated 
corpus 

answer 
extraction

answer(s)

{answer, docid}

projection

fig. 2.5 basic architecture of a question-answering system, using projection.

row, good documents/passages will likely be missed. broadly speaking,
two approaches are used to tackle this problem: well-developed ques-
tion analysis, to optimize the queries submitted to the search engine,
and iterative search, starting with a very restrictive query and relaxing
until    acceptable    hit-lists result.

with a set of candidate passages in hand, it is necessary for answer
extraction to test how well the semantics of a passage matches that
of the question. approaches use varying amounts of simple heuristics,
nlp and inferencing: counting question words present in the passage,
computing various proximity measures, matches of common syntactic
relationships, and use of proofs, to varying degrees of rigor.

the goal of answer extraction is to extract answers of the same type
sought in the question. this has implications for the appropriate size
of the answer type hierarchy, and also where/when/whether to consider
subsumption. these issues are considered later in this section.

we also discuss precision vs. recall trade-o   s and granularities of

answer types.

in the following sections, we will look at all of the components
depicted in figure 2.5 and more, as well as issues such as precision vs.

2.3 terminology

111

recall trade-o   s and granularities of answer types. we will note that
many of the techniques and methodologies of the    elds of ir, nlp and
machine learning are commonly used. in addition, knowledge repre-
sentation and inferencing are used, as well as statistical techniques.

2.3 terminology

in the next section, we will cover the building of a minimal qa system,
but    rst we need to establish some terminology. in this section, we
de   ne the following terms for use in the rest of this article:

    question phrase
    question type
    answer type
    question focus
    question topic
    candidate passage
    candidate answer
    authority file/list

2.3.1 question phrase

this is the part of the question that says what is being sought. the
simplest question phrases are the wh-words    who,       what,       which,   
   when,       where,       why,    and    how,    when they stand alone as the
sentence subjects. in general, question phrases are those phrases gen-
erated when the wh-words modify nouns, adjectives or adverbs, as in
   what company,       which president,       how long,    and    how quickly.   
we may occasionally include in this de   nition disjoined words as    cost   
in    how much . . . cost?,    since that question is equivalent (except per-
haps in emphasis) to    how expensive . . .    . for list questions, the
question phrases are identi   ed in the same way, e.g.,    what 5 compa-
nies . . .    . yes   no questions do not have question phrases.

it is a characteristic of english that di   erent syntax is typically
used when the questioner knows or suspects that there is more than
one answer to the question. thus we see

112 overview of question   answering

213: name a    ying mammal.
316: name a tiger that is extinct.
412: name a    lm in which jude law acted.

the    name a    syntax is logically identical to the corresponding
   what is    formulation, and will be treated identically here. we note
that    name    questions are really all list questions, with the number
of desired responses indicated by whether the word is followed by the
inde   nite article (indicating just one answer) or a cardinal number and
a plural noun.

2.3.2 question type

the question type is an idiosyncratic categorization of questions
for purposes of distinguishing between di   erent processing strategies
and/or answer formats. in trec, there are (from trec2003):

factoid:
list:
definition:

1894: how far is it from earth to mars?
1915: list the names of chewing gums.
1933: who is vlad the impaler?

however, there are other possibilities:

relationship: what is the connection between valentina

superlative:
yes   no:
opinion:

tereshkova and sally ride?   
what is the largest city on earth?
is osama bin laden alive?
what do most americans think of
gun control?

cause & effect: why did iraq invade kuwait?
. . .

this list is open-ended, and its types are not always mutually exclusive.

2.3.3 answer type

the answer type is the class of object (or rhetorical type of sentence)
sought by the question. for example,

2.3 terminology

113

person (from    who . . .    )
place (from    where . . .    )
date (from    when . . .    )
number (from    how many . . .    )
. . .

but also

explanation (from    why . . .    )
method (from    how . . .    )
. . .

answer types are usually tied intimately to the classes recognized
by the system   s named entity recognizer, when text search is used,
or to the predicates employed by a structured resource. there is no
generally agreed on set or number of classes for a given domain. since
answer types will crop up frequently in this article, we use this font to
distinguish them from the corresponding english words or phrases.

2.3.4 question focus

the question focus, generally a compound noun-phrase but sometimes
a simple noun, is the property or entity that is being sought by the
question [48]. in the following questions, the focus is underlined.

820: mccarren airport is located in what city?
219: what is the population of japan?
1217: what color is yak milk?

the focus is clearly intimately tied to the answer type. the focus is
a property of the question, while the answer type is an information
type intrinsic to the qa system. they often correspond, but in, for
example,

2301: what composer wrote    die gotterdammerung   ?

the focus is composer, but the answer type may well be person (depend-
ing on the granularity of the system   s answer type hierarchy).

the question focus is usually part of the question phrase. when
the question phrase is a single word (   where,       why,       who,    etc.)

114 overview of question   answering

the focus can be determined from the question phrase   s expansion (   in
what place,       for what reason,       what person   ).

2.3.5 question topic

the question topic is the object (person, place, . . . ) or event that the
question is generally about. the question might well be asking for a
property of the topic, which will be the question focus. for example, in

what is the height of mt. everest?

height is the focus, mt. everest is the topic.

when more verbose questions are asked, it typically happens that
no passage can be found that contains all of the question words (after
wh-word removal). it is almost always the case that if the topic (or a
paraphrase) is missing, the passage does not contain an answer, but
that is not so often true of the other words. for example, with

1174: where on the body is a mortarboard worn?

it is very unlikely that an answer can be found without matching the
topic, mortarboard, but body and forms of wear may well be absent. we
will see later techniques where terms are dropped from over-constrained
queries     these techniques can bene   t from topic identi   cation.

2.3.6 candidate passage

a candidate passage is a text passage (anything from a single sentence
to a whole document) retrieved by a search engine in response to a
question. depending on the query and kind of index used, there may
or may not be a guarantee that a candidate passage has any candidate
answers. candidate passages will usually have associated scores     their
relevance rankings     from the search engine.

2.3.7 candidate answer

a candidate answer, in the context of a question, is a small quantity of
text (anything from a single word to a sentence or more, but usually a
noun-phrase) that is ranked according to its suitability as an answer;
in many systems it is required to be of the same type as the answer

2.3 terminology

115

type, although the type match may be of the same type as the answer
type. in some systems, the type match may be approximate, if there
is the notion of confusability.6 candidate answers are found in candi-
date passages. for example, the following items are typical candidate
answers:

    50    queen elizabeth ii
    september 8, 2003
    by baking a mixture of    our and water

2.3.8 authority list

an authority list (or    le) is a collection of instances of an answer type
of interest, used to test a term for class membership. instances in such
a list should be derived from an authoritative source and be as close to
complete as possible.

ideally, the type implemented by an authority list is small, eas-
ily enumerated and whose members have a limited number of lexical
forms.

good types for authority lists include:
    days of week
    planets
    elements
    states/provinces/counties/countries

types that are good statistically, meaning that it is easy to collect most
instances that are likely to be encountered in common practice, but for
which it is possibly di   cult to get 100% coverage, include:

    animals
    plants
    colors

6 named-entity recognizers make errors, but usually not randomly. the errors are mostly
systematic and re   ect properties of the real world, such as places and organizations often
being named for people. armed with that knowledge, it can make sense for a system
to propose candidates with not only the correct answer type, but any that are readily
confusable with it.

116 overview of question   answering

some types may use authority lists for part of the identi   cation process,
but the lists are not su   cient by themselves. such classes have very
many instances, and are open-ended, and include:

    people
    organizations

types for which authority lists are totally inappropriate include:

(1) all numeric quantities, e.g.,

    cardinal and ordinal numbers
    measurements (numbers + units, e.g.,    200 sq. miles,   
   5ft. 6in.   )
    populations (e.g.,    500,000 people,       a population of
1,000,000   
    dates

(2) explanations, quotations, and other clausal quantities

2.4 building a qa system

to build a qa system, many of the components can be readily acquired
o   -the-shelf. in this section, we will see what needs to be done to build
a minimal qa system. we will consider both using and not using the
web;    rst we will assume that we have a substantial text corpus which
we want to query. it is possible to build systems that have no under-
standing of answer types, but this is rarely done, so we will assume
that the system is type-based, since there are some distinct associated
advantages to that. for ease of reference, we repeat figure 2.4 here in
figure 2.6.

as discussed earlier, the problem is uninteresting if the corpus is
su   ciently small that search is not required, so we will assume the
availability of a search engine and an indexer for it. con   gurable, not
to mention custom, search engines can provide considerable    exibility
as to what gets indexed, and we will see the advantages of such control
in section 4.3. for now, though, we will assume that every word in the
corpus is indexed, subject to stop-word removal and either lemmatiza-
tion or id30.

2.4 building a qa system 117

question 

question 
analysis 

keyword query

search 

web or 
corpus 

answer type

documents 
or passages

answer 
extraction

answer(s)

fig. 2.6 basic architecture of a question   answering system.

we will assume the availability of a named entity recognizer.
examples of these include bbn   s identifinder [3], she   eld   s gate
(http://www.gate.ac.uk), and ibm   s textract/resporator [8, 54, 78].
it may come with a    xed set of types, or preferably, will come with a
   standard    base set of types useful to almost any domain (e.g. person,
place, number, etc.) but be extendable by the user to other types.

given these components, two other components need to be built: a
question analysis module and an answer extraction module. however,
before question analysis can proceed, one must determine the set of
types to be known to the system.

2.4.1

inventory of answer types

the    rst thing to be decided is whether the system is to be domain-
speci   c or open-domain. while the processing is the same in both cases
in a minimal system, the set of types that the system needs to iden-
tify will be very dependent on that answer. for example, if the domain
is that of computer hardware, useful types might include process-
ingspeed and memory; if the domain is terrorism, types might include
biologicalweaponagents. it could be argued that an open-domain sys-
tem is just the union of all possible domains, but that would imply

118 overview of question   answering

the use of very many specialized types each of which is statistically
very unlikely to be used, and as a whole would require a lot of work to
implement. consequently, systems tend to identify a set of types that
cover the large majority of expected questions in the chosen domain
(often by question/query log analysis), and also incorporate a default
mechanism that operates when unexpected question types occur, in
order to determine a default answer type, or a broad list of possible
answers.

a typical qa system will have two basic mechanisms for processing
questions, based on answer type. if the question is about a type that the
system knows about, then type-speci   c processing will be initiated; oth-
erwise, some default mechanism, likely using a resource such as word-
net [45], will be used. the advantage of type-speci   c processing, it is
assumed, is that it is probabilistically more accurate than the default
mechanism (because knowledge-engineering has gone into building the
former mechanism). thus the goal in building a qa-system is to select
a set of types that cover the majority of expected questions, in order
that the overall performance of the system is more heavily weighted
in the direction of type-speci   c performance rather than default per-
formance. the cost of adding a new type is not only measured by the
work done in building the corresponding recognizers, but the potential
drop in performance if it is not done well enough.

for open-domain qa, there is no agreed-upon set of types, or even
consensus on how big the set should be. for qa systems in the liter-
ature, numbers ranging from a few dozen to many hundreds of types
have been reported, and no strong correlation with overall performance
has been identi   ed. all systems will have types person, organization,
date, and so on, but will di   er with regard to both depth and breadth.
country is a common type, but county is not; number is common, but
kitchenutensil is not.

recall that the question speci   es the answer type, which has to
be identi   ed in the corpus. therefore there are two implications of
choosing to use a given type:

    question analysis must identify questions seeking that type
    entity recognition must recognize instances of the type in
text.

2.4 building a qa system 119

with each of these are the opposing interests of precision and recall,
as we shall shortly see. an initial set of types is usually derived from
observations of training data, which in this case would be logs of ques-
tions received by other qa systems, or even search engines.7 natural
extrapolations can be made on this set: noting questions about moun-
tains in the training data, for example, it would be quite reasonable to
add not only the mountain type, but also river and lake, even if such
questions were not observed. to some extent, choosing the set of types
is still an art.

in deciding whether to adopt a type, it tends to be primarily the
entity recognition accuracy that is a consideration. each type will
have one or more nominal forms which are used in questions, e.g.,

mountain (cid:1)   (cid:2)    what mountain/peak . . . ,   
organization (cid:1)   (cid:2)    what organization/company/group/agency

. . . ,   
person (cid:1)   (cid:2)    who . . . ,   

so pattern-based approaches to identifying questions about the type are
usually straightforward, and can be done with good accuracy. the main
issue is how well instances can be recognized in text. there are di   erent
implications for precision and recall, which we will now discuss.

recognition of some basic types, principally those like dates involv-
ing numbers, involves parsing and/or complex pattern-matching, while
most others can be recognized by authority lists plus some fairly simple
pattern-matching. so for these latter types, the    rst question is one of
recall: how readily can as complete a list of instances as possible be gen-
erated? for many types there are readily-available resources that can be
used to populate such lists. id138 is very useful for everyday types    
plants, animals, colors, etc. the cia world factbook8 is very useful
for countries and other geo-political information. miscellaneous web-
sites can be found for many other types. care must be taken to assess
the authoritativeness of the lists found, but for many of these types,
reasonably complete lists can be obtained. one can run into di   culties

7 despite the fact that standard search engines operate by processing queries which are sets
of keywords, many natural-language questions are found in query logs.
8 http://www.odci.gov/cia/publications/factbook/index.html.

120 overview of question   answering

for domain-speci   c types, even of a very general nature: for example,
id138 does not have the concept governmentorganization, so it can-
not be used to populate such a type. other sets     rock bands, movies,
authors     likewise are not easy to    nd in single comprehensive lists.
thus having the type author, say, is of limited usefulness if instances
cannot be recognized when they occur     especially if the question
is about an author participating in some other activity, so responsive
text     text that contains the answer in a supporting context     will
not provide any clues that the person mentioned is an author.

precision tends to be an issue because many words are polysemous.9
whether this is a problem in any particular case depends on many
things: how many of the meanings correspond to di   erent types and
whether the named-entity recognizer allows for ambiguity, and whether
it is known what the statistically most-likely type is (both in general
and as an answer to a question).

suppose one wants to create a list of occupations and roles, as poten-
tial answers to    who is x?    questions. id138 is an excellent resource
for this, but unfortunately many of the hyponyms of person found there
(fox, sheep, shark, cow, slug, and many others) are also animals, and
in a general corpus, are more likely a priori to have    animal    rather
than    undesirable person    senses. since    who is . . .     questions are
likely to be common, as are mentions of animals in a general corpus,
the best strategy would seem to be to remove such instances from an
occupationandrole type. this would cause the system to miss any occa-
sions when the animal name was the desired person descriptor, but
playing the odds this is the right thing to do. besides, while apt, these
may well not be the best way to give a de   nition of someone.

by contrast, a di   erent situation applies to body parts, for example.
many such part names (head, back, front, side, arm, leg, and others),
are most likely to be used in a general corpus, and especially a newswire
corpus, in their metaphorical, directional and mereological senses, but
these are not necessarily recognized as another type. furthermore, out-
side of the medical domain one can expect questions about body parts
to be relatively rare, compared with other questions of general interest.
thus a bodypart type, while known in advance to have poor precision,

9 they have multiple meanings.

2.4 building a qa system 121

can be implemented directly from id138 authority lists because the
false positives are unlikely to be problematic in practice.

2.4.2 question classi   cation

part of the job of question analysis is answer type determination,
sometimes called question classi   cation. both manually and auto-
matically created classi   ers are used, and they each have their own
advantages.

the manual method is to create patterns that match all anticipated
ways to ask questions for which each type in the system   s set of answer
types is a possible answer. so person, for example, will be associated
with patterns such as:

who/whom/whose . . .

what is the name of the person who . . .

and distance will be associated with:

how far/wide/broad/narrow/tall/high . . .

what is the distance/height/breadth . . .

in general, these patterns have high precision, but it is di   cult
and very time-consuming to get high recall across a large set of types.
the more specialized the type, the easier it is to get good coverage of
these patterns: aside from disjunctions and conjunctions, it is di   cult
to imagine more than a few likely ways to ask about biologicalweapons,
for example. another advantage of having explicit patterns is that each
may be associated with information about which matched question
words to retain or drop, as discussed in the next section.

automatic classi   ers have the potential advantage of being more
easily expandable and adaptable as the type system changes. a good
example is the hierarchical learning classi   er of [33].

the question arises of whether there is a general-purpose solution
when the question does not match any patterns. the system can be
engineered so that this never (or extremely rarely) happens. if yes   no
questions are ruled out-of-scope, then every question must have a wh-
word and an identi   able question focus. if the pattern set has a default
pattern for each wh-word (including    what    when used stand-alone)

122 overview of question   answering

then the only remaining issue is how to    nd an answer type for    what
x    where x is not recognized.

one approach to this issue is statistical. prager et al. [56] describe
a statistical co-occurrence method, where, for example, it is discovered
that the question focus    death toll    collocates frequently with popula-
tion and number. another approach uses lookup in an ontology such
as id138 [52, 55]. so for example the focus    debts    in

7: what debts did qintex group leave?

is discovered to be a hyponym of       nancial condition,    which is mapped
in a human-prepared table to money.

2.4.3 query generation

given a set of answer types and a search engine, the question analysis
component must produce a query for the search engine for any question.
the query will contain some of the question words, possibly modi   ed,
will drop some and add others. stop-words and wh-words are dropped;
whether to drop the whole question phrase is somewhat problematic,
and is discussed below. id172 (lemmatization or id30,
case-id172) will proceed according to the search engine   s index-
ing regimen. id183 may take place, although as discussed
later, this may only take place as a relaxation step.

a common way to recognize answer types is to establish one or
more templates to match against questions. associated with each such
template can be an indication of which words are to be dropped from
the query. although it is clear that the wh-words will be dropped, there
is no general rule whether the entire question phrase should be; this
depends on how much of the semantics of the answer is represented by
the answer type.

for type organization, for example, in    what organization . . .     the
word    organization    should probably be dropped, because organiza-
tions can be recognized either by list membership or syntax (endings
such as    co.,       ltd.    etc.), and it is observed that most mentions of
organizations in text do not include the word    organization.    including
the word in the query is not only unnecessary but will skew the search
in unwanted directions.

2.4 building a qa system 123

the opposite case is represented by the type population, which in the
author   s system is used to represent counted collections of people. so
text strings such as    100,000 people,       50,000 residents    are recognized
as population, as well as phrases such as    a population of 2,000,000.   
since this ner design is recall-oriented, keeping    population    in the
query (and further, expanding with related terms    residents,       inhab-
itants,       people,    and    citizens   ) boosts precision.

consequently, the dropping or not of the whole question phrase is
a function of the ways in which instances of the corresponding answer
type is expressed. in general, this must be decided on a case-by-case
basis, and will in practice involve a number of experiments on test data
to determine the best behavior in each case.

2.4.4 search

the current practice in most qa systems is for query generation to
produce a bag-of-literals query, in which case any competitive o   -the-
shelf search engine (such as lucene10 or indri,11 for example) will work
well. in some designs, derived data such as entity types are indexed and
included in the search. this technique is called predictive annotation,
and is covered in section 4.3.1. depending on the exact design of the
search engine and indexer, some customization may be necessary to
implement that technique. to index and search for complex structures
such as syntactic or semantic relationships, or parse trees, considerable
support must be supplied by the search engine.

the principal design decision regarding search is in what to out-
put: passages12 or documents? the smaller (in terms of text length)
the quantity passed on to answer extraction, the easier the job the
latter component will have. less text to process means less noise (i.e.,
opportunity for selecting incorrect answers), but also jeopardizes recall.

10 http://lucene.apache.org/java/docs/index.html.
11 http://www.lemurproject.org/indri.
12 a passage is not a well-de   ned quantity in the way a document is. in the author   s system,
it is a text snippet from 1 to 3 sentences. each implementation will use its own de   nition.
for purposes of this article, a passage can be thought of as being a paragraph, but nothing
will hinge on exactly what de   nition is used. when co-reference resolution and titles are
considered, the notion can become very fuzzy indeed.

124 overview of question   answering

the usual practice is for systems to produce passage-level text
chunks at some point. in some cases, the search engine itself will return
passages, in others a post-search phase will take documents from the
search engine and extract passages. whether such a passage-extraction
component is part of the search or a separate stage is primarily a mat-
ter of convenience. the existence and importance of the interaction
between the document- and passage-retrieval levels is discussed in [73].
free-text style searches give found documents scores which are a
function of the number of times each query term is found in the doc-
ument, weighted by some tf*idf -style factor. in such cases the hit-lists
are typically very long, since any document containing even just one
instance of any query term will be on it. qa-systems employing such
searches will truncate the hit list either by number of hits or score
before passing to the next component.

boolean searches, weighted-boolean, and free-text with the
   required    operator, on the other hand, o   er two opportunities that
plain free-text searches cannot. it is much easier in these cases for what
look like reasonable searches to return no (or very few) hits. such sys-
tems can therefore decide at this point that the question has no answers
in the corpus. alternatively, they can employ relaxation, in which terms
are removed from the query or required operators are dropped and the
search re-run (cf. the falcon system, below). relaxation therefore intro-
duces loops into the previously linear qa pipeline (cf. figure 2.6). the
use of relaxation allows a high-precision initial attempt, with a back-
o    which increases recall. in systems where answer extraction uses the
search-engine score as a factor in its processing, it becomes a challenge
for a search component which employs relaxation to assign the most
meaningful scores to the documents it outputs.

2.4.4.1 smu falcon13

a good example of the use of loops in search is exhibited by the
falcon system from southern methodist university [21], the highest-
performing system in trec9. in fact, their system employs three

13 while for the most part in this article, speci   c qa systems are covered in section 4,
falcon is described here since it illustrates relaxation loops, a technique that cuts across
the qa system classi   cation scheme described later.

2.4 building a qa system 125

loops, there being tests at three points in the processing pipeline
which, if not passed, causes the processing to go back to reformulate
the query and try again.

their question analysis component generates a dependency struc-
ture called a question semantic form and an initial query as a boolean
conjunction of terms. they employed a number of heuristics to generate
keywords for the query. these heuristics were ordered by e   ectiveness,
and the keywords were labeled by the rank order of the heuristic used to
generate them. the keywords associated with the less-e   ective heuris-
tics were not initially added to the query. their search engine, based on
smart [6], retrieved passages when all query terms would fall within a
prede   ned window size. smu observed that they could establish upper
and lower bounds on the number of desirable passages, as a function
of answer type. too many hits would result in keywords being added
(from the less-e   ective heuristics, not initially included), too few would
cause some to be removed (in reverse order of heuristic e   ectiveness)
[47]. this constituted the    rst loop of the system; the part of the falcon
system    ow diagram related to loops is depicted in figure 2.7.

returned paragraphs are parsed and transformed into an answer
semantic form. when none of these can unify14 with the question

generate 
question 
semantic form 
and keywords 

does number 
of passages 
fall within 
bounds? 

y

does any
answer 
semantic form 
unify
w/question sf

y

abductive 
justification 
possible? 

y

morphological 
expansions 

n

n

n 

lexical 
alternations

answer

semantic alternations, 
paraphrases

fig. 2.7 loops in falcon.

14 in the prolog sense of uni   cation. see, e.g., http://en.wikipedia.org/wiki/uni   cation as

of may 26, 2007.

126 overview of question   answering

table 2.1 contribution of feedback loops in smu falcon system.

improvement over baseline (%)

loop 1 alone
loop 2 alone
loop 3 alone
all three loops combined

40
53
8
77

semantic form, the query is expanded via synonyms and other lexi-
cal alternations found in id138, and reissued. this constitutes the
second loop.

when the semantic forms unify, they are converted to logical
forms and an abductive justi   cation is attempted. if this fails, key-
words are expanded to include related concepts, as found in a word-
net search. the expanded queries are reissued, making the third
loop [22].

these loops were evaluated to determine their contributions to the
system   s ultimate performance scores. their    ndings for the 50-byte
task in trec9 (see section 3.1) are presented in table 2.1 (derived
from [52, table 9]). the    rst three rows show the performance improve-
ment over the no-loop baseline when each of the three loops is enabled
alone. the last row shows an impressive 77% improvement over baseline
when all three loops are simultaneously active.

2.4.5 answer extraction

answer extraction (also known as answer selection/pinpointing)
takes a number of text items (documents or passages     we will
assume passages) returned by search, the answer type as determined
by question analysis, and the question, the query or some repre-
sentation thereof, and seeks to    nd the answer to the question in
those passages. just as search engines return a hitzlist of rank-
ordered documents, answer extraction will return a rank-ordered list of
answers.

by running its named-entity recognition on the passages, a set of
candidate answers is generated. answer boundaries are determined
directly by how much of the text is matched. assuming the ner classi-
   cation is binary, what distinguishes candidate answers is not how well

2.4 building a qa system 127

they match the answer type but how well the textual context matches
the question. how and how well this is done is arguably the biggest
factor in distinguishing di   erent qa systems. broadly speaking, the
approaches fall into four categories.

    heuristic: these approaches approximate matching between
the question/query and the passage by counting the number
of term matches and computing measures such as average dis-
tance of query term to answer term, density of all matching
terms and so on, and combining these factors using weights
learned in training (see e.g., [65]).
    pattern-based: if in training it is discovered that    when did
mozart die?    can be answered by    mozart expired in 1791,   
then    when did beethoven die?    can be answered by match-
ing    beethoven expired in (cid:1)date(cid:2)    (see e.g., [67]). pattern-
based approaches are discussed in section 4.2.
    relationship-based: these approaches take advantage of the
fact that despite the bag-of-words nature of the query, the
original question was in natural language and hence relation-
ships existed between the various question words. therefore,
passages are scored based on    nding the same relationships
there (see, e.g., [15]). syntactic relationships are easier to
   nd than semantic ones, but    nding the same semantic rela-
tionships in the answer passage is a better indication of the
answer being there.
    logic-based: this type of approach uses theorem-proving,
although not necessarily in a rigorous manner
(see
section 4.5). the basic technique is to convert the question
to a goal, then for each passage found through conventional
search methods, convert it to a set of logical forms, repre-
senting individual assertions (see [49]). predicates represent-
ing subsumption and real-world knowledge are added, and
the goal is proved (or not).

we now look at the heuristic and relationship-based approaches in more
detail; the other two are covered later.

128 overview of question   answering

2.4.5.1 heuristic answer extraction

this approach uses heuristic feature sets by which candidate answers
are ranked. every text fragment in a fetched document or passage which
is of the current answer type is ranked in this approach.

this section covers the features used in [55], but see also [65]. the
basic idea is to determine feature values for each candidate answer, and
then calculate a linear combination using weights learned from training
data. first, we look at the ranking criteria that the features embody.
    good global context: the global context of a candidate answer
evaluates the relevance to the question of the passage from
which the candidate answer is extracted.
    good local context: the local context of a candidate answer
assesses the likelihood that the answer    lls in the gap in the
question (the question phrase).
    right semantic type: the semantic type of a candidate answer
should either be the same as or a subtype of the answer type
identi   ed by the question analysis component.
    redundancy: the degree of redundancy for a candidate
answer increases as more instances of the answer occur in
retrieved passages.

some features that have been found to be e   ective for determining
global context are:

    keywordsinpassage: the ratio of keywords present in a pas-
sage to the total number of keywords issued to the search
engine.
    npmatch: the number of words in noun-phrases shared by
both the question and the passage.
    sescore: the ratio of the search engine score for a passage
to the maximum achievable score.
    firstpassage: a boolean value which is true for the highest
ranked passage returned by the search engine, and false for
all other passages.

2.4 building a qa system 129

some features that have been found to be e   ective for determining local
context are

    avgdistance: the average distance between the candidate
answer and keywords that occurred in the passage.
    notinquery: the number of words in the candidate answers
that are not query keywords.

other similar measures are obviously possible, and methods of com-
bining other than linear combination might give better results, but
lacking an underlying model of meaning, the search space is huge.
more advantage can be gained by exploring more sophisticated
features.

2.4.5.2 answer extraction using relationships

the basic idea here is to use linguistic knowledge to compute passage
and candidate answer scores. syntactic processing is performed both
on the question and each candidate passage. predicate-argument and
modi   cation relationships are extracted from the parse, as are the verbs
and noun-phrases themselves. the computed score is a function of the
number of relationship matches.
so for example, the question:
who wrote the declaration of independence?

generates the relationships:

[x.write], [write, "declaration of independence"]

and the candidate passage

je   erson wrote the declaration of independence

generates:

[jefferson.write], [write, "declaration of independence"]
two scores are computed. the passage score is a simple function
of the number of instantiated relationship matches (those without the
variable    x   ), and the intermediate candidate score is a simple func-
tion of the number of relationship matches with the variable. the    nal

130 overview of question   answering

candidate score is a simple weighted mean of the passage score and the
intermediate candidate scores: weighting 2:1 in favor of the candidate
score has been found to be e   ective.

consider the question:

q: when did amtrak begin operations

with relationships:

rq: [amtrak, begin], [begin, operation], [x, begin]

and the three matching passages with corresponding relationship sets
(some non-participating relations have been omitted for clarity):

p1: in 1971, amtrak began operations,. . .

r1: [amtrak, begin], [begin, operation], [1971, begin] . . .

p2:    today, things are looking better,    said claytor, expressing
optimism about getting the additional federal funds in future
years that will allow amtrak to begin expanding its operations.

r2: [amtrak, begin], [begin, expand], [expand, oper-
ation], [today, look] . . .

p3: airfone, which began operations in 1984, has installed air-
to-ground phones . . . . airfone also operates railfone, a public
phone service on amtrak trains.

r3: [airfone, begin], [begin, operation], [1984, oper-
ation], [amtrak, train] . . .

all three answer passages contain the question words and the answer
type. it is clear that r1 matches rq better than r2 or r3 do, but it
might not be so obvious why a simple density or compactness measure
would not prefer p1 without the need for relationships. consider the
following passages15:

15 the original passage augmented with material from wikipedia, http://en.wikipedia.org/

wiki/concorde.

p1(cid:2): in 1971, the year the us cancelled its supersonic aircraft
program, amtrak began operations,. . .

2.4 building a qa system 131

or

p1(cid:2)(cid:2): in 1971, the year the concorde 001 went on a sales and
demonstration tour while the us cancelled its supersonic aircraft
program, amtrak began operations,. . .

simple proximity measures as described in section 2.6.4.1 would give
p1(cid:2) and p1(cid:2)(cid:2) much worse scores than p1. since the relationships are
derived from the parse tree, and the mere insertion of relative or other
modifying phrases and clauses do not disturb the high-level original
parse structure, the same relationships can be expected to be found in
the expanded passages as in the original (plus others which neither add
to nor detract from the score). thus the answer 1971 from p1(cid:2) and/or
p1(cid:2)(cid:2) will continue to be preferred over answers from p2 and p3.

punyakanok et al. [63] describe a somewhat similar approach, but
using entire trees rather than relationship fragments. they generate
dependency trees for both question and answer passages, and use
an approximate tree-matching algorithm that minimizes edit-distance.
arampatzis et al. [1] describe attempts in general ir where phrases are
indexed in preference to keywords, other ir approaches showing how
performance is improved using a variety of di   erent term dependence
models include [19] and [43].

2.4.6 following answer extraction

while answer extraction is the point at which actual answers are man-
ifested for the    rst time, it is not necessarily the end of the pipeline.
further processing can be done on the candidate list to adjust their
con   dence scores, often reordering the list, possibly removing some
candidates entirely. these    nal stages go by various names     we will
refer to them as answer merging and answer veri   cation.

answer merging is the operation of taking two or more answer
strings that are considered to be (di   erent) representations of the same
real-world entity, and combining them to a single string. usually, the

132 overview of question   answering

score of the combined string is calculated by a formula which will result
in a value greater than any of its inputs, on the theory that multiple
instances of an answer count as extra evidence that the answer is cor-
rect. for example, if a1 with score s1 is merged with answer a2 with
score s2, where 0 <= si <= 1, then a simple combining formula for the
resulting score might be 1     (1     s1)     (1     s2).

di   erent criteria can be used for deciding when to merge. clearly,
identical strings, if not merged in answer extraction, can be merged
here. synonymous terms, e.g.,    3    and    three,    or    holland    and    the
netherlands    are easily merged. di   erent systems of measurement can
be merged     e.g.,    6 feet    with    2 yards.    for types like populations,
an error margin can be used to merge approximately equal numbers.
in all these cases, one of the input forms will be the preferred one and
chosen for output. for types like names, it is possible that the    nal form
is not one of the inputs but a combination     for example, the system
may decide to merge    mr. john smith    with    j. a. smith    to give
   mr. john a. smith.    in general, an answer merger can use arbitrarily
complex nlp to determine if terms are comparable, but in practice
usually only the simplest heuristics are used. a fuller discussion of the
complexities of testing for identity in the context of answer merging
can be found in [61].

answer veri   cation is the process of determining the    tness of a
candidate answer by techniques that are typically unsuitable for    nding
the answer in the    rst place (or else the technique would have been
part of search/answer extraction). in later sections, we will discuss
two forms of answer veri   cation: the use of redundancy for answer
validation by [40], in section 2.5.4, and sanity-checking with cyc by
[59], in section 4.5.2.

2.5 general issues in qa

we discuss in this section some general issues in qa: the trade-o   
between getting all the answers (or at least one correct one) and the
overall accuracy of those returned; whether and how to incorporate all
modi   ers given in the question in the search for an answer; the bene   ts

2.5 general issues in qa 133

of redundancy; and some thoughts on how advanced components need
to be in order to build an e   ective qa system.

2.5.1 getting all the answers

to perform well, a qa system needs to generate as many correct candi-
date instances as possible. at answer evaluation time this translates to
recall, but internally a qa system needs to maintain as solidly correct
a list of candidates as possible, mainly (a) because not all instances
will be in contexts that the system can correctly evaluate, and (b) so
that redundancy techniques can operate. this is an issue in qa for the
same reason recall is an issue in ir: the words in the question are not
necessarily ones used by the author in a responsive passage. further-
more, the syntax may be di   erent too, despite identical semantics. for
the vocabulary-matching problem, term expansion is required, but it
can be e   ected in at least three di   erent locations in the system, for
purposes of locating matching passages.

2.5.1.1

in question analysis

here question terms can be expanded to synonyms (or hypernyms,
hyponyms, related terms); since it has been shown that automatic syn-
onym expansion does not necessarily improve results [74], it should be
done in a controlled way. question reformulation is also possible here.
some kinds of terms or expressions found in questions can be pre-
dicted to be absent in good responsive passages, spatial and temporal
expressions particularly. for example,

name a band from liverpool in england who had many number-1

hits in the 60   s

there may indeed be one or more passages talking about the
beatles    hits in the 60s, but there are also likely to be many others
that talk about speci   c years or ranges. while liverpool is clearly in
england, relatively few passages that mention liverpool will say so.
one approach is to drop the modi   er and forget about it     this some-
times works like a charm because of inherent redundancy     or else to
drop the modi   er and test for it later.

134 overview of question   answering

2.5.1.2

in search

having the search engine do expansion (especially in its inner loop) is
generally avoided for reasons of computational expense.

2.5.1.3 at indexing time

id30 or lemmatization and case-id172 are a very basic
form of expansion, and are standard (and note that the same oper-
ations should take place on query terms in question analysis). more
sophisticated forms of generating canonical forms from equivalent vari-
ants are also useful. for example, we note the following variations,
amongst many:

    numerical:    10    ==    10.00    ==    ten   
    dates:
    national:
    state:

   italy    ==    italian   
   california    ==    calif    ==    ca   

==    1/1/2006   

   january 1st 2006    ==    1st of january 2006   

clearly these kinds of variants are not always identical under all cir-
cumstances, so it is somewhat of a judgment call whether to use them
automatically. however, expansions such as those listed above would
seem to o   er better increases in recall than loss in precision.

2.5.2 getting just the correct answers

this is the goal of maximizing precision, which also can be addressed
at several locations.

2.5.2.1

in question analysis

generally speaking, the more question words included in the query,
the higher the precision. idf -style weighting can be used to indicate
preferences, with extra weighting factors for phrases and capitalized
terms.

2.5 general issues in qa 135

2.5.2.2

in search

storing part-of-speech (pos) information in the index can be useful
for polysemous terms, but it can be harmful too. for example, bear as
a verb will almost never want to match bear as a noun, but the same
cannot be said for phone (   phone him    vs.    get him on the phone   ).
there is a clear precision vs. recall trade-o    here, and the best approach
in qa is to err on the side of recall during search, in the hopes that
precision can be recovered downstream in components such as answer
extraction.

2.5.2.3

in answer extraction

tests can be constructed to reward or penalize passages that have
made it this far. for example, the pos test described above can be
applied, given a list of words whose meaning changes dramatically
with part of speech. spatial, temporal or other modi   ers dropped from
the question, as described above, can be tested for here. there is the
opportunity here for constructing specialized type-dependent equality
or containment tests, which will match, for example, 1964 with 60s or
england with uk [60]. we discuss this approach further in the next
section.

2.5.3 answer types and modi   ers

as discussed in section 2.4.4, the question focus is often a compound
noun-phrase, which in the best of circumstances corresponds directly
to a known type. the larger the type system, the more often this will
happen. inevitably, however large the type system is there will be occa-
sions where the head of the focus is a known type, but the entire focus
is not. consider the following question:

name 5 french cities.

let us assume there is no type frenchcity, but there is city. the
query will therefore contain french/france, and returned passages will
be examined for instances of type city. instances will best be ordered
by frequency. the problem is that many mentions of paris, marseilles,

136 overview of question   answering

nice, cannes etc., especially in news copy written for educated adults,
do not say they are in france. possible actions include:

    do nothing, and hope for the best. surprisingly many sys-
tems do just this.
    use deep parsing or possibly logical id136 to try to con-
clude that a passage says that the city is in france.
    use high-precision language identi   cation on the city name
(not a guarantee, but will probably re-rank candidates use-
fully).
    if a list of french cities is available, use it to    lter the list,
or use projection (section 4.6).
    use longitude/latitude information of cities and countries.

suppose now the question is:

567: name a female    gure skater

most likely there is no type femalefigureskater. most likely there
is no type figureskater either, or even skater. the basic approach is
clearly to use query terms {figure, skater} and to look for person
in returned passages, but what is to be done about female? there are
two possibilities:

1. include female in the bag of words. this relies on logic that
if    femaleness    is an interesting property, it might well be
mentioned in answer passages. this is a reasonable hypoth-
esis for skating, given that such competitions are divided
by gender, but would not apply to other categories, such as
singer or writer. the real problem here is how is the system
to know the di   erence?

2. leave out female, but test candidate answers for gender. this
requires either an authority    le or a heuristic test (for exam-
ple, statistical co-occurrence counts with the various third-
person pronouns).

by examining these and other examples, we see that there is no
one-size-   ts-all solution to the problem of query formulation. there

2.5 general issues in qa 137

is clearly an interaction between question analysis and answer extrac-
tion activities, and these in turn depend on available resources such
as authority    les or speci   c tests, statistical or otherwise. even in an
open-domain setting, certain kinds of question terms (such as nation-
ality and gender, above) can be anticipated, and heuristics deployed to
handle them.

there is a certain phenomenon which comes to the aid of qa sys-
tem developers after the most likely cases have been taken care of.
through some combination of examination of training data, extrapola-
tion, and even educated guesswork, modi   ers such as the ones discussed
above would have been encountered and dealt with (if desired), in the
sense that machinery to process those cases will be incorporated in
the developers    systems. these modi   cations are those whose seman-
tics are known and which authors (correctly) expect not to have to
explain. now, despite even large amounts of training data, systems
will in practice encounter previously unseen concepts, but their very
unexpectedness would suggest that passages referring to them will be
correspondingly explicit. for example, suppose the system has the type
bird, but not flightlessbird. the question

what    ightless bird lives in new zealand?

can, as it happens, be easily answered since many articles about the
kiwi or takahe say that they are    ightless. therefore, if the modi   cation
is not recognized as being one of a few classes of terms that require
special treatment, default (i.e., bag-of-words) processing will usually
do just    ne.

2.5.4 redundancy

the current operational de   nition of question   answering requires
unique answers to factoid questions, but in the course of their pro-
cessing, systems encounter many answer candidates, and often in these
candidate sets there are repetitions. this directly leads to the question
of whether and how systems can take advantage of this recurrence,
known as redundancy, to improve the chances of returning a correct
answer.

138 overview of question   answering

redundancy is an extremely important phenomenon in the world
of qa [35]. it is implicit in the tf component of tf*idf, and many algo-
rithms reported here use weights proportional to occurrence counts.
the very nature of projection, wherein an answer found in one location
is looked for in the target corpus (described in section 4.6), bene   ts
from redundancy since an answer so found has by de   nition been found
in at least two places. in this section, we will mention some work which
has explicitly taken advantage of redundancy.

clarke et al. [13] performed some experiments in which they varied
a weighting factor in their answer extraction module to account for or
not account for the number of times a candidate appeared in retrieved
passages. they demonstrated that eliminating the redundancy factor
reduced the mean reciprocal rank score (see section 3.1) by 38%
and the number of questions answered correctly by 28%. they did
further experiments in which they demonstrated that redundancy could
compensate to some degree for overly simplistic information extraction
techniques.

prager et al. [58] show the bene   ts of redundancy in two forms.
their piquant qa system allows the use of one or several answer-
ing agents, which are in essence qa systems in their own right,
whose answers are combined in a voting module. two of the agents
were general-purpose (the predictive annotation (pa) and statistical
agents) and three were more narrowly focused, hence higher precision
when they were invoked, but with much lesser coverage. they demon-
strated an improvement of 60.2% when all agents ran together over the
baseline pa agent.

in the same paper they described qa-by-dossier-with-constraints
wherein answers can get supported by asking di   erent questions. this
is discussed further in section 4.8.1.

magnini et al. [40] use redundancy directly to inform a statisti-
cal algorithm to perform answer validation. they generated candidate
answers {ai} to a question q, and (after stop-word and other low-
importance term removal) used the altavista search engine to calcu-
late the number of pages matched for the three queries:    q,       ai    and
   q near ai.    using any of three di   erent metrics, namely, mutual
information, maximum likelihood ratio, and corrected conditional

2.5 general issues in qa 139

id203 they were able to establish a threshold below which answers
were rejected. using approximately equal numbers of participants    cor-
rect and incorrect answers to trec2001 questions as input, their algo-
rithm was able to get performance in the region of 80%, whether mea-
sured by precision, recall or success rate.

while this approach does not help much in    nding the correct can-
didates in the    rst place, it is able to use redundancy to most often
validate correct candidates. this would be valuable in a context (unlike
trec) where credit is given for withholding answers rather than giving
wrong ones.

2.5.5 component performance

as stated earlier, and as we shall see in chapter 4 when we look at spe-
ci   c systems, qa employs many of the techniques and methodologies
of the    elds of ir, nlp, ml, and kr, as well as statistical techniques.
it is observed that what one does not    nd in the qa literature are
assertions that the best or most state-of-the-art of these technologies
is required for decent qa performance. that is not to say that bet-
ter performance will not    ow from better pos taggers, parsers, ners,
search engines etc., but that it is usually found that bigger gains can
be found by either trying to solve new problems, or old problems in
completely di   erent ways.

to take an example from the author   s own experience, a failure
analysis was performed on a run of the 500 questions from trec10.
it was determined that 18 failures could be attributed to named-entity
recognition errors. fixing these failures so that ner performance was
perfect could give a 3.6% absolute increase in system accuracy (we will
discuss metrics in chapter 3). however, this does not mean the named
entity recognition was at 96.4%     actual values averaged around
80%   90%. the exact quantitative relationship between qa component
and system performance is currently little understood.

3

evaluation

qa di   ers from ir in its requirements for an adequate response. to sim-
plify a little, in ir the more retrieved documents that are on-target the
better, where on-target means that the document is at least partially
   about    the topic in the query. in qa, on the other hand, a single
correct answer, backed up by a justifying document, is considered a
su   cient response.1 for factoid questions, which we will consider    rst,
recall is not an issue, except in the degenerate sense (was the correct
answer found at all). by contrast, for list questions it is very relevant.
in this chapter, we follow the qa evaluation regimens developed and
used by nist in the trec evaluations.

to evaluate qa systems automatically, without any human inter-
vention, would require a program which could take a proposed answer
and tell if it correctly answered the question. this is precisely the
job of answer extraction, so successful unsupervised evaluation would

1 to illustrate with a particular example, the answer to    what is the boiling point of gold?   
was found in an article on tin by the author   s qa system applied to an encyclopedia, in a
sentence where the two boiling points were compared. this was the only mention of gold
in the entire article. there were many articles which mentioned    boiling point    and    gold   
several times, but not in this relationship; these non-responsive articles were at the top of
the hit-list when a regular search engine was used.

140

3.1 factoid evaluation

141

require solving the very problem it is evaluating. in practice, what
happens is that human assessors prepare lists of answers, from their
own research and/or from inspection of returned answers, and evaluate
system responses with respect to those lists.

these same lists, or variants of them, are used by system develop-
ers to automatiscally evaluate their systems, for purposes of regression
testing and quanti   cation of improvements. this method works very
well for evaluating previously discovered answers, which will be on the
answer list, or di   erent instances of the same answers, which will match
too. the problem comes when systems discover either di   erent correct
answers, or di   erent lexicalizations of known answers, where the pat-
terns in the answer keys do not anticipate all possible correct variants.
if a developer spots these occurrences, he can extend his answer keys
accordingly, but this immediately makes comparisons with other sys-
tems invalid, unless the updated answer key is shared. one can imagine
the same or similar technology that is employed in answer extraction or
answer merging to compare two terms for identity to be used in evalu-
ation scripts, although issues of precision and exactness remain:    john
smith    and mr. smith    may be the same person, but are they both
acceptable as answers? however, this will never solve the recall prob-
lem of section 2.1.1 (systems discovering yet another prime minister
of japan).

3.1 factoid evaluation

factoid answers can be evaluated on three dimensions:

d1: how far down the answer list does one have to go to    nd a

correct answer?

d2: is the answer precise enough to be considered correct?
d3: is the answer supported (meaning, accompanied by a vali-

dating document)?

we will examine here how factoids have been evaluated in trec
over the years. the trend has been monotonically toward higher preci-
sion, in all dimensions.

142 evaluation

for d1, from trec-8 to trec2001, answers were evaluated using
the mean reciprocal rank (mrr) metric, over 5 answers. systems
returned their top 5 answers to each question, in decreasing order of
system con   dence. if the top answer was correct, it got 1 point; if the
top answer was incorrect but the second was correct, it got 1/2 point,
and so on down to 1/5th point if the    fth answer was the    rst correct
one in the list. otherwise the score was zero. after trec2001, only
one answer was allowed, and this would get a score of 1 or 0. the score
for the whole test set was the simple average of individual question
scores.

for d2, in trecs -8 and-9, there were two    tasks,    in which the sys-
tems were to return their answers as 50- and 250-byte segments of text.
if the segment wholly contained a correct answer, it was considered cor-
rect. the 250-byte task had been introduced as a low-entry-barrier task
for ir-oriented systems, since it was not required to pinpoint the answer
very exactly. this option was dropped in trec2001. from trec2002
on, the 50-byte requirement was changed to    exact-answer,    which
meant that answers were to be complete, but not include any extrane-
ous material. unfortunately, whether extra modi   ers are extraneous or
not can be highly subjective. for example, the correct exact answer to
1395:    who is tom cruise married to?    was nicole kidman. actress
nicole kidman was marked as inexact, although it was arguably a bet-
ter answer. kidman too was marked as inexact, which can make sense
if there is an umbrella ban on last-name-only answers, or if there is
some systematic correlation of allowable shortened names with fame or
recognizability (but neither was the case).

lin et al. [38] show through user studies that users prefer paragraph-
sized text chunks over exact phrases. this does not negate the utility
of exact-answer evaluations, since clearly any system able to return an
exact answer can return its enclosing passage if the user is the imme-
diate consumer, but if the answer is to further processed by another
module in a larger system, then exact answers can be important.

for d3, responses had to be accompanied by a supporting document

(that is, its id) from trec-9 onwards.

after each evaluation, nist and/or some trec participants made
available to the community answer-keys. these are lists of perl patterns

3.1 factoid evaluation

143

which match correct answers, and which can be used by evaluation
scripts available from nist (see section 3.6). these answer keys con-
tinue to provide a great bene   t to the community since they allow
teams to continue to re-evaluate their systems as development pro-
gresses. these keys do not incorporate lists of responsive documents,
so the d3 dimension is not evaluated.

3.1.1 supporting documents

in trec, for a correct answer to a factoid question to be judged cor-
rect if it must be associated with a responsive or supporting document.
most further evaluation that is done by researchers using trec ques-
tions sets uses the community answer sets, but as mentioned, these do
not test for whether the extracted answer is from a supporting docu-
ment. thus care must be taken in comparing later results with trec
snapshots.

for a question to be supported, the given document must not only
contain the answer, but contain it in a context from which it is clear
that it answers the question. in most cases there is no dispute or ambi-
guity with this requirement, but this is not guaranteed to be always the
case. the hidden assumption with this requirement is that of all the
information necessary to answer a question, most of it can be assumed
to be    general knowledge    which    everybody knows,    and that there
is only one missing property or relationship, which the supporting doc-
ument supplies.

3.1.2 what is a right answer?

voorhees and tice [77] examined the evaluation process in trec
and found a 6% disagreement between assessors on the trec-8 task.
despite this, they demonstrated that system rankings in the trec
tasks were quite stable. this is comforting for those worried about rel-
ative performance, but the possibility of (at worst) errors or (at best)
di   erences in opinion in the answer keys is of concern to those who use
the keys for further training and testing.

the move from n-byte snippets to exact answer has ameliorated
at least one problem with evaluation     that of answers which were

144 evaluation

too exact to the point of incorrectness. as voorhees and tice point
out, an answer pattern    n\.h\.    for    where is dartmouth college?   
will incorrectly match    plain   eld, n.h.    (the correct town is hanover),
while the exact pattern      n\.h\.$    will not. (an exact answer key
would presumably also include another pattern with hanover.)

for purposes of trec, if a document asserts a fact, then it is
correct, regardless of    reality.    this clearly simpli   es the evaluation
process, but it is somewhat inconsistent with tacit user models and
requirements of systems to have some world knowledge in order to
determine what information elements supporting documents need to
support. furthermore, a correct answer is sometimes not acceptable in
trec, because of meta-rules regarding fame and originality imposed
for the evaluation     the correct and acceptable answer to

73: where is the taj mahal?

depends on which taj mahal is being referred to, which clearly is a
function of the user. this and other issues related to user models are
discussed in chapter 5.

3.1.3 what is a right question?

by inspection of the question sets in the    rst several years of trec-
qa, the author estimated 1%   2% of the questions su   ered from
spelling, punctuation or other errors, despite hopeful claims from nist
that the questions would be error-free. there is a strong argument that
systems should be made to be somewhat immune to such errors, since
they occur plentifully in real-life, so these slips in question-set genera-
tion are actually a good thing. however, there are some cases that pose
a problem.
for

1511: who said    music hath charm to soothe the savage beast   ?

we note that the only responsive document in the collection has the
passage:

william shakespeare said    music hath charms to soothe the sav-
age breast,    and he was right.

3.2 list evaluation

145

let us ignore the missing    s    on charm, since systems that lem-
matize or stem will not have a problem with this (although it is an
interesting question whether words inside quotes should be so treated,
for qa). the missing    r    in breast is most likely not a typological
error but a misconception on behalf of the questioner, since that error
is a common mis-quotation. so by that token the correct answer is
   nobody    (or even    everybody   ). to make matters worse, the (cor-
rect) quote is actually by william congreve, so the document author
was wrong too!

sometimes the errors are in the form of slipshod speech that most
people can mentally correct, since there is only one reasonable inter-
pretation. consider:

68: what does el nino mean in spanish? (sic)

a literal interpretation concludes this question is asking for    el
nino    to be translated into spanish, rather than:    what does the
spanish phrase    el nino    mean (in english)?    a too-clever answer
to the original question is    el nino,    but one can imagine a literal-
minded qa-system with a complement of bilingual dictionaries having
a problem since it will be unable to    nd the phrase in any non-spanish
vocabulary list.

3.2 list evaluation

in the trec context, list questions are factoids for which multiple
answers are expected. in trec2001 and 2002, list questions provided
the target number of responses, e.g.,

l11: name 10 di   erent    avors of ben and jerry   s ice cream.

the score per question was the number of distinct responses divided
by the target number. subsequently, no target was given, e.g.,

2207: what chinese provinces have a mcdonald   s restaurant?

given the number of known answers, the number of correct distinct
responses returned by the system and the total number of responses
returned by the system, instance precision and instance recall, and from
these an f-measure, can be computed in the regular way.

146 evaluation

3.3    other    evaluation

a trec    other    question is in the form of a target, which is usually a
person, organization, event or thing (see figure 3.1 for some examples),
and systems must supply textual passages that contain properties of
the target that the assessors consider to be important. the assessor has
a list of nuggets, compiled both from his own knowledge and research,
and from inspecting the pooled responses. recall is computed from how
many of the assessor   s nuggets are conceptually present in the returned
text; no simple string-matching is employed: arbitrary id141 is
acceptable, as long as the concept is represented. precision is hard to
evaluate, so it is approximated from the inverse of the length of the
response.

speci   cally, the assessor marks his own nuggets as either vital or
ok. for example, for #3 hale bopp,    one of brightest comets of 20th
century    was considered vital, while    cult members expect to ren-
dezvous with ufo trailing hale bopp    was only ok; the complete ref-
erence nugget list for this target is given in figure 3.2. system nuggets

1:  crips
2:  fred durst
3:  hale bopp comet
5:  aarp
66:  russian submarine kursk sinks
95:  return of hong kong to chinese 
sovereignty

fig. 3.1 sample targets from trec2004/5.

cult members expect to rendezvous with ufo trailing hale bopp

3.4 1 ok
3.4 2 vital one of brightest comets of 20th century
3.4 3 vital discovered by hale and bopp
3.4 4 ok
3.4 5 vital 5 to 10 times the size of haley   s (sic) comet

total solar eclipse coincides with hale bopp arrival

fig. 3.2 nugget list for hale bopp   s    other    question.

3.3    other    evaluation

147

that, correspond (or not) in meaning to the assessor   s nuggets get cat-
egorized as vital, ok, or not ok. it is the fraction of vital
nuggets returned that determines recall. for precision, the number of
vital and ok nuggets returned is totalled and multiplied by 100 (a
so-called byte-allowance), and then divided by the total text-length in
bytes for all nuggets returned for that question. a result greater than
1 is reduced to 1.

from these precision and recall values, an f measure is evaluated.
for trec2003, a    of 5 was used, for trec2004 and trec2005 a   
of 3, where

f(  ) = (  2 + 1     precision     recall)/(  2     precision + recall)

a    greater than 1 favors recall over precision.

3.3.1 automatic de   nition question evaluation

following the success of id7 for automatic machine translation eval-
uation [51] and id8 for id54 evaluation [34],
lin and demner-fushman [36] introduced pourpre for automatic
nugget evaluation.

the pourpre method uses a simple count of word overlap
between a system and an assessor   s nugget, divided by the number
of words in the assessor   s nugget. this method was shown to have very
high correlation (mid-90s r2 correlation) with o   cial scores. attempt-
ing to re   ne the match score by weighting terms with idf values showed
a decrease in correlation.

despite the very high demonstrated agreement with human assess-
ment, caution should be exercised in using this method outside of the
trec framework, or any other framework that uses the same evalua-
tion methodology. pourpre relies on similar wording between system
and assessor nuggets; this is a fragile dependence, since vocabulary and
paraphrase di   erences are notorious for presenting nlp systems di   -
culties in assessing identity or equivalence. the apparent reason these
di   erences have not hurt pourpre   s accuracy in trec is that the
assessor nuggets were fashioned, in part, by reference to the pool of
returned system nuggets, plus the limited number of instances (hence

148 evaluation

limited lexical variations) of good nuggets in the target corpus. it has
not yet been measured how well pourpre will do when assessor
nugget lists are developed independently of the system responses, but
it is suspected that we will need a metric of a di   erent couleur.

3.3.2 pyramids

lin and demner-fushman [37] demonstrated that the so-called    pyra-
mid    method of nenkova and passonneau [50] for scoring summariza-
tions could be adapted to qa    other    questions. because of the highly
subjective nature of the vital-ok-not ok distinction for catego-
rizing nuggets, it was shown that by combining scores (votes, in e   ect)
from multiple assessors, a much more robust and reliable overall score
could be assigned. in trec2006, it was adopted as an additionally
reported measure.

in the pyramid scheme, the nuggets are evaluated by multiple asses-
sors (lin and demner-fushman used 9). each nugget is given an initial
weight equal to the number of assessors that deemed it vital. these
weights are then normalized by dividing by the maximum nugget weight
for its question. this gives each nugget a score in the range 0   1, with
1 being achievable without all assessors agreeing, as long as it had the
most agreement.

recall for a question is simply calculated as the sum of the nugget
weights for all of a system   s returned nuggets, divided by the sum of
the nugget weights for all reference nuggets. the precision calculation
is unchanged.

3.4 no answer evaluation

questions with no answer in the corpus do not present any special
problems for evaluation. the convention is that such questions return
the pseudo-answer nil (and no supporting document), and that answer
keys contain the string nil.

3.5 con   dence evaluation

ostensibly, the score a system assigns to its answers represent the sys-
tem   s con   dence in the answers. after all, the score is a function of

3.5 con   dence evaluation

149

the various features that are evaluated to measure the degree of match
of the answer in its passage with the question, so at the very least the
scores order the candidate answers according to con   dence. ideally, the
score would represent a id203 (maybe scaled by a factor of 100
or 1000 for cosmetic reasons), namely the id203 that the answer
is correct. if that is done, then a score of 0.7 (for example) means
that 70% of answers with that score will be correct. it is, however, not
easy to reliably associate real probabilities with answers     witness the
lack of any kind of score given with popular present-day web search
engines. this is presumably because the calculations that are used to
produce the score do not have a complete (or any) id203-theoretic
basis.

suppose you have two qa-systems that get answers right 50%
of the time. system a gives every answer a score of 0.5, whereas
system b gives the answers it gets right a score of 1.0, and those it
gets wrong a score of 0.0. system b is clearly the preferable one to
use, since one can be con   dent in those answers given with a score
of 1.0, and for the others one knows to look elsewhere. with sys-
tem a, one does not know what to think. in fact, system b might be
preferable even if its accuracy were less than 50%, as long as its associ-
ated scores were accurate predictors of correctness. this is an extreme
case, perhaps, but it indicates that the con   dence in an answer can
be just as critical to the system   s usefulness as the system   s inherent
accuracy.

in trec2002, one task was for systems to return their answers
to the 500-question set rank-ordered by the systems    con   dences in
them. the absolute values of the scores that the systems used to
perform this ordering were ignored, but the relative values deter-
mined the order, and hence some measure of implicit relative con-
   dence. the measure used was the con   dence weighted score
(cws), which computes average precision. for an ordered set of n
answers (most con   dent    rst), given correctness judgments, cws is
computed by

cw s =

n(cid:1)

i=1

1
n

# correct up to question i

i

.

150 evaluation

this formula has some very interesting properties. first, we observe
how it is weighted heavily in favor of the earliest entries in the list:
    the score for answer#1 participates in every term in the
summation
    the score for answer#2 participates in all but the    rst . . .
    the score for answer#n participates in only the last

the contribution to the cws sum by answer k as a function of k is
given in figure 3.3, where the total number of questions/answers, n,
is 500.

let us denote the contribution of the kth answer as ck. then the

cws formula yields the recurrence relation:

ck = ck+1 + 1/kn

cn+1 = 0

from which we can get upper and lower bounds for ck:

(cid:2)

(cid:3)

(cid:2)

(cid:3)

ln

n + 1
k + 1

    n ck     ln

n
k

+

1
k

.

suppose a system gets n answers correct out of n. given that the
earlier an answer is in the ranking, the more it contributes, it is clear
that the highest score the system can get is obtained if the    rst n
answers it returns are the correct ones, followed by all the incorrect

fig. 3.3 contribution of correct answer plotted against rank position.

3.5 con   dence evaluation

151

ones. this is a non-issue for the boundary cases of n = 0 and n = n,
but is especially important for middle values of n (see figure 3.4).

the upper curve shows the maximal score possible for the given
number of questions correct (i.e., assuming ordered optimally), and
the lower curve shows the minimal score. the most dramatic di   erence
occurs when half the questions are correct. if all of the correct questions
lead the ranked ordering (i.e., rrrrrr . . . wwwww), the cws
score will be 0.85; in the worst case, where all the incorrect questions
are put    rst (i.e., wwwwww . . . rrrrrr), the score will be 0.15.
the diagonal line and    cloud    in the center of figure 3.4 repre-
sent what happens with no attempt to sort the results. the solid
line in the center of the cloud is the ideally-uniformly-distributed
case (i.e.,
if 1/3 of the answers are right, the submitted list
goes . . . rwwrwwrwwrww . . .), and the cloud is a simulation of

i

i

n
o
s
c
e
r
p
 
e
g
a
r
e
v
a

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

1

questions correct

500

fig. 3.4 average precision (cws) as a function of questions correct.

152 evaluation

randomly-distributed rights and wrongs, given the number of correct
answers. the width of the cloud approximately represents a 3-standard-
deviation spread.

if a system does no sorting by con   dence (or if it tries, but the
answer scores are in no way correlated with correctness), the expected
cws score is n/n. on the other hand, the maximum score possible
(the upper curve in figure 3.4 is approximately:
ra = (cws score     expected score)/(max score     expected score)
ra is 1 on the upper curve,    1 on the lower curve, and 0 on the diag-
onal. in trec2002, the best ranking ability of participating teams
was 0.657. interestingly, the team with the best accuracy results had
a very mediocre ra score. the top 15 submissions, sorted by ra, are
presented in table 3.1, from [11]. there is no obvious correlation of ra
with accuracy.

one can imagine a    utility    measure for a qa system based
on both accuracy and ranking ability (maybe their product),
but to be properly motivated there would need to be extensive
user testing in real-life situations of
information need, to deter-
mine the appropriate balance of and interaction between these
factors.

table 3.1 ranking ability of top 15 submissions to trec2002.

cws % correct ranking ability
submission
limsiqalir2
0.497
ibmpqsqacyc 0.588
0.499
bbn2002c
nuslamp2002
0.396
0.589
irst02d1
0.498
isi02
0.434
fdut11qa1
0.455
ibmsqa02c
exactanswer
0.691
0.450
ilv02wt
0.512
uwmtb3
0.496
ali2002b
0.433
aranea02a
lccmain2002
0.856
0.610
pris2002

0.657
0.627
0.603
0.569
0.559
0.555
0.539
0.461
0.449
0.392
0.392
0.365
0.35
0.168
0.095

26.6
35.8
28.4
21.0
38.4
29.8
24.8
29.0
54.2
30.8
36.8
36.2
30.4
83.0
58.0

3.6 evaluation resources

153

3.6 evaluation resources

while the evaluation paradigms described here may provide an under-
standing of how rankings in trec are determined, they are maybe
more important to developers as incremental guides in the development
process. furthermore, published evaluation results are more meaningful
when common test sets and common evaluation measures are used. we
will therefore mention here a number of resources that may be useful
to the reader.

the main trec-qa resources page

on the nist site
http://trec.nist.gov/data/qamain.html provides a number of useful
   les. there are the question sets used in previous trec evaluations,
along with answer keys (earlier ones developed by nist, later ones by
ken litkowski), and evaluation scripts. there are also so-called judg-
ment    les, which are aggregates of (anonymized) answer submissions
by trec participants along with assessor   s judgments.

amongst other uses, the judgment    les can provide information
regarding near-misses. the site also contains descriptions of tasks, lists
of top documents, and lists of nuggets for de   nition questions.

a tool to evaluate de   nition and relationship questions has been
made publicly available by gregory marton at mit.2 scripts to evalu-
ate pourpre have been made available by jimmy lin at u. maryland.3
the same page also o   ers lin   s entire web-based aranea question   
answering system, as well as some other data and utilities.

3.7 some trec details

table 3.2 lists the teams/systems that have placed in the top 10 in the
main qa task since the trec8 qa in 1999, in decreasing order of
score in the main task.

3.7.1 trec corpus

many of the system and algorithm evaluations reported here were on
what is loosely known as the    trec corpus,    which has in fact changed

2 http://people.csail.mit.edu/gremio/code/nuggeteer.
3 http://www.umiacs.umd.edu/   jimmylin/downloads/index.html.

154 evaluation

.
s
r
a
e
y

e
h
t

-

r
e
v
o
a
q
c
e
r
t
n

i

s
r
e
m
r
o
f
r
e
p

p
o
t

2
.
3

e
l

b
a
t

.

h
c
e
t

.
t
s
n
i

n

i

b
r
a
h

d

l
e
   
e
h
s

.

u

i

t
m

y
n
a
b
a

l

.

u

.

u
n
a
d
u
f

a
s
n

m
b
i

,
s
e
l
a

w

.

u

r
o
g
n
a
b

m
b
i

.

u
d
n
a
l
r
a
a
s

.

u
n
a
d
u
f

d

l
e
   
e
h
s

.

u

t
s
r
i
-

c
t

i

i

t
m

e
r
o
p
a
g
n

i

s

c
c
l

.

u
n

.

e
r
o
p
a
g
n

i

s

c
c
l

.

u
n

.

e
r
o
p
a
g
n

i

s

c
c
l

.

u
n

.

m

-
t
f
o
s
t
h
g
i
s
n
i

c
c
l

5
0
0
2
c
e
r
t

4
0
0
2
c
e
r
t

3
0
0
2
c
e
r
t

2
0
0
2
c
e
r
t

1
0
0
2
c
e
r
t

e
n
o
l
c
i
x
e
l

e
r
o
p
a
g
n

i

s

.

u
n

.

i
s
i
-

c
s
u

n
b
b

i

t
m

y
n
a
b
a

l

.

u

t
s
r
i
-

c
t

i

m
b
i

t
s
r
i
-

c
t

i

m
b
i

o
o
l
r
e
t
a

w

.

u

s

m
e
t
s
y
s
o
r
c
i

m
n
u
s

,
e
g
e
l
l
o
c

i
s
i
-

c
s
u

i
s
m
i
l

n
b
b

t
f
o
s
o
r
c
i

m

b
-
m
b
i

a
-
m
b
i

.

u
e
s
u
c
a
r
y
s

e
t
n
a
c
i
l

a

.

u

t
t
n

t
f
o
s
t
h
g
i
s
n
i

e
l
c
a
r
o

c
c
l

o
o
l
r
e
t
a

w

.

u

i
s
i
-

c
s
u

9
c
e
r
t

o
o
l
r
e
t
a

w

.

u

i
s
i
-

c
s
u

u
m
s

a
-
m
b
i

b
-
m
b
i

s
n
e
e
u
q

y
n
u
c

y
n
o
h
p
m
y
c

8
c
e
r
t

t
&
t
a

u
m
s

e
p
o
r
u
e
x
o
r
e
x

d
n
a
l
y
r
a
m

.

u

m
b
i

o
c
i
x
e
m
w
e
n

.

u
e
t
a
t
s

e
r
t
m

i

t
t
n

.

u
d
n
a
l
r
a
a
s

.

u
a
e
r
o
k

.

u
n
a
d
u
f

e
t
n
a
c
i
l

a

.

u

e
g
e
l
l
o
c
s
n
e
e
u
q

e
p
o
r
u
e
x
o
r
e
x

s
t
t
e
s
u
h
c
a
s
s
a
m

.

u

y
n
u
c

3.7 some trec details

155

over the years. in trec8 the corpus for qa contained documents from
the la times, the financial times, fbis, and the federal register,
a total of 528 k documents occupying 1.9 gb. in trec9/trec2001
the federal register documents were dropped, but others from the ap
newswire, the wall street journal and the san jose mercury news were
added, totaling 979 k docs in 3 gb. in trec2002 onwards the corpus
used was what is called the aquaint corpus, consisting of associated
press and new york times newswire and the english portion of the
xinhua news agency. these numbered 1.033 million docs and again
were 3 gb in size. more details are given in [76].

4

speci   c approaches

we discuss in this chapter some of the better-performing and more
in   uential approaches that have been reported in the literature. we
group these approaches via their primary characteristics, thus we have
(1) web, (2) pattern, (3) index-augmentation, (4) statistical, (5)
logic, (6) projection, (7) de   nitional, and (8) event-based. we    nish
with a discussion of questions with no answer. we readily acknowledge
that these approaches are not mutually exclusive and that most if not
all systems use a combination of the above. the grouping, therefore,
is not so much to categorize the systems themselves but rather the
signi   cant components of the systems that are being speci   cally dis-
cussed. for example, the microsoft   s askmsr system is a web-based
system that uses patterns and projection; it is discussed in the section
on pattern-based systems, since that is where it makes the most inter-
esting contributions. we will begin with web-based approaches, since
they are conceptually the simplest.

4.1 web-based qa

web-based approaches use the web as the principal (or only) resource
for answers, and web search engines to search it. web-based qa

156

4.1 web-based qa 157

systems in e   ect provide a wrapper around the search engine, for two
reasons.

    web search engines take as input a set of keywords instead
of a natural language question. they can technically accept
natural-language questions, but they treat them as collec-
tions of keywords.
    web search engines return lists of documents, not answers,
although they do usually show document extracts where the
search terms are concentrated.

thus web-based qa systems take in natural-language questions
which they transform to keyword queries, and they post-process the
returned documents to locate answers. example systems of this kind
include those of [5, 31, 64, 66].

the process of generating a query for the web search engine is not
too di   erent from the question processing stages of non-web qa sys-
tems, since they too employ search engines that require keyword lists.
systems have the burden of adapting to the particular syntax used by
the search engine of choice: there may be issues of expressive power
(does the search engine allow arbitrary boolean expressions, for exam-
ple), individual term weighting, presence or absence of adjacency oper-
ators and so on. however, one advantage of the web over newswire
corpora, especially for questions mined from web search engine logs, is
that the answers are usually replicated very many times using many
di   erent syntactic forms. this makes it not especially critical how the
web query is formulated. the major challenge, though, lies in answer
extraction.

4.1.1 phrase reranking in nsir

the nsir system of [64] uses a probabilistic phrase reranking algorithm
to promote the correct answer toward the top of the list. they    rst
formulate a web query out of the question, and submit it to one of three
search engines (alltheweb, northernlight, and google). sentences in
the returned documents are ranked either by an id165 or vector
space model. each is given a sentence score.

158 speci   c approaches

the documents are chunked into phrases; each phrase is a potential
answer. the phrases are given an initial score, as described below, and
then reranked from consideration of the sentence scores.

first, a proximity score is assigned to each phrase. phrases con-
taining or close to most query terms get the largest such scores. next,
phrases are given a signature, which is a representation of the sequence
of parts-of-speech in the phrase. thus the signature of    alan shepherd   
is {nnp nnp}. they de   ne a set of 17 qtypes (answer types). for each
one, training material gives the a priori values for

pr(qtype | signature)

question analysis produces the top two most likely qtypes (say qt1
and qt2); the qtype score for a phrase with signature sig is then

pr(qt1 | sig) + pr(qt2 | sig)

the phrase score is then calculated as the product of the proximity and
qtype scores. finally, the phrase list is    ltered to remove any that are
not found in the top 50 sentences via the sentence score.

4.2 pattern-based qa

pattern-based approaches are very common in qa, and they occur
in named-entity recognition, search, and answer selection. these
approaches are popular because they emphasize shallow techniques
over deep nlp, are data-intensive and can be trained with minimal
supervision (human judgments).

we will look here at three successful systems that use patterns heav-

ily and in interesting ways:

    the textmap system of usc-isi.
    insight.
    microsoft   s askmsr.

singapore   s soft patterns are used in de   nition questions, and are dis-
cussed in section 4.7.5.

4.2 pattern-based qa 159

4.2.1 usc-isi   s textmap system

the usc-isi textmap system [24, 67] employs surface text pat-
terns to identify answer-bearing snippets of text. the insight behind
the idea is that questions are often about properties of objects,
and that certain relationships (i.e., object-property) are expressed
in regular ways in text. these regular ways are captured by sur-
face text patterns. what makes the idea work is that these patterns
are in general
independent of the speci   c object in the ques-
tion. a similar approach is employed in the qitekat system at
bangor [14].

there is no requirement that there be only one pattern per relation-
ship, but rather that the pattern-gathering methodology should acquire
a su   cient number of these patterns to ensure adequate coverage with
unseen test data. consequently, this technique could also be considered
to be a redundancy-based approach.

consider the simple relationship question type:

when was x born?

inspection of text corpora discovers text snippets such as:

mozart was born in 1756

gandhi (1869-1948)

in those syn-
there is nothing speci   c about mozart or gandhi
tactic patterns, which can therefore be represented by generalized
expressions:

(cid:1)name (cid:2) was born in (cid:1)birthdate (cid:2)
(cid:1)name (cid:2) ((cid:1)birthdate (cid:2) -

these patterns can be learned, and textmap uses a id64 pro-
cedure to do this. for an identi   ed question type (such as when was
x born? ), they start with some known answers for some values of x:

mozart 1756
gandhi 1969
newton 1642

160 speci   c approaches

web search queries are then issued with these as query terms. the
top documents are retrieved (they use 1,000). these documents are
post-processed (tokenized, de-tagged, etc.). they then use a su   x tree
constructor to    nd the best substrings, e.g.,

mozart (1756-1791)

and    ltered down to produce

mozart (1756-

finally, patterns are generated by substituting in variables (e.g.,
(cid:1)name (cid:2), (cid:1)answer (cid:2)) for the query strings.

the precision of each pattern so generated can easily be determined.
first, documents are retrieved using just the question term (mozart).
the patterns are applied (they will not match in most cases), but for
those that do, the precision can be calculated. patterns can then be
rejected if the precision falls below a threshold.

the outcome of this is a mapping between question type and pat-

tern set.

having trained the system, the following procedure is used to    nd

answers to unseen questions:

    determine the question type
    perform the ir query (all terms are pre   xed with the plus-
sign indicating    required    and submitted to altavista)
    do sentence segmentation and smoothing
    replace the question term by a question tag
    search for instances of patterns associated with the question
type.
    select words matching (cid:1)answer (cid:2)
    assign scores according to the precision of the pattern, fre-
quency of the answer.

i.e., replace mozart with (cid:1)name (cid:2)

4.2.2

insight

the insight system [70, 71] performed very well in trec2001 and
trec2002. like textmap, their system uses patterns     they call

4.2 pattern-based qa 161

theirs indicative patterns     which match answer contexts. for exam-
ple, the pattern:

cap word; paren; 4 digits; dash; 4 digits; paren

matches

mozart (1756-1791)
these patterns are broader than just named-entities in order to
capture the context in which the sought answers occur. their claim
is to achieve    semantics in syntax,    which they demonstrated for
the factoid questions typical of trec-qa, but it is nevertheless
unclear how much further this approach can be pushed for more gen-
eral qa. the approach relies on the redundancy of large corpora,
as all pattern-based methods do. it is not clear how they match
questions to patterns, or how named entities within patterns are
recognized.

one interesting contribution of the work is the explicit recognition
of the phenomenon they call zeroing     the e   ect of constructs in the
vicinity of potentially matching patterns which disqualify the match.
for example, in the case of seeking subjects of occupational roles, mod-
i   ers such as
    former
    elect
    deputy

will take the candidate out of consideration. so, in the right contexts,
will the word not (see section 5.3.3). the insight work is intriguing
since they performed well in trec using what appear to be straight-
forward techniques, but nobody has reported that they have been able
to replicate the results.

4.2.3 microsoft askmsr

the microsoft askmsr system is a pattern-based system which uses
the web and projection, so could equally well be covered in those other
sections; however, its major contribution to the    eld is its innovative
use of patterns, so it is described here.

162 speci   c approaches

the approach is an implementation of data-intensive qa, advo-

cated in [5]. they state as motivation:

overcoming the surface string mismatch between the
question formulation and the string containing the
answer.

their approach is based on the assumption/intuition that someone on
the web has answered the question in the same way it was asked.
because of the expected similarity between the surface forms of ques-
tion and answer, they claim that by using a huge corpus they can avoid
dealing with:

    lexical, syntactic, semantic relationships (between q & a)
    id2
    synonymy
    alternate syntax
    indirect answers

given a question, askmsr generates a set of queries that attempt to
capture possible declarative forms of the question. the set is overgen-
erated, since no consideration is given to how likely the rewrite is; if
it does not match anything in the web, no harm is done. so for the
question

what is relative humidity?

they generate the following rewrites1:

["+is relative humidity", left, 5]

["relative +is humidity", right, 5]

["relative humidity +is", right, 5]

["relative humidity", null, 2]

["relative" and "humidity", null, 1]

these are triples where the    rst component is the web query, the second
says which side of the query fragment the answer is to be located, and

1 the    +    is presumably to prevent stopwords from being dropped.

4.2 pattern-based qa 163

the last is the weight or con   dence of the particular pattern. note
that rather than relying on possibly error-prone parsing to drive the
placement of    is    in the declarative search phrase, they generate all
permutations.

the algorithm in askmsr consists of 5 steps:
    get the top 100 documents from google for each query
rewrite
    extract id165s from document summaries
    score id165s by summing the scores of the rewrites it came
from
    use tiling to merge id165s
    search for supporting documents in the trec corpus (pro-
jection)

the tiling operation constructs longer id165s out of shorter ones, so
that    a b c    and    b c d    are combined into    a b c d.    the weight
of the longer one is the maximum of the weights of its constituents.

the e   ectiveness of the approach is illustrated by
1340: what is the rainiest place on earth?

which was reformulated to match

x is the rainiest place on earth or the rainiest place on earth is x
askmsr retrieved the correct answer    mount waialeale    from the
web, and then located the following passage in the trec corpus.

. . . in misty seattle, wash., last year, 32 inches of rain fell.
hong kong gets about 80 inches a year, and even pago
pago, noted for its prodigious showers, gets only about 196
inches annually. (the titleholder, according to the national
geographic society, is mount waialeale in hawaii, where
about 460 inches of rain falls each year.) . . .

it is di   cult to imagine locating this passage directly with conventional
qa techniques, but a simple reformulation regimen and a large corpus
(plus projection to satisfy trec requirements) was clearly successful.

164 speci   c approaches

4.3

index augmentation based qa

we look in this section at approaches to qa which involve putting
extra information in the search-engine index, and corresponding extra
information in the queries issued to it. such approaches can be used
to increase both precision and recall, but maybe not at the same time.
we call this process index augmentation; it is related to but not the
same as document expansion [69], where extra terms from similar doc-
uments are in e   ect added to the document in question. with index
augmentation, no new literal terms are introduced, but rather type-
related information that is implicit in the document is realized in the
index.

we    rst look at the approach by ibm, called predictive annotation
in [54, 55], and then generalized to semantic search in [12]. in most
ir and qa applications, only corpus literals (words and sometimes
phrases) are included in the index. semantic search is the technique of
including any kind of extracted semantic information in the index; in
predictive annotation speci   cally the semantic types of corpus terms
are included.

4.3.1

indexing types     predictive annotation

the principal goal of predictive annotation is to make sure that pas-
sages retrieved by the search engine have at least one candidate answer
in them. consider what happens to a question beginning    when   
in question analysis. if the word is included in the query,
it will
probably either match instances as relative pronouns or as interrog-
ative pronouns, neither of which are helpful to answering the ques-
tion. if the word is removed, which will probably happen either as an
explicit rule or because the word is a stop-word, then there is noth-
ing in the query to direct the search-engine to    nd passages with
dates or times in them. the same argument goes for all question
phrases.

we saw in figure 2.4 that question analysis emits the answer type
which is used by answer extraction to locate candidate answers in pas-
sages returned by search. there is no a priori guarantee that such
passages will have any instances of the target type in them. to remedy

4.3 index augmentation based qa 165

this, in predictive annotation the corpus is processed by the system   s
named entity recognizer prior to indexing, and the semantic class labels
so generated which annotate the corpus literals are indexed along with
the literals. thus shakespeare is recognized as of type person, and so
the label person (suitably mutated so as not be confused with the literal
person), is indexed too.

ideally the labels are indexed at the same location as their cor-
responding base terms,2 but as a crude approximation they can be
inserted in-line: the cost is that this loses the ability to do    exclusive
matches,    described below.

predictive annotation adds two steps to the normal qa processing:

1. annotate the entire corpus and index semantic labels along

with text.

2. identify answer types in questions and include corresponding

labels in queries.

one advantage of predictive annotation is that it reduces the num-
ber of passages or documents that need to be returned from search.
some qa-systems return hundreds or even thousands of documents;
experiments described in [54] show that the optimum number of pas-
sages for the predictive annotation system under investigation was
around 10.

4.3.1.1 example

suppose the question is

244: who invented baseball?

the wh-word    who    can map to person or organization. suppose we
assume only people invent things (it does not really matter). question
analysis will construct the following query (here we add a    $    to the
end of a type name to make the string di   erent from any english word,
but any method of making the distinction can be used):

{person$ invent baseball}

2 that is, in the index the o   set for person will be the same as for shakespeare.

166 speci   c approaches

the corpus contains the passage:

. . . but its conclusion was based largely on the recollections
of a man named abner graves, an elderly mining engineer,
who reported that baseball had been    invented    by doubleday
between 1839 and 1841.

we can depict the annotations of a portion of this passage graphically,
as follows:

baseball

have

be

invent

by

doubleday

sport$

person$

this clearly will match well the query above. we note that baseball
has been annotated as a sport, irrelevant but also not distracting for
the current query. however, it does mean that the same structure is
equally e   ective at answering

what sport did doubleday invent?

via the query

{doubleday invent sport$}

4.3.1.2 xml syntax

in order to depict annotated text without the need for diagrams, we
can use an xml-based syntax, where tags are used to denote annota-
tions. this is the notation used in [12]. queries can also be represented
this way, corresponding to the xml fragments query language of [10].
thus

who invented baseball?

becomes

<person></person> invent baseball

and the text segment it matches (dropping stop-words) is

<sport>baseball</sport> invent <person>doubleday</person>

4.3 index augmentation based qa 167

4.3.1.3 exclusive match

terms and their annotations are indexed at the same o   set, and unless
care is taken both can unintentionally match query terms at the same
time. consider the question

what is the capital of france?

generating the query

<place></place> capital france

consider two candidate text passages

p1: paris is the capital of france

<place>paris</place> capital <place>france</place>

and

p2: he lived in the capital of france

live capital <place>france</place>

where both paris and france are annotated with place. we want the
query to match only passage p1, but both will match fully, since
in p2 the term <place>france</place> matches both france and
<place></place> in the query. by enforcing an exclusive match rule,
whereby a term and its annotation are prevented from both contribut-
ing to the passage score, p1 becomes the preferred match.

4.3.1.4 predictive annotation     improving precision

at no cost to recall

this discussion is illustrated with speci   c named-entity types used in
the author   s system, and occurrence counts found by the author   s sys-
tem, but similar types and values can be expected using other systems
too. consider the question:

202: where is belize located?

where can map to any of the answer types (continent, worldregion,
country, state, city, capital, lake, river . . . ) that are subtypes of place.

168 speci   c approaches

but we know from running our ner on the question that belize is a
country. if we use the heuristic that, to answer a where-is question that
asks about a place, we    nd a geographic region that encloses the place,
we can narrow down the set of possible answer types to continent and
worldregion.3 if we count occurrences in the trec corpus, we    nd:

    belize occurs 1068 times.
    belize and place co-occur in only 537 sentences.
    belize and continent or worldregion co-occur in only 128
sentences.

we only need to include the disjunction of those two types in the query.
by having to consider fewer sentences, there is less opportunity for
noise. in some cases, it makes the di   erence between having any or
no chance of success. let us assume the qa-system employs the type
zipcode. consider

what is the zipcode for the white house?

there are in the trec corpus about 25,000 documents mentioning
the white house. there are 15 that mention the white house and give
some zipcode, 4 of which are the actual white house zipcode. however,
there are none that mention both white house and the term zipcode
(or zip or zipcode). hence a system not using predictive annotation
must use just white house as the query, and must search on average
over 6,000 documents before encountering the correct answer. with the
query

+"white house" +<zipcode></zipcode>

only 15 documents are returned. experiments show that 4 of the top
5 documents contain the correct answer, and the redundancy factor in
answer extraction causes the correct answer to be given.

4.3.1.5 multiple annotations

when types are indexed, a decision must be made how to handle the
type hierarchy. consider new york city, which is of type city, which

3 a simple geographic part-of ontology can be constructed to organize these containment
relationships.

4.3 index augmentation based qa 169

is a subtype of place. for    what city . . . ?    questions we only want to
consider cities, which is achieved by annotating and indexing all known
cities as type city, and including that type in the query. but consider
an open    where    question like

where was john lennon killed?

we do not know in advance it was in a city. it could have happened on
an island, on a mountain-top, or anywhere. such open-ended questions
require using the most general type in the query, namely place. in order
that this match new york city (and all other places of all types), one
of three approaches must be used; these parallel the choices represented
in section 2.5.1 discussing recall:

1. the query is expanded to a disjunction of all sub types of

the desired type.

2. subsumption checking is done in the search engine.
3. all types are simultaneously annotated and indexed with all

their parent types.

choice #2 is ruled out for computational reasons. in [54, 55], since
disjunction in type sets was already being used to signify ambiguity
(for example, using a disjunction of person and organization for    who   
questions), choice #3 was implemented for cosmetic reasons. choices
#1 and #3 are equivalent in the sense that the same documents will be
matched, but there may be di   erences in search engine ranking scores,
depending on whether and how its query syntax allows weighting, and
how the search engine ranking algorithm handles disjunctions.

4.3.2

indexing relations

subtle di   erences in syntax can make enormous di   erences in seman-
tics, and in particular to the usefulness of a passage in question   
answering. katz and lin [29] give the examples shown in figure 4.1.

they argue that as long as nlp techniques are brittle, they should
be used selectively, and that less linguistically-informed techniques
might work better in general. that said, they identify two phenomena
that need linguistic information: semantic symmetry and ambiguous

170 speci   c approaches

the bird ate the snake.
(1)
(1   )
the snake ate the bird.
(2)
the largest planet   s volcanoes.
(2   )  the planet   s largest volcanoes.
(3)
(3   )
(4)
(4   )

the house by the river.
the river by the house.
the germans defeated the french.
the germans were defeated by the french.

fig. 4.1 examples of lexically similar text fragement pairs.

modi   cation. the former occurs when selectional restrictions overlap
(cf. #1 and 4 above; in #1 for example, eating requires an animate
subject and edible object, and birds and snakes are both), and the
latter when the restrictions are not restrictive enough (cf. #2, almost
anything can be large).

to address this problem, they generate from text, and index, ternary
relations. these are triples of the subject-relation-object kind, and so
represent syntactic relationships which can be extracted with high reli-
ability with a good parser. ternary relations are used in the start qa
system [27]. the ternary relations present in the fragments in figure 4.1
are shown in figure 4.2.

katz and lin describe experiments performed on a specially engi-
neered test-set to highlight the bene   ts of their approach. since the
relations were indexed, they could generate queries consisting entirely
of relations. the questions were of the form    who defeated the spanish

(1)
(1   )
(2)
(2   )

(3)
(3   )
(4)
(4   )

[ bird eat snake ]
[ snake eat bird ]
[ largest adjmod planet ]
[ planet poss volcano ]
[ largest adjmod volcano ]
[ planet poss volcano ]
[ house by river ]
[ river by house ]
[ germans defeat french ]
[ french defeat germans ]

fig. 4.2 ternary relations in examples in figure 4.1.

4.3 index augmentation based qa 171

armada?       what is the largest planet?,       what do frogs eat?,    which
could all be represented by ternary relations. they demonstrated a pre-
cision of 0.84 +/    0.11 with a relation query over 0.29 +/    0.11 for
a keywords-only query. in all these cases the question semantics could
be subsumed by the relations, by design. they did not address using
relations cooperatively with keywords, something that would have to
be done for more complex questions.

4.3.3 matching relations via similarity

one di   culty with the approach of katz and lin is that relation match-
ing is exact: although usual lexical id172 can be applied to the
subject and object arguments of their triples, the relations must match
exactly. cui et al. [16] address this by allowing for relations to match
approximately. although they do not index relations, this approach
is mentioned here since it is a natural extension of katz and lin, and
gives rise to interesting questions about indexing, which we will address
shortly.

cui et al.   s relation triples are like ternary relations, except that the
second argument is a relation path, rather than a relation. a relation
path between two parse-tree nodes n1 and n2 is an n-tuple, whose
terms are a series of edges which if traversed form the path between
n1 and n2. because it is di   cult to    nd in practice exact matches of
relation paths between question and answer text even when the other
arguments match, they allow for non-exact matching. by examining
about 2,500 relation path pairs from about 1,000 question   answer pairs,
they developed a pairwise similarity measure between relations, based
on mutual information and path-length; from this they develop two
path similarity measures.

they took the top 50 sentences from their passage retrieval mod-
ule, extracted candidate answers, and ranked them according to the
path similarity. they were able to achieve a factoid precision of 0.625.
it is unknown whether this approach su   ered at all from not indexing
relations. as follows from the discussion on indexing types earlier, not
requiring the desired type to be present in the retrieved document set
might result in a null candidate set, even with relaxation. the same

172 speci   c approaches

can happen with relations, as katz and lin point out. when relations
match approximately, some scheme needs to be worked out to enable
approximate matching in the search engine. one possibility is to index
with every relation every other relation that it is similar to, above
a threshold, similar to the multiple annotation indexing scheme of
section 4.3.1.5.

4.3.4

indexing semantic relations

as discussed in section 4.3.1, predictive annotation inserts the types
of recognized terms in the index along with the terms themselves, but
does not make explicit the connection between them. thus inserting
place along with paris may be helpful in answering    what is the capital
of france?,    but it does not prevent that term from matching queries
about paris hilton, or any other non-place use of the word paris. chu-
carroll et al. [12] describe how by indexing the relationship between
the term and the type, such unwanted matches can be avoided, thus
increasing precision.

a step in the direction of indexing semantic relations is to use a
search engine that can index xml documents and that has a query
language that supports xml fragments, as in section 4.3.1.2 (one could
equally use xpath queries, as is done in inex4). this method simulates
relation indexing by using inclusion spans, but does not (necessarily)
index semantic roles. recall from section 4.3.1.3 the original text:

p1: paris is the capital of france

and the terms it generates for the index:

<place>paris</place> capital <place>france</place>

while predictive annotation just indexes the terms {place$, paris,
capital, place$, france}, with appropriate o   sets, the semantic
relation index stores the information that place annotations span the
terms paris and france. the question:

where is paris hilton?

4 initiative for the evaluation of xml retrieval, http://inex.is.informatik.uni-duisburg.de/
2006/.

4.4 formal statistical models in qa 173

will generate the query

+<place></place> +<person>paris hilton</person>

assuming correct operation of the named-entity recognizer. this will
fail to match the text fragment p1, not because this instance of paris
was indexed as a place (since nested multiple annotations are allowed),
but because it is not in the context of a person annotation, as required
by the query.

one of the experiments described in chu-carroll et al. [12] con-
cerned use of semantic relation indexing for the aquaint 2005 opin-
ion pilot. here questions were of the form:

what does opinionholder think about subjectmatter?

opinionholder here stands for people or organizations such as the
u.s. government and subjectmatter to issues such as india   s nuclear
capability. since a common way opinions are reported in news cor-
pora is through quoted speech, the authors generated queries of
the form:

+opinionholder +<quotation> subjectmatter </quotation>

which had the e   ect of requiring that the terms expressing the sub-
ject matter were in the context of a quotation annotation. this is
a very simple, even simplistic, use of annotation indexing, and does
not even require (and hence does not guarantee) any relation between
the opinonholder and the subjectmatter. nevertheless, a doubling of
precision over a baseline of the same query without the nesting was
demonstrated.

4.4 formal statistical models in qa

most qa systems have at least a partly statistical nature, in as much
as they rely on corpus frequency counts to determine evidence for the
features under consideration. even the basic ir tf*idf term-weighting
formula is statistical. the intuition behind these methods is, to take
tf*idf as an example, that in lieu of being able to determine algorith-
mically that a document is about a given subject s (for example to
identify, parse and understand text that asserts it is about s),    nding

174 speci   c approaches

many more occurrences of s in the document than would be expected
on average is strongly suggestive that s is directly related to what the
document is all about. however, along with these methods, most qa
systems also use discrete symbolic methods, such as parsing, pattern
matching, and classi   cation.

qa systems

that we call

statistical use many fewer

sym-
bolic/linguistic methods. the general idea is to use statistical distribu-
tions to model likelihoods of entities of interest such as answer type and
answer. in this section, we will discuss the approaches of [18], where
a new metric space is developed for measuring the similarity between
question and candidate answer sentence, and of [26], which uses statis-
tical methods in the answer-type identi   cation and answer extraction
phases.

4.4.1 a id87

echihabi and marcu [18] developed a noisy-channel model for qa,
inspired by similar models for id103 and machine trans-
lation (amongst others), where statistical models are more commonly
used. given a set of sentences {si} returned by a search engine for a
question q, each one containing a set of candidate answers {a}, they
seek to    nd the si,a that maximizes

p(q | si,a )

in both training and testing, they take an answer sentence and abstract
it via a    cut    operation on the parse tree, so that for example in
response to

950: when did elvis presley die?

the found sentence

presley died of heart disease at graceland in 1997, and
the faithful return by the hundreds each year to mark the
anniversary.

is transformed to

presley died pp pp in a date, and snt.

4.4 formal statistical models in qa 175

where a date is the date tag with a pre   x indicating it is the
answer, and the other new tokens are part-of-speech tags. the noisy
channel model consists of a series of stochastic operations on the
sentence that gradually transform it into the question. the oper-
ations consist of adding and deleting words, followed by a substi-
tution phase, to e   ect the desired alignment. the training consists
of optimizing the probabilities associated with each operation. the
overall id203 p(q|s) is then the product of the probabilities of
the steps in the transformation. they suggest that the basic model
can be extended to cover the functionality of id138 and other
structured resources by mining and adding to the {q, a} training
set word-synonym, word-gloss, and question-factoid pairs. the use-
fulness of this is suggestive, but has not been tested in a full-scale
evaluation.

4.4.2 a maximum id178 model
ideally, one would want to maximize the id203 p(a|q) of answer a
to question q. unfortunately this is not tractable due to a huge answer
space and insu   cient training data. consequently the goal is refor-
mulated to determine the id203 of correctness c of an answer a
to question q. ittycheriah [26] rewrites this as p(c|q, a); by computing
this for all candidate answers, those with the greatest id203 value
are returned. by introducing a hidden variable e for answer type, the
id203 can be calculated by

p(c|q, a) =   ep(c, e|q, a) =   ep(c|e, q, a) p(e|q, a)

here, p(e|q, a) is what he calls the answer type model (atm), p(c|e, q, a)
is the answer selection model (asm). atm predicts, from the question
and a proposed answer, the answer type they both satisfy. given a
question, an answer and the predicted answer type, the asm seeks
to model the correctness of this con   guration. the distributions are
modeled by a maximum id178 formulation [2].

question analysis is done via the atm. questions are classi   ed into
one of 31 categories. a generic category phrase is used for questions
that cannot be    t anywhere else.

176 speci   c approaches

the search proceeds in two passes. the    rst is of an encyclope-
dia, where the highest-scoring passages are used to create expanded
queries, via local context analysis (lca) [82]. these expanded queries
are then executed against the target corpus. the top 1,000 documents
are retrieved. from these, the top 100 passages are found that

    maximize question word match.
    have the desired answer type.
    minimize dispersion of question words.
    have similar syntactic structure to the question.

candidate answers from these passages are ranked via asm.

p(c|e, q, a) is modeled by 31 features. one of these is the search
engine score from the ir stage. the others are sentence and answer-
candidate features. sentence features include counts of how many
matches occur directly, or via id138, lca expansion and depen-
dency links in parse trees. candidate features include directly observ-
able kinds, such as being proper nouns, dates, numbers, and linguistic
features, such as being adjacent to all question words but separated by
the verb    be    (an is-relationship), or a comma (apposition).

the training data for these models was developed by human judg-
ments. for the atm model, 13,000 questions were annotated with 31
answer type categories. for asm, a total of 5,000 questions were used,
culled both from prior trecs and trivia collections.

this approach was shown in trec to be competitive with all but
the very best systems. with such methods it is never clear how much
better performance can be achieved with more training data, since cre-
ating human judgments is an expensive task. it is clear, though, that
there is no free lunch. even though the selection and evaluation of can-
didate answers was done via statistical methods, some of the features
used were heavily linguistic in nature.

4.5 logic in qa

it is widely believed that to achieve ultimate success in question   
answering, as well as other    elds of nlp, the right combination
of linguistic processing and knowledge-based/deductive reasoning is

4.5 logic in qa 177

required. all nlp applications and components that employ lists of
patterns or instances     and this is all of them     are already at least
somewhat knowledge-based, but relatively few make use of any infer-
encing. the stand-out example that does is the lcc system and its
logic prover component. in this section, we will examine this system,
as well as the way that the cyc knowledge base and inferencing system
has been used in the ibm piquant qa system. in both cases we are
essentially ignoring the question analysis and ir parts of the systems,
but are focusing on what some call answer veri   cation.

4.5.1 the lcc logic prover

the lcc logic prover [49] can be seen as an outgrowth of the
abductive justi   cation component of the falcon system described in
section 2.4.4.1. the logic prover uses 5 kinds of data elements:

    the question logical form.
    candidate answers in logical form.
    extended id138 glosses (see below).
    linguistic axioms.
    lexical chains.
following common practice in theorem provers, the id136 engine
attempts to verify the answer by negating the question and proving a
contradiction. if the proof fails, predicates in the question are gradually
relaxed until either the proof succeeds or the associated proof score is
below a threshold.

4.5.1.1 extended id138

recognizing the importance of id138 to their (and other) nlp com-
ponents, the lcc group developed extended id138 (xwn). while
the isa hierarchy between terms (actually, synsets) is what id138
is primarily known for, the glosses contain implicitly a wealth of other
information about how words are related. the glosses, which can be
thought of as dictionary de   nitions, are in english, albeit a stylized
form, and thus su   er from problems of ambiguity. through a combi-
nation of automatic and manual means, the terms in the glosses were

178 speci   c approaches

disambiguated and associated with the corresponding synsets. xwn
is id138 with glosses syntactically parsed, transformed to logical
forms, and content words semantically disambiguated.5

4.5.1.2 lexical chains

when all question keywords are not present in candidate match-
ing passages, lexical chains are established between each unmatched
question and passage term, in an attempt to establish a semantic
connection. a chain is established when paths, starting from each
term via xwn links, intersect. the xwn links include the original
hypernym/hyponym/holonym links from id138, plus gloss and
rgloss (reverse-gloss). two examples will illustrate the way these
chains connect otherwise unmatched terms in the question and answer
passage.

q1540: what is the deepest lake in america?

answer: rangers at crater lake national park in oregon have
closed the hiking trail to the shore of the nation   s deepest lake

lexical chain:

(1) america:n#1 -> hypernym -> north american country:
n#1 -> hypernym -> country:n#1 -> gloss -> nation:n#1

it is fairly clear that this chain does not say that    nation    is equivalent
to    america        indeed, one can make a similar chain starting at any
country. in fact, in any passage about a given country, the unmodi   ed
phrase    the nation    would probably refer to that country. the lexical
chaining strategy would produce the wrong answer in such a case, unless
extra processing were invoked to determine the referent of    nation.   

q:1570 what is the legal age to vote in argentina?

answer: voting is mandatory for all argentines aged over 18.

5 http://xwn.hlt.utdallas.edu/.

4.5 logic in qa 179

lexical chains:

(1) legal:a#1 -> gloss -> rule:n#1 -> rgloss ->

mandatory:a#1

(2) age:n#1 -> rgloss -> aged:a#3
(3) argentine:a#1 -> gloss -> argentina:n#1

note that the existence of a lexical chain between two terms is not
a formal proof that they are equivalent in any sense, but rather that
they are related. this is exempli   ed in (1) from q1570, since legal and
mandatory are not synonymous. by associating a weight with every link
type, summing or multiplying the weights along a chain, and requir-
ing that the total passes a threshold test, the semantic drift can be
controlled.

4.5.1.3 the logic prover

the lcc cogex logic prover [46] takes lexical chains and linguistic
axioms, and    proves    the question logical form from the answer logical
form. this can be illustrated by means of the example

108: which company created the internet browser mosaic?

this is converted into the question logical form:

organization at(x2) & company nn(x2) & create vb (e1,
x2,x6) & internet nn(x3) & browser nn(x4) & mosaic nn
(x5) & nn nnc(x6,x3,x4,x5)

the lexical chain connecting develop and make is:

exists x2 x3 x4 all e2 x1 x7 (develop vb(e2,x7,x1) <->
make vb(e2,x7,x1)
&
such jj (x1) & product nn(x2)
& or cc(x4,x1,x3) &
mental jj(x3) & artistic jj(x3) & creation nn(x3))

& something nn(x1) & new jj(x1)

and between make and create:

all e1 x1 x2 (make vb(e1,x1,x2) <-> create vb(e1,x1,x2)
& manufacture vb(e1,x1,x2) & man-made jj(x2) & product nn
(x2))

180 speci   c approaches

finally, the linguistic axiom representing mosaic is an internet
browser is

all x0 (mosaic nn(x0) -> internet nn(x0) & browser nn
(x0))

these are all used to    nd an answer in the following way. using a search
engine, many candidate passages are located, including this one:

. . . mosaic, developed by the national center for supercomput-
ing applications (ncsa) at the university of illinois at urbana-
champaign . . .

which is represented by the answer logical form:

... mosaic nn(x2) & develop vb(e2,x2,x31) & by in (e2,
x8) & national nn(x3) & center nn(x4) & for nn(x5) &
supercomputing nn(x6) & application nn(x7) & nn nnc(x8,
uni
x3,x4,x5,x6,x7) &
versity nn(x10) & of nn(x11) & illinois nn(x12) & at nn
(x13) & urbana nn(x14)
& nn nnc(x15,x10,x11,x12,x13,
x14) & champaign nn(x16) ...

ncsa nn(x9) & at in(e2,x15) &

this logical form can be matched with the question logical form
by chaining through the aforementioned lexical chains and linguistic
axiom. the variable x2 in the question logical form, which stands for
the entity (the company) being sought, gets uni   ed with the variable
x8 in the answer logical form, where x8 represents the national center
for supercomputing applications.

the impact of this machinery on the lcc performance in

trec2002 is as follows (from table 2.1 in [46]).

it is readily seen from table 4.1 that cogex could answer about

2/5 of all 500 questions, and improved the lcc score by 31%.

4.5.2 use of cyc in ibm   s piquant

cyc is a large knowledge based and id136 engine from cycorp [32],
and in principle can be used to perform the very deep inferencing that

4.5 logic in qa 181

table 4.1 in   uence of cogex logic prover on lcc performance at trec20002.

questions answered by the complete system 415
206
questions answered by cogex
questions answered only by cogex
98
317
questions answered without cogex

is believed required for some qa problems, but so far this has not
been demonstrated. the subject of this section is a relatively shal-
low use of cyc in the ibm piquant system as a sanity checker [59].
in large part because of the projection problem (its answers are not
in the trec corpus     see section 4.6), cyc was not used to    nd
answers but to reject    insane    answers and boost the con   dence of
   sane    ones.

it becomes apparent that such a mechanism is desirable when a

question like

1018: what is the earth   s diameter?

results in the answer

14 feet

because the system    nds a passage talking about the diameter of a
hole in the earth. when this occurs, all of the query terms match,
the span of the matching terms has a good density and a num-
ber of the sought syntactic relationships are found, so the heuris-
tics used in answer extraction will give the candidate a fairly good
score, in this case more than any other candidate. however, with-
out real-world knowledge, it cannot know that the answer 14 feet is
insane.

the sanity checker is invoked for numerical quantities with the fol-

lowing three arguments:

    predicate (e.g.,    diameter   ).
    focus (e.g.,    the earth   ).
    candidate value (e.g.,    14 feet   ).

182 speci   c approaches

the appeal of cyc is that there are many possible ways it could deduce
answer to questions like this, all opaque to the calling qa system.
regarding the diameter of the earth, for example,

    it could know the    gure directly.
    it could reason that the earth is a planetary object, and it
might have ranges of reasonable values for sizes of planetary
objects.
    it could reason that the earth is a spheroid, is covered with
landmasses, these landmasses consist of continents which are
made up of countries, which . . . , and that objects are larger
than objects they consist of.
    it could know that the ei   el tower is smaller than the earth
and how big the ei   el tower is
    . . .

the sanity checker returns one of three possibilities:

    sane :
    insane :
    don   t know : otherwise.

when the candidate value is within + or     10%
of the value known to cyc.
when the candidate value is outside of an
established reasonable range.

the answer is rejected if    insane.    the con   dence score is highly
boosted if    sane.   
when used in trec2002, the question

1425: what is the population of    maryland?   

matched the passage

maryland   s population is 50,000 and growing rapidly.

which happened to be about the exotic species called    nutria,   
not humans. without sanity checking, piquant   s top answer was
   50,000,    despite a much more reasonable    5.1 million    in second place.
with sanity checking, since cyc believes the population of maryland to
be 5,296,486, piquant rejected the top    insane    answer, and returned
a new top answer:    5.1 million    with very high con   dence.

4.6 projection-based qa 183

despite its promise, cyc proved to be of very limited utility to

piquant at that time. the problems were twofold:

    only a limited number of questions were completely map-
pable to goals in cycl (cyc   s query language).
    cyc had limited instance information.

as a result, it was triggered in only 4% of the questions. in 2/3 of
these, the system had already gotten the correct answer in    rst place,
so there was no impact to the system   s performance. the con   dence of
those answers was improved, so would have positively a   ected the cws
score, had it been a part of that year   s evaluation. in the remaining 1/3
of 4% of the questions, it failed because of a bug. in failure analysis,
it was determined that with the given system architecture the sanity
checker could have helped in 13% of the questions; in those other 9% of
the cases the question analysis did not put out an appropriate semantic
form. the conclusion was that the technology was promising, but much
more knowledge engineering would be necessary to make it really useful.

4.6 projection-based qa

when a qa system looks for answers within a relatively small textual
collection, the chance of    nding strings/sentences that closely match
the question string is small. however, when a qa system looks for
strings/sentences that closely match the question string on the web,
the chance of    nding correct answer is much higher. (hermjakob et al.
2002).

sometimes it is very easy to    nd an answer to a question using
resource a, but the task demands that you    nd it in target resource b.
a popular solution is projection: to    rst    nd the answer in resource
a, then locate the same answer, along with original question terms, in
resource b. this is an arti   cial problem, but is real for participants in
evaluations such as trec. figure 4.3, a copy of figure 2.5, illustrates
the architecture of a system using projection.

there are basically two situations where projection makes sense:
there is a corpus or corpora that are either much bigger or more spe-
cialized to the domain of the question than the target corpus, or the

184 speci   c approaches

question

question
analysis

keyword query

search

web 
and/or 
corpus

answer type

documents 
or passages

designated
corpus

answer
extraction

answer(s)

{answer, docid}

projection

fig. 4.3 basic architecture of a question   answering system, using projection.

question is easy to understand and there is a structured resource that
covers the domain. we have encountered the former case with askmsr
in section 4.2.3. using the web in askmsr is e   ective for two reasons:

    the web is much larger than the trec corpus (3,000 : 1
and growing).
    trec questions are generated from web logs, and the style
of language (and subjects of interest) in these logs are more
similar to the web content than to newswire collections.

what characterizes both uses of projection is the greater a priori
likelihood of    nding a correct answer in the non-target resource than
by interrogating the target resource directly.

it should be mentioned here that systems can bene   t from answers
from external sources without using projection. answer candidates can
be found directly in the target corpus, while the search is simulta-
neously made elsewhere (the web, encyclopedias, etc.). the relative
frequency of the di   erent answers in the external sources can add (or
subtract) support for the ranking from the direct search, sometimes
resulting in a useful re-ranking [14, 57].

4.6 projection-based qa 185

now, when the question syntax is simple and reliably recognizable,
it can be transformed into a logical form. this form is used in the world
of id99 and reasoning, to represent assertions and
goals, as we saw in section 4.5.1 on the lcc logic prover. the log-
ical form represents the entire semantics of the question, and can be
used to access structured resources directly. most questions that ask
for properties of objects can be easily converted to logical forms, for
example:

in each of the cases in table 4.2, an    x    is used to stand for the
unknown quantity. the logical form can be interpreted by the system
as a query against structured resources, such as:

    id138.
    on-line dictionaries.
    tables of facts and    gures.
    knowledge-bases such as cyc.

the query can be processed as a direct table/database lookup, as
was done in the mit start system [27], or it could trigger some
calculation, such as in the currency conversion case.
having found the answer, projection works as follows:

1. a query is constructed with the original question terms, plus

the found answer.

2. documents/passages are retrieved from the target corpus.
3. answer extraction is run in a modi   ed mode, wherein it is

told the answer.

we should note that this technique is prone to failure due to the pro-
jection problem     a passage exists which contains the answer in a sup-
porting context, but due to vocabulary mismatches with the projected

table 4.2 sample logical forms.

when was einstein born?
how high is everest?
what is the capital of california?
what is the distance from london to paris?
how much is a pound sterling in us dollars?

birthdate (einstein, x)
height (everest, x)
capital (california, x)
distance (london, paris, x)
convert (1, ukp, x, usd)

186 speci   c approaches

answer it is never retrieved. this is the same identity determination
issue that answer merging faces (cf. section 2.4.6).

answer extraction, as discussed in section 2.4.5, evaluates candidate
answers based on contextual information     in essence, how well the
context of the candidate matches the context of the question phrase in
the original question. in this modi   ed form, the same calculation is run,
but only the given answer (and variants) are allowed to be evaluated.
by doing this, it is not the answer that is ultimately evaluated but the
enclosing passage     the passage/document that gives the best score is
the one that is returned as the support for the answer.

as discussed in section 2.4.4, there is the opportunity here for intro-
ducing the answer nil:    no answer,    if none of the contexts surround-
ing located instances of the designated answer pass a given threshold,
or indeed if the answer cannot be found anywhere in the target corpus.
if many potential answers are found by processing the logical form,
then criteria di   erent from the ones usually available to answer extrac-
tion must be applied to choose between them. we will study one such
algorithm, called virtual annotation, in the next section.

4.7 de   nition questions

de   nition questions are those of the form    what is x?    or    who is
x?    we will look at two di   erent interpretations of such questions:

1. as factoids, requiring a single phrase as an answer.
2. as requiring a list of properties.

the factoid interpretation was required for trec8/9/2001. in
the de   nition/other questions had to be

trec2003 onwards,
answered by a list of properties.

in the world in general, a commonly practiced method of de   ning
terms is the aristotelian    genus and di   erentiae    approach: determine
the class of objects the term belongs to, and then di   erentiate it from
others in the class. with id138 as a resource, one can readily    nd out
the genus (hypernym). discovering the di   erentiae is trickier; although
id138 glosses sometimes contain the information, the problem is in
parsing it out. having both items in hand and then locating a good

4.7 de   nition questions

187

passage via projection is in the author   s experience rarely successful,
because it requires too much chance agreement between the writers of
the corpus being processed and the developers of id138.

fortunately, in many cases one can get by without the di   erentiae.

for example, to a question like
463: what is a stratocaster?

the answer    a guitar    is probably su   cient for most people. whether
and to what degree this is possible depends on a model of the user and
why they might be asking the question. we will follow up this line of
enquiry in chapter 5.

4.7.1 target determination

in    who/what is x?    questions, the x is often called the target;
indeed, in recent trec evaluations the targets are supplied without
   who/what is.    it turns out that the    x    is often a compound phrase
with redundant or at least overlapping terms, and that dropping some
of them might bene   t the system   s recall, using arguments similar to
those given in section 2.6.1. examples of such targets from recent eval-
uations include those shown in figure 4.4.

the modi   ers are in most cases necessary for disambiguation, but
are undesirable to be included in a phrase with the other target terms
for purposes of document retrieval. cui et al. [15] describe a method
for determining the    bull   s eyes    within these targets. for each pair of
topic terms they calculate their pointwise mutual information, based
on hits when the terms are used as google queries. they establish
a threshold, whereby terms above it are grouped, and terms in such

1972: abraham in the old testament.
1987: eta in spain.
2148: ph in biology (sic).
2348: the medical condition shingles.
t11: the band nirvana.
t18: boxer floyd patterson.
t24: architect frank gehry.

fig. 4.4 sample targets with potentially redundant modi   ers,

188 speci   c approaches

groups are connected by and in subsequent queries (di   erent groups
are connected by or).

we follow with descriptions of three approaches: the ibm team   s
method of virtual annotation to    nd factoid answers, and then
columbia   s defscriber system which, heavily informed by their summa-
rization work, attempts to produce a cohesive set of maximally-di   erent
informative sentences about the subject. we end the section with a
description of bbn   s method of removing redundant information.

4.7.2 de   nition questions by virtual annotation

we describe here the approach taken by prager et al. [62]. the main
idea is to use id138 to    nd hypernyms, which are candidate de   ning
terms for the subject of the question. the problem is twofold:

    the word may be polysemous (e.g., from trec: sake, mold,
battery).
    the immediate parent hypernym may not be the best choice.

the general approach is to use corpus statistics to select the best
choice(s). this approach is somewhat like that described by mihalcea
and moldovan [44] as an approach to id51.
consider the question

354: what is a nematode?

the hypernym hierarchy from id138 is given in table 4.3.
viewing this table one can conclude that either the parent (a nema-
tode is a kind of worm) or a synonym (a nematode is also called a
roundworm) would be a su   cient answer (either was acceptable in

table 4.3 id138 hierarchy for    nematode.   

level
0
1
2
3
4
5

synset

{nematode, roundworm}
{worm}
{invertebrate}
{animal, animate being, beast, brute, creature, fauna}
{life form, organism, being, living thing}
{entity, something}

4.7 de   nition questions

189

table 4.4 id138 hypernyms for    meerkat.   

level
0
1
2
3
4
5
6
7
8
9

synset

{meerkat, mierkat}
{viverrine, viverrine mammal}
{carnivore}
{placental, placental mammal, eutherian, eutherian mammal}
{mammal}
{vertebrate, craniate}
{chordate}
{animal, animate being, beast, brute, creature, fauna}
{life form, organism, being, living thing}
{entity, something}

trec). however, a super   cially similar question:

358: what is a meerkat?

has the hypernyms as shown in table 4.4

here neither the synonym nor the immediate parent seems like a
good response (even possibly to zoologists), but the other ancestors
contain some common terms that would be readily understood by a
typical questioner     but which to choose?

the phenomenon of natural categories is described in [68]. there
we learn that according to psychological testing, these are categoriza-
tion levels of intermediate speci   city that people tend to use in uncon-
strained settings. that is, there will be a natural tendency to use a
certain level of granularity to describe objects, such as the subjects of
de   nition questions.

for purposes of answering such questions, we hypothesize that we
can expect to    nd instances of these natural categories in text, in the
right contexts, and in greater quantities than other candidate terms.
we further hypothesize that these terms will serve as good answers.

there are many common syntactic structures used in english to

express a term t and its parent p. these include:

. . . t and other p . . .
. . . p, such as t, . . .
. . . t p . . .

the technique of virtual annotation proceeds as follows:

1. find all parents of the target term in id138.

190 speci   c approaches

2. look for co-occurrences of the term and the parent in
the text corpus. many di   erent phrasings are possible, so
we just look for proximity, rather than parse. this is in
contrast to the defscriber system of [4], described in the
next section, which explicitly looks for such de   nitional
predicates.

3. score candidate parents are as follows:

    count co-occurrences of each parent with search
term, and divide by level number (only levels >= 1),
generating level-adapted count (lac).
    exclude very highest levels (too general).
    select parent with highest lac plus any others with
lac within 20%.

the level-adapted count numbers for nematode and meerkat are
shown after the terms so counted in tables 4.5 and 4.6. the absence
of a number means there were no occurrences found in the corpus.

table 4.5 id138 hypernyms for    nematode,    including level-adapted scores.

level
0
1
2
3
4
5

synset

{nematode, roundworm}
{worm(13)}
{invertebrate}
{animal(2), animate being, beast, brute, creature, fauna}
{life form(2), organism(3), being, living thing}
{entity, something}

table 4.6 id138 hypernyms for    meerkat,    including level-adapted scores.

level
0
1
2
3
4
5
6
7
8
9

synset

{meerkat, mierkat}
{viverrine, viverrine mammal}
{carnivore}
{placental, placental mammal, eutherian, eutherian mammal}
{mammal}
{vertebrate, craniate}
{chordate}
{animal(2), animate being, beast, brute, creature, fauna}
{life form, organism, being, living thing}
{entity, something}

4.7 de   nition questions

191

we see that worm is indeed selected as the best choice for de   ning
nematode, but perhaps somewhat surprisingly we    nd the very general
term animal, rather than more speci   c terms like mammal, is chosen
for meerkat. but animal is how the thing was described in the corpus,
so by de   nition is the appropriate natural category for this case. as
illustration, corpus passages that contain these two targets and their
de   ning parent terms are shown below.

what is a nematode? ->
such genes have been found in nematode worms but not yet in
higher animals.

what is a meerkat? ->
south african golfer butch kruger had a good round going in the
central orange free state trials, until a mongoose-like animal
grabbed his ball with its mouth and dropped down its hole. kruger
wrote on his card:    meerkat.   

4.7.3 defscriber

the virtual annotation technique described in the previous section
is a hybrid technique in as much as it uses a symbolic/structural
approach to    nd candidate answers, followed by a statistical method
to rank them. columbia   s defscriber system [4]
it
uses a pattern-based goal-driven approach to    nd certain speci   c
candidate sentences, followed by a data-driven approach to comple-
ment them.

is also hybrid:

in training, a number of lexicosyntactic patterns are developed by
hand to identify genus-species de   nitions. these are similar to the
de   nition patterns used by lcc [20]. the defscriber patterns are par-
tially speci   ed syntax trees, which are claimed to o   er more    exibility
over    at patterns.

when a question is processed, a series of increasingly-relaxed queries
is submitted to a search engine until a predetermined threshold number
of documents is retrieved. these are then examined for presence of
de   nitional predicates.

192 speci   c approaches

s

vp

vp

np

np

dt? term

formative vb

np

np

pp

genus

prep species

fig. 4.5 extracted partial syntax-tree pattern.

the lexicosyntactic patterns are applied to the documents returned
from search to extract de   nitional sentences about the subject in hand.
for example, the training sentence:

the hindu kush represents the boundary between two major
plates: the indian and the eurasian.

generates the pattern in figure 4.5:
then in response to the question

what is the hajj?

matching this pattern against retrieved documents    nds the sentence:

the hajj, or pilgrimage to makkah (mecca) is the central
duty of islam.
the correspondence between relevant terms in the training and test

sentences is shown in table 4.7.

a precision of 96% was reported on a small test sestet, in which
human judges evaluated the quality of system-generated de   nitions of
19 test terms.

the data-driven phase is a redundancy-based approach that uses
techniques from id57 to identify themes

4.7 de   nition questions

193

table 4.7 example of defscriber   s use of lexicosyntactic patterns.

node in figure 4.5 training sentence
dt? term
the hindu kush

formative vb
genus
prep species

represents
the boundary
between two major plates: the
indian and the eurasian

test sentence
the hajj, or pilgrimage
makkah (mecca)
is
the central duty
of islam

to

that recur across documents. the lexicosyntactic patterns previously
described are part of a broader set of non-speci   c de   nitional pat-
terns. this broader set is applied to the retrieved document set to
   lter out non-de   nitional sentences; the resulting set of sentences is
then processed as follows.

by using cosine-distance measures on word-vectors, a de   nition
centroid is generated from the sentence collection. the sentences are
ordered in increasing distance from the centroid. after ordering, a
sequential non-hierarchical id91 is performed to generate the top
n clusters. the top-ranking genus-species sentence followed by the
   rst sentence from each of the top nclusters generates a summary.

although immaterial for purposes of trec-style nugget scoring, as
described in section 3.3, cohesion is important for summaries for human
consumption. the defscriber system improves cohesion by choosing
the second and following sentences by iteratively considering closeness
to the de   nition centroid and the previously-chosen sentence   s cluster
centroid. the summary they generate for the hajj question is:

the hajj, or pilgrimage to makkah [mecca], is the cen-
tral duty of islam. more than two million muslims are
expected to take the hajj this year. muslims must per-
form the hajj at least once in their lifetime if physically
and    nancially able. the hajj is a milestone event in
a muslim   s life. the annual hajj begins in the twelfth
month of the islamic year (which is lunar, not solar,
so that hajj and ramadan fall sometimes in summer,
sometimes in winter). the hajj is a week-long pilgrim-
age that begins in the 12th month of the islamic lunar
calendar. another ceremony, which was not connected

194 speci   c approaches

with the rites of the ka   ba before the rise of islam, is
the hajj, the annual pilgrimage to    arafat, about two
miles east of mecca, toward mina. the hajj is one of    ve
pillars that make up the foundation of islam. not only
was the kissing of this stone incorporated into islam, but
the whole form of the hajj pilgrimage today is funda-
mentally that of the arabs before islam. rana mikati of
rochester will make a pilgrimage, or hajj, to the holy
site of mecca next week.

4.7.4 removal of redundant answers

while much emphasis has been given to recall in generating lists of
answering nuggets, precision is important too, and one way to improve
precision is to weed out redundant answers. the previously described
defscriber system achieves this indirectly by using id91, which is
an approach to avoid generating redundant answers in the    rst place.
in general, the more a system    knows what it is doing    the better able
it is to remove redundant nuggets. this is exempli   ed by the bbn
de   nition-question system [83].

their system generated a set of kernel facts from extracted sen-
tences that matched the question target. these facts were extracted
and ordered by the following criteria, most important    rst:

1. appositions and copular constructions.
2. structured patterns (like de   nition patterns/predicates men-

tioned earlier).

3. special propositions (i.e., simple predicate argument struc-

tures, matching a prede   ned list).

4. relations (matching a relation de   ned by ace [39]).
5. ordinary propositions (i.e., simple predicate argument struc-

tures, outside the prede   ned list).

6. sentences.

within each grouping, facts were ordered with respect to similarity to
a question pro   le.

broadly, the procedure used is to start with an empty set s, then
add facts to it until prede   ned stopping criteria are met. at each step,

a fact is only added if it is not considered redundant with respect to
the facts already in s. three criteria are used:

4.7 de   nition questions

195

1. a proposition is redundant if another proposition in s shares
the same predicate and the same head noun for each of the
arguments.

2. a structured pattern is redundant if two facts already in s

had been extracted using the same rule.

3. otherwise a fact is redundant if >70% of its content words

are already in facts in s.

4.7.5 soft pattern matching

pattern-based techniques for factoid questions were discussed in
section 4.2. a basic problem with using patterns that are con-
structed by hand, or learned over the web, is that they are in   exi-
ble: they match according to the constraints in the patterns but do
not allow for the almost unlimited variations in writing that one will
encounter in practice. to rectify this, cui et al. [15] developed an
approach to use soft pattern matching. they used this in the con-
text of de   nition questions, but it is in principle extendable to other
kinds.

cui et al. developed a series of heuristics for generalizing de   nition
sentences, or at least a window around de   ned terms in a de   nition
sentence. they    rst identi   ed highly topical words, which they called
centroid words. the heuristics consisted of performing the following
mappings:

1. the

search term (de   nition term)

is

replaced with

(cid:1)sch term(cid:2).

2. centroid words get generalized to their syntactic classes.
3. chunked noun phrases become np.
4. adjectival and adverbial modi   ers are deleted.
5. parts of to be become be$.
6. articles become dt$.
7. numerics become cd$.
8. all other words remain untouched.

196 speci   c approaches

thus the passage

the channel iqra is owned by the arab radio and television
company and is the brainchild of the saudi millionaire, saleh
kamel.

gets transformed to the soft pattern:

dt$ nn <sch term> be$ owned by dt$ np and be$ dt$ brain-
child of np.

they also manually generate a set of generic de   nitional patterns rep-
resenting appositives, copulas and the like, such as

<sch term> be$ dt$ nnp

a pattern-matching method between test sentences and these patterns
is de   ned, both on slot-slot matches and on token sequences. the over-
all algorithm is as follows:

1. candidate de   nition sentences are generated by issuing ir

queries.

2. these sentences are ranked statistically, informed by the cen-

troid words.

3. the top n (10) sentences are extracted.
4. a set of soft patterns is generated.
5. the manual patterns are added.
6. the sentences are re-ranked, using both statistical and soft-

pattern match weights.

7. the best sentences, subject to redundancy    ltering, are

output.

4.8 qa with domain constraints

just as in mathematics where problems can be solved either from    rst
principles or by using an established formula (or a combination of the
two), then so can questions be answered by    nding an answer from a
resource that directly attests to it, or indirectly from other facts that
entail the sought answer. this is pursued further in the later discussion

4.8 qa with domain constraints

197

on decomposition in section 5.3.1. the reason that indirect approaches
work, both in mathematics and qa, is that there is a consistent under-
lying world model out there, where the current question/problem is
just seeking one aspect of it; that aspect does not exist in isolation but
is consistent with and constrained by the rest of the model.

qa systems are beginning to take advantage of this broader view in
order to help the answering process. in this section, we look at two sys-
tems that take quite di   erent advantage of the consistent world model.
the ibm piquant system [58] uses qa-by-dossier-with-constraints to
ask related questions to the one in hand. the singapore qualifier
system [84] treats questions as aspects of events.

4.8.1 asking constraining questions

prager et al. [58] described qa-by-dossier-with-constraints wherein
answers can get supported by asking di   erent questions. for example,
the initial answers to

when did leonardo paint the mona lisa?

were in a ranked list of 5 answers, with the correct one in 4th place.
by also getting the top 5 answers to

when was leonardo born?

and

when did leonardo die?

and applying natural constraints between allowable answers, the cor-
rect answer was promoted to    rst place. this approach is in princi-
ple general-purpose, but in practice depends on the existence of the
aforementioned natural constraints. promising subdomains are those
of timelines (people, organizations and even artifacts have lifecycles),
geography (constraints come from proximity and containment), kin-
ship (most relationships have reciprocals), de   nitions (a term and its
de   nition are generally interchangeable), and any context with part-
whole relationships (most numerical measures of a part are less than
the corresponding measure of the whole).

198 speci   c approaches

4.8.2 event-based qa

factoid questions can be broadly classi   ed as being one of two kinds:
about properties of entities or events. in some sense, questions about
properties of entities (capitals of countries, heights of mountains etc.)
are the simpler kind. the question foci and question topics are rel-
atively simple to identify in the question, and must be present in
answer passages, where the relationships are usually straightforward
to extract. while not absolutely required to be so, these properties
are mostly static, so that temporal, spatial or other conditional mod-
i   cation is usually absent. event questions, by contrast, are not so
constrained, and tend to give qa-systems bigger headaches. the sin-
gapore system qualifier [84] attacks them head on by modeling
events.

the central idea in their system is that an event is a point in a
multi-dimensional event space (where the dimensions are time, space
and other quantities), that a question provides some of those elements
and asks about one or some of the unknown ones. one can think of the
event itself as being a    hidden variable    between the question and the
answer.

to more precisely pinpoint the event, some of the missing dimen-
sions or event elements need to be instantiated. qualifier uses a
combination of id138 and the web to do this. they describe this
process as consisting of two parts: a    linear    process which uses word-
net to generate strongly related terms, and a structured one which is
event-based, and which is described in more detail here.

4.8.2.1 quali   er event analysis

in qualifier, terms in the original question are used to construct a
query to google, retrieving the top n documents. for each query term,
a list of terms c that co-occur with it in a sentence in the retrieved set
is built. each term is given a weight according to what proportion of the
terms    occurrences are co-occurrences. each of the original query terms
is looked up in id138, and associated terms from their synsets s and
glosses g are extracted (subject to simple    ltering to approximate word
sense disambiguation).

4.8 qa with domain constraints

199

for each term in c, its weight is increased if it is in s or g. after
id172 and thresholding, a    nal set of terms is generated. so,
for the question:

1411: what spanish explorer discovered the mississippi river?

the original query terms are

{spanish, explorer, mississippi, river}

and the    nal, re-ranked query list is

{mississippi, spanish, hernando, soto, de, 1541,
explorer, first, european, french, river}

this forms what they call the linear query. for the structured query,
the following steps are taken.

    three distance measures are computed, based on lexical
correlation, co-occurrence correlation and physical distance
in text.
    the terms in c + s + g are clustered into disjoint sets,
using those distance measures.
    for each group, its cohesiveness is computed, by summing
distances of elements to the group   s dominant member.
terms in highly cohesive groups are all required, and are con-
joined, while terms in the less cohesive groups are disjoined.
for the mississippi example, this gives:

(mississippi) & (french|spanish) & (hernando
& soto & de) & (1541) & (explorer) &
(first|european|river)

    boolean retrieval is then used. in the event that the query
is over-constrained, they relax up to 4 times by recomputing
a threshold for removing terms or clusters from the query
computation, although the    rst two relaxations made the
biggest di   erence. they call this process successive constraint
relaxation (scr).

they measured precision of the baseline bag of words, the linear
and the structured query with and without scr. they showed that

200 speci   c approaches

the baseline query improved from .256 to .438 (71%) with scr. the
linear query improved from .346 to .588 (70%) with scr. at 0.634, the
best precision they reported, the structured query with scr was 34%
better than the structured query without.

4.9 no-answer (nil) questions

it is important for systems to be able to signal that they can    nd no
answer to a question. there are three basic variations of this notion,
which often get con   ated together. this is unfortunate, since di   erent
processing is required in each case.

s1:    there is no answer to this question in this corpus.   
s2:    there is no answer to this question. period.   
s3:    i don   t know.   

situation s1 is peculiar to trec or similar situations where, to be
acceptable, an answer must be found in a designated corpus. s2 entails
s1, but for trec the two are equivalent.

a simple strategy for a system is to examine its performance on
training data, and determine a threshold t whereby if the score of the
top candidate answer is less than t , nil is output. this is to declare
s1 from s3, but is reasonable as a simplistic approach.

s2 can be determined whenever a closed-world assumption holds.
such an assumption can also lead to high-con   dence assertions of s1.
closed-world assumptions can be made when data is held in structural
form such as a data-base, and developers can assert that the table
is complete. suppose one has a data-base of world capitals (possibly
including state and province capitals     it does not matter). then sup-
pose the system gets a question of the form:

what is the capital of x?

if the system can be con   dent it has analyzed the question correctly
(not di   cult for simple questions like this), and there is no such x as
a key, then it can assert both s1 and s2.

if the data-base lookup results in an answer (thus ruling out s2),
then projection techniques can be used to locate the answer in a corpus.

4.9 no-answer (nil) questions

201

if no such location is found, or if the answer is not found in an appro-
priate context, then s1 can be asserted.

in a setting where one knows the a priori id203 p of a nil
question, and there is at least a moderate correlation of answer score
with correctness, the following procedure can be followed. suppose for
this example only the top answer is scored.

examine the scores and correctness of a collection of test questions.
at least several hundred will be required. divide the range of scores
into buckets     say 10 or 20     and allocate questions with those scores
into those buckets. count the percentage correct per bucket. if there
is a large enough number of questions per bucket, the so generated
con   dence histogram of average correctness per bucket will be mono-
tonically increasing (with bucket score). the number of buckets can be
increased and the allocation redone as long as the resulting curve (the
envelope of the histogram) is monotonic.

the average correctness per bucket is the expected evaluation score
(based on 1 for correct, 0 for incorrect) for an answer whose qa score
is in that bucket. if you know p, then    nd the lowest bucket with an
average correctness greater than p, and use the low end of the score
range that de   nes the bucket as the threshold t for nil answers.

some variations of this are possible.

1. if more than the top answer is to be output, then nil can
be generated in other than the top position. the algorithm
above can be recomputed, but using p/m instead of p when
considering position m in the answer list.

2. if more than the top answer is to be output, always output

nil in position m(> 1).

3. determine if the training data shows systematic di   erent
average correctness across answer types. with a large enough
set of questions in the training data, histograms can be gen-
erated per answer type, or group of answer type.

4.9.1 nil and cws

in evaluation section 3.5, we discussed the con   dence-weighted score.
the question arises, if an answer is replaced by nil, as above or

202 speci   c approaches

otherwise, then what is its new con   dence and score? a simple
approach is to determine the average accuracy of nil answers in
training data, and use that to determine its score, via a con   dence
histogram.

we examine here the approach of [11], based on these ideas, both
to determine which answers should be made nil and what their
scores should be (so that they would be optimally placed in the rank-
ing input to the cws calculation). this approach is entirely sta-
tistical in nature, and only makes sense in the event of the system
not having any real idea of the no-answer status of any individual
question. trec2001 data is used to derive parameters to apply to
trec2002.

their approach followed the observation that the lowest-con   dence
answers in trec2001 were more often no-answer than correct. thus
converting all answers below a certain score to nil would improve the
overall correctness and also the average con   dence of the block. moving
that converted block to the rank with the same average precision would
boost cws too.

figure 4.6 shows the answer evaluations to 491 trec2001 ques-
tions, ordered from most con   dent to least. the use of 50 answers
per row is somewhat arbitrary but happens in this case to allow an
easy identi   cation of the nil threshold. the columns to the side of the
answer display show the number of nil (   -   ) and correct (   x   ) answers
per row (incorrect are    .   ). it is readily seen that for the bottom two

                        nil  correct

         xxxxxxxxxxxxxx.xx.xxxxxxxxxxxxxx.x..xx.xx   0      35
xxxxxx-x.-x.xxxxxxxx..x-xxxxxxxxxx.xxxxx.x.xxx.-xx   4      38
xx.....x.-xx.....xx....x.xx.x..xxx.xx...xx.x..xx.x   1      22
.-...x.xx-..x..x.xx....xx.x...xx.....x..xxx....xx.   2      18
........x....x..xxxx...x...xx....xxxxx--......xxx.   2      17
..x.xxx...-x-...xx.....x...xx--.xx-....xx..x..x...   5      16
..x.x.-......x....x.x-.x.xx...-x-x-x-...-..x-x.x.x   8      15
x..-x.....x.x.....-..........-...-..x.-....-..x...   6       6
.x--......xx....-.-..x.-....-.-..x...........--...   9       5
-.-.-..--...-x.xx....-.-x......-.....-..-...-.x.-.  13       5

k

c

fig. 4.6 trec2001 answers ordered from most to least con   dent, coded by an    x    for
correct,    .    for incorrect and    -    for nil.

4.9 no-answer (nil) questions

203

rows the number of nils6 surpasses the number of correct answers.
so by changing to nil all answers in this bottom block of two rows
would gain 22     10 = 12 correct answers. the con   dence of the leading
element of the block = c is noted.

the precision of the block = p = 22/100 is calculated. then the
point in the answer distribution with the same local precision p is
found. the con   dence at this point = k is noted.

having derived these parameters,

they must be applied to
trec2002 answers. con   dence scores are known, but correctness is
not. laying out the answers again in order of decreasing con   dence,
there will be some point with local con   dence c. from here to the
end is the block to be transformed     it happens to have size 147. all
answers in this block are made nil, and k-c is added to each con-
   dence. the point with local con   dence k is found again. the block
of nils is inserted at this point. for all answers to the right of this
insertion point, c is subtracted from their con   dences. this generated
a block of nils with the same con   dence features as the training data.
the results of this exercise were as follows: 29 out of 46 nil answers
were located     this represents a recall of 0.63. 9 previously-correct
answers were lost, representing an overall gain of 20 correct questions,
or 4%. however, there was a minimal (<0.5%) improvement in the    nal
cws; this was because all the movement took place in questions well
down the distribution, which from figure 3.1 we see contribute very
little to the cws total.

6 that is, questions for which the correct answer was nil. at this point, the system had
not asserted any nils.

5

user models and question complexity

in this chapter we ask how well de   ned a question is, and how do we
rank questions by complexity. the    rst of these is a matter of addressing
user expectation, the second is pertinent to research agendas, and to a
limited extent to dispatcher modules in the more complex qa systems.

5.1 what is a question?

or rather, what is a question suitable for a qa system? if we look
at the questions in figure 5.1, we see that it is doubtful if any except
the    rst is within the range of current qa systems, unless by    uke a
corpus repeats the question in declarative form. this is because they
are unanswerable by any kind of    lookup,    whether textual or tabular,
not because they are so di   cult from a human perspective.

we might begin by enumerating criteria by which to discard inap-
propriate questions (those that are clearly mathematical or logical in
nature, that require estimation, etc.) but are quickly led into the amor-
phous    those that require reasoning.    so we could adopt the approach
taken by nist and assert that an appropriate question is one which
is answered by the text in a document in a designated resource, along

204

5.1 what is a question?

205

450: what does caliente mean (in english)?
what is one plus one?
how many $2 pencils can i buy for $10?
how many genders are there?
how many legs does a person have?
how many books are there (approximately) in a local library?
what was the dilemma facing haid113t?

fig. 5.1 a selection of di   cult questions for a traditional qa system.

with knowledge that any educated adult might have. the problem that
arises, though, is that often with a careful reading of the text one real-
izes that the supporting document does not fully support the given
answer     even if the answer is correct.

5.1.1 support

there is of course room for argument over what    everyone    is supposed
to know, and what su   ces for a supporting document. suppose we have
the question:

when was j.f.k. killed?

and a passage

j.f.k. was assassinated in 1963.
there is no argument that this is a supporting passage, since    every-
one knows    that assassination is a form of killing. in fact, id138 is
often used as the primary resource to relate synonyms and hypernyms,
and it has never been an issue whether a relation found there is widely
known. suppose though, that the question is:

whom did j.f.k.   s widow marry?

and a passage

former    rst lady jacqueline kennedy married aristotle onassis.

this would on the face of it supply both the answer and justi   cation.
but one can imagine (at a bit of a stretch) that    everyone    already

206 user models and question complexity

knows that jacqueline kennedy married aristotle onassis, and that
what was needed to claim this as the answer was that j.f.k. had pre-
viously married jacqueline. or that wives of presidents are known as
   rst ladies, and that j.f.k. was a president. where is this knowledge?
should not the source of this knowledge be returned along with the
answer?

consider:

1260: what color does litmus paper turn when it comes into con-
tact with a strong acid?

the correct answer    red    was found in the passage

litmus is the compound that turns red in acid solution.
while apparently a good supporting passage, it required three miss-

ing pieces of knowledge.

1.    litmus paper    is a kind of paper. most often compound
nouns formed by the apposition of two nouns are a type
of the head noun, and the object behaves accordingly. for
example, a dinner jacket is a kind of jacket, not of dinner, and
its relation to dinner is unspeci   ed. thus it is unwarranted
to conclude without further information that litmus paper
behaves like litmus.

2. the question asks about a strong acid, but the passage talks
about acid of indeterminate strength. it turns out that the
acid does not have to be particularly strong to turn litmus
red, but this is not stated and it is questionable if this is
   general knowledge.   

3. it is a fact of physical chemistry that objects in solution are

in contact with the other molecules in the solution.

one could formalize the answering/justi   cation process by creating
logical forms and axioms for this information and the question, and
using theorem-proving techniques to establish the answer, in the style
of cogex (section 4.5.1.3). one can imagine an individual already
knowing any subset of the supporting facts, so for him/her a useful sup-
porting document would be one that contained the missing information.

5.2 user models

207

therefore, it should be understood that what makes a document sup-
porting is relative to the expected state of knowledge of the user. user
models are discussed further in section 5.2.

the question faced by evaluators is in how much latitude to allow
in situations like the one above. up to now the assessors at nist have
been relatively liberal, but it is not clear whether this will be useful in all
qa domains; for example intelligence analysts might seriously not want
the system to jump to conclusions. in any case, these considerations
support the view that qa-systems must o   er as much evidence as
possible (but maybe not formal proofs!) along with their answers, so
the end-users can be the    nal arbiters.

5.2 user models

where is chicago?

this was a question in the development set used by teams to prepare
for trec8. it seems innocent enough, until one realizes that there are
di   erent expectations for the answer depending on who is asking the
question. for a young child in japan, say, who has just heard the word
on television, the answer    the united states    (in japanese!) might
be perfectly su   cient. for various other people, in various circum-
stances, the answers    the mid-west,       illinois,       the shores of lake
michigan,       cook county,       down interstate 94,       where grandma
lives    might all be optimal. for star trek   s mr. spock, even    earth   
might do.

so granularity is an issue. but in this case, as with others, there
is ambiguity too. the question might not be about the city, but,
in the right conversational context, might be about where the rock
band is performing, where the play or movie is showing, where the
bulls or bears are competing, where mr. chicago has got to, or
where the university is     maybe other more parochial interpretations
exist too.

clearly, knowing what the user knows and what he or she is expect-
ing would be helpful, although telling that to present qa-systems would
be a challenge. the one thing that systems can do reliably is to com-
pute likelihood on a statistical basis     this was demonstrated with

208 user models and question complexity

virtual annotation in section 4.7.2. however, this sometimes runs into
trouble.
consider the now infamous:

73: where is the taj mahal?
there are numerous such entities in the world, including many
restaurants, but the two that are best known are the mausoleum in
agra, india and the casino in atlantic city, new jersey. a ground-rule
of trec was that in the case of ambiguity, it was the famous object,
not a model or replica, that was to be assumed to be the subject of
questions. the casino, though, is neither a model nor replica. so how
does a system determine which instance is the well-known one? one
possibility is to have a list of such entities, but where does one draw
the line? presumably a better alternative is to count instances in a non-
domain-speci   c corpus, written for the same kind of reader as might
be asking the question, to determine degree of fame.

mentions of the taj mahal were counted by the author in the corpus
used by trec8, which consisted mainly of newswire articles. instances
of the casino outnumbered instances of the mausoleum by a ratio of
7:1. despite this, nist insisted that the indian building was the only
one acceptable as an answer. this made presumably no meaningful
di   erence to the system rankings in trec8, but it does have a deeper
signi   cance. developers use past question sets and their answer keys
to train their systems, and thus face the dilemma of which answer (or
both) to use as correct.

it is not immediately apparent what the right approach is for ques-
tions like this one. one possibility is to make a list of really famous
sites and force them to override any more-frequently occurring simi-
larly named places, but as mentioned above, it is not clear what really
famous is. another approach is to disregard this particular problem.
maybe the best solution is not to get oneself into a situation where one
is forced to make a disambiguating decision without enough context
information and without the recourse of asking the user for clari   cation.
web search engines    nesse the problem because, while getting the
correct answer in    rst place is desirable, getting the correct answer
somewhere in the top 10 places (thus not requiring the user to scroll

5.2 user models

209

down) is even more desirable. techniques such as maximal marginal
relevance [9] can be used to enforce diversity in the top answers, thus
maximizing the id203 that the desired answer is somewhere in
the top few.

5.2.1 user models and de   nition questions

in section 4.7, it was mentioned that the right way to answer a de   ni-
tion question depended on both knowledge of the user and why he/she
might be asking the question. these are not always given, nor are they
always easy to estimate. one can in theory calculate the most likely
user model given a question, but this might not help much with the
task. the most likely asker of    where is chicago?    is probably not
an educated english-speaking adult, but that is the audience that the
original material in the corpora used in trec (english newswire) were
written for.

trec questions have in the past been generated from query logs of
certain web search engines. the status of the questioners is unknown.
consider the following list of what-is questions, observed in the same
logs. they are not necessarily de   nitional at all.

what is a powerful adhesive?

this most probably is asking for an instance of the class adhesive,
furthermore one that is powerful.

what is an antacid?

this could indeed be a de   nition question, but it could equally well be
asking for a brand name.

what is a yellow spotted lizard?

this is probably an inverse de   nition     asking for an entity that is so
described.

434: what is a nanometer?

this is asking for a value.

what is a star fruit?

210 user models and question complexity

the questioner probably recognizes that this is a fruit, so is asking how
this object is distinguished from other fruits (the di   erentiae).

these guesses of what was intended were made by the author. on
re   ection, they were based on an estimation of how likely the ques-
tioner knew the question term. for a qa-system to perform well,
it will not only have to be able to answer these di   erent styles of
questions, but be able to determine the most likely characteristics of
the user.

5.2.2 user models     summary

having said all that, the most important thing to note is that the end
user does not care about any of it. to an end-user, all are questions,
and a question   answering system should be able to answer any and all
of them. the user does not care that only some of them are amenable
to the techniques employed by a given system. so to be maximally
useful to that user, any qa system should ideally be able to identify
out-of-range questions and tell the user so.

5.3 question complexity

in 2001, a qa roadmap was issued [7]; this was supposed to map out
the evolution of the    eld over the next several years, and indeed in
the two following years the qa track at trec did incorporate ele-
ments from the roadmap. the roadmap posited that questions could
be located in a    spectrum    of four levels:

level 1    casual questioner   
level 2    template questioner   
level 3    cub reporter   
level 4    professional information analyst   

this strati   cation assumed that the complexity of the question (and/or
answer) was a function of the sophistication of the questioner. while
this may be true to some degree,
it turns out to be not terribly
helpful for developers of qa systems. to them the complexity is of
two kinds.

5.3 question complexity

211

5.3.1 syntactic complexity

the    rst of these is syntactic complexity. this is the situation where
questions can take many words to express, but can be decomposed into
multiple simpler questions. for example,

what is the population of the capital of the country whose soccer
team won the world cup in 2002?

this decomposes into:

what country   s soccer team won the world cup in 2002? (call
it a.)
what is the capital of a? (call it b.)
what is the population of b?

this kind of question is interesting because it is not always clear when
decomposition is necessary. for example, to answer

what is the population of the capital of tajikistan?

one might think one has to    rst    nd the capital of tajikistan
(dushanbe) and then ask what is the population of that. however,
with relatively obscure places like dushanbe, any article mentioning
its population is quite likely to also mention that it is the capital of
tajikistan. this being so, the best approach is to ignore the question   s
decomposability altogether and issue the question as-is.
the opposite can also be true. a simple question like

who is the wife of the president?

might not be immediately recognized as decomposable, but might be
better answered by    rst    nding out who the president is.

decomposable questions, such as the ones above and the kennedy-
onassis one in section 5.1.1 are characterized by having one or more
intermediate or hidden variables, which need to be found en route to
the desired answer. to answer such questions will clearly take the    eld
in the direction of general problem solving. another phenomenon with
similar characteristics is over-speci   cation.

212 user models and question complexity

5.3.2 over-speci   cation

as has been indicated many times earlier, redundancy in answer sets
is a good thing, since it gives more evidence for the recurring answer.
however, redundancy in questions in the form of overspeci   cation of
answers can be problematic. consider

469: who coined the term    cyberspace    in his novel    neuro-
mancer   ?

because it is not the case that di   erent authors coined the term
   cyberspace    in di   erent novels, the prepositional phrase    in his novel
neuromancer    is descriptive rather than restrictive in its function, and
hence can     at least in theory     be dropped. alternatively, two ques-
tions can be asked.

who coined the term    cyberspace   ?
who wrote the novel    neuromancer   ?

with the added constraint that the answer to the two must be
the same. the di   culty for qa systems is to know when to do
this. we could argue that in this case cyberspace and neuromancer
are strongly enough linked that it is quite likely that articles can
be found mentioning the two together (cf. the tajikistan example
above).

many times the overspeci   cation is not as complete as in question

469, but rather helps to narrow down the answer set. consider

916: what river in the us is known as the big muddy?

the phrase in the united states is not restrictive, since there are (as
far as we know) no big muddies anywhere else, but it should be useful
to help eliminate wrong answers. in this case, though, since it is not
so likely that in english-language newswire mentions of the mississippi
will be tagged with united states, it is of dubious value to include the
mention of united states in the question. this is to be contrasted with
questions like

548: what   s the most famous tourist attraction in rome?

since here the modifying phrase    in rome    is restrictive.

5.3 question complexity

213

from a logical point of view, in all these cases there are multiple
predications on the answer variable. this much can be derived from a
deep parse. the issue for the qa system is how and when to apply all
of these predications. we can identify three general approaches.

1. con   gurational analysis: from inspection of the examples
given above, we can see that the likely properties of the
question are independent of the speci   c entities in them,
but rather    ow from its general form. so for example, for
any question of the kind

what   s the [superlative adjective] [noun] in [place]?

the [place]
is likely to be critical. factoid question sets
do tend to have repeating question patterns, so such an
analysis would likely be fruitful, but would entail consider-
able knowledge engineering for either linguistic or statistical
approaches, and getting high coverage would be di   cult.

2. relaxation: if it is not known a priori whether the modi-
   ers are restrictive or descriptive, a reasonable approach is
to try to use all of them in a search query, and if the ini-
tial search generates empty answer sets then drop them in
turn until answers are found     cf. the successive constraint
relaxation of nus quali   er, discussed in 4.8.2. care must
be taken not to drop too many query terms, since empty
answer sets may correctly indicate the absence of a correct
answer in the corpus.

3. answer validation/veri   cation: regardless of which predi-
cations are used to locate the answer, all of them must hold
true if the answer is correct. thus they can be split into a
subset used to generate keywords for search, and a subset for
checking the answer (cf. sections 2.5.4 and 4.5.2). all possi-
ble subdivisions can be attempted, on the assumption that
less useful partitions can be identi   ed and disregarded: over-
constrained queries will give empty answer sets, and under-
constrained queries will give overly large lists of candidates,
none having stand-out scores or con   dences.

214 user models and question complexity

5.3.3 other issues in syntactic complexity

some questions can be considered syntactically complex, not because
of any inherent di   culty in parsing, but because their full meaning
only    ows from considering all the question words, which is typically
not done in automatic systems. instead, words are stemmed or lemma-
tized, stop-words are dropped, and adverbs whose meaning is position-
dependent are largely ignored. we will list here a few examples of
syntactic problems.

5.3.3.1 spelling and grammatical errors

in the real world, these are widespread and inevitable. as previously
mentioned, trec attempts to clean its question sets before release,
but errors creep in. in the author   s opinion this is a good thing, since a
certain amount of fault-tolerance in a qa-system is a desirable feature.

5.3.3.2 syntactic precision

an example of syntactic precision is whether articles are correctly
inserted. the question

1224: what is mold?

would take on a di   erent meaning entirely if it were

what is a mold?

of course, if the question were asked by anyone whose native language
does not have articles then the original question could have both mean-
ings simultaneously.

5.3.3.3 negation

using standard qa techniques, the question

212: who invented the electric guitar?

will match the passage

while mr. fender did not invent the electric guitar, he did rev-
olutionize and perfect it.

5.3 question complexity

215

most current qa-systems do not pay su   cient attention to negation,
even though its presence can completely change the viability of a can-
didate answer. however, not all instances of negation will invalidate
a candidate     it all depends on the scope of the negation and where
the candidate sits in the sentence. given the need for often deep but
always correct parsing to determine scope, such reluctance is under-
standable, and it all boils down to a bet: that leading candidates will
be in negative contexts a small enough percentage of the time that the
development e   ort to process negation properly will not pay o   . as the
   eld progresses, the parameters of this bet will likely change.

5.3.3.4 negation in questions

negation in questions requires very di   erent considerations from nega-
tion in the corpus. consider the di   erence between the two questions:

name a us state where cars are manufactured.

and

name a us state where cars are not manufactured.

certain kinds of negative events or instances are rarely asserted explic-
itly in text, but must be deduced by other means. in any case, negation
in questions cannot be ignored since doing so will almost guarantee
incorrect answers being found.

5.3.3.5 other adverbial modi   ers

like not, adverbs such as only and just and others can signi   cantly
change the meaning of a question. consider

name an astronaut who nearly made it to the moon

it is very unlikely that a responsive passage will be phrased in terms
of nearly. to satisfactorily answer such questions, one needs to know
the di   erent ways in which events can fail to happen. in this case there
are several, including: mechanical failure, illness, insu   cient seniority,
canceled mission, mission with di   erent objective. it is by no means
clear what needs to be done to    teach    a qa system to come up with

216 user models and question complexity

these reasons. one can imagine starting with an ontology of actions and
adding in pre-and post-conditions, but considerably more knowledge
would need to be incorporated for a system to begin to answer these
questions.

5.3.4

impedance match

beyond these syntactic issues, complexity is not a function of question
alone, but rather the pair {question, corpus}. in general, it is a function
of the question and the resources to answer it, which include text cor-
pora, databases, knowledge bases, ontologies, and processing modules.
according to [53],

complexity     impedance match1

if asked to rank the following three questions in order of increasing
complexity,

q1: when was queen victoria born?
q2: should the federal reserve bank raise interest rates?
q3: what is the meaning of life?

one might reasonably argue that they are already in order. suppose
now that the responsive passages to those questions are as follows.

to q1, there are three passages, possibly in entirely di   erent docu-

ments:

. . . king george iii   s only granddaughter to survive infancy was
born in 1819 . . .
. . . victoria was the only daughter of edward, duke of kent . . .
. . . george iii   s fourth son edward became duke of kent . . .

to q2, there is (prior to 2006)

   all of the current leading economic indicators point in the direc-
tion of the federal reserve bank raising interest rates at next
week   s meeting.    alan greenspan, fed chairman.

1 in electrical engineering, matching the output impedance of a source to the input
impedance of a load will maximize the energy transfer.

5.3 question complexity

217

and to q3:

the meaning of life is 42. (the hitchhiker   s guide to the galaxy)

in this admittedly contrived situation, q3 is answered as a sim-
ple factoid, q2 requires considerable nlp, along with knowledge that
alan greenspan is an authority, and q1 requires nlp, knowledge, and
problem-solving ability. the message here is that the complexity or
di   culty of a question is not intrinsic but depends on the resources
available to answer it.

6

discussion

6.1 status of question   answering

question   answering is a thriving    eld. there are many qa papers sub-
mitted for publication every year, and conference sessions and work-
shops in qa continue to be held. much progress has been made in the
last decade, giving rise to both a substantial literature base and a num-
ber of working systems. in the commercial world, there are companies
that o   er qa products, and on the web companies like google1 and
microsoft2 o   er live question   answering services.

work in the exploratory systems that have been the principal
focus of this article has established a number of tools and techniques
that are now considered important if not critical to the    eld. sys-
tems using appropriate combinations of such implementations can
be expected to perform at least moderately well against unseen
questions.

not all questions are equally easy for current systems to answer cor-
rectly. questions about properties of objects (   what is the x of y    )

1 http:/www.google.com. type a simple natural-language question instead of keywords.
2 at http://www.msdewey.com/.

218

6.2 thoughts about the future of qa 219

tend to be the most manageable, since language variations can usu-
ally be overcome using simple synonym lookup, or web search. maybe
the most challenging questions are long ones with much descriptive
material, since the chance of    nding a direct match in a single pas-
sage (the goal of most qa systems) is in general quite low, and
knowing how to best trim or decompose questions is not yet well
understood.

while this has not been proven in a formal way, it would seem that in
open-domain qa, the larger the corpus the better systems can perform.
using precise answer extraction and/or redundancy, the presence of
multiple instances of the correct answer seems to outweigh the extra
noise.

of the eight successful techniques highlighted in chapter 4, namely
web, pattern, index-augmentation, formal statistical models, logic,
projection, de   nition and event-based, it is suggested that logic- and
event-based techniques have been least exploited, and o   er the best
opportunity for making the greatest advances in the near future. sta-
tistical models too continue to o   er promise, assuming clever selection
of features and su   cient training data.

6.2 thoughts about the future of qa

with so much progress, why is not qa a solved problem? consider the
following three trec questions. each is accompanied by an answer
passage found by the author   s qa system.

q1: who shot abraham lincoln?
ap1: in it, actor edwin booth tells us his version of his life story,
in which he and not his brother john wilkes shoots abraham
lincoln at ford   s theater.

q2: how many astronauts have been on the moon?
ap2: apollo (three-man capsules) eleven    ights, 33 astronauts;
from oct. 11-22, 1968, to dec.7-19, 1972. six landed on the
moon, three circled; the longest was apollo 17, 301 hours, 51
minutes.

220 discussion

q3: what do penguins eat?
ap3: adult krill live in vast schools and are a favorite food of
whale, penguin and seal.

some systems might get the answer    john wilkes    to q1 just by
a simple proximity calculation, although to construct the full correct
answer    john wilkes booth    requires knowledge of writing styles and
real-world knowledge. however, to    prove    that it was he rather than
edwin who did the shooting would require unwrapping the implicit
double negative.

the passage retrieved for q2 contains all the necessary question
terms, and several instances of the answer type, wholenumber. how-
ever, none of them is the correct answer, which is not even derivable
from the given facts without knowing that two of the three astronauts
from every manned landing mission descended to the surface, and that
there were no repeats in apollos 11   17.

the author   s system extracted    favorite food    as the top answer
to q3. but we know that everyone knows that people and animals eat
food (by de   nition), so such an answer could never be right (in the
sense of both correct and useful).

at its core, the problem of qa (as with all nlp) is understand-
ing. if a computer system could understand the question and the
text (or other resources), the problem would go away, but a system
which could do that consistently would be to be able to beat the
turing test. we could attempt to narrow the problem down some-
what to that of    aboutness        characterizing what text is about, but
we are still not close to anything operationizable. aboutness is the
inherent scienti   c goal of the    eld of summarization, as exempli   ed
by the duc conference3 (whose practical goal is producing textual
summaries). however, aboutness is essentially the problem of reading
comprehension, which is itself considered a modern recasting of the
turing test.

digging further, we

id136, also
called id123, as in the pascal recognizing textual

consider local textual

3 http://duc.nist.gov/.

6.2 thoughts about the future of qa 221

entailment (rte)4 challenge [17]. given a question and a corpus, along
with the ability to determine answer types in the question and instances
of those types in text, one could in principle extract in turn each pas-
sage in the corpus, convert the question to a declarative statement
using answer-type instances found there, and test for entailment. suc-
cess will    prove    that the substituted entity is the correct answer to
the question, as attested by the corresponding passage. this is cur-
rently impractical, both because of the computational requirements,
and because rte is far from solved. however, we note that id136
or entailment may have an important role in veri   cation of candidate
answers, as discussed in section 4.5.

there are three sub   elds of nlp which permeate qa activities,
the combined solution to which we assert would go a long way to an
e   ective qa solution.

1. word-sense disambiguation (wsd): many words are pol-
ysemous (have multiple meanings) and wsd is the act of
determining which of several senses is the intended one. is
   bank    the side of the river or the    nancial institution, for
example?

2. co-reference resolution: a given entity can be referred to
in many di   erent ways in text, even in the same document,
and the challenge is to tie together all mentions of the same
entity, regardless of the surface form. in terms of mappings,
co-reference resolution is the opposite of wsd.

3. id14: how are the di   erent entities in
text related to each other and to the inherent actions and
events? if we are looking for dogs that bite people, we are
not interested in    nding articles about people biting dogs.5

many e   orts have been undertaken in an attempt to address (or
some would say skirt) these problems; in a qa context at least, each
of these e   orts is accompanied by the need to make trade-o   s. for the
most part, e   ciency and usability, major concerns in most engineering

4 http://www.pascal-network.org/challenges/rte/.
5 well maybe we are, but not for the stated information need.

222 discussion

problems, have not been an issue here (at least as reported in the
literature), but accuracy and coverage are. these usually surface in the
traditional ir measures of precision (avoidance of false positives) and
recall (avoidance of misses, or false negatives).

there are three kinds of property or characteristic which compo-
nents of qa systems (and many other applications) seek in pursuit of
their pragmatic and scienti   c goals, and which have become a thread
through the discussions in this article.

    identity: establishing whether two items are the same, or if
an item belongs to a given class.
    analogy: if item a is like item b, then any properties of b
or roles that b plays should likewise apply to a
    redundancy: the more instances there are of an item in a
given qa context, the more evidence this supplies for the
   correctness    of the item for the qa problem.

these properties have the advantage of being much more of a syn-
tactic nature than the three previously mentioned nlp problems, and
hence are much more tractable, and to the extent that their solution
correlates with solving the problems of semantics, such approaches
are proving to be very useful. furthermore, statistical algorithms
have proved and continue to prove very useful in establishing these
properties.

a common denominator of these properties is context, which deter-
mines whether the relationships can be meaningfully applied. one
reason for the discovery by voorhees [74] that automatic synonym
expansion is not always helpful is that it is rarely the case that a pair of
words are synonyms in all contexts, and hence always mutually inter-
changeable. for example, it has been reported that one qa system once
answered    what are invertebrates?    with    michael dukakis,    because
it found an article where someone called him spineless. on another
occasion, the author   s qa system was observed trying to determine
the number of african   american keys on a piano.

from a knowledge-based point-of-view, the activities of seeking
identity, analogy and redundancy are heuristics employed on unstruc-

6.2 thoughts about the future of qa 223

tured information as a substitute for understanding what is going on.
while much of qa achieves its success by clever counting and symbol
manipulation, it is clear that to solve the general qa problem, a sys-
tem must at some deep level understand the question, understand the
corpus text and extract and justify the answer.

acknowledgments

the author would like to thank david ferrucci and jennifer chu-
carroll for their support and feedback; the anonymous reviewers, for
their detailed, insightful and to-the-point comments and criticisms;
and lastly the invaluable input and guidance from the series editor
jamie callan. this work was supported in part by the disruptive tech-
nology o   ce (dto)   s advanced question   answering for intelligence
(aquaint) program under contract number h98230-04-c-1577.

224

references

[1] a. t. arampatzis, t. tsoris, c. h. a. koster, and t. p. van der weide,    phrase-
based information retrieval,    information processing and management, vol. 34,
no. 6, pp. 693   707, 1998.

[2] a. l. berger, v. d. pietra, and s. d. pietra,    a maximum id178 approach to
natural language processing,    computational linguistics, vol. 22, no. 1, 1996.
[3] d. bikel, r. schwartz, and r. weischedel,    an algorithm that learns what   s in

a name,    machine learning, vol. 34, no. 1   3, pp. 211   231, 1999.

[4] s. blair-goldensohn, k. r. mckeown, and a. h. schlaikjer,    answering de   ni-
tional questions: a hybrid approach,    in new directions in question answer-
ing, (m. maybury, ed.), pp. 47   57, aaai press, 2004.

[5] e. brill, j. lin, m. banko, s. dumais, and a. ng,    data-intensive question   
answering,    in proceedings of the 10th text retrieval conference (trec2001),
nist, gaithersburg, md, 2002.

[6] c. buckley, m. mitra, j. a. walz, and c. cardie,    smart high precision:
trec 7,    in proceedings of the 7th text retrieval conference (trec7),
pp. 230   243, gaithersburg, md, 1998.

[7] j. burger, c. cardie, v. chaudhri, r. gaizauskas, s. harabagiu, d. israel,
c. jacquemin, c.-y. lin, s. maiorano, g. miller, d. moldovan, b. ogden,
j. prager, e. rilo   , a. singhal, r. srihari, t. strzalkowski, e. voorhees,
and r. weishedel,    issues, tasks and program structures to roadmap
research in question & answering (q&a) arda qa roadmap (http://www-
nlpir.nist.gov/projects/duc/papers/qa, roadmap-paper v2.doc),    2001.

[8] r. byrd and y. ravin,    identifying and extracting relations in text,    in pro-
ceedings of 4th international conference on applications of natural language
and information systems (nldb 99), klagenfurt, austria, 1999.

225

226 references

[9] j. g. carbonell and j. goldstein,    the use of mmr, diversity-based reranking
for reordering documents and producing summaries,    in proceedings of the 21st
annual international acm sigir conference on research and development
in information retrieval (sigir98), melbourne, australia, august 1998.

[10] d. carmel, y. s. maarek, m. mandelbrod, y. mass, and a. so   er,    searching
xml documents via xml fragments,    in proceedings of the 26th annual inter-
national acm sigir conference on research and development in information
retrieval, 2003.

[11] j. chu-carroll, j. prager, c. welty, k. czuba, and d. ferrucci,    a multi-
strategy and multi-source approach to id53,    in proceedings of
the 11th text retrieval conference (trec2002), gaithersburg, md, 2003.

[12] j. chu-carroll, j. m. prager, k. czuba, d. ferrucci, and p. duboue,    semantic
search via xml fragments: a high precision approach to ir,    in proceedings
of the 29th annual international acm sigir conference on research and
development in information retrieval (sigir06), seattle, wa, 2006.

[13] c. l. a. clarke, g. v. cormack, and t. r. lynam,    exploiting redundancy in
id53,    in proceedings of the 24th annual international acm
sigir conference on research and development in information retrieval
(sigir01), new orleans, september 2001.

[14] t. clifton, a. colquhoun, and w. teahan,    bangor at trec 2003: q&a
and genomics tracks,    in proceedings of the 12th text retrieval conference
(trec2003), gaithersburg, md, 2004.

[15] h. cui, m.-y. kan, and t.-s. chua,    unsupervised learning of soft patterns
for generating de   nitions from online news,    in proceedings of the thirteenth
world wide web conference (www 2004), pp. 90   99, new york, may 17   22
2004.

[16] h. cui, k. li, r. sun, t.-s. chua, and m.-y. kan,    national university of
singapore at the trec-13 id53 main task,    in proceedings of
the 13th text retrieval conference (trec2004), gaithersburg, md, 2005.

[17] i. dagan, o. glickman, and b. magnini,    the pascal recognising textual
entailment challenge,    in proceedings of the pascal challenges workshop on
recognising id123, 2005.

[18] a. echihabi and d. marcu,    a noisy-channel approach to id53,   
in proceedings of the 41st annual meeting of the association for computational
linguistics (acl2003), pp. 16   23, 2003.

[19] j. gao, j.-y. nie, g. wu, and g. cao,    dependence language model for infor-
mation retrieval,    in proceedings of the 27th annual international acm sigir
conference on research and development in information retrieval (sigir04),
she   eld, uk, july 25   29, 2004.

[20] s. harabagiu, d. moldovan, c. clark, m. bowden, j. williams, and j. bensley,
   answer mining by combining extraction techniques with abductive reasoning,   
in proceedings of the 12th text retrieval conference (trec2003), gaithers-
burg, md, 2004.

[21] s. harabagiu, d. moldovan, m. pasca, r. mihalcea, m. surdeanu, r. bunescu,
r. girju, v. rus, and p. morarescu,    falcon: boosting knowledge for
answer engines,    in proceedings of the 9th text retrieval conference (trec9),
pp. 479   488, nist, gaithersburg md, 2001.

references

227

[22] s. harabagiu, d. moldovan, m. pasca, r. mihalcea, m. surdeanu, r. bunescu,
r. girju, v. rus, and p. morarescu,    the role of lexico-semantic feedback in
open-domain textual question-answering,    in proceedings of the association for
computational linguistics, pp. 274   281, july 2001.

[23] g. g. hendrix, e. d. sacerdoti, d. sagalowicz, and j. slocum,    developing a
natural language interface to complex data,    very large data bases (vldb),
p. 292, 1977.

[24] u. hermjakob, a. echihabi, and d. marcu,    natural language based refor-
mulation resource and web exploitation for id53,    in pro-
ceedings of the 11th text retrieval conference (trec2002), gaithersburg,
md, 2003.

[25] l. hirschman and r. gaizauskas,    natural language id53: the
view from here,    natural language engineering, vol. 7, no. 4, pp. 275   300,
2001.

[26] i. ittycheriah, m. franz, and s. roukos,    ibm   s statistical question-answering
system     trec-10,    in proceedings of the 10th text retrieval conference
(trec10), pp. 258   264, nist, gaithersburg, md, 2001.

[27] b. katz,    annotating the world wide web using natural language,    in proceed-
ings of the 5th riao conference on computer assisted information searching
on the internet, 1997.

[28] b. katz, s. felshin, d. yuret, a. ibrahim, j. lin, g. marton, a. j. mcfarland,
and b. temelkuran,    omnibase: uniform access to heterogeneous data for ques-
tion answering,    in proceedings of the 7th international workshop on applica-
tions of natural language to information systems (nldb), 2002.

[29] b. katz and j. lin,    selectively using relations to improve precision in question
answering,    in proceedings of the eacl-2003 workshop on natural language
processing for id53, april 2003.

[30] j. kupiec,    murax: a robust linguistic approach for id53 using
an on-line encyclopedia,    in proceedings of the 16th annual international acm
sigir conference on research and development in information retrieval
(sigir93), pp. 181   190, 1993.

[31] c. kwok, o. etzioni, and d. s. weld,    scaling id53 to the web,   
in proceedings of the 10th world wide web conference (www 2001), hong
kong, pp. 150   161, 2001.

[32] d. b. lenat,    cyc: a large-scale investment in knowledge infrastructure,    com-

munications of the acm, vol. 38, no. 11, 1995.

[33] x. li and d. roth,    learning question classi   ers: the role of semantic infor-
mation,    journal of natural language engineering, vol. 12, no. 3, pp. 229   249,
2006.

[34] c.-y. lin and e. hovy,    automatic evaluation of summaries using id165
co-occurrence statistics,    in proceedings of the human language technology
conference (hlt/naacl), 2003.

[35] j. lin,    an exploration of the principles underlying redundancy-based fac-
toid id53,    acm transactions on information systems, in press,
2007.

228 references

[36] j. lin and d. demner-fushman,    automatically evaluating answers to de   ni-
tion questions,    in proceedings of the human language technology conference
(hlt/emnlp2005), vancouver, canada, october 2005.

[37] j. lin and d. demner-fushman,    will pyramids built of nuggets topple
over?,    in proceedings the human language technology conference (hlt-
naacl2006), new york, ny, june 2006.

[38] j. lin, d. quan, v. sinha, k. bakshi, d. huynh, b. katz, and d. r. karger,
   what makes a good answer? the role of context in question   answering,   
in proceedings of the ninth ifip tc13 international conference on human-
computer interaction (interact 2003), zurich, switzerland, 2003.

[39] linguistic data consortium (ldc),    ace phase 2: information for ldc anno-

tators,    http://www.ldc.upenn.edu/projects/ace2, 2002.

[40] b. magnini, m. negri, r. prevete, and h. tanev,    is it the right answer?
exploiting web redundancy for answer validation,    association for computa-
tional linguistics 40th anniversary meeting (acl-02), university of pennsyl-
vania, philadelphia, pp. 425   432, july 7   12, 2002.

[41] d. marr,    early processing of visual information, philosophic,    transactions of

the royal soc ijt b, vol. 275, pp. 1377   1388, 1976.

[42] m. maybury, ed., new directions in id53, aaai press, 2004.
[43] d. metzler and w. b. croft,    a markov random    eld model for term depen-
dencies,    in proceedings of the 28th annual international acm sigir con-
ference on research and development in information retrieval (sigir 2005),
pp. 472   479, 2005.

[44] r. mihalcea and d. moldovan,    a method for id51 of
unrestricted text,    in proceedings of the 37th annual meeting of the association
for computational linguistics (acl-99), pp. 152   158, college park, md, 1999.
[45] g. miller,    id138: a lexical database for english,    communications of the

acm, vol. 38, no. 11, pp. 39   41, 1995.

[46] d. moldovan, c. clark, s. harabagiu, and s. maiorano,    cogex: a logic
prover for question   answering,    in proceedings of the human language tech-
nology conference (hlt-naacl03), pp. 87   93, 2003.

[47] d. moldovan, s. harabagiu, m. pasca, r. mihalcea, r. girju, r. goodrum, and
v. rus,    the structure and performance of an open-domain question answer-
ing system,    in proceedings of the 38th annual meeting of the association for
computational linguistics (acl00), pp. 563   570, 2000.

[48] d. moldovan, s. harabagiu, m. pasca, r. mihalcea, r. goodrum, r. girju, and
v. rus,    lasso: a tool for sur   ng the answer net,    in proceedings of the 8th
text retrieval conference (trec8), pp. 175   183, gaithersburg, md, 1999.

[49] d. i. moldovan and v. rus,    logic form transformation of id138 and its
applicability to id53,    in proceedings of the 39th annual meeting
of the association for computational linguistics (acl01), toulouse, france,
july 2001.

[50] a. nenkova and r. passonneau,    evaluating content selection in summariza-
tion: the pyramid method,    in proceedings of the human language technology
conference (naacl-hlt), 2004.

references

229

[51] k. papineni, s. roukos, t. ward, and w.-j. zhu,    id7: a method for auto-
matic evaluation of machine translation,    in proceedings of the 40th annual
meeting of the association for computational linguistics (acl02), 2002.

[52] m. pasca and s. harabagiu,    high performance question/answering,    in pro-
ceedings of the 24th annual international acm sigir conference on research
and development in information retrieval (sigir-2001), pp. 366   374, new
orleans la, september 2001.

[53] j. m. prager,    a curriculum-based approach to a qa roadmap,    in lrec 2002
workshop on id53: strategy and resources, las palmas, may
2002.

[54] j. m. prager, e. w. brown, a. coden, and r. radev,    id53 by
predictive annotation,    in proceedings of the 23rd annual international acm
sigir conference on research and development in information retrieval
(sigir00), pp. 184   191, athens, greece, 2000.

[55] j. m. prager, j. chu-carroll, e. w. brown, and k. czuba,    id53
by predictive annotation,    in advances in open-domain question-answering,
(t. strzalkowski and s. harabagiu, eds.), pp. 307   347, springer, 2006.

[56] j. m. prager, j. chu-carroll, and k. czuba,    statistical answer-type iden-
ti   cation in open-domain question   answering,    in proceedings of the human
language technology conference (hlt 2002), san diego, ca, march 2002.

[57] j. m. prager, j. chu-carroll, and k. czuba,    a multi-agent approach to using
redundancy and reinforcement in question   answering,    in new directions in
id53, (m. maybury, ed.), pp. 237   252, aaai press, 2004.

[58] j. m. prager, j. chu-carroll, and k. czuba,    question   answering using con-
straint satisfaction: qa-by-dossier-with-constraints,    in proceedings of the 42nd
annual meeting of the association for computational linguistics (acl04),
pp. 575   582, barcelona, spain, 2004.

[59] j. m. prager, j. chu-carroll, k. czuba, c. welty, a. ittycheriah, and
r. mahindru,    ibm   s piquant in trec2003,    in proceedings of the 12h
text retrieval conference (trec2003), gaithersburg, md, 2004.

[60] j. m. prager, p. duboue, and j. chu-carroll,    improving qa accuracy by
question inversion,    in coling-acl 2006, pp. 1073   1080, sydney, australia,
2006.

[61] j. m. prager, s. k. k. luger, and j. chu-carroll,    type nanotheories: a frame-
work for type comparison,    acm sixteenth conference on information and
knowledge management (cikm 2007), to appear.

[62] j. m. prager, d. r. radev, and k. czuba,    answering what-is questions by vir-
tual annotation,    in proceedings of human language technologies conference
(hlt), san diego, ca, march 2001.

[63] v. punyakanok, d. roth, and w. yih,    natural language id136 via depen-
dency tree mapping: an application to id53,    ai & math,
january 2004.

[64] d. r. radev, w. fan, h. qi, h. wu, and a. grewal,    probabilistic question
answering on the web,    in proc. of the int. www conf., pp. 408   419, 2002.
[65] d. r. radev, j. m. prager, and v. samn,    ranking suspected answers to
natural language questions using predictive annotation,    in proceedings 6th

230 references

applied natural language processing conference (anlp2000), pp. 150   157,
seattle, wa, 2000.

[66] d. r. radev, h. qi, z. zheng, s. blair-goldensohn, z. zhang, w. fan, and
j. m. prager,    mining the web for answers to natural language questions,    in
proceedings of the 2001 acm cikm international conference on information
and knowledge management, altlanta ga, 2001.

[67] d. ravichandran and e. hovy,    learning surface text patterns for a question
answering system,    in proceedings of the 40th annual meeting of the associ-
ation for computational linguistics (acl02), pp. 41   47, philadelphia, july
2002.

[68] e. rosch et al.,    basic objects in natural categories,    cognitive psychology,

vol. 8, pp. 382   439, 1976.

[69] a. singhal, j. choi, d. hindle, j. hirschberg, f. pereira, and w. whittaker,
   at&t at trec-7 sdr track,    in proceedings of the broadcast news tran-
scription and understanding workshop (bntuw   099), 1999.

[70] m. soubbotin,    patterns of potential answer expressions as clues to the right
answers,    in proceedings of the 10th text retrieval conference (trec2001),
pp. 293   302, nist, gaithersburg, md, 2002.

[71] m. soubbotin and s. soubbotin,    use of patterns for detection of answer
strings: a systematic approach,    in proceedings of the 11th text retrieval con-
ference (trec2002), pp. 325   331, nist, gaithersburg, md, 2003.

[72] t. strzalkowski and s. harabagiu, eds., advances in open-domain question-

answering, springer, 2006.

[73] s. tellex, b. katz, j. lin, g. marton, and a. fernandes,    quantitative evalu-
ation of passage retrieval algorithms for id53,    in proceedings of
the 26th annual international acm sigir conference on research and devel-
opment in information retrieval (sigir 2003), pp. 41   47, toronto, canada,
july 2003.

[74] e. voorhees,    id183 using lexical-semantic relations,    in proceed-
ings of the seventeenth international acm-sigir conference on research and
development in information retrieval, (w. croft and c. van rijsbergen, eds.),
pp. 61   69, 1994.

[75] e. m. voorhees and h. t. dang,    overview of the trec 2005 question
answering track,    in proceedings of the 14th text retrieval conference, nist,
gaithersburg, md, 2006.

[76] e. m. voorhees and d. k. harman, eds., trec: experiment and evaluation

in information retrieval, mit press, 2005.

[77] e. m. voorhees and d. tice,    building a id53 test collection,   
in 23rd annual international acm sigir conference on research and devel-
opment in information retrieval, pp. 200   207, athens, august 2000.

[78] n. wacholder, y. ravin, and m. choi,    disambiguation of proper names in
text,    in proceedings of the fifth conference on applied natural language pro-
cessing (anlp   97), washington, dc, april 1997.

[79] d. h. d. warren and f. c. n. pereira,    an e   cient easily adaptable system
for interpreting natural language queries,    computational linguistics, vol. 8,
no. 3   4, pp. 110   122, 1982.

references

231

[80] t. winograd,    procedures as a representation for data in a computer program
for under-standing natural language,    cognitive psychology, vol. 3, no. 1, 1972.
[81] w. a. wood,    progress in natural language understanding     an application
to lunar geology,    afips conference proceedings, vol. 42, pp. 441   450, 1973.
[82] j. xu and w. b. croft,    id183 using local and global document
analysis,    in proceedings of the 19th annual international acm sigir confer-
ence on research and development in information retrieval, pp. 4   11, zurich,
switzerland, august 18   22 1996.

[83] j. xu, a. licuanan, and r. weischedel,    trec 2003qa at bbn: answering
de   nitional questions,    in proceedings of the 12th text retrieval conference
(trec2003), gaithersburg, md, 2004.

[84] h. yang, t. chua, s. wang, and c. koh,    structured use of external knowledge
for event-based open domain id53,    in proceedings of the 26th
annual international acm sigir conference on research and development
in information retrieval, pp. 33   40, 2003.

