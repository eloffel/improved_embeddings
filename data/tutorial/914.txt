id14
past, present and future

llu    s m`arquez

talp research center

tecnhical university of catalonia

tutorial at acl-ijcnlp 2009

suntec     singapore

august 2, 2009

   version from august 3, 2009   

1

introduction

2 state-of-the-art

3 empirical evaluation and lessons learned

4 problems and challenges

5 conclusions

introduction:

tutorial overview

3

1

introduction

problem de   nition and properties
main computational resources and systems

2 state-of-the-art

3 empirical evaluation and lessons learned

4 problems and challenges

5 conclusions

introduction: problem de   nition and properties

4

tutorial overview

1

introduction

problem de   nition and properties
main computational resources and systems

2 state-of-the-art

3 empirical evaluation and lessons learned

4 problems and challenges

5 conclusions

introduction: problem de   nition and properties

5

id14: the problem

srl

def
= detecting basic event structures such as who did what to
whom, when and where

[ie point of view]

introduction: problem de   nition and properties

6

id14: the problem

srl

def
= detecting basic event structures such as who did what to
whom, when and where

[ie point of view]

s

np

np

vp

np

pp

the luxury auto maker last year sold 1,214 cars in the u.s.

a0
agent

am   tmp
temporal
marker

p
predicate

a1
object

am   loc
locative
marker

introduction: problem de   nition and properties

7

id14: the problem

srl

def
= identify the arguments of a given verb and assign them
semantic labels describing the roles they play in the predicate
(i.e., identify predicate argument structures) [cl point of view]

s

np

np

vp

np

pp

the luxury auto maker last year sold 1,214 cars in the u.s.

a0
agent

am   tmp
temporal
marker

p
predicate

a1
object

am   loc
locative
marker

introduction: problem de   nition and properties

8

id14: the problem

syntactic variations

temp

hitter

thing hit

instrument

{
z
yesterday,

}|

z }| {
kristina hit

z }| {
scott

{
z
with a baseball

}|

scott was hit by kristina yesterday with a baseball

yesterday, scott was hit with a baseball by kristina

with a baseball, kristina hit scott yesterday

yesterday scott was hit by kristina with a baseball

kristina hit scott with a baseball yesterday

example from (yih & toutanova, 2006)

introduction: problem de   nition and properties

9

id14: the problem

syntactic variations

temp

hitter

thing hit

instrument

{
z
yesterday,

}|

z }| {
kristina hit

z }| {
scott

{
z
with a baseball

}|

scott was hit by kristina yesterday with a baseball

yesterday, scott was hit with a baseball by kristina

with a baseball, kristina hit scott yesterday

yesterday scott was hit by kristina with a baseball

kristina hit scott with a baseball yesterday

example from (yih & toutanova, 2006)

introduction: problem de   nition and properties

10

id14: the problem

structural view
mapping from input to output structures:

input is text (enriched with morpho-syntactic information)

output is a sequence of labeled arguments

sequential segmenting/labeling problem

    mr. smith sent the report to me this morning .    

[mr. smith]agent sent [the report]obj to [me]recip [this morning]tmp .

mr.b   agent smithi sent theb   obj reporti too meb   recip thisb   tmp
morningi .o

introduction: problem de   nition and properties

11

id14: the problem

structural view
mapping from input to output structures:

input is text (enriched with morpho-syntactic information)

output is a sequence of labeled arguments

sequential segmenting/labeling problem

    mr. smith sent the report to me this morning .    

[mr. smith]agent sent [the report]obj to [me]recip [this morning]tmp .

mr.b   agent smithi sent theb   obj reporti too meb   recip thisb   tmp
morningi .o

introduction: problem de   nition and properties

12

id14: the problem

structural view
mapping from input to output structures:

input is text (enriched with morpho-syntactic information)

output is a sequence of labeled arguments

sequential segmenting/labeling problem

    mr. smith sent the report to me this morning .    

[mr. smith]agent sent [the report]obj to [me]recip [this morning]tmp .

mr.b   agent smithi sent theb   obj reporti too meb   recip thisb   tmp
morningi .o

introduction: problem de   nition and properties

13

id14: the problem

structural view

instrument

agent

patient

pat.

r   pat.

s

agent

s

np

vp

np

pp

np

np

np

vp

the

cat

trapped

the

rat

with

hata

that

i

had .

output is a hierarchy of labeled arguments

introduction: problem de   nition and properties

14

id14: the problem

structural view

instrument

agent

patient

pat.

r   pat.

s

agent

s

np

vp

np

pp

np

np

np

vp

the

cat

trapped

the

rat

with

hata

that

i

had .

output is a hierarchy of labeled arguments

introduction: problem de   nition and properties

15

id14: the problem

linguistic nature of the problem

argument identi   cation is strongly related to syntax

s

np

np

vp

np

pp

the luxury auto maker

last year sold 1,214 cars in the u.s.

a0
agent

am   tmp
temporal
marker

p
predicate

a1
object

am   loc
locative
marker

role labeling is a semantic task

e.g., selectional preferences should play an important role

introduction: problem de   nition and properties

16

id14: the problem

linguistic nature of the problem

argument identi   cation is strongly related to syntax

s

np

np

vp

np

pp

the luxury auto maker

last year sold 1,214 cars in the u.s.

a0
agent

am   tmp
temporal
marker

p
predicate

a1
object

am   loc
locative
marker

role labeling is a semantic task

e.g., selectional preferences should play an important role

introduction: problem de   nition and properties

17

id14: applications

is srl really useful for nlp applications?

1

information extraction (surdeanu et al., 2003; frank et al., 2007)

2 question & answering (narayanan and harabagiu, 2004)

3 id54 (melli et al., 2005)

4 coreference resolution (ponzetto and strube, 2006)

5 machine translation (boas, 2002; gim  enez and m`arquez, 2007;

wu and fung, 2009a;2009b)

6 etc. [more on srl and applications in the last section]

introduction: problem de   nition and properties

18

id14: applications

is srl really useful for nlp applications?

1

information extraction (surdeanu et al., 2003; frank et al., 2007)

2 question & answering (narayanan and harabagiu, 2004)

3 id54 (melli et al., 2005)

4 coreference resolution (ponzetto and strube, 2006)

5 machine translation (boas, 2002; gim  enez and m`arquez, 2007;

wu and fung, 2009a;2009b)

6 etc. [more on srl and applications in the last section]

introduction: problem de   nition and properties

19

id14: applications

is srl really useful for nlp applications?

1

information extraction (surdeanu et al., 2003; frank et al., 2007)

2 question & answering (narayanan and harabagiu, 2004)

3 id54 (melli et al., 2005)

4 coreference resolution (ponzetto and strube, 2006)

5 machine translation (boas, 2002; gim  enez and m`arquez, 2007;

wu and fung, 2009a;2009b)

6 etc. [more on srl and applications in the last section]

introduction: problem de   nition and properties

20

id14: applications

is srl really useful for nlp applications?

1

information extraction (surdeanu et al., 2003; frank et al., 2007)

2 question & answering (narayanan and harabagiu, 2004)

3 id54 (melli et al., 2005)

4 coreference resolution (ponzetto and strube, 2006)

5 machine translation (boas, 2002; gim  enez and m`arquez, 2007;

wu and fung, 2009a;2009b)

6 etc. [more on srl and applications in the last section]

introduction: problem de   nition and properties

21

id14: applications

is srl really useful for nlp applications?

1

information extraction (surdeanu et al., 2003; frank et al., 2007)

2 question & answering (narayanan and harabagiu, 2004)

3 id54 (melli et al., 2005)

4 coreference resolution (ponzetto and strube, 2006)

5 machine translation (boas, 2002; gim  enez and m`arquez, 2007;

wu and fung, 2009a;2009b)

6 etc. [more on srl and applications in the last section]

srl in information extraction

(surdeanu et al., 2003)

walk-through example
walk-through example

the space shuttle challenger flew apart over florida like a billion-dollar confetti killing six astronauts.

s

np

vp

advp pp

pp

s

loc

vp

np

the space shuttle challenger

flew apart

over

florida like a billion-dollar confetti killing six astronauts

arg0

pred

arg1

agent_of_death

manner_of_death deceased

ne+parsing

srl

mapping rules

templette filling 

9

introduction: problem de   nition and properties

23

id14: in context

is srl a new problem/task?

srl = shallow semantic analysis (id29)

computational semantics is not a new area in cl
(actually, it is as old as ai itself)

for decades: manual development of lexicons, grammars and
other semantic resources
(hirst, 1987; pustejovsky, 1995; copestake & flickinger, 2000)

last six years: availability of semantically annotated corpora
(e.g., propbank, framenet)

proliferation of automatic srl systems based on
statistical learning

introduction: problem de   nition and properties

24

id14: in context

is srl a new problem/task?

srl = shallow semantic analysis (id29)

computational semantics is not a new area in cl
(actually, it is as old as ai itself)

for decades: manual development of lexicons, grammars and
other semantic resources
(hirst, 1987; pustejovsky, 1995; copestake & flickinger, 2000)

last six years: availability of semantically annotated corpora
(e.g., propbank, framenet)

proliferation of automatic srl systems based on
statistical learning

introduction: problem de   nition and properties

25

id14: in context

is srl a new problem/task?

srl = shallow semantic analysis (id29)

computational semantics is not a new area in cl
(actually, it is as old as ai itself)

for decades: manual development of lexicons, grammars and
other semantic resources
(hirst, 1987; pustejovsky, 1995; copestake & flickinger, 2000)

last six years: availability of semantically annotated corpora
(e.g., propbank, framenet)

proliferation of automatic srl systems based on
statistical learning

introduction: problem de   nition and properties

26

id14: in context

is srl a new problem/task?

other related tasks on predicate semantics (related with
syntactic structure at sentence level):

verb id91 according to argument structure properties
(merlo & stevenson, 2001; schulte im walde, 2006)

acquisition of subcategorization patterns and selectional
preferences (briscoe & carroll, 1997)

classi   cation of semantic relations in noun phrases
(moldovan et al., 2004; rosario & hearst, 2004)

semantic classi   cation of prepositions (litkowski et al., 2005)
prediction of glarf (grammatical and logical representation
framework) dependency structures (meyers et al., 2009)

introduction: problem de   nition and properties

27

id14: in context

is srl a new problem/task?

see (yih & toutanova, 2006) tutorial for a comparison of srl
to other related tasks and applications: information extraction,
id29 for speech dialogs and nl interfaces to dbs, deep
id29, and prediction of function tags and case markers

introduction: problem de   nition and properties

28

id14: in context

focus of this tutorial

we will concentrate on:

development and learning of computational srl systems

speci   c points

statistical modeling and learning strategies
resources and feature engineering
evaluation and results
current shortcomings and future challenges

introduction: problem de   nition and properties

29

id14: in context

focus of this tutorial

we will concentrate on:

development and learning of computational srl systems

speci   c points

statistical modeling and learning strategies
resources and feature engineering
evaluation and results
current shortcomings and future challenges

introduction: main computational resources and systems

30

tutorial overview

1

introduction

problem de   nition and properties
main computational resources and systems

2 state-of-the-art

3 empirical evaluation and lessons learned

4 problems and challenges

5 conclusions

introduction: main computational resources and systems

31

srl: computational resources

from theory to computational resources

since (fillmore, 1968), considerable linguistic research has been
devoted to the nature of semantic roles
two broad families exist:

1 syntax-based approach : explaining the varied expression of

verb arguments within syntactic positions : levin (1993) verb
classes =    verbnet (kipper et al., 2000) =    propbank
(palmer et al., 2005) : focused on verbs

2 situation-based approach (a word activates/invokes a frame of

semantic knowledge that relates linguistic semantics to
encyclopedic knowledge) : frame semantics (fillmore, 1976)
=    framenet (fillmore et al., 2004) : words with other pos
can invoke frames too (e.g., nouns, adjectives)

introduction: main computational resources and systems

32

srl: computational resources

from theory to computational resources

since (fillmore, 1968), considerable linguistic research has been
devoted to the nature of semantic roles
two broad families exist:

1 syntax-based approach : explaining the varied expression of

verb arguments within syntactic positions : levin (1993) verb
classes =    verbnet (kipper et al., 2000) =    propbank
(palmer et al., 2005) : focused on verbs

2 situation-based approach (a word activates/invokes a frame of

semantic knowledge that relates linguistic semantics to
encyclopedic knowledge) : frame semantics (fillmore, 1976)
=    framenet (fillmore et al., 2004) : words with other pos
can invoke frames too (e.g., nouns, adjectives)

introduction: main computational resources and systems

33

srl: computational resources

from theory to computational resources

since (fillmore, 1968), considerable linguistic research has been
devoted to the nature of semantic roles
two broad families exist:

1 syntax-based approach : explaining the varied expression of

verb arguments within syntactic positions : levin (1993) verb
classes =    verbnet (kipper et al., 2000) =    propbank
(palmer et al., 2005) : focused on verbs

2 situation-based approach (a word activates/invokes a frame of

semantic knowledge that relates linguistic semantics to
encyclopedic knowledge) : frame semantics (fillmore, 1976)
=    framenet (fillmore et al., 2004) : words with other pos
can invoke frames too (e.g., nouns, adjectives)

introduction: main computational resources and systems

34

id14: corpora

framenet

(fillmore et al., 2004)

framenet project: http://framenet.icsi.berkeley.edu

based on the theory of semantic frames (fillmore, 1976)
methodology followed by lexicographers:

de   ne a situation based frame (e.g., arrest)

identify lexical items that invoke the frame
(lexical units, e.g.,    aprehend   ,    bust   )

de   ne appropriate roles for the frame
(frame elements, e.g., suspect, authorities, o   ense)
find example sentences in the corpus and annotate them

introduction: main computational resources and systems

35

id14: corpora

framenet

(fillmore et al., 2004)

framenet project: http://framenet.icsi.berkeley.edu

based on the theory of semantic frames (fillmore, 1976)
methodology followed by lexicographers:

de   ne a situation based frame (e.g., arrest)

identify lexical items that invoke the frame
(lexical units, e.g.,    aprehend   ,    bust   )

de   ne appropriate roles for the frame
(frame elements, e.g., suspect, authorities, o   ense)
find example sentences in the corpus and annotate them

introduction: main computational resources and systems

36

id14: corpora

framenet

main characteristics

(fillmore et al., 2004)

computational frame lexicon + corpus of examples annotated
with semantic roles (mostly bnc)

   800 semantic frames
>9,000 lexical units
   150,000 annotated sentences

frame speci   c roles

corpus is not a representative sample of text

introduction: main computational resources and systems

37

id14: corpora

framenet

main characteristics

(fillmore et al., 2004)

computational frame lexicon + corpus of examples annotated
with semantic roles (mostly bnc)

   800 semantic frames
>9,000 lexical units
   150,000 annotated sentences

frame speci   c roles

corpus is not a representative sample of text

introduction: main computational resources and systems

38

id14: corpora

propbank

(palmer et al., 2005)

annotation of all verbal predicates in wsj (id32)

http://verbs.colorado.edu/   mpalmer/projects/ace.html

add a semantic layer to the syntactic trees

s

np

np

vp

np

pp

the luxury auto maker

last year sold 1,214 cars in the u.s.

introduction: main computational resources and systems

39

id14: corpora

propbank

(palmer et al., 2005)

annotation of all verbal predicates in wsj (id32)

http://verbs.colorado.edu/   mpalmer/projects/ace.html

add a semantic layer to the syntactic trees

s

arg0

np

am   tmp
np

vp

arg1

np

am   loc
pp

the luxury auto maker

last year sold 1,214 cars in the u.s.

introduction: main computational resources and systems

40

id14: corpora

propbank

(palmer et al., 2005)

theory neutral numeric core roles (arg0, arg1, etc.)

interpretation of roles: verb-speci   c framesets
arg0 and arg1 usually correspond to prototypical agent and
patient/theme roles. other arguments do not consistently
generalize across verbs
di   erent senses have di   erent framesets
syntactic alternations that preserve meaning are kept
toghether in a single frameset

closed set of 13 general labels for adjuncts (e.g., temporal,
manner, location, etc.)

introduction: main computational resources and systems

41

id14: corpora

propbank

(palmer et al., 2005)

theory neutral numeric core roles (arg0, arg1, etc.)

interpretation of roles: verb-speci   c framesets
arg0 and arg1 usually correspond to prototypical agent and
patient/theme roles. other arguments do not consistently
generalize across verbs
di   erent senses have di   erent framesets
syntactic alternations that preserve meaning are kept
toghether in a single frameset

closed set of 13 general labels for adjuncts (e.g., temporal,
manner, location, etc.)

introduction: main computational resources and systems

42

id14: corpora

propbank

(palmer et al., 2005)

theory neutral numeric core roles (arg0, arg1, etc.)

interpretation of roles: verb-speci   c framesets
arg0 and arg1 usually correspond to prototypical agent and
patient/theme roles. other arguments do not consistently
generalize across verbs
di   erent senses have di   erent framesets
syntactic alternations that preserve meaning are kept
toghether in a single frameset

closed set of 13 general labels for adjuncts (e.g., temporal,
manner, location, etc.)

introduction: main computational resources and systems

43

id14: corpora

propbank: frame    les

(palmer et al., 2005)

sell.01: commerce: seller
arg0=   seller    (agent); arg1=   thing sold    (theme); arg2=   buyer   
(recipient); arg3=   price paid   ; arg4=   benefactive   
[al brownstein]arg0 sold [it]arg1 [for $60 a bottle]arg3

sell.02: give up
arg0=   entity selling out   
[john]arg0 sold out

sell.03: sell until none is/are left
arg0=   seller   ; arg1=   thing sold   ; ...
[the new harry potter]arg1 sold out [within 20 minutes]argm   tmp

introduction: main computational resources and systems

44

id14: corpora

propbank: frame    les

(palmer et al., 2005)

sell.01: commerce: seller
arg0=   seller    (agent); arg1=   thing sold    (theme); arg2=   buyer   
(recipient); arg3=   price paid   ; arg4=   benefactive   
[al brownstein]arg0 sold [it]arg1 [for $60 a bottle]arg3

sell.02: give up
arg0=   entity selling out   
[john]arg0 sold out

sell.03: sell until none is/are left
arg0=   seller   ; arg1=   thing sold   ; ...
[the new harry potter]arg1 sold out [within 20 minutes]argm   tmp

introduction: main computational resources and systems

45

id14: corpora

propbank: frame    les

(palmer et al., 2005)

sell.01: commerce: seller
arg0=   seller    (agent); arg1=   thing sold    (theme); arg2=   buyer   
(recipient); arg3=   price paid   ; arg4=   benefactive   
[al brownstein]arg0 sold [it]arg1 [for $60 a bottle]arg3

sell.02: give up
arg0=   entity selling out   
[john]arg0 sold out

sell.03: sell until none is/are left
arg0=   seller   ; arg1=   thing sold   ; ...
[the new harry potter]arg1 sold out [within 20 minutes]argm   tmp

introduction: main computational resources and systems

46

id14: corpora

propbank: frame    les

(palmer et al., 2005)

sell.01: commerce: seller
arg0=   seller    (agent); arg1=   thing sold    (theme); arg2=   buyer   
(recipient); arg3=   price paid   ; arg4=   benefactive   
[al brownstein]arg0 sold [it]arg1 [for $60 a bottle]arg3

sell.02: give up
arg0=   entity selling out   
[john]arg0 sold out

sell.03: sell until none is/are left
arg0=   seller   ; arg1=   thing sold   ; ...
[the new harry potter]arg1 sold out [within 20 minutes]argm   tmp

introduction: main computational resources and systems

47

id14: corpora

propbank

main characteristics

representative sample of text
[but: limited genre of wsj text]

(palmer et al., 2005)

non situation speci   c labels
[but: core labels do not (completely) generalize across verbs]

has become the primary resource for research in srl

introduction: main computational resources and systems

48

id14: corpora

propbank

main characteristics

representative sample of text
[but: limited genre of wsj text]

(palmer et al., 2005)

non situation speci   c labels
[but: core labels do not (completely) generalize across verbs]

has become the primary resource for research in srl

introduction: main computational resources and systems

49

id14: corpora

propbank

main characteristics

representative sample of text
[but: limited genre of wsj text]

(palmer et al., 2005)

non situation speci   c labels
[but: core labels do not (completely) generalize across verbs]

has become the primary resource for research in srl

introduction: main computational resources and systems

50

id14: corpora

nombank

(meyers et al., 2004)

nombank project: http://nlp.cs.nyu.edu/meyers/nombank.html

annotation of the nominal predicates in wsj   penntreebank

ibm appointed john
john was appointed by ibm
ibm   s appointment of john
the appointment of john by ibm
john is the current ibm appointee

annotation similar to propbank

[her]arg0 gift of [a book]arg1 [to john]arg2

introduction: main computational resources and systems

51

id14: corpora

nombank

(meyers et al., 2004)

nombank project: http://nlp.cs.nyu.edu/meyers/nombank.html

annotation of the nominal predicates in wsj   penntreebank

ibm appointed john
john was appointed by ibm
ibm   s appointment of john
the appointment of john by ibm
john is the current ibm appointee

annotation similar to propbank

[her]arg0 gift of [a book]arg1 [to john]arg2

introduction: main computational resources and systems

52

id14: corpora

languages other than english

chinese propbank
http://verbs.colorado.edu/chinese/cpb/

korean propbank
http://www.ldc.upenn.edu/

ancora corpus: spanish and catalan
http://http://clic.ub.edu/ancora/

prague dependency treebank: czech
http://ufal.m   .cuni.cz/pdt2.0/

penn arabic treebank: arabic
http://www.ircs.upenn.edu/arabic/

others are under development, e.g., scandinavian and baltic
languages

introduction: main computational resources and systems

53

id14: corpora

other extensions

framenet for german (salsa corpus), spanish and japanese

ontonotes corpus: treebank + propbank + word senses +
coreference annotation
http://www.bbn.com/nlp/ontonotes

conll   2008 shared task: joint representation for syntactic
and semantic dependencies
http://www.yr-bcn.es/conll2008/

conll   2009 shared task: extension to multiple languages
(catalan, chinese, czech, english, german, japanese, spanish)
http://ufal.m   .cuni.cz/conll2009-st/

introduction: main computational resources and systems

54

id14: systems available

tools available online that produce srl structures

assert (automatic statistical semantic role tagger)
http://cemantix.org/assert

uiuc system
http://l2r.cs.uiuc.edu/~cogcomp/srl-demo.php

swirl
http://www.surdeanu.name/mihai

shalmaneser: framenet-based system from salsa project
http://www.coli.uni-saarland.de/projects/salsa/shal/

state-of-the-art:

tutorial overview

55

1

introduction

2 state-of-the-art
architecture
feature engineering
srl systems in detail

3 empirical evaluation and lessons learned

4 problems and challenges

5 conclusions

state-of-the-art: architecture

tutorial overview

56

1

introduction

2 state-of-the-art
architecture
feature engineering
srl systems in detail

3 empirical evaluation and lessons learned

4 problems and challenges

5 conclusions

state-of-the-art: architecture

srl: step by step

the problem

57

given a sentence and a designated predicate p

every subsequence of words (not necessarily contiguous) is a
potential argument of p

arguments can be discontinuous:

srl can be formalized as a mapping from word substrings to
the set of argument labels plus    non-argument   

this is clearly impractical. we need to    lter the set of
candidates...

state-of-the-art: architecture

srl: step by step

the problem

58

given a sentence and a designated predicate p

every subsequence of words (not necessarily contiguous) is a
potential argument of p

arguments can be discontinuous:
   one troubling aspect of dec   s results, analysts said, was its
performance in europe   

srl can be formalized as a mapping from word substrings to
the set of argument labels plus    non-argument   

this is clearly impractical. we need to    lter the set of
candidates...

state-of-the-art: architecture

srl: step by step

the problem

59

given a sentence and a designated predicate p

every subsequence of words (not necessarily contiguous) is a
potential argument of p

arguments can be discontinuous:
[one troubling aspect of dec   s results]arg1 , [analysts]arg0 said, [was
its performance in europe]c    arg1 .

srl can be formalized as a mapping from word substrings to
the set of argument labels plus    non-argument   

this is clearly impractical. we need to    lter the set of
candidates...

state-of-the-art: architecture

srl: step by step

the problem

60

given a sentence and a designated predicate p

every subsequence of words (not necessarily contiguous) is a
potential argument of p

arguments can be discontinuous:
[one troubling aspect of dec   s results]arg1 , [analysts]arg0 said, [was
its performance in europe]c    arg1 .

srl can be formalized as a mapping from word substrings to
the set of argument labels plus    non-argument   

this is clearly impractical. we need to    lter the set of
candidates...

state-of-the-art: architecture

srl: step by step

61

step 1: select argument candidates

given a sentence and a designated predicate

parse the sentence
identify candidates in tree constituents (   ltering/pruning)

simple heuristic rules can be used, which maintain a high recall
(xue & palmer, 2004)

key point: 95% of semantic arguments coincide with unique
syntactic constituents in the gold parse tree (propbank)

matching is still    90% when using automatic parsers

state-of-the-art: architecture

srl: step by step

62

step 1: select argument candidates

given a sentence and a designated predicate

parse the sentence
identify candidates in tree constituents (   ltering/pruning)

simple heuristic rules can be used, which maintain a high recall
(xue & palmer, 2004)

key point: 95% of semantic arguments coincide with unique
syntactic constituents in the gold parse tree (propbank)

matching is still    90% when using automatic parsers

state-of-the-art: architecture

srl: step by step

63

step 2: local scoring of candidates

apply classi   ers to assign con   dence scores to argument
candidates (all labels +    non-argument   )

candidates are treated independently of each other
identi   cation and classi   cation may be performed separately

computational reasons but also modularity in feature
engineering

many ml paradigms have been used: not big di   erences

features are more important

state-of-the-art: architecture

srl: step by step

64

step 2: local scoring of candidates

apply classi   ers to assign con   dence scores to argument
candidates (all labels +    non-argument   )

candidates are treated independently of each other
identi   cation and classi   cation may be performed separately

computational reasons but also modularity in feature
engineering

many ml paradigms have been used: not big di   erences

features are more important

state-of-the-art: architecture

srl: step by step

65

step 2: local scoring of candidates

apply classi   ers to assign con   dence scores to argument
candidates (all labels +    non-argument   )

candidates are treated independently of each other
identi   cation and classi   cation may be performed separately

computational reasons but also modularity in feature
engineering

many ml paradigms have been used: not big di   erences

features are more important

state-of-the-art: architecture

srl: steps 1 + 2

66

s

scotty

said

the  same words more loudly

state-of-the-art: architecture

srl: steps 1 + 2

67

s

vp

np

np

advp

nnp

vbd dt jj nns rbr rb

scotty

said

the  same words more loudly

state-of-the-art: architecture

srl: steps 1 + 2

68

s

vp

np

np

advp

nnp

vbd dt jj nns rbr rb

scotty

said

the  same words more loudly

state-of-the-art: architecture

srl: steps 1 + 2

69

s

vp

np

np

advp

nnp

vbd dt jj nns rbr rb

scotty

said

the  same words more loudly

state-of-the-art: architecture

srl: steps 1 + 2

70

s

vp

arg

np

arg

np

arg

advp

vbd dt jj nns rbr rb

x

nnp

scotty

said

the  same words more loudly

state-of-the-art: architecture

srl: steps 1 + 2

71

s

vp

np

np

advp

nnp

vbd dt jj nns rbr rb

scotty

said

the  same words more loudly

state-of-the-art: architecture

srl: steps 1 + 2

72

s

vp

np

a0

np

a1

advp

am   
mnr

nnp

vbd dt jj nns rbr rb

scotty

said

the  same words more loudly

state-of-the-art: architecture

73

srl: motivating next step (joint scoring)

s

vp

np

sc(a0)=0.78 
sc(a1)=0.06
...
sc(none)=0.01

np

advp

nnp

vbd dt jj nns

rbr rb

scotty

said

the  same words more loudly

state-of-the-art: architecture

74

srl: motivating next step (joint scoring)

s

vp

sc(a0)=0.07 
sc(a1)=0.80
...
sc(none)=0.02

sc(a0)=0.03
sc(a1)=0.01
...
sc(none)=0.04

np

advp

np

sc(a0)=0.78 
sc(a1)=0.06
...
sc(none)=0.01

nnp

vbd dt jj nns rbr rb

scotty

said

the  same words more loudly

state-of-the-art: architecture

75

srl: motivating next step (joint scoring)

s

vp

sc(a0)=0.07 
sc(a1)=0.80
...
sc(none)=0.02

sc(a0)=0.03
sc(a1)=0.01
...
sc(none)=0.04

np

advp

np

sc(a0)=0.78 
sc(a1)=0.06
...
sc(none)=0.01

nnp

vbd dt jj nns rbr rb

scotty

said

the  same words more loudly

state-of-the-art: architecture

76

srl: motivating next step (joint scoring)

global score = 0.30

s

vp

sc(a0)=0.07 
sc(a1)=0.80
...
sc(none)=0.02

sc(a0)=0.03
sc(a1)=0.01
...
sc(none)=0.04

np

advp

np

sc(a0)=0.78 
sc(a1)=0.06
...
sc(none)=0.01

nnp

vbd dt jj nns rbr rb

scotty

said

the  same words

more loudly

state-of-the-art: architecture

77

srl: motivating next step (joint scoring)

s

vp

sc(a0)=0.07 
sc(a1)=0.80
...
sc(none)=0.02

sc(a0)=0.03
sc(a1)=0.01
...
sc(none)=0.04

np

advp

gs = 0.45

np

sc(a0)=0.78 
sc(a1)=0.06
...
sc(none)=0.01

nnp

vbd dt jj nns rbr rb

scotty

said

the  same words more loudly

s

vp

sc(a0)=0.07 
sc(a1)=0.80
...
sc(none)=0.02

sc(a0)=0.03
sc(a1)=0.01
...
sc(none)=0.04

np

advp

gs = 0.15

np

sc(a0)=0.78 
sc(a1)=0.06
...
sc(none)=0.01

nnp

vbd dt jj nns rbr rb

scotty

said

the  same words more loudly

s

vp

sc(a0)=0.07 
sc(a1)=0.80
...
sc(none)=0.02

sc(a0)=0.03
sc(a1)=0.01
...
sc(none)=0.04

np

advp

gs = 0.09

np

sc(a0)=0.78 
sc(a1)=0.06
...
sc(none)=0.01

nnp

vbd dt jj nns rbr rb

scotty

said

the  same words more loudly

...

s

vp

sc(a0)=0.07 
sc(a1)=0.80
...
sc(none)=0.02

sc(a0)=0.03
sc(a1)=0.01
...
sc(none)=0.04

np

advp

gs = 0.001

np

sc(a0)=0.78 
sc(a1)=0.06
...
sc(none)=0.01

nnp

vbd dt jj nns rbr rb

scotty

said

the  same words more loudly

state-of-the-art: architecture

78

srl: motivating next step (joint scoring)

s

vp

sc(a0)=0.07 
sc(a1)=0.80
...
sc(none)=0.02

sc(a0)=0.03
sc(a1)=0.01
...
sc(none)=0.04

np

advp

arg max
gs = 0.45

np

sc(a0)=0.78 
sc(a1)=0.06
...
sc(none)=0.01

nnp

vbd dt jj nns rbr rb

scotty

said

the  same words more loudly

s

vp

sc(a0)=0.07 
sc(a1)=0.80
...
sc(none)=0.02

sc(a0)=0.03
sc(a1)=0.01
...
sc(none)=0.04

np

advp

gs = 0.15

np

sc(a0)=0.78 
sc(a1)=0.06
...
sc(none)=0.01

nnp

vbd dt jj

nns rbr rb

scotty

said

the  same words more loudly

s

vp

sc(a0)=0.07 
sc(a1)=0.80
...
sc(none)=0.02

sc(a0)=0.03
sc(a1)=0.01
...
sc(none)=0.04

np

advp

gs = 0.09

np

sc(a0)=0.78 
sc(a1)=0.06
...
sc(none)=0.01

nnp

vbd dt jj nns rbr rb

scotty

said

the  same words more loudly

...

s

vp

sc(a0)=0.07 
sc(a1)=0.80
...
sc(none)=0.02

sc(a0)=0.03
sc(a1)=0.01
...
sc(none)=0.04

np

advp

gs = 0.001

np

sc(a0)=0.78 
sc(a1)=0.06
...
sc(none)=0.01

nnp

vbd dt jj nns rbr rb

scotty

said

the  same words more loudly

state-of-the-art: architecture

srl: step by step

79

step 3: joint scoring     paradigmatic examples

combine local predictions through ilp to    nd the best
solution according to structural and linguistic constraints
(koomen et al., 2005; punyakanok et al., 2008)
   learning +dependencies +search

re-ranking of several candidate solutions
(haghighi et al., 2005; toutanova et al., 2008)
+learning +dependencies    search

global search integrating joint scoring: tree crfs
(cohn & blunsom, 2005)
+learning +/   dependencies +/   search

state-of-the-art: architecture

srl: step by step

80

step 3: joint scoring     paradigmatic examples

combine local predictions through ilp to    nd the best
solution according to structural and linguistic constraints
(koomen et al., 2005; punyakanok et al., 2008)
   learning +dependencies +search

re-ranking of several candidate solutions
(haghighi et al., 2005; toutanova et al., 2008)
+learning +dependencies    search

global search integrating joint scoring: tree crfs
(cohn & blunsom, 2005)
+learning +/   dependencies +/   search

state-of-the-art: architecture

srl: step by step

81

step 4: post-processing

application of a set of heuristic rules to:

correct frequent errors

enforce consistency in the solution

state-of-the-art: architecture

srl: step by step

82

exceptions to the standard architecture

1 joint treatment of all predicates in the sentence

(carreras et al., 2004; surdeanu et al., 2007)

2 specialized parsing for srl

syntactic parser trained to predict argument candidates
(yi & palmer, 2005)
joint parsing and srl: id29
(musillo & merlo, 2006; merlo & musillo, 2008)
srl based on id33 (johansson & nugues, 2007)
systems from the conll   2008 and 2009 shared tasks
(surdeanu et al., 2008; haji  c et al., 2009)

3 sequential labeling instead of tree traversing. motivated by:
the lack of full parse trees (carreras & m`arquez, conll-2004)
allowing e   cient search in joint id136 (m`arquez et al., 2005)

state-of-the-art: architecture

srl: step by step

83

exceptions to the standard architecture

1 joint treatment of all predicates in the sentence

(carreras et al., 2004; surdeanu et al., 2007)

2 specialized parsing for srl

syntactic parser trained to predict argument candidates
(yi & palmer, 2005)
joint parsing and srl: id29
(musillo & merlo, 2006; merlo & musillo, 2008)
srl based on id33 (johansson & nugues, 2007)
systems from the conll   2008 and 2009 shared tasks
(surdeanu et al., 2008; haji  c et al., 2009)

3 sequential labeling instead of tree traversing. motivated by:
the lack of full parse trees (carreras & m`arquez, conll-2004)
allowing e   cient search in joint id136 (m`arquez et al., 2005)

state-of-the-art: architecture

srl: step by step

84

exceptions to the standard architecture

1 joint treatment of all predicates in the sentence

(carreras et al., 2004; surdeanu et al., 2007)

2 specialized parsing for srl

syntactic parser trained to predict argument candidates
(yi & palmer, 2005)
joint parsing and srl: id29
(musillo & merlo, 2006; merlo & musillo, 2008)
srl based on id33 (johansson & nugues, 2007)
systems from the conll   2008 and 2009 shared tasks
(surdeanu et al., 2008; haji  c et al., 2009)

3 sequential labeling instead of tree traversing. motivated by:
the lack of full parse trees (carreras & m`arquez, conll-2004)
allowing e   cient search in joint id136 (m`arquez et al., 2005)

state-of-the-art: feature engineering

tutorial overview

85

1

introduction

2 state-of-the-art
architecture
feature engineering
srl systems in detail

3 empirical evaluation and lessons learned

4 problems and challenges

5 conclusions

state-of-the-art: feature engineering

86

srl: feature engineering

features: local scoring

(gildea & jurafsky, 2002)

highly in   uential for the srl work.
they characterize:

1 the candidate argument (constituent) and its context:

phrase type, head word, governing category of the constituent

2 the verb predicate and its context: lemma, voice,

subcategorization pattern of the verb

3 the relation between the consituent and the predicate:

position of the constituent with respect to the verb, category
path between them.

state-of-the-art: feature engineering

87

srl: feature engineering

features: local scoring

(gildea & jurafsky, 2002)

highly in   uential for the srl work.
they characterize:

1 the candidate argument (constituent) and its context:

phrase type, head word, governing category of the constituent

2 the verb predicate and its context: lemma, voice,

subcategorization pattern of the verb

3 the relation between the consituent and the predicate:

position of the constituent with respect to the verb, category
path between them.

state-of-the-art: feature engineering

88

srl: feature engineering

features: local scoring

(gildea & jurafsky, 2002)

highly in   uential for the srl work.
they characterize:

1 the candidate argument (constituent) and its context:

phrase type, head word, governing category of the constituent

2 the verb predicate and its context: lemma, voice,

subcategorization pattern of the verb

3 the relation between the consituent and the predicate:

position of the constituent with respect to the verb, category
path between them.

state-of-the-art: feature engineering

89

srl: feature engineering

features: local scoring

(gildea & jurafsky, 2002)

highly in   uential for the srl work.
they characterize:

1 the candidate argument (constituent) and its context:

phrase type, head word, governing category of the constituent

2 the verb predicate and its context: lemma, voice,

subcategorization pattern of the verb

3 the relation between the consituent and the predicate:

position of the constituent with respect to the verb, category
path between them.

state-of-the-art: feature engineering

90

srl: feature engineering

features: local scoring     extensions

   brute force    features. applied to the constituent and
possibly to parent and siblings:

first and last words/pos in the constituent, bag-of-words,
id165s of pos, and sequence of top syntactic elements in the
constituent.

linguistically   inspired features

content word, named entities (surdeanu et al., 2003), syntactic
frame (xue & palmer, 2004), path variations, semantic
compatibility between constituent head and predicate (zapirain
et al., 2007;2009), etc.

signi   cant (and cumulative) increase in performance

state-of-the-art: feature engineering

91

srl: feature engineering

features: local scoring     extensions

   brute force    features. applied to the constituent and
possibly to parent and siblings:

first and last words/pos in the constituent, bag-of-words,
id165s of pos, and sequence of top syntactic elements in the
constituent.

linguistically   inspired features

content word, named entities (surdeanu et al., 2003), syntactic
frame (xue & palmer, 2004), path variations, semantic
compatibility between constituent head and predicate (zapirain
et al., 2007;2009), etc.

signi   cant (and cumulative) increase in performance

state-of-the-art: feature engineering

92

srl: feature engineering

features: local scoring     extensions

   brute force    features. applied to the constituent and
possibly to parent and siblings:

first and last words/pos in the constituent, bag-of-words,
id165s of pos, and sequence of top syntactic elements in the
constituent.

linguistically   inspired features

content word, named entities (surdeanu et al., 2003), syntactic
frame (xue & palmer, 2004), path variations, semantic
compatibility between constituent head and predicate (zapirain
et al., 2007;2009), etc.

signi   cant (and cumulative) increase in performance

state-of-the-art: feature engineering

93

srl: feature engineering

features: joint scoring

richer features taking into account information from several
arguments at a time

best example: when doing re-ranking one may codify patterns
on the whole candidate argument structure
(hiaghighi et al., 2005; toutanova et al., 2008)

good for capturing global preferences

(more on this approach in a while)

state-of-the-art: feature engineering

94

srl: feature engineering

features: joint scoring

richer features taking into account information from several
arguments at a time

best example: when doing re-ranking one may codify patterns
on the whole candidate argument structure
(hiaghighi et al., 2005; toutanova et al., 2008)

good for capturing global preferences

(more on this approach in a while)

state-of-the-art: feature engineering

95

srl: feature engineering

features: the kernel approach

knowledge poor approach

let the id81 to compute the similarity/di   erences
between examples by considering all possible substructures as
features

motivation: avoid intense knowledge engineering

potentially useful for rapid system development and working
with under resourced languages

mostly variants of collins    all-subtrees convolution kernel
(collins & du   y 2001; moschitti et al., 2008)

state-of-the-art: feature engineering

96

srl: feature engineering

features: the kernel approach

knowledge poor approach

let the id81 to compute the similarity/di   erences
between examples by considering all possible substructures as
features

motivation: avoid intense knowledge engineering

potentially useful for rapid system development and working
with under resourced languages

mostly variants of collins    all-subtrees convolution kernel
(collins & du   y 2001; moschitti et al., 2008)

state-of-the-art: feature engineering

97

srl: feature engineering

features: the kernel approach

knowledge poor approach

let the id81 to compute the similarity/di   erences
between examples by considering all possible substructures as
features

motivation: avoid intense knowledge engineering

potentially useful for rapid system development and working
with under resourced languages

mostly variants of collins    all-subtrees convolution kernel
(collins & du   y 2001; moschitti et al., 2008)

state-of-the-art: feature engineering

98

srl: feature engineering

features: the kernel approach

problems with the structural kernel approach

1 uncontrolled explosion of features

2 low e   ciency

3

inability to use linguistic knowledge

some works in the previous directions

id14 using a grammar-driven convolution
tree kernel. includes approximate matching at substructure
and node levels (zhang et al., 2008)

feature selection in kernel space and lineariziation of tree
id81s (pighin & moschitti, 2009)

state-of-the-art: feature engineering

99

srl: feature engineering

features: the kernel approach

problems with the structural kernel approach

1 uncontrolled explosion of features

2 low e   ciency

3

inability to use linguistic knowledge

some works in the previous directions

id14 using a grammar-driven convolution
tree kernel. includes approximate matching at substructure
and node levels (zhang et al., 2008)

feature selection in kernel space and lineariziation of tree
id81s (pighin & moschitti, 2009)

state-of-the-art: srl systems in detail

tutorial overview

100

1

introduction

2 state-of-the-art
architecture
feature engineering
srl systems in detail

3 empirical evaluation and lessons learned

4 problems and challenges

5 conclusions

state-of-the-art: srl systems in detail

101

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

architecture

1

identify argument candidates

pruning (xue & palmer, 2004)
argument identi   cation: binary classi   cation (using snow)

2 classify argument candidates

argument classi   er: multi-class classi   cation (snow)

3

id136

use the estimated id203 distribution given by the
argument classi   er
use structural and linguistic constraints
infer the optimal global output

state-of-the-art: srl systems in detail

102

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

architecture

1

identify argument candidates

pruning (xue & palmer, 2004)
argument identi   cation: binary classi   cation (using snow)

2 classify argument candidates

argument classi   er: multi-class classi   cation (snow)

3

id136

use the estimated id203 distribution given by the
argument classi   er
use structural and linguistic constraints
infer the optimal global output

state-of-the-art: srl systems in detail

103

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

architecture

1

identify argument candidates

pruning (xue & palmer, 2004)
argument identi   cation: binary classi   cation (using snow)

2 classify argument candidates

argument classi   er: multi-class classi   cation (snow)

3

id136

use the estimated id203 distribution given by the
argument classi   er
use structural and linguistic constraints
infer the optimal global output

state-of-the-art: srl systems in detail

104

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

id136

the output of the argument classi   er often violates some
constraints, especially when the sentence is long

finding the best legitimate output is formalized as an
optimization problem and solved via integer linear
programming (roth & yih, 2004)
input formed by:

the id203 estimation (by the argument classi   er)
structural and linguistic constraints

allows incorporating expressive constraints (non-sequential)
on the variables (the arguments types)

state-of-the-art: srl systems in detail

105

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

id136

the output of the argument classi   er often violates some
constraints, especially when the sentence is long

finding the best legitimate output is formalized as an
optimization problem and solved via integer linear
programming (roth & yih, 2004)
input formed by:

the id203 estimation (by the argument classi   er)
structural and linguistic constraints

allows incorporating expressive constraints (non-sequential)
on the variables (the arguments types)

state-of-the-art: srl systems in detail

106

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

id136

the output of the argument classi   er often violates some
constraints, especially when the sentence is long

finding the best legitimate output is formalized as an
optimization problem and solved via integer linear
programming (roth & yih, 2004)
input formed by:

the id203 estimation (by the argument classi   er)
structural and linguistic constraints

allows incorporating expressive constraints (non-sequential)
on the variables (the arguments types)

state-of-the-art: srl systems in detail

107

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

id136

the output of the argument classi   er often violates some
constraints, especially when the sentence is long

finding the best legitimate output is formalized as an
optimization problem and solved via integer linear
programming (roth & yih, 2004)
input formed by:

the id203 estimation (by the argument classi   er)
structural and linguistic constraints

allows incorporating expressive constraints (non-sequential)
on the variables (the arguments types)

state-of-the-art: srl systems in detail

108

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

integer id135 id136

for each candidate argument ai (1     i     n),
set up a boolean variable: ai ,t indicating whether ai is
classi   ed as argument type t
goal is to maximize: pi score(ai = t)    ai ,t
subject to the (linear) constraints

if score(ai = t) = p(ai = t), the objective is to    nd the
assignment that maximizes the expected number of
arguments that are correct and satis   es the constraints

state-of-the-art: srl systems in detail

109

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

constraints: examples

i =1 ai ,arg 0     1

no duplicate argument classes: pn
on discontinuous arguments (c-arg)
   j(1     j     n), pj   1
on reference arguments (r-arg)
   j(1     j     n), pi 6=j ai ,arg 0     aj ,r   arg 0
many other possible constraints:

i =1 ai ,arg 0     aj ,c    arg 0

unique labels
no overlapping or embedding
relations between number of arguments; order constraints
if verb is of type a, no argument of type b

ilp id136 can be used to combine di   erent srl systems

state-of-the-art: srl systems in detail

110

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

constraints: examples

i =1 ai ,arg 0     1

no duplicate argument classes: pn
on discontinuous arguments (c-arg)
   j(1     j     n), pj   1
on reference arguments (r-arg)
   j(1     j     n), pi 6=j ai ,arg 0     aj ,r   arg 0
many other possible constraints:

i =1 ai ,arg 0     aj ,c    arg 0

unique labels
no overlapping or embedding
relations between number of arguments; order constraints
if verb is of type a, no argument of type b

ilp id136 can be used to combine di   erent srl systems

state-of-the-art: srl systems in detail

111

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

constraints: examples

i =1 ai ,arg 0     1

no duplicate argument classes: pn
on discontinuous arguments (c-arg)
   j(1     j     n), pj   1
on reference arguments (r-arg)
   j(1     j     n), pi 6=j ai ,arg 0     aj ,r   arg 0
many other possible constraints:

i =1 ai ,arg 0     aj ,c    arg 0

unique labels
no overlapping or embedding
relations between number of arguments; order constraints
if verb is of type a, no argument of type b

ilp id136 can be used to combine di   erent srl systems

state-of-the-art: srl systems in detail

112

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

constraints: examples

i =1 ai ,arg 0     1

i =1 ai ,arg 0     aj ,c    arg 0

no duplicate argument classes: pn
on discontinuous arguments (c-arg)
   j(1     j     n), pj   1
on reference arguments (r-arg)
[the deregulation]arg1 of railroads and trucking companies
[that]r   arg1 began [in 1980]am   tmp enabled ...
   j(1     j     n), pi 6=j ai ,arg 0     aj ,r   arg 0
many other possible constraints:

unique labels
no overlapping or embedding
relations between number of arguments; order constraints
if verb is of type a, no argument of type b

ilp id136 can be used to combine di   erent srl systems

state-of-the-art: srl systems in detail

113

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

constraints: examples

i =1 ai ,arg 0     1

no duplicate argument classes: pn
on discontinuous arguments (c-arg)
   j(1     j     n), pj   1
on reference arguments (r-arg)
   j(1     j     n), pi 6=j ai ,arg 0     aj ,r   arg 0
many other possible constraints:

i =1 ai ,arg 0     aj ,c    arg 0

unique labels
no overlapping or embedding
relations between number of arguments; order constraints
if verb is of type a, no argument of type b

ilp id136 can be used to combine di   erent srl systems

state-of-the-art: srl systems in detail

114

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

constraints: examples

i =1 ai ,arg 0     1

no duplicate argument classes: pn
on discontinuous arguments (c-arg)
   j(1     j     n), pj   1
on reference arguments (r-arg)
   j(1     j     n), pi 6=j ai ,arg 0     aj ,r   arg 0
many other possible constraints:

i =1 ai ,arg 0     aj ,c    arg 0

unique labels
no overlapping or embedding
relations between number of arguments; order constraints
if verb is of type a, no argument of type b

ilp id136 can be used to combine di   erent srl systems

state-of-the-art: srl systems in detail

115

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

constraints: examples

i =1 ai ,arg 0     1

no duplicate argument classes: pn
on discontinuous arguments (c-arg)
   j(1     j     n), pj   1
on reference arguments (r-arg)
   j(1     j     n), pi 6=j ai ,arg 0     aj ,r   arg 0
many other possible constraints:

i =1 ai ,arg 0     aj ,c    arg 0

unique labels
no overlapping or embedding
relations between number of arguments; order constraints
if verb is of type a, no argument of type b

ilp id136 can be used to combine di   erent srl systems

state-of-the-art: srl systems in detail

116

generalized id136     ilp (koomen et al., 2005; punyakanok et al., 2008)

f1

79.44

col

char

char-2

char-3

char-4

char-5

wsj

brown

67.75

combined

50

60

70

80

90

id136 with many parsers improves results     2.6 f1 points

best results at conll-2005 shared task (carreras & m`arquez, 2005)

state-of-the-art: srl systems in detail

117

joint system based on reranking

(toutanova et al., 2008)

architecture

use a probabilistic local srl model to produce multiple
(n-best) candidate solutions for the predicate structure

use a feature   rich reranking model to select the best solution
among them

main goal: is to build a rich model for joint scoring, which
takes into account the dependencies among the labels of
argument phrases

use a second layer of reranking by combining di   erent
solutions coming from alternatieve input syntactic parses

state-of-the-art: srl systems in detail

118

joint system based on reranking

(toutanova et al., 2008)

architecture

use a probabilistic local srl model to produce multiple
(n-best) candidate solutions for the predicate structure

use a feature   rich reranking model to select the best solution
among them

main goal: is to build a rich model for joint scoring, which
takes into account the dependencies among the labels of
argument phrases

use a second layer of reranking by combining di   erent
solutions coming from alternatieve input syntactic parses

state-of-the-art: srl systems in detail

119

joint system based on reranking

(toutanova et al., 2008)

models

simple local scoring model with strong independence
assumptions, trained with id148 (maxent):
p(labels|tree) = qnodei    tree p(labelsi |nodei )
find top n non-overlapping assignments for local model using
id145

select the best assignment among top n using a joint
log-linear model (collins, 2000)

the resulting id203 of a complete labeling l of the tree
for a predicate p is given by:
psrl(l|tree, p) = log (pjoint (l|tree, p)) +   log (plocal(l|tree, p))

state-of-the-art: srl systems in detail

120

joint system based on reranking

(toutanova et al., 2008)

models

simple local scoring model with strong independence
assumptions, trained with id148 (maxent):
p(labels|tree) = qnodei    tree p(labelsi |nodei )
find top n non-overlapping assignments for local model using
id145

select the best assignment among top n using a joint
log-linear model (collins, 2000)

the resulting id203 of a complete labeling l of the tree
for a predicate p is given by:
psrl(l|tree, p) = log (pjoint (l|tree, p)) +   log (plocal(l|tree, p))

state-of-the-art: srl systems in detail

121

joint system based on reranking

(toutanova et al., 2008)

features: joint scoring

slide from (yih & toutanova, 2006)

joint model features

s
s

a0

np

am-tmp

np

vp

a1

np

am-tmp

np

yesterday  ,   kristina       hit        scott   hard

repetition features: count of arguments with a given label c(am-tmp)=2

complete sequence syntactic-semantic features for the core arguments:

[np_a0 hit np_a1] , [np_a0 vbd np_a1]  (backoff)

[np_a0 hit] (left backoff)

[np_arg hit np_arg] (no specific labels)

[1 hit 1] (counts of left and right core arguments)

66

state-of-the-art: srl systems in detail

122

joint system based on reranking

(toutanova et al., 2008)

enhancement by using multiple trees

for top k trees from charniak   s parser, t1, t2, . . . , tk ,    nd
corresponding best srl assignments l1, l2, . . . , lk and
choose the tree and assignment that maximize the score
(approx. joint id203 of tree and assignment)
score(li , ti ) =   log (p(ti )) + log (psrl(li |ti ))

final results (2nd best at conll):
wsj-23: 78.45 (f1), 79.54 (prec.), 77.39 (rec.)
brown: 67.71 (f1), 70.24 (prec.), 65.37 (rec.)

improvement due to the joint model: > 2 f1 points

state-of-the-art: srl systems in detail

123

joint system based on reranking

(toutanova et al., 2008)

enhancement by using multiple trees

for top k trees from charniak   s parser, t1, t2, . . . , tk ,    nd
corresponding best srl assignments l1, l2, . . . , lk and
choose the tree and assignment that maximize the score
(approx. joint id203 of tree and assignment)
score(li , ti ) =   log (p(ti )) + log (psrl(li |ti ))

final results (2nd best at conll):
wsj-23: 78.45 (f1), 79.54 (prec.), 77.39 (rec.)
brown: 67.71 (f1), 70.24 (prec.), 65.37 (rec.)

improvement due to the joint model: > 2 f1 points

state-of-the-art: srl systems in detail

124

joint system based on reranking

(toutanova et al., 2008)

enhancement by using multiple trees

for top k trees from charniak   s parser, t1, t2, . . . , tk ,    nd
corresponding best srl assignments l1, l2, . . . , lk and
choose the tree and assignment that maximize the score
(approx. joint id203 of tree and assignment)
score(li , ti ) =   log (p(ti )) + log (psrl(li |ti ))

final results (2nd best at conll):
wsj-23: 78.45 (f1), 79.54 (prec.), 77.39 (rec.)
brown: 67.71 (f1), 70.24 (prec.), 65.37 (rec.)

improvement due to the joint model: > 2 f1 points

state-of-the-art: srl systems in detail

125

state-of-the-art: other systems, approaches, etc.

srl using di   erent syntactic parsers:

id35 parser (gildea and hockenmaier, 2005; boxwell et al., 2009)
hpsg parsers with handcrafted grammars (zhang et al., 2008;
2009)

srl using markov logic (meza-ruiz & riedel, 2008; 2009)

unsupervised approaches to srl (swier & stevenson, 2004;2005;
grenager & manning, 2006; abend et al., 2009)

corpora development: cross-lingual annotation projection
(fung & chen, 2004; pad  o & lapata 2006; fung et al., 2007; pad  o 2007;
pad  o & pitel, 2007)

state-of-the-art: srl systems in detail

126

state-of-the-art: other systems, approaches, etc.

srl using di   erent syntactic parsers:

id35 parser (gildea and hockenmaier, 2005; boxwell et al., 2009)
hpsg parsers with handcrafted grammars (zhang et al., 2008;
2009)

srl using markov logic (meza-ruiz & riedel, 2008; 2009)

unsupervised approaches to srl (swier & stevenson, 2004;2005;
grenager & manning, 2006; abend et al., 2009)

corpora development: cross-lingual annotation projection
(fung & chen, 2004; pad  o & lapata 2006; fung et al., 2007; pad  o 2007;
pad  o & pitel, 2007)

state-of-the-art: srl systems in detail

127

state-of-the-art: other systems, approaches, etc.

srl using di   erent syntactic parsers:

id35 parser (gildea and hockenmaier, 2005; boxwell et al., 2009)
hpsg parsers with handcrafted grammars (zhang et al., 2008;
2009)

srl using markov logic (meza-ruiz & riedel, 2008; 2009)

unsupervised approaches to srl (swier & stevenson, 2004;2005;
grenager & manning, 2006; abend et al., 2009)

corpora development: cross-lingual annotation projection
(fung & chen, 2004; pad  o & lapata 2006; fung et al., 2007; pad  o 2007;
pad  o & pitel, 2007)

state-of-the-art: srl systems in detail

128

state-of-the-art: other systems, approaches, etc.

srl using di   erent syntactic parsers:

id35 parser (gildea and hockenmaier, 2005; boxwell et al., 2009)
hpsg parsers with handcrafted grammars (zhang et al., 2008;
2009)

srl using markov logic (meza-ruiz & riedel, 2008; 2009)

unsupervised approaches to srl (swier & stevenson, 2004;2005;
grenager & manning, 2006; abend et al., 2009)

corpora development: cross-lingual annotation projection
(fung & chen, 2004; pad  o & lapata 2006; fung et al., 2007; pad  o 2007;
pad  o & pitel, 2007)

129

empirical evaluation and lessons learned:

tutorial overview

1

introduction

2 state-of-the-art

3 empirical evaluation and lessons learned

4 problems and challenges

5 conclusions

empirical evaluation and lessons learned:

130

empirical evaluation of srl systems

evaluation exercises

up to 9 evaluation exercises in the last 5 years

conll-2004/2005 shared tasks
(carreras & m`arquez, 2004; 2005)

senseval   3 (litkowski, 2004)

semeval-2007 (pradhan et al., 2007; m`arquez et al., 2007)
(baker et al., 2007; litkowski & hargraves, 2007)

conll-2008 shared task (surdeanu et al., 2008)

conll-2009 shared task (haji  c et al., 2009)

empirical evaluation and lessons learned:

131

empirical evaluation: on propbank

on propbank: conll-2004/2005 shared tasks

input: words, pos, nes, syntax; output: srl annotation

conll-2004 =    conll-2005:

10 teams =    19 teams
partial parsing =    full parsing
   200kw training =       1mw training

best overall results:    80% f1 measure

identifying arguments is more di   cult than classifying them:
recall    81%, class. accuracy    95% on the previous set

core arguments vs. adjuncts: 70%   90% vs. <60%

   good    results on unseen predicates:    70% f1

empirical evaluation and lessons learned:

132

empirical evaluation: on propbank

on propbank: conll-2004/2005 shared tasks

input: words, pos, nes, syntax; output: srl annotation

conll-2004 =    conll-2005:

10 teams =    19 teams
partial parsing =    full parsing
   200kw training =       1mw training

best overall results:    80% f1 measure

identifying arguments is more di   cult than classifying them:
recall    81%, class. accuracy    95% on the previous set

core arguments vs. adjuncts: 70%   90% vs. <60%

   good    results on unseen predicates:    70% f1

empirical evaluation and lessons learned:

133

empirical evaluation: on propbank

on propbank: conll-2004/2005 shared tasks

input: words, pos, nes, syntax; output: srl annotation

conll-2004 =    conll-2005:

10 teams =    19 teams
partial parsing =    full parsing
   200kw training =       1mw training

best overall results:    80% f1 measure

identifying arguments is more di   cult than classifying them:
recall    81%, class. accuracy    95% on the previous set

core arguments vs. adjuncts: 70%   90% vs. <60%

   good    results on unseen predicates:    70% f1

empirical evaluation and lessons learned:

134

empirical evaluation: on propbank

on propbank: conll-2004/2005 shared tasks

input: words, pos, nes, syntax; output: srl annotation

conll-2004 =    conll-2005:

10 teams =    19 teams
partial parsing =    full parsing
   200kw training =       1mw training

best overall results:    80% f1 measure

identifying arguments is more di   cult than classifying them:
recall    81%, class. accuracy    95% on the previous set

core arguments vs. adjuncts: 70%   90% vs. <60%

   good    results on unseen predicates:    70% f1

empirical evaluation and lessons learned:

135

empirical evaluation: on propbank

on propbank: conll-2005: system combination

observation: the 4 best scoring systems at conll-2005 were
combined systems

main reason: combination increases diversity and gets more
robustness from parsing errors

what to combine? the output of di   erent srl base systems
vs. several outputs from the same system trained using
di   erent input settings (e.g., using di   erent parse trees)

combination scheme: ranking of complete solutions vs.
combining argument candidates
combination improves results 2   5 f1 points

empirical evaluation and lessons learned:

136

empirical evaluation: on propbank

on propbank: conll-2005: system combination

observation: the 4 best scoring systems at conll-2005 were
combined systems

main reason: combination increases diversity and gets more
robustness from parsing errors

what to combine? the output of di   erent srl base systems
vs. several outputs from the same system trained using
di   erent input settings (e.g., using di   erent parse trees)

combination scheme: ranking of complete solutions vs.
combining argument candidates
combination improves results 2   5 f1 points

empirical evaluation and lessons learned:

137

empirical evaluation: on propbank

on propbank: conll-2005: system combination

observation: the 4 best scoring systems at conll-2005 were
combined systems

main reason: combination increases diversity and gets more
robustness from parsing errors

what to combine? the output of di   erent srl base systems
vs. several outputs from the same system trained using
di   erent input settings (e.g., using di   erent parse trees)

combination scheme: ranking of complete solutions vs.
combining argument candidates
combination improves results 2   5 f1 points

empirical evaluation and lessons learned:

138

empirical evaluation: on propbank

on propbank: conll-2005: system combination

observation: the 4 best scoring systems at conll-2005 were
combined systems

main reason: combination increases diversity and gets more
robustness from parsing errors

what to combine? the output of di   erent srl base systems
vs. several outputs from the same system trained using
di   erent input settings (e.g., using di   erent parse trees)

combination scheme: ranking of complete solutions vs.
combining argument candidates
combination improves results 2   5 f1 points

empirical evaluation and lessons learned:

139

empirical evaluation: on propbank

on propbank: conll-2005: system combination

observation: the 4 best scoring systems at conll-2005 were
combined systems

main reason: combination increases diversity and gets more
robustness from parsing errors

what to combine? the output of di   erent srl base systems
vs. several outputs from the same system trained using
di   erent input settings (e.g., using di   erent parse trees)

combination scheme: ranking of complete solutions vs.
combining argument candidates
combination improves results 2   5 f1 points

empirical evaluation and lessons learned:

140

empirical evaluation: on propbank

system combination

(surdeanu et al., 2007)

reliance on full syntax

model 1

model 2

model 3

candidate argument

pool

candidate
generation

learning
(batch)

learning
(online)

candidate
scoring

constraint
satisfaction
engine

dynamic
programming
engine

dynamic
programming
engine

id136

solution

solution

solution

id136 with
id124

id136 with
local learning

id136 with
global learning

empirical evaluation and lessons learned:

141

empirical evaluation: on propbank

system combination

(surdeanu et al., 2007)

combining n-best systems from conll-2005

wsj

c2
c4
c6
c8
c10

local ranker
recall

f1

prec.

pprops
50.69% 86.60% 73.90% 79.75  0.7
55.14% 86.67% 76.63% 81.38  0.7
54.85% 87.45% 76.34% 81.52  0.6
54.36% 87.49% 76.12% 81.41  0.6
53.90% 87.48% 75.81% 81.23  0.6

best results up to date on conll-2005 datasets

empirical evaluation and lessons learned:

142

empirical evaluation: on propbank

on propbank: semeval-2007 task #17

(pradhan et al., 2007)

srl + wsd in a set of 50 selected verbal predicates

double annotation and evaluation: comparison of the
propbank roleset with a verbnet-based roleset containing
general semantic roles

only two participant systems

results consistent with conll-2005

systems predicted verbnet-based roles as accurately as
propbank roles

empirical evaluation and lessons learned:

143

empirical evaluation: on propbank

on propbank: semeval-2007 task #17

(pradhan et al., 2007)

srl + wsd in a set of 50 selected verbal predicates

double annotation and evaluation: comparison of the
propbank roleset with a verbnet-based roleset containing
general semantic roles

only two participant systems

results consistent with conll-2005

systems predicted verbnet-based roles as accurately as
propbank roles

empirical evaluation and lessons learned:

144

empirical evaluation: on framenet

on framenet: senseval   3

(litkowski, 2004)

replicating the experimental setting of gildea & jurafsky (2002)

subset of 40 selected frames

simple task (role classi   cation): best result    92%

complete srl task: best result    83%

empirical evaluation and lessons learned:

145

empirical evaluation: on framenet

on framenet: senseval   3

(litkowski, 2004)

replicating the experimental setting of gildea & jurafsky (2002)

subset of 40 selected frames

simple task (role classi   cation): best result    92%

complete srl task: best result    83%

empirical evaluation and lessons learned:

146

empirical evaluation: on framenet

on framenet: semeval-2007 task #19

(baker et al., 2007)

realistic setting:

label running text with framenet semantic roles
output a graph representation of the sentence semantics
test was newly annotated material: contained some new
frames and roles not in the framenet lexicon

three teams submitted results

precision percentages in the 60s
but recall percentages in the 30s

empirical evaluation and lessons learned:

147

empirical evaluation: on framenet

on framenet: semeval-2007 task #19

(baker et al., 2007)

realistic setting:

label running text with framenet semantic roles
output a graph representation of the sentence semantics
test was newly annotated material: contained some new
frames and roles not in the framenet lexicon

three teams submitted results

precision percentages in the 60s
but recall percentages in the 30s

empirical evaluation and lessons learned:

148

empirical evaluation: other languages

other languages at semeval-2007

spanish and catalan: task #9: only 2 participants

arabic: task #19: no participants in srl

czech: task #3: cancelled

empirical evaluation and lessons learned:

149

empirical evaluation: other languages

other languages at semeval-2007

spanish and catalan: task #9: only 2 participants

arabic: task #19: no participants in srl

czech: task #3: cancelled

empirical evaluation and lessons learned:

150

empirical evaluation: other languages

semeval-2007: task #9 on spanish and catalan

multilevel semantic annotation of catalan and spanish
http://www.lsi.upc.edu/   nlp/semeval/msacs.html
(m`arquez et al., 2007)

empirical evaluation and lessons learned:

151

empirical evaluation: other languages

semeval-2007: task #9 on spanish and catalan

s

gv

sn   suj

espec

grup.nom

quedan

sp

prep

sp

las

conclusiones

sp

para

prep

sn

prep

sn

de

espec

grup.nom

la

comision grup.nom

zapatero

despues_del

grup.nom

verano

empirical evaluation and lessons learned:

152

empirical evaluation: other languages

semeval-2007: task #9 on spanish and catalan

s

gv

argm   tmp

sp

sn   suj

arg0   tem

espec

grup.nom

quedan

prep

sp

las

conclusiones

sp

prep

b3

sn

de

espec

grup.nom

la

comision grup.nom

srl + sc

zapatero

para

prep

sn

despues_del

grup.nom

verano

empirical evaluation and lessons learned:

153

empirical evaluation: other languages

semeval-2007: task #9 on spanish and catalan

s

gv

argm   tmp

sp

sn   suj

arg0   tem

espec

grup.nom

quedan

prep

sp

las

conclusiones

sp

prep

b3

sn

org

de

espec

grup.nom

para

prep

sn

despues_del

grup.nom

verano

ner

srl + sc

la

comision grup.nom

per

zapatero

empirical evaluation and lessons learned:

154

empirical evaluation: other languages

semeval-2007: task #9 on spanish and catalan

s

gv

argm   tmp

sp

sn   suj

arg0   tem

espec

grup.nom

quedan

prep

sp

las

conclusiones

sp

05059980n

prep

b3

sn

org

para

prep

sn

despues_del

grup.nom

de

espec

grup.nom

ner
nsd
srl + sc

la

comision grup.nom

06172564n

zapatero

per

verano

10946199n

empirical evaluation and lessons learned:

155

empirical evaluation: other languages

semeval-2007: task #9 on spanish and catalan

multilevel semantic annotation of catalan and spanish

goal: joint resolution of all three semantic tasks, exploiting
interdependencies among them

results: best system (from ilk) showed that srl for catalan
and spanish is possible with comparable accuracy to
state-of-the-art english systems (using gold parse trees)

but: nobody tried the joint learning challenge!

empirical evaluation and lessons learned:

156

empirical evaluation: other languages

semeval-2007: task #9 on spanish and catalan

multilevel semantic annotation of catalan and spanish

goal: joint resolution of all three semantic tasks, exploiting
interdependencies among them

results: best system (from ilk) showed that srl for catalan
and spanish is possible with comparable accuracy to
state-of-the-art english systems (using gold parse trees)

but: nobody tried the joint learning challenge!

empirical evaluation and lessons learned:

157

empirical evaluation: other languages

semeval-2007: task #9 on spanish and catalan

multilevel semantic annotation of catalan and spanish

goal: joint resolution of all three semantic tasks, exploiting
interdependencies among them

results: best system (from ilk) showed that srl for catalan
and spanish is possible with comparable accuracy to
state-of-the-art english systems (using gold parse trees)

but: nobody tried the joint learning challenge!

empirical evaluation and lessons learned:

158

empirical evaluation: recent conll shared tasks

conll-2008 shared task

joint parsing of syntactic and semantic dependencies
http://www.yr-bcn.es/conll2008/
(surdeanu et al., 2008)
main features:

srl using a dependency-based representation

not only verbal predicates (from propbank) but also nominal
predicates (from nombank)
more complex syntactic dependencies

merged representation for syntax and semantics

empirical evaluation and lessons learned:

159

empirical evaluation: recent conll shared tasks

conll-2008 shared task

joint parsing of syntactic and semantic dependencies
http://www.yr-bcn.es/conll2008/
(surdeanu et al., 2008)
main features:

srl using a dependency-based representation

not only verbal predicates (from propbank) but also nominal
predicates (from nombank)
more complex syntactic dependencies

merged representation for syntax and semantics

empirical evaluation and lessons learned:

160

empirical evaluation: recent conll shared tasks

conll-2008 shared task

research questions:

is the dependency-based representation better for srl than
the constituent-based formalism?
is the merged representation more helpful than the individual
ones?

more motivations:

ease adoption of nlp parsing technology: linear time
processing possible (good    t for applications)
identifying the semantic dependencies between predicates and
modi   ers (heads of semantic arguments) could be easier and
enough for application needs

empirical evaluation and lessons learned:

161

empirical evaluation: recent conll shared tasks

conll-2008 shared task: graphical representation of data

empirical evaluation and lessons learned:

162

empirical evaluation: recent conll shared tasks

conll-2008 shared task: some details

main di   culties:

input: words + pos; output: dependency tree, predicate
identi   cation and disambiguation (sense in the frame    le), srl
for all predicates
semantic structure does not match the syntactic dependency
tree (nor any known graph representation with fast id136
and learning algorithms) =    di   cult to devise joint systems

open/close challenges

full task vs. srl-only

main evaluation score: global measure as a weighted average
of las (syntax) and semantic f1

empirical evaluation and lessons learned:

163

empirical evaluation: recent conll shared tasks

conll-2008 shared task: some details

main di   culties:

input: words + pos; output: dependency tree, predicate
identi   cation and disambiguation (sense in the frame    le), srl
for all predicates
semantic structure does not match the syntactic dependency
tree (nor any known graph representation with fast id136
and learning algorithms) =    di   cult to devise joint systems

open/close challenges

full task vs. srl-only

main evaluation score: global measure as a weighted average
of las (syntax) and semantic f1

empirical evaluation and lessons learned:

164

empirical evaluation: recent conll shared tasks

conll-2008 shared task: results and conclusions

55 groups signed up for the task; 22 submitted results
best results (johanson & nugues, 2008):

wsj: las=90.13; f1=81.75; overall: 85.95
brown: las=82.81; f1=69.06; overall: 75.95

mostly pipeline architectures. 5 systems combined the
syntactic and semantic subtasks to some extent (the
best-performing system, among others).
but only 2 were truly joint systems

the best of such scored 80.19 (wsj) and 70.34 (brown)
(henderson et al., 2008); 5 points below the best system

empirical evaluation and lessons learned:

165

empirical evaluation: recent conll shared tasks

conll-2008 shared task: results and conclusions

55 groups signed up for the task; 22 submitted results
best results (johanson & nugues, 2008):

wsj: las=90.13; f1=81.75; overall: 85.95
brown: las=82.81; f1=69.06; overall: 75.95

mostly pipeline architectures. 5 systems combined the
syntactic and semantic subtasks to some extent (the
best-performing system, among others).
but only 2 were truly joint systems

the best of such scored 80.19 (wsj) and 70.34 (brown)
(henderson et al., 2008); 5 points below the best system

empirical evaluation and lessons learned:

166

empirical evaluation: recent conll shared tasks

conll-2008 shared task: on the research questions

comparison to conll-2005:

results on the dependency representation are slightly better
than those on constituents. fair post-competition comparison
by johansson (2008)

observation from systems addressing syntax and srl jointly:
(compared to the pipeline approach) joint id136 seems not
to degrade syntactic results, but to boost the f1 score on
semantic dependencies
(henderson et al., 2008; llu    s & m`arquez, 2008)

empirical evaluation and lessons learned:

167

empirical evaluation: recent conll shared tasks

conll-2008 shared task: on the research questions

comparison to conll-2005:

results on the dependency representation are slightly better
than those on constituents. fair post-competition comparison
by johansson (2008)

observation from systems addressing syntax and srl jointly:
(compared to the pipeline approach) joint id136 seems not
to degrade syntactic results, but to boost the f1 score on
semantic dependencies
(henderson et al., 2008; llu    s & m`arquez, 2008)

empirical evaluation and lessons learned:

168

empirical evaluation: recent conll shared tasks

conll-2009 shared task

syntactic and semantic dependencies in multiple languages
http://ufal.m   .cuni.cz/conll2009-st/
(haji  c et al., 2009)

very similar task setting and goals to those of 2008
particularities

extension to 7 languages from di   erent typologies: catalan,
chinese, czech, english, german, japanese, spanish
signi   cant di   erences among languages (e.g, corpora size, avg.
sentence length, size and granularity of the syntactic and
semantic tagsets, etc.)
results on all languages had to be submitted
   predicates    identi   ed both in training and test

empirical evaluation and lessons learned:

169

empirical evaluation: recent conll shared tasks

conll-2009 shared task

syntactic and semantic dependencies in multiple languages
http://ufal.m   .cuni.cz/conll2009-st/
(haji  c et al., 2009)

very similar task setting and goals to those of 2008
particularities

extension to 7 languages from di   erent typologies: catalan,
chinese, czech, english, german, japanese, spanish
signi   cant di   erences among languages (e.g, corpora size, avg.
sentence length, size and granularity of the syntactic and
semantic tagsets, etc.)
results on all languages had to be submitted
   predicates    identi   ed both in training and test

empirical evaluation and lessons learned:

170

empirical evaluation: recent conll shared tasks

conll-2009 shared task: results and conclusions

68 registrations, 34 licenses for evaluation data, 20 groups
submitted results

results:

macro avg.: las=85.77; f1=80.47; overall: 82.64
at least one team per language beat the state-of-the-art
syntactic parser provided by organizers
best result on english from 2008, overall f1=85.95
(johansson & nugues), was beat by 4 systems in 2009
(with best f1=87.69)

one surprise (about the lack of surprises): no signi   cant
changes in results among languages

empirical evaluation and lessons learned:

171

empirical evaluation: recent conll shared tasks

conll-2009 shared task: results and conclusions

68 registrations, 34 licenses for evaluation data, 20 groups
submitted results

results:

macro avg.: las=85.77; f1=80.47; overall: 82.64
at least one team per language beat the state-of-the-art
syntactic parser provided by organizers
best result on english from 2008, overall f1=85.95
(johansson & nugues), was beat by 4 systems in 2009
(with best f1=87.69)

one surprise (about the lack of surprises): no signi   cant
changes in results among languages

empirical evaluation and lessons learned:

172

empirical evaluation: recent conll shared tasks

conll-2009 shared task: results and conclusions

68 registrations, 34 licenses for evaluation data, 20 groups
submitted results

results:

macro avg.: las=85.77; f1=80.47; overall: 82.64
at least one team per language beat the state-of-the-art
syntactic parser provided by organizers
best result on english from 2008, overall f1=85.95
(johansson & nugues), was beat by 4 systems in 2009
(with best f1=87.69)

one surprise (about the lack of surprises): no signi   cant
changes in results among languages

empirical evaluation and lessons learned:

173

empirical evaluation: recent conll shared tasks

conll-2009 shared task: results and conclusions

system architectures

best systems are still pipelined (syntax, then semantics)
four joint models were presented. the best of them scored
only 0.5 f1 points below the winner (gesmundo et al., 2009)
conclusions with joint models are similar to those obtained in
2008

no further insights on the two fundamental research questions

a lot of analysis can still be done on the competition
materials. datasets (available through ldc soon), systems   
outputs, etc. represent a very valuable multilingual resource
for the future research

empirical evaluation and lessons learned:

174

empirical evaluation: recent conll shared tasks

conll-2009 shared task: results and conclusions

system architectures

best systems are still pipelined (syntax, then semantics)
four joint models were presented. the best of them scored
only 0.5 f1 points below the winner (gesmundo et al., 2009)
conclusions with joint models are similar to those obtained in
2008

no further insights on the two fundamental research questions

a lot of analysis can still be done on the competition
materials. datasets (available through ldc soon), systems   
outputs, etc. represent a very valuable multilingual resource
for the future research

empirical evaluation and lessons learned:

175

empirical evaluation: recent conll shared tasks

conll-2009 shared task: results and conclusions

system architectures

best systems are still pipelined (syntax, then semantics)
four joint models were presented. the best of them scored
only 0.5 f1 points below the winner (gesmundo et al., 2009)
conclusions with joint models are similar to those obtained in
2008

no further insights on the two fundamental research questions

a lot of analysis can still be done on the competition
materials. datasets (available through ldc soon), systems   
outputs, etc. represent a very valuable multilingual resource
for the future research

176

problems and challenges:

tutorial overview

1

introduction

2 state-of-the-art

3 empirical evaluation and lessons learned

4 problems and challenges

generalization to new domains
dependence on syntax
srl systems in applications

5 conclusions

problems and challenges: generalization to new domains

177

tutorial overview

1

introduction

2 state-of-the-art

3 empirical evaluation and lessons learned

4 problems and challenges

generalization to new domains
dependence on syntax
srl systems in applications

5 conclusions

problems and challenges: generalization to new domains

178

domain dependence

all statistical ml systems su   er from domain dependence

how large is this dependence in the case of srl?

conll-2005 evaluation: out-of-domain test corpus (brown)
=       10 f1 point drop in performance

similar evaluations at conll-2008/2009 shared tasks

problems and challenges: generalization to new domains

179

domain dependence: conll-2005

results on wsj and brown tests

f1: 70% ~ 80%
small differences

every system
suffers from
cross-domain
test (~10%)

75

problems and challenges: generalization to new domains

180

domain dependence

reasons for the low generalization ability

training corpus is not representative and big enough (and it
will never be)

the linguistic processors experiment a similar drop in
performance

the loss in accuracy takes place in assigning the semantic
roles, not in identi   cation     semantic explanation
(pradhan et al., 2008)

problems and challenges: generalization to new domains

181

domain dependence

generalization of role sets

does propbank numbered core roles allow to generalize
across verbs and to unseen predicates in new corpora?

aren   t thematic role labels (e.g., agent, patient, theme,
experiencer, source, bene   ciary, etc.) closer to application
needs?

opportunity: semlink maps propbank annotation into
verbnet thematic roles. it covers most of the corpus.
sl: http://verbs.colorado.edu/semlink/
vn: http://verbs.colorado.edu/   mpalmer/projects/verbnet.html

problems and challenges: generalization to new domains

182

domain dependence

generalization of role sets

does propbank numbered core roles allow to generalize
across verbs and to unseen predicates in new corpora?

aren   t thematic role labels (e.g., agent, patient, theme,
experiencer, source, bene   ciary, etc.) closer to application
needs?

opportunity: semlink maps propbank annotation into
verbnet thematic roles. it covers most of the corpus.
sl: http://verbs.colorado.edu/semlink/
vn: http://verbs.colorado.edu/   mpalmer/projects/verbnet.html

problems and challenges: generalization to new domains

183

domain dependence

generalization of role sets

loper et al. (2007) show that arg2 generalizes better (in
brown) when training the system from a verbnet mapped
version of propbank.

zapirain et al. (2008) show a negative result:

training on propbank arguments is more robust under several
training settings
also, it is more productive to train on the propbank roleset
and (naively) mapping the output into verbnet roles, than
doing all the process using the verbnet version of propbank

more related studies will be presented at acl-ijcnlp 2009

problems and challenges: generalization to new domains

184

domain dependence

generalization of role sets

loper et al. (2007) show that arg2 generalizes better (in
brown) when training the system from a verbnet mapped
version of propbank.

zapirain et al. (2008) show a negative result:

training on propbank arguments is more robust under several
training settings
also, it is more productive to train on the propbank roleset
and (naively) mapping the output into verbnet roles, than
doing all the process using the verbnet version of propbank

more related studies will be presented at acl-ijcnlp 2009

problems and challenges: generalization to new domains

185

domain dependence

generalization of role sets

loper et al. (2007) show that arg2 generalizes better (in
brown) when training the system from a verbnet mapped
version of propbank.

zapirain et al. (2008) show a negative result:

training on propbank arguments is more robust under several
training settings
also, it is more productive to train on the propbank roleset
and (naively) mapping the output into verbnet roles, than
doing all the process using the verbnet version of propbank

more related studies will be presented at acl-ijcnlp 2009

problems and challenges: generalization to new domains

186

domain dependence

new articles at acl-ijcnlp 2009

merlo & van der plaas: abstraction and generalisation in
semantic role labels: propbank, verbnet or both?

critizism on the experimental settings of (loper et al. 2007)
and (zapirain et al. 2008): task-oriented evaluation (srl
systems); syntax based; skewed distributions of role labels
in the new paper authors analyze how good the two schemes
are at capturing the linguistic generalizations that are known
to hold for semantic role labels
analyses and statistical measures avoid using syntactic
properties or parsing techniques
conclusions: verbnet is more verb speci   c and better able to
generalize to new semantic role instances; propbank better
capture structural constraints among roles

matsubayashi et al.: a comparative study on generalization of
semantic roles in framenet

problems and challenges: generalization to new domains

187

domain dependence

new articles at acl-ijcnlp 2009

merlo & van der plaas: abstraction and generalisation in
semantic role labels: propbank, verbnet or both?

critizism on the experimental settings of (loper et al. 2007)
and (zapirain et al. 2008): task-oriented evaluation (srl
systems); syntax based; skewed distributions of role labels
in the new paper authors analyze how good the two schemes
are at capturing the linguistic generalizations that are known
to hold for semantic role labels
analyses and statistical measures avoid using syntactic
properties or parsing techniques
conclusions: verbnet is more verb speci   c and better able to
generalize to new semantic role instances; propbank better
capture structural constraints among roles

matsubayashi et al.: a comparative study on generalization of
semantic roles in framenet

problems and challenges: generalization to new domains

188

domain dependence

new articles at acl-ijcnlp 2009

merlo & van der plaas: abstraction and generalisation in
semantic role labels: propbank, verbnet or both?

critizism on the experimental settings of (loper et al. 2007)
and (zapirain et al. 2008): task-oriented evaluation (srl
systems); syntax based; skewed distributions of role labels
in the new paper authors analyze how good the two schemes
are at capturing the linguistic generalizations that are known
to hold for semantic role labels
analyses and statistical measures avoid using syntactic
properties or parsing techniques
conclusions: verbnet is more verb speci   c and better able to
generalize to new semantic role instances; propbank better
capture structural constraints among roles

matsubayashi et al.: a comparative study on generalization of
semantic roles in framenet

problems and challenges: generalization to new domains

189

domain dependence

new articles at acl-ijcnlp 2009

merlo & van der plaas: abstraction and generalisation in
semantic role labels: propbank, verbnet or both?

critizism on the experimental settings of (loper et al. 2007)
and (zapirain et al. 2008): task-oriented evaluation (srl
systems); syntax based; skewed distributions of role labels
in the new paper authors analyze how good the two schemes
are at capturing the linguistic generalizations that are known
to hold for semantic role labels
analyses and statistical measures avoid using syntactic
properties or parsing techniques
conclusions: verbnet is more verb speci   c and better able to
generalize to new semantic role instances; propbank better
capture structural constraints among roles

matsubayashi et al.: a comparative study on generalization of
semantic roles in framenet

problems and challenges: generalization to new domains

190

domain dependence

semantic features for srl

motivation

up to now: preeminence of syntactic information in srl
systems

semantic information comes from the raw lexical features
but lexical features are sparse and generalize badly to new
corpora

some works explore the incorporation of selectional
preferences as a way to generalize lexical features and gain
semantic coherence in the predicate argument structure
(zapirain et al., 2007;2009; erk, 2007)

not easy: a key problem is the noise introduced by lexical
ambiguity

problems and challenges: generalization to new domains

191

domain dependence

semantic features for srl

motivation

up to now: preeminence of syntactic information in srl
systems

semantic information comes from the raw lexical features
but lexical features are sparse and generalize badly to new
corpora

some works explore the incorporation of selectional
preferences as a way to generalize lexical features and gain
semantic coherence in the predicate argument structure
(zapirain et al., 2007;2009; erk, 2007)

not easy: a key problem is the noise introduced by lexical
ambiguity

problems and challenges: generalization to new domains

192

domain dependence

semantic features for srl

zapirain et al., (2009)

study the use of automatically acquired selectional preferences
(sp) for argument classi   cation

setting: given a verb occurrence and a constituent head word
dependant on that verb, assign the most plausible role to the
head word according to the selectional preference model

distributional sp models vs. id138-based
lexical features have a high precision but very low recall
sp features improve over the baseline: 17 f1 points on the
wsj datasets and 41 f1 points on the brown
sp features help to alleviate the lexical sparseness problem

initial experiments show signi   cant improvements in a full
   edged srl system (ongoing work)

problems and challenges: generalization to new domains

193

domain dependence

semantic features for srl

zapirain et al., (2009)

study the use of automatically acquired selectional preferences
(sp) for argument classi   cation

setting: given a verb occurrence and a constituent head word
dependant on that verb, assign the most plausible role to the
head word according to the selectional preference model

distributional sp models vs. id138-based
lexical features have a high precision but very low recall
sp features improve over the baseline: 17 f1 points on the
wsj datasets and 41 f1 points on the brown
sp features help to alleviate the lexical sparseness problem

initial experiments show signi   cant improvements in a full
   edged srl system (ongoing work)

problems and challenges: generalization to new domains

194

domain dependence

semantic features for srl

zapirain et al., (2009)

study the use of automatically acquired selectional preferences
(sp) for argument classi   cation

setting: given a verb occurrence and a constituent head word
dependant on that verb, assign the most plausible role to the
head word according to the selectional preference model

distributional sp models vs. id138-based
lexical features have a high precision but very low recall
sp features improve over the baseline: 17 f1 points on the
wsj datasets and 41 f1 points on the brown
sp features help to alleviate the lexical sparseness problem

initial experiments show signi   cant improvements in a full
   edged srl system (ongoing work)

problems and challenges: dependence on syntax

195

tutorial overview

1

introduction

2 state-of-the-art

3 empirical evaluation and lessons learned

4 problems and challenges

generalization to new domains
dependence on syntax
srl systems in applications

5 conclusions

problems and challenges: dependence on syntax

196

impact of syntactic processing in srl

srl results strongly depend on syntax (bottleneck)

gold vs. automatic parses:    90% vs.    80% f1

drop in performance occurs in identifying argument
boundaries

problems and challenges: dependence on syntax

197

impact of syntactic processing in srl

srl results strongly depend on syntax (bottleneck)

gold vs. automatic parses:    90% vs.    80% f1

drop in performance occurs in identifying argument
boundaries

problems and challenges: dependence on syntax

198

impact of syntactic processing in srl

srl results strongly depend on syntax (bottleneck)

gold vs. automatic parses:    90% vs.    80% f1

drop in performance occurs in identifying argument
boundaries

problems and challenges: dependence on syntax

199

impact of syntactic processing in srl

partial vs. full parsing

(conll-2004/2005)

motivation: partial parsing can be more robust to changing
application domains

conll-2005 vs. conll-2004:    80% vs.    70% f1

...but the corpus size was the main factor

the real performance drop when using partial parsing (base
chunks + clause boundaries) is    2 f1 points
(surdeanu et al., 2007; punyakanok et al., 2008)

bad news: partial parsers degraded their performance as much
as full parsers when applied to brown

problems and challenges: dependence on syntax

200

impact of syntactic processing in srl

partial vs. full parsing

(conll-2004/2005)

motivation: partial parsing can be more robust to changing
application domains

conll-2005 vs. conll-2004:    80% vs.    70% f1

...but the corpus size was the main factor

the real performance drop when using partial parsing (base
chunks + clause boundaries) is    2 f1 points
(surdeanu et al., 2007; punyakanok et al., 2008)

bad news: partial parsers degraded their performance as much
as full parsers when applied to brown

problems and challenges: dependence on syntax

201

impact of syntactic processing in srl

partial vs. full parsing

(conll-2004/2005)

motivation: partial parsing can be more robust to changing
application domains

conll-2005 vs. conll-2004:    80% vs.    70% f1

...but the corpus size was the main factor

the real performance drop when using partial parsing (base
chunks + clause boundaries) is    2 f1 points
(surdeanu et al., 2007; punyakanok et al., 2008)

bad news: partial parsers degraded their performance as much
as full parsers when applied to brown

problems and challenges: dependence on syntax

202

integration of syntactic parsing and srl

first attempt

(yi & palmer, 2005)

syntactic parser trained to predict argument candidates

merge the id32 and propbank to generate training
parse trees with enriched labels including semantic arguments

independent classi   cation of the arguments predicted by the
specialized parser

results did not improve the conventional architecture

possible explanations: weaker base parser / increase in the
number of syntactic labels to predict

problems and challenges: dependence on syntax

203

integration of syntactic parsing and srl

first attempt

(yi & palmer, 2005)

syntactic parser trained to predict argument candidates

merge the id32 and propbank to generate training
parse trees with enriched labels including semantic arguments

independent classi   cation of the arguments predicted by the
specialized parser

results did not improve the conventional architecture

possible explanations: weaker base parser / increase in the
number of syntactic labels to predict

problems and challenges: dependence on syntax

204

integration of syntactic parsing and srl

id29

(merlo & musillo, 2008)

enrich the annotation of training syntactic trees with
semantic role labels

 

 

 

 

 

 

 

 

 

 

 

s
h
 

 

h

h

h

h

h

h

h

h

h

h

h

np-a1
  
pp

 

p

 

 

 

p

p

p

p

 

 

 
p
the government   s borrowing authority

p

                  

 

 

 

 

 

 

 

 

h

h

xxxxxxxxxxxxxxxxxx

h

h

h

h

h

vbd-rel

pp-am-tmp

np-am-tmp

h
vp
 
h

dropped

 
in

at

  

hh

h
np

nnp

tuesday

to

nn

midnight

qp
pp
  

 

p

p
 
$ 2.80 trillion

from

qp
pp
  

 

p

p
 
$ 2.87 trillion

h
pp-a4
h
 

 
to

h
np

pp-a3
h
 

 
in

h
np

problems and challenges: dependence on syntax

205

integration of syntactic parsing and srl

id29

(merlo & musillo, 2008)

train a state-of-the-art parser to produce this new kind of
structures (titov & henderson, 2007)

devise procedures (rule and ml   based) for extracting
predicate-argument structures from the enriched trees

evaluation on the conll-2005 datasets shows very high
precision results (at the price of a low recall)

once combined with the best system at conll-2005 the
results raise to 80.5% precision, 81.4% recall, and 81.0
f1-measure for section 23.

problems and challenges: dependence on syntax

206

integration of syntactic parsing and srl

id29

(merlo & musillo, 2008)

train a state-of-the-art parser to produce this new kind of
structures (titov & henderson, 2007)

devise procedures (rule and ml   based) for extracting
predicate-argument structures from the enriched trees

evaluation on the conll-2005 datasets shows very high
precision results (at the price of a low recall)

once combined with the best system at conll-2005 the
results raise to 80.5% precision, 81.4% recall, and 81.0
f1-measure for section 23.

problems and challenges: dependence on syntax

207

integration of syntactic parsing and srl

syntactic and semantic dependencies

(conll-2008/2009)

a key di   culty: the joint structure is not a dependency tree
anymore (directed graph); traditional id33
algorithms work on dependency trees

three di   erent approaches (from simple to complex)

1

2

3

(morante et al., 2009)
(llu    s & m`arquez, 2008; llu    s et al., 2009)
(henderson et al, 2008; gesmundo et al., 2009)

problems and challenges: dependence on syntax

208

integration of syntactic parsing and srl

syntactic and semantic dependencies

(conll-2008/2009)

a key di   culty: the joint structure is not a dependency tree
anymore (directed graph); traditional id33
algorithms work on dependency trees

three di   erent approaches (from simple to complex)

1

2

3

(morante et al., 2009)
(llu    s & m`arquez, 2008; llu    s et al., 2009)
(henderson et al, 2008; gesmundo et al., 2009)

problems and challenges: dependence on syntax

209

integration of syntactic parsing and srl

approach 1

(morante et al., 2009)

forget about di   cult structures and work at word level

word classi   cation with extended syntactic-semantic labels

problems and challenges: dependence on syntax

210

integration of syntactic parsing and srl

approach 1

(morante et al., 2009)

three di   erent granularities are considered for class labels
(i.e., three overlapping classi   cation problems are de   ned)

make use of memory based learning (insensitivity to large
number of classes)

add a second layer to construct the structured solution based
on the predictions of all word-level classi   ers (ranking   based)

(still) low results at conll-2009 shared task

possible reasons: features, heuristics to construct solution,
large number of classes, etc.

problems and challenges: dependence on syntax

211

integration of syntactic parsing and srl

approach 1

(morante et al., 2009)

three di   erent granularities are considered for class labels
(i.e., three overlapping classi   cation problems are de   ned)

make use of memory based learning (insensitivity to large
number of classes)

add a second layer to construct the structured solution based
on the predictions of all word-level classi   ers (ranking   based)

(still) low results at conll-2009 shared task

possible reasons: features, heuristics to construct solution,
large number of classes, etc.

problems and challenges: dependence on syntax

212

integration of syntactic parsing and srl

approach 1

(morante et al., 2009)

three di   erent granularities are considered for class labels
(i.e., three overlapping classi   cation problems are de   ned)

make use of memory based learning (insensitivity to large
number of classes)

add a second layer to construct the structured solution based
on the predictions of all word-level classi   ers (ranking   based)

(still) low results at conll-2009 shared task

possible reasons: features, heuristics to construct solution,
large number of classes, etc.

problems and challenges: dependence on syntax

213

integration of syntactic parsing and srl

approach 1

(morante et al., 2009)

three di   erent granularities are considered for class labels
(i.e., three overlapping classi   cation problems are de   ned)

make use of memory based learning (insensitivity to large
number of classes)

add a second layer to construct the structured solution based
on the predictions of all word-level classi   ers (ranking   based)

(still) low results at conll-2009 shared task

possible reasons: features, heuristics to construct solution,
large number of classes, etc.

problems and challenges: dependence on syntax

214

integration of syntactic parsing and srl

approach 2

(llu    s & m`arquez, 2008; llu    s et al., 2009)

force semantic information to be learnt with the syntactic
dependency tree

extend regular syntactic id33 algorithms:

minimum spanning tree family
eisner algorithm
trained with structure id88

problems and challenges: dependence on syntax

215

integration of syntactic parsing and srl

approach 2

(llu    s & m`arquez, 2008; llu    s et al., 2009)

problems and challenges: dependence on syntax

216

integration of syntactic parsing and srl

approach 2

(llu    s & m`arquez, 2008; llu    s et al., 2009)

obj, a1, a1, _

obj, _, _, su

sbj, a0, _, a0

amod, _, am   tmp, _

nmod , _, _, _

nmod , _, _, _

problems and challenges: dependence on syntax

217

integration of syntactic parsing and srl

eisner   s first order id33 algorithm

dependency d = hh, m, li

best tree(x) = argmaxy    y(x) score tree(y , x)
score tree(y , x) = phh,m,li   y score (hh, m, li , x)
score(hh, m, l i , x) =    (hh, m, l i , x)    wl

where

x is an input sentence
y is a dependency tree
y(x) is the set of all dependency trees for input x
   is a feature extraction function
wl is the weight vector for dependency label l

problems and challenges: dependence on syntax

218

integration of syntactic parsing and srl

eisner   s first order id33 algorithm

dependency d = hh, m, li

best tree(x) = argmaxy    y(x) score tree(y , x)
score tree(y , x) = phh,m,li   y score (hh, m, li , x)
score(hh, m, l i , x) =    (hh, m, l i , x)    wl

where

x is an input sentence
y is a dependency tree
y(x) is the set of all dependency trees for input x
   is a feature extraction function
wl is the weight vector for dependency label l

problems and challenges: dependence on syntax

219

integration of syntactic parsing and srl

eisner   s first order id33 algorithm

dependency d = hh, m, li

best tree(x) = argmaxy    y(x) score tree(y , x)
score tree(y , x) = phh,m,li   y score (hh, m, li , x)
score(hh, m, l i , x) =    (hh, m, l i , x)    wl

where

x is an input sentence
y is a dependency tree
y(x) is the set of all dependency trees for input x
   is a feature extraction function
wl is the weight vector for dependency label l

problems and challenges: dependence on syntax

220

integration of syntactic parsing and srl

eisner   s first order id33 algorithm

the eisner algorithm is a id145 search
algorithm that computes the best    rst-order factorized tree in
o(n3) (i.e., solves the argmax function).

all binary linear classi   ers can be trained on-line using
structure preceptron (collins & du   y 2001; carreras et al.,
2007;2008)

can be naturally extended to higher order factorizations,
e.g., (carreras, 2007)

problems and challenges: dependence on syntax

221

integration of syntactic parsing and srl

approach 2

(llu    s & m`arquez, 2008; llu    s et al., 2009)

obj, a1, a1, _

obj, _, _, su

sbj, a0, _, a0

amod, _, am   tmp, _

nmod , _, _, _

nmod , _, _, _

problems and challenges: dependence on syntax

222

integration of syntactic parsing and srl

approach 2

(llu    s & m`arquez, 2008; llu    s et al., 2009)

an extended dependency is:

d = (cid:10)h, m, lsyn, lsem p1 , . . . , lsem pq(cid:11)

h is the head
m the modi   er
lsyn the syntactic label
lsem pi one semantic label for each sentence predicate pi

problems and challenges: dependence on syntax

223

integration of syntactic parsing and srl

approach 2

(llu    s & m`arquez, 2008; llu    s et al., 2009)

best tree(x, y    ) = argmaxy    y(x) score tree(y , x, y    )
score tree(y , x, y    ) = phh,m,lsyn ,li   y score(hh, m, lsyn, li , x, y    )
score (hh, m, lsyn, li , x, y    ) =

synt score (hh, m, lsyni , x) + sem score (hh, m,li , x, y    )

l = lsem p1 , . . . , lsem pq are the semantic labels for predicates pi

sem score (cid:0)h, m,lsem p1 , . . . , lsem pq , x, y    (cid:1) =
  sem (hh, m,lsem pi i , pi , x, y    )    w(lsem pi )
x
lsem pi

q

problems and challenges: dependence on syntax

224

integration of syntactic parsing and srl

approach 2

(llu    s & m`arquez, 2008; llu    s et al., 2009)

best tree(x, y    ) = argmaxy    y(x) score tree(y , x, y    )
score tree(y , x, y    ) = phh,m,lsyn ,li   y score(hh, m, lsyn, li , x, y    )
score (hh, m, lsyn, li , x, y    ) =

synt score (hh, m, lsyni , x) + sem score (hh, m,li , x, y    )

l = lsem p1 , . . . , lsem pq are the semantic labels for predicates pi

sem score (cid:0)h, m,lsem p1 , . . . , lsem pq , x, y    (cid:1) =
  sem (hh, m,lsem pi i , pi , x, y    )    w(lsem pi )
x
lsem pi

q

problems and challenges: dependence on syntax

225

integration of syntactic parsing and srl

approach 2

(llu    s & m`arquez, 2008; llu    s et al., 2009)

eisner id136 unchanged (the only change occurs at
dependency scoring)

standard syntactic and srl features

on-line training of w vectors using structure id88

extension to second-order parsing is straightforward

moderate results at conll-2008 and 2009 shared tasks

di   culties: 1) too complex decisions at dependency level
(semantic structure is not exploited); 2) adjustment of the
relative weight of syntactic and semantic contributions

problems and challenges: dependence on syntax

226

integration of syntactic parsing and srl

approach 2

(llu    s & m`arquez, 2008; llu    s et al., 2009)

eisner id136 unchanged (the only change occurs at
dependency scoring)

standard syntactic and srl features

on-line training of w vectors using structure id88

extension to second-order parsing is straightforward

moderate results at conll-2008 and 2009 shared tasks

di   culties: 1) too complex decisions at dependency level
(semantic structure is not exploited); 2) adjustment of the
relative weight of syntactic and semantic contributions

problems and challenges: dependence on syntax

227

integration of syntactic parsing and srl

approach 2

(llu    s & m`arquez, 2008; llu    s et al., 2009)

eisner id136 unchanged (the only change occurs at
dependency scoring)

standard syntactic and srl features

on-line training of w vectors using structure id88

extension to second-order parsing is straightforward

moderate results at conll-2008 and 2009 shared tasks

di   culties: 1) too complex decisions at dependency level
(semantic structure is not exploited); 2) adjustment of the
relative weight of syntactic and semantic contributions

problems and challenges: dependence on syntax

228

integration of syntactic parsing and srl

approach 2

(llu    s & m`arquez, 2008; llu    s et al., 2009)

problems and challenges: dependence on syntax

229

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

deal with syntax and semantics as separate structures

but synchronize the generation of both structures

and establish dependencies between both levels in the form of
latent variables

transition-based model of parsing (shift-reduce style or
history-based)

new operation (swap) for on-line planarisation of the semantic
graph

synchronous derivations are modeled with an incremental
sigmoid belief network (isbn; titov and henderson   s parser,
2007)

problems and challenges: dependence on syntax

230

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

deal with syntax and semantics as separate structures

but synchronize the generation of both structures

and establish dependencies between both levels in the form of
latent variables

transition-based model of parsing (shift-reduce style or
history-based)

new operation (swap) for on-line planarisation of the semantic
graph

synchronous derivations are modeled with an incremental
sigmoid belief network (isbn; titov and henderson   s parser,
2007)

problems and challenges: dependence on syntax

231

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

deal with syntax and semantics as separate structures

but synchronize the generation of both structures

and establish dependencies between both levels in the form of
latent variables

transition-based model of parsing (shift-reduce style or
history-based)

new operation (swap) for on-line planarisation of the semantic
graph

synchronous derivations are modeled with an incremental
sigmoid belief network (isbn; titov and henderson   s parser,
2007)

problems and challenges: dependence on syntax

232

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

root hope

seems

doomed

to failure

p(td , ts)

slides by james henderson

problems and challenges: dependence on syntax

233

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

de   ne two separate derivations, one for the syntactic structure
and one for the semantic structure.

p(td , ts ) = p(d 1

d , ..., d md

d , d 1

s , ..., d ms
s )

use an intermediate synchronization granularity, between full
predications and individual actions: synchronization at each word
prediction

c t = d
p(d 1

e t
d

bt
d
d , ..., d
d , d 1

s

s , ..., d e t

d , shiftt , d bt
s , ..., d ms

s , shiftt
s ) = p(c 1, . . . , c n)

s

d , ..., d md

results in one shared input queue
allows two separate stacks

problems and challenges: dependence on syntax

234

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

root hope

p(c1)

slides by james henderson

problems and challenges: dependence on syntax

235

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

root hope

seems

p(c 1) p(c2|c1)

slides by james henderson

problems and challenges: dependence on syntax

236

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

root hope

seems

doomed

p(c 1) p(c 2|c 1) p(c3|c1, c2)

slides by james henderson

problems and challenges: dependence on syntax

237

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

root hope

seems

doomed

to

p(c 1) p(c 2|c 1) p(c 3|c 1, c 2) p(c4|c1, c2, c3)

slides by james henderson

problems and challenges: dependence on syntax

238

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

root hope

seems

doomed

to failure

p(c 1) p(c 2|c 1) p(c 3|c 1, c 2) p(c 4|c 1, c 2, c 3) p(c5|c1, c2, c3, c4)

slides by james henderson

problems and challenges: dependence on syntax

239

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

derivation example:

root hope

slides by james henderson

problems and challenges: dependence on syntax

240

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

derivation example:

root hope

seems

slides by james henderson

problems and challenges: dependence on syntax

241

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

derivation example:

root hope

seems

slides by james henderson

problems and challenges: dependence on syntax

242

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

derivation example:

root hope

seems

slides by james henderson

problems and challenges: dependence on syntax

243

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

derivation example:

root hope

seems

slides by james henderson

problems and challenges: dependence on syntax

244

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

derivation example:

root hope

seems

doomed

slides by james henderson

problems and challenges: dependence on syntax

245

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

derivation example:

root hope

seems

doomed

slides by james henderson

problems and challenges: dependence on syntax

246

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

derivation example:

root hope

seems

doomed

slides by james henderson

problems and challenges: dependence on syntax

247

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

derivation example:

root hope

seems

doomed

slides by james henderson

problems and challenges: dependence on syntax

248

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

derivation example:

root hope

seems

doomed

to

slides by james henderson

problems and challenges: dependence on syntax

249

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

derivation example:

root hope

seems

doomed

to

slides by james henderson

problems and challenges: dependence on syntax

250

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

derivation example:

root hope

seems

doomed

to

slides by james henderson

problems and challenges: dependence on syntax

251

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

derivation example:

root hope

seems

doomed

to failure

slides by james henderson

problems and challenges: dependence on syntax

252

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

derivation example:

root hope

seems

doomed

to failure

slides by james henderson

problems and challenges: dependence on syntax

253

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

derivation example:

root hope

seems

doomed

to failure

slides by james henderson

problems and challenges: dependence on syntax

254

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

isbns are dynamic id110s for modeling
structures,

with vectors of latent variables annotating derivation states

connections between latent states re   ect locality in the
syntactic or semantic structure,

explicit conditioning features of the history are also speci   ed

t   c

s

t   1

s

t
s

t   c

d

t   1

d

t
d

problems and challenges: dependence on syntax

255

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

the model maximizes the joint id203 of the syntactic
and semantic dependencies (=    enforces that the output
structure be globally coherent)

good results at conll-2008: joint parsing improves the
semantic part by 3.5 f1 points

very good results at conll-2009: f1 score 82.14 (3rd
position; almost tied with the two    rst). the parser proved to
be very robust across languages and data domains

problems and challenges: dependence on syntax

256

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

the model maximizes the joint id203 of the syntactic
and semantic dependencies (=    enforces that the output
structure be globally coherent)

good results at conll-2008: joint parsing improves the
semantic part by 3.5 f1 points

very good results at conll-2009: f1 score 82.14 (3rd
position; almost tied with the two    rst). the parser proved to
be very robust across languages and data domains

problems and challenges: dependence on syntax

257

impact of syntactic processing in srl

approach 3

(henderson et al., 2008; gesmundo et al., 2009)

the model maximizes the joint id203 of the syntactic
and semantic dependencies (=    enforces that the output
structure be globally coherent)

good results at conll-2008: joint parsing improves the
semantic part by 3.5 f1 points

very good results at conll-2009: f1 score 82.14 (3rd
position; almost tied with the two    rst). the parser proved to
be very robust across languages and data domains

problems and challenges: srl systems in applications

258

tutorial overview

1

introduction

2 state-of-the-art

3 empirical evaluation and lessons learned

4 problems and challenges

generalization to new domains
dependence on syntax
srl systems in applications

5 conclusions

problems and challenges: srl systems in applications

259

srl in applications

examples of applications of srl

information extraction (surdeanu et al., 2003)

question & answering (narayanan and harabagiu, 2004; frank et
al., 2007)

id54 (melli et al., 2005)

coreference resolution (ponzetto and strube, 2006)

see (yih & toutanova, 2006) tutorial for a discussion on all
previous works

problems and challenges: srl systems in applications

260

srl in applications

examples of applications of srl

information extraction (surdeanu et al., 2003)

question & answering (narayanan and harabagiu, 2004; frank et
al., 2007)

id54 (melli et al., 2005)

coreference resolution (ponzetto and strube, 2006)

see (yih & toutanova, 2006) tutorial for a discussion on all
previous works

problems and challenges: srl systems in applications

261

srl in applications

other applications of srl

machine translation evaluation
(gim  enez and m`arquez, 2007)

machine translation
(boas, 2002; wu and fung, 2009a;2009b)

id123
(tatu & moldovan, 2005; burchardt et al., 2007)

modeling early id146 (connor et al., 2008;2009)

pictorial communication systems (goldberg, et al., 2008)

...

we will concentrate on machine translation

problems and challenges: srl systems in applications

262

srl in applications

other applications of srl

machine translation evaluation
(gim  enez and m`arquez, 2007)

machine translation
(boas, 2002; wu and fung, 2009a;2009b)

id123
(tatu & moldovan, 2005; burchardt et al., 2007)

modeling early id146 (connor et al., 2008;2009)

pictorial communication systems (goldberg, et al., 2008)

...

we will concentrate on machine translation

problems and challenges: srl systems in applications

263

srl in machine translation

automatic mt evaluation

gim  enez and m`arquez (2007;2008)

introduced a new set of automatic metrics for mt evaluation
based on rich linguistic information (including similarity at
lexical, shallow/deep syntactic, shallow/deep semantic levels)

srl provides an important subset of these features
measuring the overlap of semantic roles between the system   s
output and reference target sentences helps improving
correlation with human judgement of translation quality

problems and challenges: srl systems in applications

264

srl in machine translation

automatic mt evaluation

gim  enez and m`arquez (2007;2008)

introduced a new set of automatic metrics for mt evaluation
based on rich linguistic information (including similarity at
lexical, shallow/deep syntactic, shallow/deep semantic levels)

srl provides an important subset of these features
measuring the overlap of semantic roles between the system   s
output and reference target sentences helps improving
correlation with human judgement of translation quality

problems and challenges: srl systems in applications

265

srl in machine translation

automatic mt evaluation

gim  enez and m`arquez (2007;2008)

surprinsingly robust srl parsing for ill-formed sentences
(works well for system comparison and ranking)

better than id7-like lexical measures, especially in
heterogeneous scenarios and out-of-domain evaluation
iqmt suite is freely available
http://www.lsi.upc.edu/   nlp/iqmt/

problems and challenges: srl systems in applications

266

srl in machine translation

automatic mt evaluation

gim  enez and m`arquez (2007;2008)

surprinsingly robust srl parsing for ill-formed sentences
(works well for system comparison and ranking)

better than id7-like lexical measures, especially in
heterogeneous scenarios and out-of-domain evaluation
iqmt suite is freely available
http://www.lsi.upc.edu/   nlp/iqmt/

problems and challenges: srl systems in applications

267

srl in machine translation

exploring the application of srl in smt

wu and fung (2009a)

present a series of experiments to study the potential impact of
srl in improving mt accuracy. three basic questions:

1 do current smt systems produce good translations at

predicate structure level? not really (even when the predicate
is correctly translated)

2 does incorporating sr analysis contribute anything beyond the

current work on syntactic smt models? sr enforce
cross-lingual translation patterns more correctly

3 what is the potential quantitative impact of realistic sr

guidance to smt systems? signi   cant id7 and meteor
improvement by >2 points

problems and challenges: srl systems in applications

268

srl in machine translation

exploring the application of srl in smt

wu and fung (2009a)

present a series of experiments to study the potential impact of
srl in improving mt accuracy. three basic questions:

1 do current smt systems produce good translations at

predicate structure level? not really (even when the predicate
is correctly translated)

2 does incorporating sr analysis contribute anything beyond the

current work on syntactic smt models? sr enforce
cross-lingual translation patterns more correctly

3 what is the potential quantitative impact of realistic sr

guidance to smt systems? signi   cant id7 and meteor
improvement by >2 points

problems and challenges: srl systems in applications

269

srl in machine translation

exploring the application of srl in smt

wu and fung (2009a)

present a series of experiments to study the potential impact of
srl in improving mt accuracy. three basic questions:

1 do current smt systems produce good translations at

predicate structure level? not really (even when the predicate
is correctly translated)

2 does incorporating sr analysis contribute anything beyond the

current work on syntactic smt models? sr enforce
cross-lingual translation patterns more correctly

3 what is the potential quantitative impact of realistic sr

guidance to smt systems? signi   cant id7 and meteor
improvement by >2 points

problems and challenges: srl systems in applications

270

srl in machine translation

first smt system with srl

(wu and fung, 2009b)

hybrid smt system incorporating id14 and
phase-based smtmodels

two-pass architecture: 1) phrase-based smt system;
2) reordering guided by shallow semantic parsers

srl is performed    rst into source and output sentences in
order to identify predicate structures and constituents to be
re-ordered.

then, a set of candidate re-ordered sentences are generated
(by moving sr-mismatched constituents)

finally, a srl parser is applied to the candidate translations
and the best match with the input structure is returned

problems and challenges: srl systems in applications

271

srl in machine translation

first smt system with srl

(wu and fung, 2009b)

hybrid smt system incorporating id14 and
phase-based smtmodels

two-pass architecture: 1) phrase-based smt system;
2) reordering guided by shallow semantic parsers

srl is performed    rst into source and output sentences in
order to identify predicate structures and constituents to be
re-ordered.

then, a set of candidate re-ordered sentences are generated
(by moving sr-mismatched constituents)

finally, a srl parser is applied to the candidate translations
and the best match with the input structure is returned

problems and challenges: srl systems in applications

272

srl in machine translation

first smt system with srl

(wu and fung, 2009b)

hybrid smt system incorporating id14 and
phase-based smtmodels

the hybrid model produces a slight but signi   cant
improvement in the quality of the translations
(measured with id7 score)

chinese-english translation on newswire texts

273

conclusions:

tutorial overview

1

introduction

2 state-of-the-art

3 empirical evaluation and lessons learned

4 problems and challenges

5 conclusions

conclusions:

general conclusions

274

srl is an important problem in nlp with strong connections
to applications requiring some degree of semantic
interpretation

it is a very active topic of research, which has generated an
important body of work in the last 6 years

some news are good but...

srl still has to face important challenges before we see
systems in real open-domain applications

good opportunities for future research on the topic

conclusions:

general conclusions

275

srl is an important problem in nlp with strong connections
to applications requiring some degree of semantic
interpretation

it is a very active topic of research, which has generated an
important body of work in the last 6 years

some news are good but...

srl still has to face important challenges before we see
systems in real open-domain applications

good opportunities for future research on the topic

conclusions:

general conclusions

276

srl is an important problem in nlp with strong connections
to applications requiring some degree of semantic
interpretation

it is a very active topic of research, which has generated an
important body of work in the last 6 years

some news are good but...

srl still has to face important challenges before we see
systems in real open-domain applications

good opportunities for future research on the topic

conclusions:

speci   c conclusions

277

generalization to new events/domains/corpora is a very weak
point of statistical srl systems

system portability must be improved (e.g., id20,
appropriate role sets, lexical semantic generalization, etc.)

system complexity is increasing in a higher scale than
performance

srl systems have to be more e   cient for massive text
processing

conclusions:

speci   c conclusions

278

generalization to new events/domains/corpora is a very weak
point of statistical srl systems

system portability must be improved (e.g., id20,
appropriate role sets, lexical semantic generalization, etc.)

system complexity is increasing in a higher scale than
performance

srl systems have to be more e   cient for massive text
processing

conclusions:

speci   c conclusions

279

srl systems for languages other than english should be
developed and made available to the nlp community

reduce the cost of producing semantically annotated corpora
for under resourced languages (e.g., making use of
semi-supervised training, corpora in other languages, etc.)

conclusions:

speci   c conclusions

280

srl technology should provide signi   cant improvements in
widely used nlp applications. a jump is needed from the
laboratory conditions to the real world.

investigate learning architectures that take advantage of the
joint resolution of several syntactic   semantic levels (parsing,
srl, wsd, nes, coreference, etc.)

conclusions:

speci   c conclusions

281

srl technology should provide signi   cant improvements in
widely used nlp applications. a jump is needed from the
laboratory conditions to the real world.

investigate learning architectures that take advantage of the
joint resolution of several syntactic   semantic levels (parsing,
srl, wsd, nes, coreference, etc.)

conclusions:

acknowledgements

thanks to

282

acl   ijcnlp 2009 organizers and tutorial chairs (diana
mccarthy and chengqing zong)

all people that directly or indirectly helped me with the
materials presented in this tutorial: mihai surdeanu, xavier
carreras, xavier llu    s, dan roth, kristina toutanova, wen-tau yih,
jan haji  c, jes  us gim  enez, paola merlo and james henderson

spanish ministry of education and science for the partial
funding of author   s research (opeid4, tin2006-15307-c03-01)

last, but not least, thanks to all tutorial attendees!

id14
past, present and future

llu    s m`arquez

talp research center

tecnhical university of catalonia

tutorial at acl-ijcnlp 2009

suntec     singapore

august 2, 2009

   version from august 3, 2009   

