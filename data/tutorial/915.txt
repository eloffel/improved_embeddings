hlt-naacl-06 tutorial

automatic id14

automatic id14

scott wen-tau yih        kristina toutanova

microsoft research

1

natural language understanding

id53

whom

what

who

when

kristina hit    scott with a baseball yesterday

  who hit scott with a baseball?
  whom did kristina hit with a baseball?
  what did kristina hit scott with?
  when did kristina hit scott with a baseball?

2

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

syntactic analysis (1/2)

syntactic analysis (2/2)

3

4

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

syntactic variations

yesterday, kristina hit scott with a baseball

scott was hit by kristina yesterday with a baseball

yesterday, scott was hit with a baseball by kristina

with a baseball, kristina hit scott yesterday

yesterday scott was hit by kristina with a baseball

kristina hit scott with a baseball yesterday

agent, hitter

thing hit

instrument

temporal adjunct

id14    

giving semantic labels to phrases

[agent john] broke [theme the window]

[theme the window] broke

[agentsotheby   s] .. offered [recipient the dorrance heirs]
[theme a money-back guarantee]

[agent sotheby   s] offered [theme a money-back guarantee] to
[recipient the dorrance heirs]

[theme a money-back guarantee] offered by [agent sotheby   s]

[recipient the dorrance heirs] will [arm-neg  not] 
be offered [theme a money-back guarantee]

 

 

 

 

 

 

5

6

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

why is srl important    

applications

  id53

  q: when was napoleon defeated?
  look for: [patient napoleon]

[pred defeat-synset] [argm-tmp *ans*]

  machine translation

english  (svo)
[agent the little boy]
[pred kicked]                      
[theme the red ball]
[argm-mnr hard]

farsi  (sov)

[agent pesar koocholo] boy-little
[theme toop germezi]
ball-red
[argm-mnr moqtam] hard-adverb 
[pred zaad-e]                 hit-past

  document summarization

  predicates and heads of roles summarize content

 

information extraction
  srl can be used to construct useful rules for ie

quick overview

  part i. introduction

(cid:2) what is id14?
  from manually created grammars to statistical approaches

  early work
  corpora     framenet, propbank, chinese propbank, nombank

  the relation between id14 and other tasks

  part ii. general overview of srl systems

  system architectures
  machine learning models

  part iii. conll-05 shared task on srl
  details of top systems and interesting systems
  analysis of the results
  research directions on improving srl systems

  part iv. applications of srl

7

8

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

moving toward statistical approaches

  early work [hirst 87] [dolan, richardson, vanderwende, 93&98]

  available corpora 

  framenet [fillmore et al. 01]

  http://framenet.icsi.berkeley.edu

  propbank [palmer et al. 05]

main focus

  http://www.cis.upenn.edu/~mpalmer/project_pages/ace.htm

  corpora in development

  chinese propbank

  http://www.cis.upenn.edu/~chinese/cpb/

  nombank

  http://nlp.cs.nyu.edu/meyers/nombank.html

9

early work [hirst 87]

  semantic interpretation

   the process of mapping a syntactically analyzed text of 
natural language to a representation of its meaning.   
  absity     semantic interpreter by hirst

  based on manually created semantic rules
  input: nadiasubj bought the bookobj from a store in the mall.
  output:

(a ?u

(buy ?u

(agent = (the ?x (person ?x

(propername =    nadia   ))))
(patient = (the ?y (book ?y)))
(source = (a ?z (store ?z

(location = (the ?w (mall ?w)))))))

example taken from [hirst 87]

10

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

early work [dolan, richardson, vanderwende, 93 & 98]

  mindnet:

  a graph of words labeled with semantic relations automatically 

acquired from on-line dictionaries and encyclopedias

  mindnet identifies 24 labeled semantic relations based on manually 

created semantic rules

  relations are weighted based on vertex frequency

http://research.microsoft.com/mnex

framenet [fillmore et al. 01]

  sentences from the british national corpus (bnc)
  annotated with frame-specific semantic roles

  various participants, props, and other conceptual roles

frame: hit_target
(hit, pick off, shoot)

agent
target

means
place

instrument

manner

purpose
subregion

time

lexical units (lus):
words that evoke the frame
(usually verbs)

non-core

frame elements (fes):
the involved semantic roles

core

[agent kristina] hit [target scott] [instrument with a baseball] [time yesterday ].

11

12

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

framenet     continued

  methodology of constructing framenet

  define/discover/describe frames
  decide the participants (frame elements)
  list lexical units that invoke the frame
  find example sentences in the corpus (bnc) and annotate them

  corpora

  framenet i     british national corpus only
  framenet ii     ldc north american newswire corpora

  size

  >8,900 lexical units, >625 frames, >135,000 sentences

http://framenet.icsi.berkeley.edu

13

proposition bank (propbank) [palmer et al. 05]

  transfer sentences to propositions
  kristina hit scott     hit(kristina,scott)

  id32     propbank

  add a semantic layer on id32
  define a set of semantic roles for each verb
  each verb   s roles are numbered

   [a0 the company] to     offer [a1 a 15% to 20% stake] [a2 to the public]
   [a0 sotheby   s]     offered [a2 the dorrance heirs] [a1 a money-back 
guarantee]
   [a1 an amendment] offered [a0 by rep. peter defazio]    
   [a2 subcontractors] will be offered [a1 a settlement]    

14

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

proposition bank (propbank)

define the set of semantic roles

  it   s difficult to define a general set of semantic 

roles for all types of predicates (verbs).

  propbank defines semantic roles for each verb 

and sense in the frame files.

  the (core) arguments are labeled by numbers. 

  a0     agent; a1     patient or theme
  other arguments     no consistent generalizations

  adjunct-like arguments     universal to all verbs
  am-loc, tmp, ext, cau, dir, pnc, adv, mnr, 

neg, mod, dis

15

proposition bank (propbank)

frame files

  hit.01    strike   

(cid:3) a0: agent, hitter; a1: thing hit; 

a2: instrument, thing hit by or with

[a0 kristina] hit [a1 scott] [a2 with a baseball] yesterday.

  look.02    seeming   

(cid:3) a0: seemer; a1: seemed like; a2: seemed to

[a0 it] looked [a2 to her] like [a1 he deserved this].

am-tmp

time

  deserve.01    deserve   

(cid:3) a0: deserving entity; a1: thing deserved; 

a2: in-exchange-for

it looked to her like [a0 he] deserved [a1 this].

proposition:

a sentence and 
a target verb

16

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

proposition bank (propbank)

add a semantic layer

a0

a1

a2

am-tmp

[a0 kristina] hit [a1 scott] [a2 with a baseball] [am-tmp yesterday].

proposition bank (propbank)

add a semantic layer     continued

a1

c-a1

a0

[a1 the worst thing about him] said [a0 kristina ] [c-a1 is his laziness].

17

18

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

proposition bank (propbank)

final notes

  current release (mar 4, 2005): proposition bank i

  verb lexicon: 3,324 frame files
  annotation: ~113,000 propositions

http://www.cis.upenn.edu/~mpalmer/project_pages/ace.htm

  alternative format: conll-04,05 shared task

  represented in table format
  has been used as standard data set for the shared 

tasks on id14
http://www.lsi.upc.es/~srlconll/soft.html

19

corpora in development

  chinese propbank http://www.cis.upenn.edu/~chinese/cpb/

  similar to propbank, it adds a semantic layer on penn 

chinese treebank

  a pre-release version has 250k words and 10,364 

sentences; ~55%

  nombank http://nlp.cs.nyu.edu/meyers/nombank.html

  label arguments that co-occur with nouns in propbank

[a0 her] [rel gift] of [a1 a book] [a2 to john]

  current release: sep. 2005

  93,809 instances of nouns; 2,805 different words; ~80%
  high frequency (>600) nouns have been completed

20

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

quick overview

  part i. introduction

(cid:2) what is id14?
(cid:2) from manually created grammars to statistical approaches

  early work
  corpora     framenet, propbank, chinese propbank, nombank

  the relation between id14 and other tasks

  part ii. general overview of srl systems

  system architectures
  machine learning models

  part iii. conll-05 shared task on srl

  details of top systems and interesting systems
  analysis of the results
  research directions on improving srl systems

  part iv. applications of srl

relation to other tasks

 

information extraction

  id29 for speech dialogues

  deep id29

  id32 function tagging

  predicting case markers

  aspects of comparisons

coverage

depth of semantics

direct application

srl

broad

shallow

no

21

22

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

related task: information extraction

  example (hub event-99 evaluations, [hirschman et al. 99])

  a set of domain dependent templettes, summarizing information 

about events from multiple sentences 

<market_change_1>:=

instrument

london [gold]

amount_change

fell [$4.70] cents

current_value

$308.45

date:

daily

time for our daily market report from nasdaq. 
london gold fell $4.70 cents to $308.45.

  many other task specifications: extracting information about 

products, relations among proteins, authors of books, etc.

information extraction versus 

id14

characteristic

ie

coverage

narrow

depth of semantics

shallow

srl

broad

shallow

directly connected to 
application

sometimes

no

  approaches to task: diverse

  depends on the particular task and amount of available data
  hand written syntactic-semantic grammars compiled into fsa
  sequence labeling approaches (id48, crf, cmm)
  survey materials: http://scottyih.org/ie-survey3.htm

[appelt & israel 99], [muslea 99]

23

24

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

related task: speech dialogs

  spoken language understanding: extract the semantics from 

an utterance

  must deal with uncertainly and disfluencies in speech input
  example: task setup in a narrow flight reservations domain 

(atis evaluations, [price 90])

<showflight>

<flight>

<dcity filler=   city   > seattle </dcity>
<acity filler=   city   > boston </acity>

</flight>

</showflight>

sentence:    show me all flights from seattle to boston   

atis parsing versus 
id14

characteristic

coverage

atis

narrow

depth of semantics

deeper

directly connected to 
application

yes

srl

broad

shallow

no

  approaches to atis parsing (overview in [wang et al. 05]):

  simultaneous syntactic/id29 [miller et al. 96], knowledge-

based approach [ward 94, dowding et al. 93]

  current best: small semantic grammar and a sequence labeling model 

(no full syntactic parsing information) error 3.8% ([wang et al. 06]). 

25

26

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

related task: id29 for 

nl interfaces to databases

  example: geoquery domain (a domain of facts for us 

geography) [zelle & mooney 96]
sentence: how many cities are there in the us?
meaning representation:

answer(count(city(loc_2(countryid(usa)))))

  characteristics:

  a restricted domain for which we have a complete domain model
  sentences are usually short but could be ungrammatical
  syntax of target representation is more complex compared to the 

atis task

  need to represent quantifiers (the largest, the most populated, etc.)

27

id29 for nl interfaces to 

databases versus id14

characteristic

nl interfaces to db

srl

coverage

depth of semantics

directly connected to 
application

narrow

deep

yes

broad

shallow

no

  approaches

  hand-built grammars [androutsopoulos et al. 05] (overview)
  machine learning of symbolic grammars     e.g. [zelle & mooney 96]
  learned statistical syntactic/semantic grammar [ge & mooney 05]

(supervised); [zettlemoyer & collins 05], [wong & mooney 06]
(unsupervised)

28

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

related task: deep parsing 

  hand-built broad-coverage grammars create simultaneous 

syntactic and semantic analyses
  the core language engine [alshawi 92]
  lexical functional grammar lfg ([bresnan 01], 

[maxwell & kaplan 93])

  head driven phrase structure grammar ([pollard & sag 94],  

[copestake & flickinger 00])

  model more complex phenomena

  quantifiers, quantifier scope, not just verb semantics, anaphora, 

aspect, tense

  a set of analyses is possible for each sentence according 

to the grammar: need to disambiguate

  until recently: no publicly available datasets or 

specifications for semantics

  difficult to create and expand

deep parsing versus 
id14

characteristic

deep parsing

srl

coverage

depth of semantics

directly connected to 
application

broad

deep

no

broad

shallow

no

  approach

  hand-build grammar (possibly expand automatically) 
  treated as a parsing problem (joint syntactic and semantic 

disambiguation)

  for lfg ([riezler et al. 02])
  for hpsg ([toutanova et al. 04], [miyao & tsujii 05])

29

30

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

related task: prediction of function tags 

[blaheta&charniak 00]

the id32 contains annotation of function tags for 
some phrases: subject, logical subject, adjuncts (temporal, 
locative, etc.)

slide from don blaheta 03 thesis defense

31

prediction of function tags versus 

id14

characteristic

predicting 
function tags

srl

coverage

broad

depth of semantics

shallower

directly connected 
to application

no

broad

shallow

no

  approach: a classifier based on voted perceptions and 

other ml techniques
  using rich syntactic information from id32 parse trees
  grammatical tags f1 96.4, other tags f1 83.8 [blaheta 03]

32

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

related task: predicting case markers

  some languages have case markers

  they indicate the syntactico-semantic 

relation between a phrase and 
the phrase it modifies

  needed for machine translation,                                 

foreign language learning

 

in japanese, case markers indicate e.g subject, 
object, location. 
  more similar to function tags than to semantic role labels

  good news: no annotated data is required!
  the case markers are part of the surface string

33

predicting case markers versus 

id14

characteristic

predicting case 
markers

coverage

broad

depth of semantics

shallower

directly connected to 
application

yes

srl

broad

shallow

no

  approaches

  using content words from the target language only plus 

dependency information [uchimoto et al. 02]

  using syntactic and word features from the source and 

target languages [suzuki & toutanova 06]; per case marker error 
using automatic parses: 8.4%

34

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

summary of part i     introduction

  what is id14?
  corpora for id14

  we will discuss mainly propbank.

  related tasks to srl
  information extraction
  deep id29
  id32 function tagging
  predicting case markers

  next part: overview of srl systems

quick overview

  part i. introduction

(cid:2) what is id14?
(cid:2) from manually created grammars to statistical approaches

  early work
  corpora     framenet, propbank, chinese propbank, nombank

(cid:2) the relation between id14 and other tasks

  part ii. general overview of srl systems

  system architectures
  machine learning models

  part iii. conll-05 shared task on srl

  details of top systems and interesting systems
  analysis of the results
  research directions on improving srl systems

  part iv. applications of srl

35

36

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

part ii: overview of srl systems

  definition of the srl task

  evaluation measures

  general system architectures
  machine learning models

  features & models
  performance gains from different techniques

development of srl systems

  gildea & jurafsky 2002

  first statistical model on framenet

  7+ papers in major conferences in 2003
  19+ papers in major conferences 2004, 2005

  3 shared tasks 

  senseval 3 (framenet)     8 teams participated
  conll 04 (propbank)     10 teams participated
  conll 05 (propbank)     19 teams participated

37

38

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

task formulation

  most general formulation: determine a labeling on (usually 

but not always contiguous) substrings (phrases) of the 
sentence s, given a predicate p

[a0 the queen] broke [a1 the window].
[a1 by working hard], [a0 he] said, [c-a1 you can get exhausted].

  every substring c can be represented by a set of word 

indices 

  more formally, a id14 is a mapping from 
the set of substrings of s to the label set l. l includes all 
argument labels and none.

subtasks

 

identification:
  very hard task: to separate the argument substrings from the 

rest in this exponentially sized set

  usually only 1 to 9 (avg. 2.7) substrings have labels arg and 

the rest have none for a predicate

  classification:

  given the set of substrings that have an arg label, decide the 

exact semantic label

  core argument id14: (easier)

  label phrases with core argument labels only. the modifier 

arguments are assumed to have label none.

39

40

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

evaluation measures

correct: [a0 the queen] broke [a1 the window] [am-tmp yesterday]
guess: [a0 the queen] broke the [a1 window] [am-loc yesterday]

correct
{the queen}    a0
{the window}    a1
{yesterday} ->am-tmp
all other     none

guess
{the queen}    a0
{window}    a1
{yesterday} ->am-loc
all other     none

  precision ,recall, f-measure {tp=1,fp=2,fn=2} p=r=f=1/3
  measures for subtasks

 

identification (precision, recall, f-measure) {tp=2,fp=1,fn=1} p=r=f=2/3

  classification (accuracy) acc = .5 (labeling of correctly identified phrases)
  core arguments (precision, recall, f-measure) {tp=1,fp=1,fn=1} 

p=r=f=1/2

part ii: overview of srl systems

(cid:2) definition of the srl task

(cid:2) evaluation measures

  general system architectures
  machine learning models

  features & models
  performance gains from different techniques

41

42

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

terminology: local and joint models

  local models decide the label of each substring 

independently of the labels of other substrings

  this can lead to inconsistencies 

  overlapping argument strings 

by [a1 working  [a1 hard ] , he] said , you can achieve a lot.

 

repeated arguments

by [a1 working] hard , [a1 he] said , you can achieve a lot.

  missing arguments

[a0 by working hard , he ] said , [a0 you can achieve a lot].

  joint models take into account the dependencies 

among labels of different substrings

43

basic architecture of a generic srl system

local scores for 
phrase labels do not 
depend on labels of 
other phrases

joint scores take 
into account 
dependencies 
among the labels 
of multiple phrases

44

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

annotations used

  syntactic parsers

  collins   , charniak   s (most systems) 

id35 parses 
([gildea & hockenmaier 03],[pradhan et al. 05])
tag parses ([chen & rambow 03])

  shallow parsers

[npyesterday] , [npkristina] [vphit] [npscott] [ppwith] [npa baseball].

  semantic ontologies (id138, automatically derived), 

and named entity classes

(v) hit (cause to move by striking) 

id138 hypernym

propel, impel (cause to move forward with force)

annotations used - continued

  most commonly, substrings that have argument labels 

correspond to syntactic constituents 

 

 

 

in propbank, an argument phrase corresponds to exactly one parse
tree constituent in the correct parse tree for 95.7% of the arguments; 

  when more than one constituent correspond to a single argument 
(4.3%), simple rules can join constituents together (in 80% of these
cases, [toutanova 05]);

in propbank, an argument phrase corresponds to exactly one parse
tree constituent in charniak   s automatic parse tree  for approx 
90.0% of the arguments.

  some cases (about 30% of the mismatches) are easily recoverable with 

simple rules that join constituents ([toutanova 05])

in framenet, an argument phrase corresponds to exactly one parse
tree constituent in collins    automatic parse tree for 87% of the 
arguments.

45

46

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

labeling parse tree nodes

  given a parse tree t, label 
the nodes (phrases) in the 
tree with semantic labels

  to deal with discontiguous 

arguments

 

in a post-processing step, 
join some phrases using 
simple rules

  use a more powerful 

labeling scheme, i.e. c-a0 
for continuation of a0

a0

none

another approach: labeling chunked 
sentences. will not describe in this section. 

47

local scoring models

  notation: a constituent node c, a tree t, a predicate 

node p , feature map for a constituent                               

  target labels 

c

p

  two (probabilistic) models 

 

identification model

  classification model

  sometimes one model 

48

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

why split the task into 

identification and classification

  different features are helpful for each task

  syntactic features more helpful for identification, lexical features 

more helpful for classification

  example: the identity of the predicate, e.g. p=   hit    is much more 

important for classification than for identification ([pradhan et al. 04]):

identification all features: 93.8 no predicate: 93.2
 
  classification all features: 91.0 no predicate: 82.4

  some features result in a performance decrease for one and an 

increase for the other task [pradhan et al. 04]

  splitting the task increases computational efficiency in 

training 

 

 

in identification, every parse tree constituent is a candidate (linear 
in the size of the parse tree, avg. 40)
in classification, label a small number of candidates (avg. 2.7)

49

combining identification and 

classification models

step 1. pruning.
using a hand-
specified filter. 

a0

a1

step 3. classification.
classification model  
assigns one of the 
argument labels to selected 
nodes (or sometimes 
possibly none) 

step 2. identification.
identification model 
(filters out candidates 
with high id203 of 
none)  

50

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

combining identification and 

classification models     continued

or

one step. 
simultaneously 
identify and classify 
using

a0

a1

combining identification and 

classification models     continued

  [gildea&jurafsky 02] 

  identification + classification for local scoring 

experiments

  one step for joint scoring experiments

  [xue&palmer 04] and [punyakanok et al. 04, 05]

  pruning + identification + classification

  [pradhan et al. 04] and [toutanova et al. 05] 

  one step

51

52

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

joint scoring models

a0

am-tmp

none

a1

am-tmp

  these models have scores for a whole labeling of a tree 

(not just individual labels)
  encode some dependencies among the labels of different nodes

53

combining local and joint scoring 

models

  tight integration of local and joint scoring in a single 

probabilistic model and exact search [cohn&blunsom 05]
[m  rquez et al. 05],[thompson et al. 03]
  when the joint model makes strong independence assumptions

  re-ranking or approximate search to find the labeling 

which maximizes a combination of local and a joint score 
[gildea&jurafsky 02] [pradhan et al. 04] [toutanova et al. 05]
  usually exponential search required to find the exact maximizer

  exact search for best assignment by local model satisfying 

hard joint constraints 
  using integer id135 [punyakanok et al 04,05] (worst 

case np-hard)

  more details later

54

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

part ii: overview of srl systems

(cid:2) definition of the srl task

(cid:2) evaluation measures

(cid:2) general system architectures
  machine learning models

  features & models

  for local scoring
  for joint scoring

  performance gains from different techniques

gildea & jurafsky (2002) features

  key early work

  future systems use these 

features as a baseline

  constituent independent
  target predicate (lemma)
  voice
  subcategorization

  constituent specific

  path
  position (left, right)
  phrase type
  governing category 

(s or vp)

  head word

target
voice
subcategorization
path
position
phrase type
gov cat
head word

broke
active
vp   vbd np
vbd   vp   s   np
left
np
s
she

55

56

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

evaluation using correct and 

automatic parses

for a correct parse, 95.7% of 
arguments correspond to a single 
constituent and their boundaries 
are easy to consider

for an automatic parse (charniak   s 
parser), about 90% of the arguments 
correspond to a single constituent; 

- the arguments for which the parser 
made a bracketing error are difficult to 
get 

- additionally, attachment errors and 
labeling errors make the task much 
harder

wrong!

57

performance with baseline features 

using the g&j model

  machine learning algorithm: interpolation of relative 

frequency estimates based on subsets of the 7 features 
introduced earlier

100

90

framenet 
results

propbank 
results

80

70

60

50

40

100

90

80

70

60

50

40

82.0

69.4

automatic
parses

59.2

id

class

integrated

82.8

79.2

67.6

53.6

automatic
parses
correct parses

class

integrated

just by changing the learning algorithm 67.6     80.8 using 
id166s [pradhan et al. 04]),    

58

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

surdeanu et al. (2003) features

  content word (different from 

head word)

  head word and content word 

pos tags

  ne labels (organization, 

location, etc.)

head word

content  word

baseline features

added features

89.0

84.6

83.7

78.8

head word

content word

100

90

80

70

60

50

40

id

class

gains from the new features using correct parses; 28% error 
reduction for identification and 23% error reduction for classification

59

pradhan et al. (2004) features

  more structural/lexical context (31% error reduction from 

baseline due to these + surdeanu et al. features)

first word / pos

parent constituent 
phrase type / 
head word/ pos

left constituent 
phrase type / 
head word/ pos

right constituent 
phrase type / 
head word/ pos

last word / pos

60

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

pradhan et al. (2004) results

baseline features

added features

added features

91.0

87.9

86.7

80.8

e
r
u
s
a
e
m

-
f

100

93.8

90.4

90

80

70

60

50

40

id

class

integrated

e
r
u
s
a
e
m

-
f

100

90

80

70

60

50

40

86.0

90.0

79.4

id

class

integrated

results on correct parse trees

results on automatic parse trees

baseline results higher than gildea and jurafsky   s due to a 
different classifier - id166

these are the highest numbers on propbank version july 2002

xue & palmer (2004) features

np_give_np_np

np_give_curr_np

np_v_np_np

  added explicit feature 

conjunctions in a 
maxent model, e.g. 
predicate + phrase type 
  syntactic frame feature 

(helps a lot)

  head of pp parent 

(helps a lot)

 

if the parent of a 
constituent is a pp, 
the identity of the 
preposition (feature 
good for propbank 
feb 04)

61

62

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

xue & palmer (2004) results

baseline features

added features

93.0

88.1

88.5

82.9

a newer version of 
propbank     february 
2004

100

90

80

70

60

50

40

class

integrated

all arguments correct parse

correct parse

automatic parse

correct parse

automatic parse

90.6

78.2

100

95.0

90

80

70

60

50

40

100

93.0

90

80

70

60

50

40

88.5

76.2

class

integrated

class

integrated

core arguments

all arguments

results not better than [pradhan et al. 04], but comparable.

63

machine learning models used

  back-off lattice-based relative frequency 

models ([gildea&jurafsky 02], [gildea& palmer 02])

  id90 ([surdeanu et al. 03])
  support vector machines ([pradhan et al. 04])
  id148 ([xue&palmer 04][toutanova et al. 05])
  snow ([punyakanok et al. 04,05])
  adaboost, tbl, crfs,    

64

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

joint scoring: enforcing hard 

constraints

  constraint 1: argument phrases do not overlap

by [a1 working  [a1 hard ] , he] said , you can achieve a lot.

  pradhan et al. (04)     greedy search for a best set of non-

overlapping arguments

  toutanova et al. (05)     exact search for the best set of non-

overlapping arguments (id145, linear in the size 
of the tree)

  punyakanok et al. (05)     exact search for best non-overlapping 

arguments using integer id135

  other constraints ([punyakanok et al. 04, 05])

  no repeated core arguments (good heuristic)
  phrases do not overlap the predicate

 

(more later)

gains from enforcing hard 

constraints

  argument phrases do not overlap

  pradhan et al. (04) good gains for a baseline system: 80.8    

81.6 correct parses

  toutanova et al. (05) a small gain from non-overlapping for a 

model with many features 88.3     88.4 correct parses

  other hard constraints (no repeating core arguments, set 

of labeled arguments allowable, etc.)
  punyakanok et al. (04) evaluation of this aspect only when using
chunked sentences (not full parsing) 87.1     88.1 correct parses 
67.1     68.2 automatic parses 

65

66

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

joint scoring: integrating soft 

preferences

a0

am-tmp

a1

am-tmp

  there are many statistical tendencies for the sequence 

of roles and their syntactic realizations
  when both are before the verb, am-tmp is usually before a0
  usually, there aren   t multiple temporal modifiers
  many others which can be learned automatically

joint scoring: integrating soft 

preferences

  gildea and jurafsky (02)     a smoothed relative frequency estimate 

of the id203 of frame element multi-sets:

  gains relative to local model 59.2     62.9 framenet automatic parses 

  pradhan et al. (04 )     a language model on argument label 

sequences (with the predicate included)

  small gains relative to local model for a baseline system 88.0     88.9 on 

core arguments propbank correct parses

  toutanova et al. (05)     a joint model based on crfs with a rich set 

of joint features of the sequence of labeled arguments (more later)
  gains relative to local model on propbank correct parses 88.4     91.2

(24% error reduction); gains on automatic parses 78.2     80.0

67

68

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

combining annotations and 

combining systems

  punyakanok et al. (05) combine information from systems trained on 
top n parse trees produced by charniak   s parser and collins    parser.
  effectively constituents from all trees can be selected as arguments
  constraints for non-overlap and other constraints are enforced through ilp
  gains 74.8     77.3 on automatic parses (conll 05 dev set)

  haghighi et al. (05) combine top n charniak parse trees 

  this is achieved in a bayesian way: sum over the parse trees 

approximated by max

  gains 79.7     80.3 on automatic parses (conll 05 test set)

  pradhan et al. (05) combine different syntactic views 

  charniak syntactic parse, id35 parse 
  gains 77.0     78.0 on automatic parses (conll 05 dev set)

  other systems in conll 2005

  more later on all of these

69

summary of part ii    

system overview

 

introduced srl system architecture:
  annotations, local scoring, joint scoring

  described major features helpful to the task

  showed that large gains can be achieved by improving the features

  described methods for local scoring, combining identification

and classification models

  described methods for joint scoring

  gains from incorporating hard constraints
  gains from incorporating soft preferences
introduced the concept of combining systems and annotations
  significant gains possible

 

  next part: more details on the systems in conll 2005

70

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

break!!

[a0 we] [am-mod will] see [a1 you] [am-tmp after the break].

quick overview

  part i. introduction

(cid:2) what is id14?
(cid:2) from manually created grammars to statistical approaches

  early work
  corpora     framenet, propbank, chinese propbank, nombank

(cid:2) the relation between id14 and other tasks

(cid:2) part ii. general overview of srl systems

(cid:2) system architectures
(cid:2) machine learning models

  part iii. conll-05 shared task on srl

  details of top systems and interesting systems
  analysis of the results
  research directions on improving srl systems

  part iv. applications of srl

71

72

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

part iii: conll-05 shared task on srl

  details of top systems and interesting 

systems
  introduce the top 4 systems
  describe 3 spotlight systems

  analysis of the overall results

  general performance
  system properties
  per argument performance

  directions for improving srl systems

73

details of conll-05 systems

  top performing systems

#3 m  rquez et al. (technical university of catalonia)
#4 pradhan et al. (university of colorado at boulder)
#1 punyakanok et al. (u. of illinois at urbana-champaign)
#2 haghighi et al. (stanford university)

kristina   s system

scott   s system

  spotlight systems

  yi & palmer     integrating syntactic and id29
  cohn & blunsorn     srl with tree crfs
  carreras     system combination

74

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

srl as sequential tagging [m  rquez et al.]

  a conceptually simple but competitive system
  srl is treated as a flat sequential labeling 

problem represented in the bio format.

  system architecture

  pre-processing (sequentialization)

  fpcha: full-parse, based on charniak   s parser
  ppupc: partial-parse, based on upc chunker & clauser

  learning using adaboost
  greedy combination of two systems

75

sequentialization     full parse

[m  rquez et al.]     continued

  explore the sentence regions defined by the clause boundaries.

  the top-most constituents in the regions are selected as tokens.

  equivalent to [xue&palmer 04] pruning process on full parse trees

kristina

b-a0

o

b-a1

b-a2

hit

scott

with
a
baseball

yesterday

b-am-tmp

s
s

vp

np

a0

np

a1

pp

a2

np

am-tmp

np

kristina   hit   scott with a baseball yesterday

76

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

sequentialization     partial parse

[m  rquez et al.]     continued

  only clauses and base chunks are available.

  chunks within the same clause are selected as tokens.

kristina

b-a0

hit

scott

with

a
baseball

o

b-a1

b-a2

i-a2

yesterday

b-am-tmp

a0

a1

a2

am-tmp

a2

greedy combination
[m  rquez et al.]     continued

 

join the maximum number of arguments from 
the output of both systems
  more impact on recall

  different performance on different labels

  fpcha: better for a0 and a1; ppupc: better for a2-a4

  combining rule

1. adding arguments a0 and a1 from fpcha
2. adding arguments a2, a3, and a4 from ppupc
3. repeat step 1&2 for other arguments
  drop overlapping/embedding arguments

77

78

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

results

[m  rquez et al.]     continued

  overall results on development set

ppupc
fpcha
combined

f1

73.57

75.75

76.93

prec.

76.86

78.08

78.39

rec.

70.55

73.54

75.53

  final results on test sets
  wsj-23 (2416 sentences)

  77.97 (f1), 79.55 (prec.), 76.45 (rec.)

  brown (426 sentences; cross-domain test) 

  67.42 (f1), 70.79 (prec.), 64.35 (rec.)

79

semantic role chunking combining 

complementary syntactic views [pradhan et al.]

  observation: the performance of an srl system depends 

heavily on the syntactic view
  syntactic parse trees generated by full parsers 

  charniak   s, collins   ,    

  partial syntactic analysis by chunker, clauser, etc.

  usage of syntactic information

  features (e.g., path, syntactic frame, etc.)
  argument candidates (mostly the constituents)

  strategy to reduce the impact of incorrect syntactic info.

  build individual srl systems based on different syntactic parse trees 

(charniak   s and collins   )

  use the predictions as additional features
  build a final srl system in the sequential tagging representation

80

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

constituent views
[pradhan et al.]     continued

parse tree #1

s
s

parse tree #2

s
s

np

vp

np

vp

np

pp

np

pp

np

np

kristina   hit  scott with  a  baseball

kristina   hit  scott    with  a  baseball

a0

a1

a2

a0

a1

81

chunk view

[pradhan et al.]     continued

  sequentialization using base chunks [hacioglu&ward 03]
  chunker: yamcha [kudo&matsumoto 01]

  http://chasen.org/~taku/software/yamcha/

chunks

true label

pred #1

pred #2

kristina

b-a0

b-a0

b-a0

hit

scott

with

a
baseball

o

b-a1

b-a2

i-a2

o

b-a1

b-a2

i-a2

o

b-a1

i-a1

i-a2

82

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

algorithm

[pradhan et al.]     continued

  generate features from charniak   s and collins   

parse trees

  add a few features from one to the other, and 

construct two srl systems

  represent the output as semantic bio tags, and 

use them as features

  generate the final semantic role label set using 

a phrase-based chunking paradigm

83

architecture

[pradhan et al.]     continued

charniak

collins

words

phrases

bio

bio

bio

features

chunker

bio

semantic role labels

slide from pradhan et al. (conll 2005)

84

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

results

[pradhan et al.]     continued

  overall results on development set

system

charniak

collins

combined

f1
77

76

78

prec

rec

80

79

81

75

74

76

  performance (f1) on test sets

  submitted system: wsj-23 77.4, brown 67.1
  bug-fixed system: wsj-23 78.6, brown 68.4

  software: assert (automatic statistical semantic role tagger)

http://oak.colorado.edu/assert

generalized id136 [punyakanok et al.]

  the output of the argument classifier often violates some 

constraints, especially when the sentence is long.

  use the integer id135 id136 procedure 

[roth&yih 04]

 

input: the local scores (by the argument classifier), and 
structural and linguistic constraints

  output: the best legitimate global predictions
  formulated as an optimization problem and solved via 

integer id135.

  allows incorporating expressive (non-sequential) 

constraints on the variables (the arguments types).

85

86

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

integer id135 id136

[punyakanok et al.]     continued

  for each argument ai and label t

  set up a boolean variable: ai,t     {0,1}

  indicating if ai is classified as t

  goal is to maximize
     i score(ai = t ) ai,t
  subject to the (linear) constraints

  any boolean constraint can be encoded this way.

  if score(ai = t) = p(ai = t), then the objective is
  find the assignment that maximizes the expected 

number of arguments that are correct

  subject to the constraints.

87

examples of constraints
[punyakanok et al.]     continued

  no duplicate argument classes

  a     potarg  x{a = a0}     1

any boolean rule can be encoded 
as a set of linear constraints.

  c-arg

if there is a c-arg phrase, there is an arg before it

   a                 potarg ,

   (a     potarg)    (a is before a             ) x{a = a0}     x{a             = c-a0}

  many other possible constraints:

  no overlapping or embedding
 

if the verb is of type a, no argument of type b

  hit can take only a0-a2 but not a3-a5

  relations between number of arguments

joint id136 can be used also to combine different srl systems.

88

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

results

[punyakanok et al.]     continued

  char: charniak   s parser (5-best trees)
  col: collins    parser

f1

wsj

brown

79.44

col

char

char-2

char-3

char-4

char-5

67.75

combined

50

60

70

80

90

online demo: http://l2r.cs.uiuc.edu/~cogcomp/srl-demo.php

a joint model for srl [haghighi et al.]

  the main idea is to build a rich model for joint scoring, 
which takes into account the dependencies among the 
labels of argument phrases.

one possible labeling suggested by local models

a0

am-tmp

a1

am-tmp

89

90

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

joint discriminative reranking

[haghighi et al.]     continued

  for computational reasons: start with local scoring model 

with strong independence assumptions

  find top n non-overlapping assignments for local model 

using a simple dynamic program [toutanova et al. 05]

  select the best assignment among top n using a joint 

log-linear model [collins 00]

  the resulting id203 of a complete labeling l of the 

tree for a predicate p is given by:

91

joint model features
[haghighi et al.]     continued

a0

am-tmp

a1

am-tmp

repetition features: count of arguments with a given label c(am-tmp)=2

complete sequence syntactic-semantic features for the core arguments: 

[np_a0 hit np_a1] , [np_a0 vbd np_a1]  (backoff) 

[np_a0 hit] (left backoff)

[np_arg hit np_arg] (no specific labels)

[1 hit 1] (counts of left and right core arguments)

92

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

using multiple trees
[haghighi et al.]     continued

  using the best charniak   s parse, on development set

  local model: 74.52(f1); joint model: 76.71(f1)

  further enhanced by using top k trees
t
, 2
t
1
l
1,
ll
k

  for top k trees from charniak   s parser                          find 

corresponding best srl assignments                  and choose the 
tree and assignment that maximize the score (approx. joint 
id203 of tree and assignment)

l

kt

,
,

,

  final results: 

  wsj-23: 78.45 (f1), 79.54 (prec.), 77.39 (rec.)
  brown: 67.71 (f1), 70.24 (prec.), 65.37 (rec.)
  bug-fixed post-evaluation: wsj-23 80.32 (f1) brown 68.81 (f1)

93

details of conll-05 systems

(cid:2) top performing systems

(cid:2) m  rquez et al. (technical university of catalonia)
(cid:2) pradhan et al. (university of colorado at boulder)
(cid:2) punyakanok et al. (u. of illinois at urbana-champaign)
(cid:2) haghighi et al. (stanford university)

  spotlight systems

  yi & palmer     integrating syntactic and id29
  cohn & blunsom     srl with tree crfs
  carreras     system combination

94

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

the integration of syntactic parsing and 

id14 [yi & palmer]

  the bottleneck of the srl task: parsing

  with [xue&palmer 04] pruning, given different parsers: 

12%~18% arguments are lost (development set: wsj-22)

  what do we want from syntactic parsing?

  correct constituent boundaries
  correct tree structures: expressing the dependency between the 

target verb and its arguments (e.g., the path feature)

  the proposed approach:

  combine syntactic parsing & argument identification (different 

cut of the task)

  train a new parser on the training data created by merging the 

id32 & the propbank (sec 02-21)

slide from yi&palmer (conll 2005)

95

data preparation & base parser

[yi & palmer]     continued

  data preparation steps

  strip off the id32 function tags
  2 types of sub-labels to represent the propbank arguments

  an: core arguments
  am: adjunct-like arguments

  train new maximum-id178 parsers [ratnaparkhi 99]

s

np-an

vp

np-an

pp-an

np-am

np

kristina    hit      scott with a baseball yesterday
96
96

based on yi&palmer   s slides (conll 2005)

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

results & discussion

[yi & palmer]     continued

  overall results on development set

an-parser

am-parser

charniak

combined

f1

67.28

69.31

69.98

72.73

prec.

71.31

74.09

76.31

75.70

rec.

63.68

65.11

64.62

69.99

  final f1     wsj-23: 75.17, brown: 63.14

  worse than using charniak   s directly

  because of weaker base parser?

  hurt both parsing and argument identification?

srl with tree crfs [cohn & blunsom]

  a different joint model     apply tree crfs

  generate the full parse tree using collins    parser
  prune the tree using [xue&palmer 04]
  label each remaining constituent the semantic role

or none

  learn the crfs model

  efficient crf id136 methods exist for trees

  maximum likelihood training: sum-product algorithm
  finding the best in testing: max-product algorithm

97

98

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

tree labeling

[cohn & blunsom]     continued

none

v

a0

a1

a2

am-tmp

none

99

model and results

[cohn & blunsom]     continued

  definition of crfs
  maximum log-likelihood training

xy

)(
x

z

p

(

)

|

=

exp

1

       

  
k

cf
,(
k

),
xy

c

   
cc

k

e

),(~
p
yx

[

f

k

   

]

e

[

f

k

0]

=

p

),(
yx

 

  use sum-product to calculate marginal
id136
  use max-product to find the best labeling

e yx

[),(

p

f

k

]

  results: final f1     wsj-23: 73.10, brown: 63.63
  findings [cohn&blunsom conll-05 slides]:

  crfs improved over maxent classifier (+1%)
  charniak parses more useful (+3%)
  very few inconsistent ancestor/dependent labelings
  quite a number of duplicate argument predictions

data from cohn&blunsom   s slide (conll 2005)

100

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

system combination [carreras et al.]

  how much can we gain from combining different 

participating systems at argument level?
  each system proposes arguments, scored according 

to overall f1 on development

  the final score for an argument is the sum of scores 

given by systems
  greedy selection

  repeat, until no more arguments in the candidate list

  select argument candidate with the best score
  removing overlapping arguments from candidate list

results & discussion
[carreras et al.]     continued

wsj-23

punyakanok+haghighi+pradhan

punyakanok

brown

f1

80.21

79.44

f1

haghighi+marquez+pradhan+tsai

69.74

punyakanok

67.75

prec.

79.10

82.28

prec.

69.40

73.38

rec.

81.36

76.78

rec.

70.10

62.93

  the greedy method of combing systems 
increases recall but sacrifices precision.

  the gain on f1 is not huge.

101

102

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

part iii: conll-05 shared task on srl

(cid:2) details of top systems and interesting 

systems
(cid:2) introduce the top 4 systems
(cid:2) describe 3 spotlight systems

  analysis of the overall results

  general performance
  system properties
  per argument performance

  directions for improving srl systems

103

results on wsj and brown tests

f1: 70% ~ 80%
small differences

every system 
suffers from 
cross-domain 
test (~10%)

figure from carreras&m  rquez   s slide (conll 2005)

104

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

system properties

  learning methods

  snow, maxent, adaboost, id166, crfs, etc.
  the choice of learning algorithms is less important.

  features 

  all teams implement more or less the standard features 

with some variations.

  a must-do for building a good system!
  a clear feature study and more feature engineering will 

be helpful.

105

system properties     continued

  syntactic information

  charniak   s parser, collins    parser, clauser, chunker, etc.
  top systems use charniak   s parser or some mixture
  quality of syntactic information is very important!

  system/information combination

  8 teams implement some level of combination
  greedy, re-ranking, stacking, ilp id136
  combination of systems or syntactic information is a 

good strategy to reduce the influence of incorrect 
syntactic information!

106

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

per argument performance

conll-05 results on wsj-test

  core arguments 

  adjuncts (freq. ~30%)

(freq. ~70%)

best f1
88.31

freq.

25.58%

79.91

35.36%

70.26

8.26%

65.26

1.39%

77.25

1.09%

a0

a1

a2

a3

a4

arguments that need

to be improved

best f1
78.21

freq.

6.86%

59.73

3.46%

80.45

2.05%

tmp

adv

dis

mnr

59.22

2.67%

loc

60.99

2.48%

mod

98.47

3.83%

cau

64.62

0.50%

neg

98.91

1.36%

data from carreras&m  rquez   s slides (conll 2005)

107

groups of verbs in wsj-test

  by their frequencies in wsj-train

0

1-20

21-100

101-500

501-1000

verbs

props

args.

34

37

70

418

568

359

1098

1049

2066

149

1896

3559

18

765

1450

  conll-05 results on wsj-test     core arguments

0

1-20

21-100

101-500

501-1000

args. %
best f1

0.9

12.8

25.2

43.4

73.38

76.05

80.43

81.70

17.7

80.31

arguments of low-frequency 
verbs need to be improved

data from carreras&m  rquez   s slides (conll 2005)

108

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

part iii: conll-05 shared task on srl

(cid:2) details of top systems and interesting 

systems
(cid:2) introduce the top 4 systems
(cid:2) describe 3 spotlight systems

(cid:2) analysis of the overall results

(cid:2) general performance
(cid:2) system properties
(cid:2) per argument performance

  directions for improving srl systems

directions for improving srl

  better feature engineering

  maybe the most important issue in practice

  joint modeling/id136

  how to improve current approaches?

  fine-tuned learning components

  can a more complicated system help?

  cross domain robustness

  challenge to applying srl systems

109

110

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

better feature engineering

gildea&jurafsky    02
   

target predicate

    voice
    subcategorization
    path
    position (left, right)
    phrase type
    governing category
    head word

surdeanu et al    03
    content word
    head word pos
    content word pos
    named entity

xue&palmer    04
   

feature conjunctions

    syntactic frame
    head of pp parent

pradhan et al    04
    phrase type / head word / pos of

left/right/parent constituent
first/last word/pos

   

 

individual feature contribution is not clear
  every set of features provide some improvement, but   
  different system, different corpus, different usage

111

joint model/id136

  unless pure local model reaches prefect results, joint 

model/id136 often can improve the performance

  greedy rules

(cid:2) fast & effective
(cid:5) with no clear objective function
(cid:5) often increase recall by sacrificing precision

 

integer id135 id136 [roth&yih 04]
(cid:2) with clear objective function
(cid:2) can represent fairly general hard constraints
(cid:5) more expensive to integrate soft (statistical) constraints

  joint model [toutanova et al. 05] [cohn&blunsom 05]

(cid:2) capture statistical and hard constraints directly from the data
(cid:5) need re-ranking to avoid complexity problems [toutanova et al. 05]
(cid:5) capture only local dependency [cohn&blunsom 05]

112

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

fine-tuned learning components

  separate core arguments and adjuncts
  adjuncts are independent of the target verb
  performance may be enhanced with specific features

  pradhan et al. (2005) did feature selection for each argument type

  train systems for different (groups of) verbs

  verbs (or senses) may have very different role sets
  example: stay.01(remain) vs. look.02 (seeming)

[a1 consumer confidence] stayed [a3 strong] in october.
[a0 the demand] looked [a1 strong] in october.

cross domain robustness

  the performance of srl systems drops 

significantly when applied on a different corpus
  ~10% f1 from wsj to brown
  the performance of all the syntactic taggers and 

parsers drops significantly

  all trained on wsj

  may not build a robust system without data

  semi-supervised learning
  active learning

113

114

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

summary of part iii: 

conll-05 shared task on srl

  described the details of top performing srl systems

 

implement generally all standard features

  use good syntactic information     charniak   s parser & more
  deploy system/information combination schemes
  achieve ~80% f1 on wsj, ~70% f1 on brown

 

introduced some interesting systems
  train syntactic parser and argument identifier together
  apply tree crfs model

 

investigate the performance of a large system combination

summary of part iii: 

conll-05 shared task on srl     continued

  analyzed the results of the conll-05 systems

  general performance

  performance on wsj is between 70% and 80%
  the differences among systems are small
  every system suffers from cross-domain test; ~10% f1

drop on brown corpus

  per argument performance

  core arguments a1 and a2 and some frequent adjunct 

arguments need to be improved

  arguments of low-frequency verbs need to be improved

115

116

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

summary of part iii: 

conll-05 shared task on srl     continued

  directions for improving srl systems

  perform careful feature study 
  design better features
  enhance current joint model/id136 techniques
  separate models for different argument sets
  improve cross domain robustness

  next part: applications of srl systems

quick overview

(cid:2) part i. introduction

(cid:2) what is id14?
(cid:2) from manually created grammars to statistical approaches

(cid:2) early work
(cid:2) corpora     framenet, propbank, chinese propbank, nombank

(cid:2) the relation between id14 and other tasks

(cid:2) part ii. general overview of srl systems

(cid:2) system architectures
(cid:2) machine learning models

(cid:2) part iii. conll-05 shared task on srl

(cid:2) details of top systems and interesting systems
(cid:2) analysis of the results
(cid:2) research directions on improving srl systems

  part iv. applications of srl

117

118

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

part iv: applications

  information extraction

  reduce development time

  summarization

  sentence matching 
  id53

  understand questions better

  id123

  deeper semantic representation

119

srl in information extraction 

[surdeanu et al. 03]

  information extraction (hub event-99 evaluations, 

[hirschman et al 99] )
  a set of domain dependent templettes, summarizing 

information about events from multiple sentences 

<market_change_1>:=

instrument

london [gold]

amount_change

fell [$4.70] cents

current_value

$308.45

date:

daily

time for our daily market report from nasdaq. 
london gold fell $4.70 cents to $308.45.

120

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

srl in information extraction 

[surdeanu et al. 03]-continued

  find predicate argument relations and 

map resulting structures into templettes 
via hand-written simple rules

np

s

arg1 and market_change_verb => 
instrument

arg2 and (money or percent or  
qauntity) and 
market_change_verb => 
amount_change

(arg4 or argm_dir) and number and 
market_change_verb=> 
current_value

vp

np

pp

norwalk-based micro warehouse

fell  5     to 34   

arg1

arg2 argm-dir

instrument

amnt_change

curr_value

121

srl in information extraction 

[surdeanu et al. 03]-continued

  results
  srl 1

  identification 71.9
  classification 78.9

  srl 2

  identification 89.0
  classification 83.7

  fsa is a traditional 
finite state approach

91.3

82.8

68.9

100

90

80

70

60

50

40

72.7

67.0

58.4

srl 1

srl 2

fsa

market change

death

better srl leads to significantly 
better ie performance.

the fsa approach does better but requires intensive human 
effort (10 person days). 

the systems using srl require 2 hours of human effort.

122

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

srl in summarization
(squash, [melli et al. 05] sfu)

  the task is to generate a 250-word summary from 

multiple documents
  given a specified topic and level of detail (specific, general)
title: american tobacco companies overseas 
narrative: in the early 1990's, american tobacco companies tried to expand

their business overseas. what did these companies do or try to do and 
where? how did their parent companies fare? 

granularity: specific 

  the system uses srl extensively for:

  estimating a significance score for a sentence

  which entities participate in which semantic relations

  estimating sentence similarity

  which entities participating in which semantic relations are contained 

in two sentences

srl in summarization 

(squash, [melli et al. 05]-continued)

 

 

it is not possible to remove just the srl component from 
the system since srl is used throughout
improving the srl system improves summarization 
performance (id8-2 scores on the development set)
  na  ve srl 0.0699
  assert srl 0.0731

  this is a pretty large improvement considering the 

impact of other successful features
  bias toward the first sentences 0.0714     0.0738

  the overall placement of an earlier version of squash 

was 7th out of 25 systems in duc 2005

123

124

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

srl in id53 

[narayanan & harabagiu 04]

  parsing questions

q: what kind of materials were stolen from the russian navy?

pas(q): what [a1 kind of nuclear materials] were [predicate:stolen] 
[a2 from the russian navy]?    

  parsing answers

a(q): russia   s pacific fleet has also fallen prey to nuclear theft; in 1/96, approximately 7 kg   of 
heu was reportedly stolen from a naval base in sovetskaya gavan.

pas(a(q)):  [a1(p1) russia   s pacific fleet] has [am-dis(p1) also] 

[p1: fallen] [a1(p1) prey to nuclear theft]; 
[am-tmp(p2) in 1/96], [a1(p2) approximately 7 kg of heu]
was [am-adv(p2) reportedly] [p2: stolen]  
[a2(p2) from a naval base] [a3(p2)in sovetskawa gavan]

  result: exact answer=    approximately 7 kg of heu   

slide from harabagiu and narayanan (hlt 2004)

125

srl in id53
[narayanan & harabagiu 04]-continued

  parsing questions

q: what kind of materials were stolen from the russian navy?

fs(q): what [goods kind of nuclear materials] were [target-predicate stolen] 
[victim from the russian navy]?    

  parsing answers

a(q): russia   s pacific fleet has also fallen prey to nuclear theft; in 1/96, approximately 7 kg   of 
heu was reportedly stolen from a naval base in sovetskaya gavan.

fs(a(q)):  [victim(p1) russia   s pacific fleet] has also fallen prey to [goods(p1) nuclear ]

[target-predicate(p1) theft]; in 1/96, [goods(p2) approximately 7 kg of heu]
was reportedly [target-predicate (p2) stolen]  
[victim (p2) from a naval base] [source(p2) in sovetskawa gavan]

  result: exact answer=    approximately 7 kg of heu   

slide from harabagiu and narayanan (hlt 2004)

126

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

srl in id53  
[narayanan & harabagiu 04]-continued

  evaluation of gains due to predicate-argument 

information. 

structure used
answer hierarchy
propbank analyses
framenet analyses

percent of questions

12%
32%
19%

  percent of questions for which the correct answer type was 

identified through using each structure. 

  question: what is the additional value compared to 

matching based on syntactic analyses?

srl in id123 

[braz et al. 05]

  does a given text s entail a given sentence t

  s: the bombers had not managed to enter the building

  t: the bombers entered the building

  evaluating entailment by matching predicate 

argument structure
  s1: [arg0the bombers] had [argm_negnot] managed to 

[predenter] [arg1 the building]

  t1: [arg0the bombers] [predentered] [arg1 the building]

s does not entail t because they do not have the 
same set of arguments

127

128

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

srl in id123

[braz et al. 05]-continued

  srl forms the basis of the algorithm for deciding 

entailment.

 

it is also extensively used in rewrite rules which preserve 
semantic equivalence. 

  not possible to isolate the effect of srl and unknown 

whether a syntactic parse approach can do similarly well.

  results on the pascal rte challenge 2005

  word based baseline: 54.7

  system using srl and syntactic parsing: 65.9

  the system placed 4th out of 28 runs by 16 teams in the 

pascal rte challenge

summary of part iv: applications

 

information extraction
  srl has advantages in development time; good srl     good ie
  fsa systems are still about 10% better.

  summarization

  sophisticated sentence matching using srl

 

improving srl improves summarization.

  id53

  having more complex semantic structures increases the number of 

questions that can be handled about 3 times.

  id123

  srl enables complex id136s which are not allowed using 

surface representations.

  action item: evaluate contributions of srl vs. syntactic parsing

  none of the systems performs a careful comparison

129

130

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

conclusions

  id14 is relatively new but has 

attracted a lot of interest

  large corpora with annotated data are available

  framenet, propbank

  it provides a novel broad-coverage level of 

semantic interpretation
  shallower than some alternatives (deep parsing for 

limited and broad domains)

  deeper than others (id32 analyses with 

function tags)

  tasks which profit from id32 syntactic 

analyses should profit from this semantic layer

131

conclusions 

current state of the art systems

  achieve about 80% per-argument f-measure 

(60% whole propositions correct)

  performance is respectable but still there is a lot of 

a0

a1

 

room for improvement
inter-annotator agreement is 99% for all nodes given gold-standard
syntactic parses (chance agreement is 88%); not comparable to 
system results

  build on the strength of statistical parsing models

  perform poorly when the syntactic parsers do so

  use syntactic information extensively
  have mechanisms for increasing robustness to parser error
  use powerful machine learning techniques
  model dependencies among argument labels

132

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

conclusions

directions for improving srl

  increase robustness to syntactic parser error
  find ways to collect additional knowledge 

  use unlabeled data
  share information across verbs
  can applications create more data for srl 

automatically?

  improve the statistical models

  other features, other dependencies

  improve search/id136 procedures

conclusions 

major challenges

  need to connect srl to natural language 

applications
  study the additional value of semantic labels 

compared to surface representations and syntactic 
analyses

  apply srl to other applications

  more information extraction applications
  atis labeling and nl interfaces to databases

  have we defined the corpora well?

  validate the annotation standards through application domains

  what level of accuracy is needed in order for srl to 

be useful?

133

134

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

final remarks

  id14 is an exciting area of research!

  progress is fast 
  there is still room for large contributions

  provides robust broad-coverage semantic representations
  easy integration with applications (information extraction, 
id53, summarization, id123)
  good results in tasks

  tools available online that produce srl structures

  assert (automatic statistical semantic role tagger) 

http://oak.colorado.edu/assert

  uiuc system (http://l2r.cs.uiuc.edu/~cogcomp/srl-demo.php)

135

136

acknowledgments

  we   d like to thank the following people, who kindly provided their 

slides to us or helped us understand their systems.
  lucy vanderwende, sameer pradhan, xavier carreras, llu  s m  rquez, 

szu-ting yi, mihai surdeanu, anoop sarkar, srini narayanan, sanda
harabagiu, and mark sammons.

  we are very grateful to joshua goodman, who gave us many 

valuable comments and helped us to prepare the materials.

  we are also thankful to our colleagues and friends who attended our 

practical talk and gave us useful feedback.

  finally, we thank the audience of our tutorial for their interest and 

also the questions and discussions.

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

references: introduction 

 

  hiyan alshawi, editor. the core language engine. mit press, 1992.
ion androutsopoulos, graeme ritchie, and peter thanisch. natural 
language interfaces to databases - an introduction. in journal of natural 
language engineering 1(1), 1995. 

  douglas appelt and david israel. introduction to information extraction 

technology. tutorial at ijcai 1999. 

  don blaheta and eugene charniak. assigning function tags to parsed text.

in proceedings of naacl 2000.

  don blaheta. function tagging. phd thesis, brown cs department, 2003.

 

joan bresnan. lexical-functional syntax. blackwell, 2001.

  ann copestake and dan flickinger. an open-source grammar development 

 

environment and broad-coverage english grammar using hpsg.  in 
proceedings of lrec-2000.
john dowding, jean gawron, doug appelt, john bear, lynn cherny, 
robert moore, and douglas moran. gemini: a natural language system for 
spoken-language understanding. in  proceedings of acl 1993.

references: introduction

  charles j. fillmore, charles wooters, and collin f. baker. building a large 

lexical databank which provides deep semantics. in proceedings of the 
pacific asian conference on language, information and computation 2001.

  ruifang ge and raymond mooney. a statistical semantic parser that 

integrates syntax and semantics. in proceedings of conll 2005.

 

  graeme hirst. semantic interpretation and the resolution of ambiguity 
(studies in natural language processing). cambridge university press, 
1987. 
lynette hirschman, patricia robinson, lisa ferro, nancy chinchor, erica 
brown, ralph grishman, and beth sundheim. hub 4 event99 general 
guidelines and templettes, 1999. 
john t. maxwell and ronald m. kaplan. the interface between phrasal and 
functional constraints. in computational linguistics,19(4), 1993.

 

137

138

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

references: introduction

 

ion muslea. extraction patterns for information extraction tasks: a survey. in 
proceedings of the aaai workshop on machine learning for ie, 1999.

  yusuke miyao and jun'ichi tsujii. probabilistic disambiguation models for 

wide-coverage hpsg parsing. in proceedings of acl 2005.

  scott miller, robert bobrow, robert ingria, and richard schwartz. a fully 
statistical approach to id139. in proceedings of acl 
1996.

  martha palmer, dan gildea, and paul kingsbury. the proposition bank: an 

annotated corpus of semantic roles. in computational linguistics, 31(1), 
2005.

  carl pollard and ivan a. sag. head-driven phrase structure grammar. 

university of chicago press, 1994.

  patti price. evaluation of spoken language systems: the atis domain. in 

proceedings of the third darpa speech and natural language workshop, 
1990.

references: introduction

  stefan riezler, tracy h. king, ronald m. kaplan, richard crouch, john t. 

maxwell iii, and mark johnson. parsing the wall street journal using a 
lexical-functional grammar and discriminative estimation techniques. in 
proceedings of acl 2002.

  hisami suzuki and kristina toutanova. learning to predict case markers in 

japanese. in proceedings of acl-coling 2006.

  kristina toutanova, penka markova, and christopher d. manning. the leaf 
projection path view of parse trees: exploring string kernels for hpsg parse 
selection. in proceedings of emnlp 2004.

  kiyotaka uchimoto, satoshi sekine and hitoshi isahara. text generation

from keywords. in proceedings of coling 2002.

  ye-yi wang, john lee, milind mahajan, and alex acero. combining 
statistical and knowledge-based spoken language understanding in 
conditional models. in proceedings of acl-coling 2006.

139

140

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

references: introduction

  ye-yi wang, li deng, and alex acero. spoken language understanding: an 

introduction to the statistical framework. in ieee signal processing 
magazine, vol 27 no. 5. 2005.

  wayne ward. recent improvements in the cmu spoken language 

understanding system. in proceedings of human language technology 
workshop, 1994.

  yuk wah wong and raymond mooney. learning for id29 with 

 

 

statistcial machine translation. in proceedings of hlt/naacl 2006.
john zelle and raymond mooney. learning to parse database queries 
using inductive logic programming. in proceedings of aaai 1996.
luke zettlemoyer and michael collins. learning to map sentences to logical 
form: structured classification with probabilistic categorial grammars. in
proceedings of uai 2005.

references: overview of srl systems

 

john chen and owen rambow. use of deep linguistic features for  the 
recognition and labeling of semantic arguments. in proceedings of emnlp 
2003. 

  xavier carreras and llu  s m  rquez. introduction to  the conll-2005 shared 

task: id14. in proceedings of conll 2005. 

  trevor cohn and  philip blunsom.  semantic role labelling with tree 

id49. in proceedings of conll 2005.

  daniel gildea and daniel jurafsky. automatic labeling of semantic roles. in 

computational linguistics, 28(3), 2002. 

  daniel gildea and martha palmer. the necessity of parsing for predicate 

argument recognition. in proceedings of acl 2002 .

  daniel gildea and julia hockenmaier. identifying semantic roles using 

id35. in proceedings of emnlp 2003.

141

142

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

references: overview of srl systems

  aria haghighi, kristina toutanova, and christopher manning. a joint model 

 

for id14. in proceedings of conll 2005.
llu  s m  rquez,  pere comas, jes  s gim  nez, and neus catal  .
id14 as sequential tagging. in proceedings of conll
2005.

  sameer pradhan, wayne ward, kadri hacioglu, james h. martin and dan 

jurafsky. id14 using different syntactic views. in 
proceedings of acl 2005.

  sameer pradhan, wayne ward, kadri hacioglu, james martin, and dan 

jurafsky. shallow id29 using support vector machines. in 
proceedings of hlt 2004. 

  vasin punyakanok, dan roth, wen-tau yih and dav zimak. semantic role 

labeling via integer id135 id136. in proceedings of 
coling 2004.

references: overview of srl systems

  vasin punyakanok, dan roth, and wen-tau yih. the necessity of syntactic  

parsing for id14. in proceedings of ijcai 2005.

  mihai surdeanu, sanda harabagiu, john williams, and paul aarseth. using 
predicate-argument structures for information extraction. in proceedings of 
acl 2003.

  kristina toutanova. effective statistical models for syntactic and semantic 

disambiguation. phd thesis, stanford cs department, 2005.

  kristina toutanova, aria haghighi, and christopher d. manning. joint 
learning improves id14. in proceedings of acl 2005.

  nianwen xue and martha palmer. calibrating features for semantic role 

labeling. in proceedings of emnlp 2004.

143

144

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

references: 

conll-05 shared task on srl

  xavier carreras and llu  s m  rquez. introduction to  the conll-2005 shared 

task: id14. in proceedings of conll 2005.

  trevor cohn and  philip blunsom.  semantic role labelling with tree 

id49. in proceedings of conll-2005.

  michael collins and terry koo. discriminative reranking for natural 

language parsing. in computational linguistics 31(1), 2005.

  daniel gildea and daniel jurafsky. automatic labeling of semantic roles. in 

computational linguistics, 28(3), 2002.

  kadri hacioglu and wayne ward. target word detection and semantic role 

chunking using support vector machines. in proceedings of hlt-naccl 
2003. 

  aria haghighi, kristina toutanova, and christopher manning. a joint model 

for id14. in proceedings of conll-2005.

145

references: 

conll-05 shared task on srl

  vasin punyakanok, dan roth, and wen-tau yih. generalized id136 with 

multiple id14 systems. in proceedings of conll-2005.

  taku kudo and yuji matsumoto. chunking with support vector machines. in 

 

proceedings of naacl 2001. 
llu  s m  rquez,  pere comas, jes  s gim  nez, and neus catal  .
id14 as sequential tagging. in proceedings of conll 2005.

  sameer pradhan, wayne ward, kadri hacioglu, james martin, and dan 

jurafsky. shallow id29 using support vector machines. in 
proceedings of hlt 2004. 

  sameer pradhan, kadri hacioglu, wayne ward, james h. martin, and daniel 
jurafsky. semantic role chunking combining complementary syntactic views. 
in proceedings of conll 2005.

  dan roth and wen-tau yih. a id135 formulation for global 

id136 in natural language tasks. in proceedings of coling 2004. 

146

wen-tau yih & kristina toutanova

hlt-naacl-06 tutorial

automatic id14

references: 

conll-05 shared task on srl

  mihai surdeanu, sanda harabagiu, john williams, and paul aarseth. using 
predicate-argument structures for information extraction. in proceedings of 
acl 2003.

  kristina toutanova, aria haghighi, and christopher d. manning. joint 
learning improves id14. in proceedings of acl 2005.

  szu-ting yi and martha palmer. the integration of syntactic parsing and 

id14. in proceedings of conll 2005.

  nianwen xue and martha palmer. calibrating features for semantic role 

labeling. in proceedings of emnlp 2004.

references: applications

  rodrigo de salvo braz, roxana girju, vasin punyakanok, dan roth, and 

 

mark sammons. an id136 model for semantic entailment in natural 
language. in proceedings of aaai 2005.
lynette hirschman, patricia robinson, lisa ferro, nancy chinchor, erica 
brown, ralph grishman, and beth sundheim. hub 4 event99 general 
guidelines and templettes, 1999.

  gabor melli, yang wang, yudong liu, mehdi m. kashani, zhongmin shi, 

baohua gu, anoop sarkar and fred popowich. description of squash, the 
sfu id53 summary handler for the duc-2005 summarization 
task. in proceedings of duc 2005.

  srini narayanan and sanda harabagiu.  id53 based on 

semantic structures. in proceedings of coling 2004.

  mihai surdeanu, sanda harabagiu, john williams, and paul aarseth. using 
predicate-argument structures for information extraction. in proceedings of 
acl 2003.

147

148

wen-tau yih & kristina toutanova

