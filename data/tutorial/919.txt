acl 2008:  semi-supervised 

learning tutorial

john blitzer and xiaojin zhu

http://ssl-acl08.wikidot.com

what is semi-supervised learning (ssl)?

    labeled data (entity classification)

       , says mr. cooper, vice  

president of    

       firing line inc., a 

philadelphia gun shop.

    lots more unlabeled data

       , yahoo   s own jerry yang is 
right    

       the details of obama   s san 
francisco mis-adventure    

labels

person 

location

organization

can we build a 
better model from 
both labeled and 
unlabeled data?

who else has worked on ssl?

    canonical nlp problems

    tagging    (haghighi and klein 2006)

    chunking, ner (ando & zhang 2005)

    parsing (mcclosky & charniak 2006)

    outside the classic nlp canon

    entity-attribute extraction (bellare et al. 2007)

    id31 (goldberg & zhu 2006)

    link spam detection (zhou et al. 2007)

    your problem?

anti-ssl arguments: practice

   

if a problem is important, we   ll find the time / money / 
linguists to label more data

ip-hln

penn chinese treebank 

np-sbj

vv
      

2 years to annotate 4000 
sentences

np

np

dt
   

nn

nn
                    

nn

the national track & field championships concluded

i want to parse the baidu zhidao question-answer database. 

who   s going to annotate it for me?

anti-ssl arguments: theory

       but tom cover said   : (castelli & cover 1996)

    under a specific generative model, labeled samples are 

exponentially more useful than unlabeled

    the semi-supervised models in this tutorial 
make different assumptions than c&c (1996)

    today we   ll also discuss new, positive 

theoretical results in semi-supervised learning

why semi-supervised learning?

    i have a good idea, but i can   t afford to label 

lots of data!

    i have lots of labeled data, but i have even 

more unlabeled data

    ssl:  it   s not just for small amounts of labeled data 

anymore!

    id20: i have labeled data from 1 
domain, but i want a model for a different domain

goals of this tutorial

1) cover the most common classes of semi-

supervised learning algorithms

2) for each major class, give examples of 

where it has been used for nlp

3) give you the ability to know which type of 

algorithm is right for your problem

4) suggest advice for avoiding pitfalls in semi-

supervised learning

overview

1) id64 (50 minutes)

    co-training

    latent variables with linguistic side information

2) graph-id173 (45 minutes)

3) structural learning (55 minutes)

    entity recognition, id20, and 

theoretical analysis

some notation

id64: outline

    the general id64 procedure

    co-training and co-boosting

    applications to entity classification and 

entity-attribute extraction

    ssl with latent variables, prototype 

learning and applications

id64

    on labeled data, minimize error

    on unlabeled data, minimize a proxy for error 

derived from the current model

    most semi-supervised learning models in nlp

1) train model on labeled data

2) repeat until converged

a) label unlabeled data with current model

b) retrain model on unlabeled data

back to named entities

    na  ve bayes model

features:  <left word =    mr.   >

label:     person   

    parameters estimated from counts 

id64 step

data

update action

estimate parameters

says mr. cooper, vice president

label unlabeled data

mr. balmer has already faxed

label balmer    person   

retrain model

mr. balmer has already faxed

id64 folk wisdom

    id64 works better for generative 

models than for discriminative models

    discriminative models can overfit some features

    generative models are forced to assign id203 

mass to all features with some count 

    id64 works better when the na  ve 

bayes assumption is stronger

       mr.    is not predictive of    balmer    if we know the 

entity is a person

two views and co-training

    make id64 folk wisdom explicit

    there are two views of a problem. 

    assume each view is sufficient to do good 

classification

    named entity classification (nec)

    2 views:  context vs. content

    says mr. cooper, a vice president of . . .

general co-training procedure

    on labeled data, maximize accuracy 

    on unlabeled data, constrain models from 

different views to agree with one another

    with multiple views, any supervised learning 

algorithm can be co-trained

co-boosting for named entity classification

collins and singer (1999)

    a brief review of supervised boosting

    boosting runs for t=1   t rounds.

    on round t, we choose a base model            and weight  

    for nlp, the model at round t,

identifies the presence of a particular 
feature and guesses or abstains

    final model:  

boosting objective

current model, 
steps 1. . .t-1

s
s
o

l

0-1 loss

exp loss

co-boosting objective

superscript: view

view 2 
loss

subscript: round 
of boosting

view 1 
loss

unlabeled co-regularizer

scores of individual ensembles ( x- and y-axis ) vs.

co-regularizer term ( z-axis )

score magnitude important 
for disagreement

score magnitude not 
important for agreement

co-boosting updates

    optimize each view separately.  

    set hypothesis     ,     to minimize

    similarly for view 1

    each greedy update is guaranteed to 

decrease one view of the objective

basic co-boosting walk-through

labeled: mr. balmer has already faxed 

unlabeled:

says mr. smith, vice president of  
adam smith wrote    the wealth of nations   

co-boosting step

data

update action

update context view

mr. balmer has already faxed

label unlabeled data

says mr. smith, vice president

label     person   

update content view

says mr. smith, vice president

label unlabeled data

adam smith wrote    the wealth 

label     person   

of nations   

update context view

adam smith wrote . . .

co-boosting nec results

    data:  90,000 unlabeled named entities

    seeds:  location     new york, california, u.s

person context     mr.
organization name     i.b.m., microsoft
organization context     incorporated

    create labeled data using seeds as rules
    whenever i see mr. ____, label it as a person

    results

    baseline (most frequent) 45%  co-boosting: 91%

entity-attribute extraction

bellare et al. (2008)

    entities: companies, countries, people

    attributes: c.e.o., market capitalization, border, prime 

minister, age, employer, address

    extracting entity-attribute pairs from sentences

the
l

population of

china

exceeds

x

m    

y

r

l,m,r = context

x,y = content

data and learning

    input: seed list of entities and attributes

    2 views: context and content

    training: co-training decision lists and self-

trained maxent classifier

    problem: no negative instances

    just treat all unlabeled instances as negative

    re-label most confident positive instances

examples of learned attributes

    countries and attributes

    <malaysia, problems>, <nepal, districts>,  

<colombia, important highway>

    companies and attributes

    <limited too, chief executive>,                                

<intel, speediest chip>, <uniroyal, former chairman>

where can co-boosting go wrong?

    co-boosting enforces agreement on unlabeled data

    if only 1 view can classify correctly, this causes errors 

co-boosting step

data

update action

update context view

mr. balmer has already faxed

label unlabeled data

says mr. cooper, vice president

label     person   

update content view

says mr. cooper, vice president

label unlabeled data

cooper tires spokesman john

label     person   

ssl with latent variables

    maximize likelihood treating unlabeled labels as hidden

y=<person>

y=<hidden>

rw=<president>

rw=<has>

mw=<cooper>

mw=<balmer>

lw=<mr.>

lw=<mr.>

    labeled data gives us basic label-feature structure. 

maximum likelihood (id113) via em fills in the gaps

where can id113 go wrong?

    unclear when likelihood and error are related

    collins & singer (1999) : co-boosting 92%, em: 83% 

    mark johnson.  why doesn   t em find good id48 

pos-taggers?  emnlp 2007.

    how can we fix id113? 

    good solutions are high likelihood, even if they   re not 

maximum likelihood

    coming up: constrain solutions to be consistent with 

linguistic intuition

prototype-driven learning

haghighi and klein 2006

standard ssl

prototype learning (part of speech)

labeled data

unlabeled data

prototypes

training data

nn

president

percent

vbd

said, was

had

jj

new, last,

other

    each instance is partially labeled

    prototypes force representative instances to be consistent

using prototypes in id48s

<y=vbd>

<y hidden>

<y=nn>

. . .

says

mr. cooper,

vice president

of

. . .  

mw=president

lw = vice

suffix = dent

    em algorithm: constrained forward-backward

    haghighi and klein (2006) use markov random fields

incorporating distributional similarity

    represent each word by bigram counts 

president

with most frequent words on left & right

lw=   vice   : 0.1

    k-dimensional representation via svd

    similarity between a word and prototype

lw=   the   : 0.02

. . .

rw=   of   : 0.13

. . .

rw=   said   : 0.05

    we   ll see a similar idea when discussing structural learning

results: id52

prototype examples (3 prototypes per tag)

nn

president

in

of

jj

new

vbd said

nns

shares

det

the

cc

and

to

to

cd

million

nnp mr.

punc .  

vbp

are

results

base

proto

proto+sim

46.4%

67.7%

80.5%

results: classified ads

goal: segment housing advertisements

size

restrict

terms

location

remodeled 2 bdrms/1 bath, spacious upper unit, located in hilltop mall 

area. walking distance to shopping, public transportation, and schools.

paid water and garbage. no dogs allowed. 

prototype examples

location

near, shopping

terms

size

paid, utilities

large, spacious

restrict

dogs, smoking

results

base

proto

46.4%

53.7%

proto+sim 71.5%

computed from bag-of-
words in current sentence

comments on id64

    easy to write down and optimize.  

    hard to predict failure cases

    co-training encodes assumptions as 2-view agreement

    prototype learning enforces linguistically consistent 

constraints on unsupervised solutions

    co-training doesn   t always succeed 

    structural learning section

    prototype learning needs good sim features to perform well

id178 and id64

    haffari & sarkar 2007.  analysis of semi-supervised 

learning with the yarowsky algorithm.
    variants of yarowsky algorithm minimize id178 of 

p(y | x) on unlabeled data.

    other empirical work has looked at minimizing 

id178 directly.

    id178 is not error.

    little recent theoretical work connecting id178 & error

more id64 work

    mcclosky & charniak (2006).  effective self-training 

for parsing. self-trained charniak parser on wsj & nanc.

    aria haghighi   s prototype sequence toolkit.  
http://code.google.com/p/prototype-sequence-toolkit/

    mann & mccallum (2007). expectation id173. 

similar to prototype learning, but develops a id173 

framework for id49.

graph-based semi-supervised learning

    from items to graphs

    basic graph-based algorithms

    mincut

    label propagation and harmonic function

    manifold id173

    advanced graphs

    dissimilarities

    directed graphs

text classification: easy example

38

    two classes: astronomy vs. travel

    document = 0-1 bag-of-word vector

    cosine similarity

easy, by 

x1=   bright asteroid   , y1=astronomy

word overlap

x2=   yellowstone denali   , y2=travel

x3=   asteroid comet   ?

x4=   camp yellowstone   ?

hard example

39

x1=   bright asteroid   , y1=astronomy

x2=   yellowstone denali   , y2=travel

x3=   zodiac   ?

x4=   airport bike   ?

    no word overlap

    zero cosine similarity

    pretend you don   t know english

40

hard example

x1

x3

x4

x2

1

1

asteroid

bright

comet

zodiac

airport

bike

yellowstone

denali

1

1

1

1

1

unlabeled data comes to the rescue

41

x1

x5 x6

x7

x3

x4

x8

x9

x2

1

1

1

1

1

1

1

1

1

asteroid

bright

comet

zodiac

airport

bike

yellowstone

denali

1

1

1

1

1

1

1

1

1

intuition

1. some unlabeled documents are similar 
to the labeled documents     same label

2. some other unlabeled documents are 

similar to the above unlabeled 
documents     same label

3. ad infinitum

we will formalize this with graphs.

the graph

    nodes

    weighted, undirected edges

    large weight     similar

    known labels  

    want to know
    transduction:

    induction:  

d1d2d4d3how to create a graph

    empirically, the following works well:

1. compute distance between i, j

2. for each i, connect to its knn.  k very 

small but still connects the graph

3. optionally put weights on (only) those 

edges

4.  tune    

mincut (st-cut)

mincut example: subjectivity

    task: classify each sentence in a 

document into objective/subjective. 
(pang,lee. acl 2004)

    nb/id166 for isolated classification

    subjective data (y=1): movie review snippets  

   bold, imaginative, and impossible to resist   

    objective data (y=0): imdb

    but there is more   

mincut example: subjectivity

    key observation: sentences next to each 

other tend to have the same label

    two special labeled nodes (source, sink)

    every sentence connects to both:

mincut example: subjectivity

some issues with mincut

    multiple equally min cuts, but different 

    lacks classification confidence

    these are addressed by harmonic 

functions and label propagation

harmonic function

an electric network interpretation

+1 voltwijr  =ij110label propagation

the graph laplacian

closed-form solution

harmonic example 1: wsd

    wsd from context, e.g.,    interest   ,    line    

(niu,ji,tan acl 2005)

    xi: context of the ambiguous word, features: 

pos, words, collocations

    dij: cosine similarity or js-divergence

    wij: knn graph

    labeled data: a few xi   s are tagged with their 

word sense.

harmonic example 1: wsd

    senseval-3, as percent labeled:

(niu,ji,tan acl 2005)

harmonic example 2: sentiment

    rating (0-3) from movie reviews 

(goldberg,zhu. naacl06 workshop)

    xi: movie reviews

    wij: cosine similarity btw    positive sentence 

percentage    (psp) vectors of xi, xj

    psp classifier trained on    snippet    data 

(pang,lee. acl 2005)

harmonic example 2: sentiment 

graph                          accuracy

some issues with harmonic function

    it fixes the given labels yl

    what if some labels are wrong?

    it cannot easily handle new test items

    transductive, not inductive

    add test items to graph, recompute

    manifold id173 addresses these 

issues

manifold id173

manifold example

    text classification 

(sindhwani,niyogi,belkin.icml 2005)

    xi: mac/windows.  tfidf.

    wij: weighted knn graph

l=50;u=1411,test=485advanced topics

    so far edges denote symmetric similarity

    larger weights     similar labels

    what if we have dissimilarity knowledge?

       two items probably have different labels   

    what if the relation is asymmetric?

   

related to      but      not always related to

dissimilarity

    political view classification 

(goldberg, zhu, wright. aistats 2007)

> deshrubinator:    you were the one who thought it should be 

investigated last week.   

dixie: no i didn   t, and i made it clear. you are insane! you 

are the one with no ****ing respect for democracy!

    they disagree     different classes

    indicators: quoting, !?, all caps (internet 

shouting), etc.

dissimilarity

    recall to encode similarity between i,j:

    wrong ways: small w = no preference; negative 

w nasty optimization

    one solution (also see (tong,jin.aaai07))

    overall

depends on 
dissim, sim

directed graphs

    spam vs. good webpage classification 

(zhou,burges,tao. airw 2007)

    hyperlinks as graph edges, a few webpages

manually labeled

directed graphs

    directed hyperlink edges

x

x

spam

x more likely spam

spam

x may be good

    can define an analogous    directed graph 

laplacian    + manifold id173

caution

    advantages of graph-based methods:

    clear intuition, elegant math 

    performs well if the graph fits the task

    disadvantages:

    performs poorly if the graph is bad: sensitive 

to graph structure and edge weights

    usually we do not know which will happen!

structural learning: outline

    the structural learning algorithm

    application to id39

    id20 with structural 

correspondence learning 

    relationship between structural and two-

view learning

structural learning

    ando and zhang (2005).  use unlabeled data to 

constrain structure of hypothesis space

    given a target problem (entity classification)

    design auxiliary problems

    look like target problem

    can be trained using unlabeled data

    regularize target problem hypothesis to be 
close to auxiliary problem hypothesis space

what are auxiliary problems?

2 criteria for auxiliary problems

1) look like target problem

2) can be trained from unlabeled data

named entity classification: predict presence or 
absence of left / middle / right words

left

mr.
president

middle

thursday

john
york

right

corp.
inc.
said

auxiliary problems for sentiment classification

running with scissors: a memoir
running with scissors: a memoir

title: horrible book, horrible.
title: horrible book, horrible.

labels

this book was horrible.  i read half of it, 
this book was horrible.  i read half of it, 

suffering from a headache the entire time, 
suffering from a headache the entire time, 

and eventually i lit it on fire.  one less 
and eventually i lit it on fire.  one less 

copy in the world...don't waste your 
copy in the world...don't waste your 

money.  i wish i had the time spent 
money.  i wish i had the time spent 

positive

negative

auxiliary problems

reading this book back so i could use it for 
reading this book back so i could use it for 

better purposes.  this book wasted my life
better purposes.  this book wasted my life

presence or absence of 
frequent words and bigrams

don   t_waste,  horrible,  suffering

auxiliary problem hypothesis space

consider linear, binary 
auxiliary predictors:

weight vector for auxiliary problem i 

given a new hypothesis 
weight vector     , how far is 
it from                ?

two steps of structural learning

step 1:  use unlabeled data and auxiliary problems to 
learn a representation     :  an approximation to 

1
0
.
.
.
0
1

0.3

-1.0...

0.7
-2.1

features:  
<left word =    mr.   >

low-dimensional 
representation

weights learned 
from labeled data

step 2:  use labeled data to learn weights for the new 
representation

unlabeled step: train auxiliary predictors

for each unlabeled instance, create a binary presence / 
absence label

(1) the book is so repetitive that 
i found myself yelling    . i will 
definitely not buy another.

(2) an excellent book.  once 
again, another wonderful novel 
from grisham

binary problem: does    not buy    appear here?

    mask and predict pivot features using other features

    train n linear predictors, one for each binary problem

    auxiliary weight vectors give us clues about feature conditional 
covariance structure

unlabeled step: id84

   

gives n new features

    value of ith feature is the 
propensity to see    not buy    in 
the same document

    we want a low-dimensional representation

    many pivot predictors give similar information 

       horrible   ,    terrible   ,    awful   

    compute svd & use top left singular vectors 

step 2:  labeled training

step 2:  use      to regularize labeled objective

original, high-dimensional 
weight vector

low-dimensional weight 
vector for learned features

only high-dimensional features have quadratic 
id173 term

step 2:  labeled training

    comparison to prototype similarity

    uses predictor (weight vector) space, 
rather than counts

    similarity is learned rather than fixed

results: id39

    data: conll 2003 shared task

    labeled: 204 thousand tokens of reuters news data

    annotations:  person, location, organization, miscellaneous

    unlabeled: 30 million words of reuters news data

    a glance of some of the rows of 

row # features

4

9

11

15

ltd, inc, plc, international, association, group

pct, n/a, nil, dec, bln, avg, year-on-year

san, new, france, european, japan

peter, sir, charles, jose, paul, lee

numerical results (f-measure)

data size 10k tokens

204k tokens

model

baseline

co-training

structural

72.8

73.1

81.3

85.4

85.4

89.3

    large difference between co-training here and 
co-boosting (collins & singer 1999)

    this task is entity recognition, not classification

    we must improve over a supervised baseline

id20 with structural learning

blitzer et al. (2006): structural correspondence learning (scl)

blitzer et al. (2007):  for sentiment: books & kitchen appliances

running with scissors: a memoir
running with scissors: a memoir

avante deep fryer, chrome & black
avante deep fryer, chrome & black

title: horrible book, horrible.
title: horrible book, horrible.

title: lid does not work well...
title: lid does not work well...

this book was horrible.  i read half of it, 
this book was horrible.  i read half of it, 

i love the way the tefal deep fryer 
i love the way the tefal deep fryer 

suffering from a headache the entire time, 
suffering from a headache the entire 

cooks, however, i am returning my 
cooks, however, i am returning my 

and eventually i lit it on fire.  one less 
time, and eventually i lit it on fire.  one 

error increase: 13%     26%

second one due to a defective lid 
second one due to a defective lid 

copy in the world...don't waste your 
less copy in the world...don't waste your 

closure.  the lid may close initially, but 
closure.  the lid may close initially, but 

money.  i wish i had the time spent 
money.  i wish i had the time spent 

after a few uses it no longer stays 
after a few uses it no longer stays 

reading this book back so i could use it for 
reading this book back so i could use it for 

closed. i will not be purchasing this one 
closed. i will not be purchasing this one 

better purposes.  this book wasted my life
better purposes.  this book wasted my life

again.
again.

pivot features

pivot features are features which are shared across domains

unlabeled kitchen contexts

unlabeled books contexts

    do not buy the shark portable 
steamer    . trigger mechanism is 
defective. 

    the book is so repetitive that i 
found myself yelling    . i will 
definitely not buy another.

    the very nice lady assured me that i 
must have a defective set    . what 
a disappointment!

    a disappointment    . ender 
was talked about for <#> pages
altogether.

    maybe mine was defective    . the 
directions were unclear

    it   s unclear    . it   s repetitive and 
boring

use presence of pivot features as auxiliary problems

choosing pivot features: mutual information

pivot selection (scl):  select top features      by shared counts

pivot selection (scl-mi):  select top features in two passes

(1) filter feature      if min count in both domains < k

(2) select top filtered features by 

books-kitchen example

in scl, not scl-mi

in scl-mi, not scl

book   one   <num>   so   all   

very   about   they   like   good   

a_must a_wonderful

loved_it
weak   don   t_waste awful   

when

highly_recommended and_easy

sentiment classification data

    product reviews from amazon.com

    books, dvds, kitchen appliances, electronics

    2000 labeled reviews from each domain

    3000     6000 unlabeled reviews

    binary classification problem 

    positive if 4 stars or more, negative if 2 or less

    features: unigrams & bigrams

    pivots: scl & scl-mi

    at train time: minimize huberized hinge loss (zhang, 2004)

visualizing 

(books & kitchen)

negative

vs.                 positive

books

engaging must_read

plot

<#>_pages

predictable

fascinating

grisham

poorly_designed

awkward_to

espresso

years_now

the_plastic

leaking

are_perfect

a_breeze

kitchen

empirical results: books & dvds

baseline loss due to adaptation: 7.6%

scl-mi loss due to adaptation: 0.7%

on average, scl-mi 
reduces error due to 
adaptation by 36%

657075808590d->be->bk->bb->de->dk->dbaselinesclscl-mibooks72.876.879.770.775.475.470.966.168.680.482.477.274.075.870.674.376.272.775.476.9dvdstructural learning: why does it work?

    good auxiliary problems = good representation 

    structural learning vs. co-training

    structural learning separates unsupervised and 

supervised learning

    leads to a more stable solution

    structural learning vs. graph id173

    use structural learning when auxiliary problems 

are obvious, but graph is not

understanding structural learning: goals

    develop a relationship between structural 

learning and multi-view learning 

    discuss assumptions under which structural 

learning can perform well

    give a bound on the error of structural learning 

under these assumptions

structural and multi-view learning

context 
pivots

orthography 

features

context 
features

orthography 

pivots

lw=mr.

rw=said

rw=corp.

balmer

smith

yahoo

general electric

rw=expounded

lw=senator

rw=llc

lw=the

brown

microsoft

smith

canonical correlation analysis

canonical correlation analysis     cca (hotelling, 1936)

mr.
said

correlated features 

smith

from different views 

are mapped to similar 

microsoft

areas of space

structural learning and cca

some changes to structural learning

(1) minimize squared loss for auxiliary predictors

(2) block svd by view: train auxiliary predictors for view 

1 using features from view 2 and vice versa

cca and semi-supervised learning

kakade and foster (2007).  multi-view regression 
via canonical correlation analysis.

assume:

contrast with co-training:  k&f don   t
assume independence

semi-supervised learning procedure

training error using 
transformed inputs

regularize based on 
amount of correlation

a bound on squared error under cca

main theorem of kakade & foster (2007)

expected error of learned, 
transformed predictor

expected error of 
best model

assumption:  how good 
is single view compared 
to joint model?

number of training 
examples

amount of 
correlation

when can structural learning break?

    hard-to-define auxiliary problems

    id33:  how to define auxiliary 

problems for an edge?

    mt alignment:  how to define auxiliary problems for a 

pair of words?

    combining real-valued & binary features

high-
dimensional, 
sparse

    scaling, optimization

low-
dimensional, 
dense

other work on structural learning

    scott miller et al. (2004). name tagging with word 

clusters and discriminative training.

    hierarchical id91, not structural learning.

    representation easily combines with binary features

    rie ando, mark dredze, and tong zhang (2005).  trec 

2005 genomics track experiments at ibm watson.

    applying structural learning to information retrieval

    ariadna quattoni,  michael collins, and trevor darrel 
(cvpr 2007).  learning visual representations using 
images with captions.

ssl summary

    id64

    easy to write down.  hard to analyze.  

    graph-based id173  

    works best when graph encodes information not easily 

represented in normal feature vectors

    structural learning

    with good auxiliary problems, can improve even with lots 

of training data

    difficult to combine with standard feature vectors

two take-away messages

1) semi-supervised learning yields good 

results for small amounts of labeled data

2)    i have lots of labeled data    is not an excuse 

not to use semi-supervised techniques

http://ssl-acl08.wikidot.com

