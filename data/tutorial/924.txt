university of sheffield, nlp

opinion mining: exploiting the 

sentiment of the crowd

diana maynard

adam funk

kalina bontcheva

university of sheffield, uk

 

 

   the university of sheffield, 1995-2012
this work is licenced under the creative commons attribution-noncommercial-sharealike licence 

university of sheffield, nlp

burning questions you may have

    which is more accurate: "phone a friend" or "ask the 

audience"?

    could the "fear index" become reality?
    what does the royal wedding have to do with 

pilates classes?

    do people feel more miserable when stock prices 

    can twitter predict earthquakes?
    can id31 find us the perfect husband 

fall?

or wife?

 

university of sheffield, nlp

aims of this tutorial

    introduce the concepts of opinion mining and 

id31 from unstructured text
    why are they useful?
    what tools and techniques are available?

    introduce some general rule-based and machine 

learning techniques

    take a look at what kind of problems are posed by 

opinion mining in general

    take a look at some problems specific to processing 

social media

 

university of sheffield, nlp

tutorial structure

09.00     10.00 introduction to opinion mining
10.00     10.30 machine learning applications
10.30     11.00 coffee break
11.00     12.00 rule-based applications
12.00     13.00 opinion mining and social media

 

university of sheffield, nlp

introduction to opinion mining

 

university of sheffield, nlp

what is opinion mining?

    om is a relatively recent discipline that studies the extraction of 

opinions using ir, ai and/or nlp techniques. 

    more informally, it's about extracting the opinions or sentiments 

given in a piece of text

    also referred to as id31 (though technically this 

is a more specific task)

    web 2.0 nowadays provides a great medium for people to 

share things.

    this provides a great source of unstructured information 

(especially opinions) that may be useful to others (e.g. 
companies and their rivals, other consumers...) 

 

university of sheffield, nlp
it's about finding out what people think...

 

university of sheffield, nlp

opinion mining is big business

    someone who wants to buy a camera

    looks for comments and reviews
    someone who just bought a camera

    comments on it
    writes about their experience

    camera manufacturer

    gets feedback from customer
    improve their products
    adjust marketing strategies

 

university of sheffield, nlp
venus williams causes controversy...

 

university of sheffield, nlp

opinion mining exposes these insights

 

university of sheffield, nlp

online social media sentiment apps

    try a search of your own on one of these:

    twitter sentiment http://twittersentiment.appspot.com/
    twends: http://twendz.waggeneredstrom.com/

http://twendz.waggeneredstrom.com/

    twittratr: http://twitrratr.com/
    socialmention: http://socialmention.com/

    easy to search for opinions about famous people, brands and so on
    hard to search for more abstract concepts, perform a non-keyword 

based string search

    e.g. to find opinions about venus williams, you can only search on 

   venus williams    to get hits

 

university of sheffield, nlp

why are these sites unsuccessful?

    they don't work well at more than a very basic level
    they mainly use dictionary lookup for positive and negative 

words

    they classify the tweets as positive or negative, but not with 

respect to the keyword you're searching for

    first, the keyword search just retrieves any tweet mentioning 

it, but not necessarily about it as a topic

    second, there is no correlation between the keyword and the 

sentiment: the sentiment refers to the tweet as a whole
    sometimes this is fine, but it can also go horribly wrong

 

university of sheffield, nlp

whitney houston wasn't very popular...

 

university of sheffield, nlp
or was she?

 

university of sheffield, nlp

opinion mining for stock market prediction

   

it might be only fiction, but using 
opinion mining for stock market 
prediction has been already a reality 
for some years

    research shows that opinion mining 

outperforms event-based 
classification for trend prediction 
[bollen2011]

    at least one investment company 

currently offers a product based on 
opinion mining

 

university of sheffield, nlp

using twitter for stock market prediction

   hey jon, derek in scunthorpe's having a bacon and egg, 
er, butty. is that good for wheat futures?   

 

university of sheffield, nlp

derwent capital markets

    derwent capital markets have launched a   25m fund that makes 

its investments by evaluating whether people are generally 
happy, sad, anxious or tired, because they believe it will predict 
whether the market will move up or down. 

    bollen told the sunday times: "we recorded the sentiment of the 

online community, but we couldn't prove if it was correct. so we 
looked at the dow jones to see if there was a correlation. we 
believed that if the markets fell, then the mood of people on 
twitter would fall.   
"but we realised it was the other way round     that a drop in the 
mood or sentiment of the online community would precede a fall 
in the market.    

   

 

university of sheffield, nlp

 

university of sheffield, nlp

but don't believe all you read...

   

   

it's been suggested recently that there are actually many flaws 
in bollen's work, and that it's impossible to predict the stock 
market in this way
if it were really possible, surely bollen would be a millionaire by 
now and everyone would be using this technology?

    there's quite a lot of sloppiness in the reporting of methodology 

and results, so it's not clear what can really be trusted

    the advertised results are biased by selection (they picked the 

winners after the race and tried to show correlation)

    the accuracy claim is too general to be useful (you can't predict 

individual stock prices, only the general trend)

    http://sellthenews.tumblr.com/post/21067996377/noitdoesnot

 

university of sheffield, nlp

who wants to be a millionaire? 

ask the audience?

or phone a friend?

which do you think is better?

 

university of sheffield, nlp

what's the capital of spain?

a: barcelona

b: madrid

c: valencia

d: seville

 

university of sheffield, nlp

what's the height of mt kilimanjaro?

a: 19,341 ft

b: 23,341 ft

c: 15,341 ft

d: 21,341 ft

 

university of sheffield, nlp

 go for the majority or trust an expert?

   

   

it depends what kind of question you're asking
in who wants to be a millionaire, people tend to ask the 
audience fairly early on, because once the questions get hard, 
they can't rely on the audience getting it right

what's the height of mt 
kilimanjaro?

what's the capital of spain?

a: 19,341 ft
b: 23,341 ft
c: 15,341 ft
d: 21,341 ft

a: barcelona
b: madrid
c: valencia
d: seville

 

university of sheffield, nlp

why bother with opinion mining?

    it depends what kind of information you want
    don't use opinion mining tools to help you win money 

on quiz shows

    recent research has shown that one knowledgeable 

analyst is better than gathering general public 
sentiment from lots of analysts and taking the 
majority opinion

    but only for some kinds of tasks

 

university of sheffield, nlp

whose opinion should you trust?

    opinion mining gets difficult when the users are 
exposed to opinions from more than one analyst
    intuitively, one would probably trust the opinion 

supported by the majority.

    but some research shows that the user is better off 

trusting the most credible analyst.

    then the question becomes: who is the most 

credible analyst?

    notions of trust, authority and influence are all 

related to opinion mining

 

university of sheffield, nlp

all opinions are not equal

    opinion mining needs to take into account how much influence any 

single opinion is worth

    this could depend on a variety of factors, such as how much trust 
we have in a person's opinion, and even what sort of person they 
are

    need to account for:

    experts vs non-experts
    spammers
    frequent vs infrequent posters
       experts    in one area may not be expert in another
    how frequently do other people agree?

 

university of sheffield, nlp

trust recommenders
    two types of trust:

     relationship (local) trust
     reputation (global) trust. 

    relationship trust: if you and i both rate the same things, and our 

opinions on them match closely, we have high relationship trust. 
this can be extended to a social networking group --> web of trust.
    reputation trust: if you've recommended the same thing as other 

people, and usually your recommendation is close to what the 
majority of people think, then you're considered to be more of an 
expert and have high reputation trust.

    we can extend relationship trust to form clusters of interests and 

likes/dislikes

    we can narrow reputation trust to opinions about similar topics

 

university of sheffield, nlp

opinion mining subtasks

    opinion extraction: extract the piece of text which represents 

the opinion
    i just bought a new camera yesterday. it was a bit 

expensive, but the battery life is very good.

    sentiment classification/orientation: extract the polarity of 

the opinion (e.g. positive, negative, neutral, or classify on a 
numerical scale)
    negative: expensive
    positive: good battery life

    opinion summarisation: summarise the overall opinion about 

something
    price:negative, battery life: positive --> overall 7/10

 

university of sheffield, nlp

feature-opinion association

    feature-opinion association: given a text with target features and 

opinions extracted, decide which opinions comment on which 
features.
       the battery life is good but not so keen on the picture quality   

    target identification: which thing is the opinion referring to?
    source identification: who is holding the opinion?
    there may be attachment and co-reference issues

       the camera comes with a free case but i don't like the colour 

much.   

    does this refer to the colour of the case or the camera?

 

university of sheffield, nlp

getting the target of the opinion right is crucial

 

university of sheffield, nlp

opinion spamming

 

university of sheffield, nlp

spam opinion detection (fake reviews)

    sometimes people get paid to post    spam    opinions supporting a 

product, organisation or even government

    an article in the new york times discussed one such company who 

gave big discounts to post a 5-star review about the product on 
amazon

    http://www.nytimes.com/2012/01/27/technology/for-2-a-star-a-retailer-gets-5-star-reviews.html?_r=3&ref=business
    could be either positive or negative opinions
    generally, negative opinions are more damaging than positive ones

 

university of sheffield, nlp

how to detect fake opinions?

    review content: lexical features, content and style 

inconsistencies from the same user, or simlarities between 
different users

    complex relationships between reviews, reviewers and 

products

    publicly available information about posters (time posted, 

posting frequency etc)

    see anything wrong with these reviews? 

http://www.amazon.com/gp/pdp/profile/a3urrtizee8r7w

 

university of sheffield, nlp
it's not just about cameras and dresses ...

    film, theatre, books, fashion etc 
     impacts on the whole industry
    predictions about changing society, trends etc.

    monitoring political views
    feedback/opinions about multimedia productions, e.g. 

documentaries, broadcasts etc.

    feedback about events, e.g. conferences
    scientific and technological monitoring, competitor surveillance 

etc.

    monitoring public opinion
    creating community memories
 

university of sheffield, nlp

and it's not always as easy as it looks...

   rubbish hotel in madrid   

 

university of sheffield, nlp

opinion mining and social media

    social media provides a wealth of information about a user's behaviour 

and interests:
    explicit: john likes tennis, swimming and classical music

implicit: people who like skydiving tend to be big risk-takers

   

    associative: people who buy nike products also tend to buy apple 

products

    while information about individuals isn't useful on its own, finding defined 

clusters of interests and opinions is
if many people talk on social media sites about fears in airline security, life 
insurance companies might consider opportunities to sell a new service
    this kind of predictive analysis is all about understanding your potential 
audience at a much deeper level - this can lead to improved advertising 
techniques such as personalised ads to different groups

 

university of sheffield, nlp

analysing and preserving opinions

    useful to collect, store and later retrieve public opinions about 

events and their changes or developments over time

    one of the difficulties lies in distinguishing what is important
    opinion mining tools can help here 
    not only can online social networks provide a snapshot of such 
situations, but they can actually trigger a chain of reactions and 
events

    ultimately these events might lead to societal, political or 

administrative changes

 

university of sheffield, nlp

pippa middleton's assets

    one of the biggest royal wedding 

stories on social media sites

    her bottom has its own twitter account, 

facebook page and website.

    pilates classes have become incredibly 

popular since the royal wedding, 
solely as a result of all the social media

 

university of sheffield, nlp

accuracy of twitter sentiment apps

    mine the social media sentiment apps and you'll find a huge 

difference of opinions about pippa middleton:
    tweetfeel: 25% positive, 75% negative
    twendz: no results
    tiptop: 42% positive, 11% negative
    twitter sentiment: 62% positive, 38% negative

    try searching for    gaddafi    and you may be surprised at some of 

the results.

 

university of sheffield, nlp

twittrater's view of the olympics

    a keyword search for olympics shows exactly how existing 

systems fail to cut the mustard

    lookup of sentiment words is not enough if

    they're part of longer words
    they're used in different contexts
    the tweet itself isn't relevant
    they're used in a negative or sarcastic sentence
    they're ambiguous

 

university of sheffield, nlp
tracking opinions over time

    opinions can be extracted with a time stamp and/or a geo-

location

    we can then analyse changes to opinions about the same 

entity/event over time, and other statistics

    we can also measure the impact of an entity or event on the 
overall sentiment about an entity or another event, over the 
course of time (e.g. in politics)

    also possible to incorporate statistical (non-linguistic) 

techniques to investigate dynamics of opinions, e.g. find 
statistical correlations between interest in certain topics or 
entities/events and number/impact/influence of tweets etc.

 

university of sheffield, nlp
viewing opinion changes over time

 

university of sheffield, nlp

mapping dynamics from social media: uk riots demo

 

university of sheffield, nlp

predicting the future

 

university of sheffield, nlp

predicting other people's decisions

   

it would be useful to predict what products people will buy, 
what films they want to see, or what political party they'll 
support

 

university of sheffield, nlp

predicting presidential candidates

    michael wu from lithium did a study of sentiment data on 
various social web apps about presidential candidates in 
march 2012

    http://lithosphere.lithium.com/t5/building-community-the-platform/big-data-big-prediction-looking-through-the-predictive-window/ba-p/41068
    his analysis involved taking the positive sentiments minus the 
negative sentiments, over a 2 week period, and also including 
the neutral sentiments

    neutral sentiments were weighted at 1/10 and added to the net 

sentiment

    he saw a close correlation between his analysis and the 

gallup polls, but he warns us to be cautious...

 

university of sheffield, nlp

predictive analysis windows
    predictive analytics is about trying to look into the future through the 

predictive window of your data.
if you try to look outside this window, your future will look very blurry.
it's like weather forecasting     the smaller the window, the more accurate 
you'll be

   

   

    the important question is not whether social media data can predict 

election outcome, but    how far ahead can it be predicted?   

    for something that changes very quickly like the financial market, the 

predictive window will be very short.

    for things that do not change as fast, the predictive window will be 

longer.

    for social media sentiment data, the window for election forecasting is 

about 1.5 to 2 weeks, (1 to be conservative).

 

university of sheffield, nlp

aggregate sentiment finding

    aggregate sentiment finding (e.g. o'connor et al 2010) uses shallow 

techniques based on sentiment word counting.

   

 idea is that if you're only trying to find aggregates then such techniques are 
sufficient, even though they're far from perfect.

    although the error rate can be high, with a fairly large number of 

measurements, these errors will cancel out relative to the quantity we are 
interested in estimating (aggregate public opinion). 

    the claim is that using standard text analytics techniques on such data can 

actually be harmful, because they're designed to optimise per-document 
classification accuracy rather than assessing aggregate population 
proportions.

    their method shows some correlation with public sentiment polls but they 

conclude that better opinion mining would be beneficial.

 

university of sheffield, nlp

social media and politics

    twitter provides real-time feedback on debates that's much faster than 

traditional polling. campaigns are paying close attention. that's because 
such chatter can gauge how a candidate's message is being received or 
even warn of a popularity dive.

    campaigns that closely monitor the twittersphere have a better feel of voter 
sentiment. that allows candidates to fine-tune their message for a particular 
state:    playing to your audience". 

    however, applying complex algorithms to twitter data, blogs, news sites and 

other media isn't yet perfect for predicting politics, e.g. you can't detect 
sarcasm reliably.

    nevertheless, twitter has played a role in intelligence gathering on uprisings 

around the world, showing accuracy at gauging political sentiment.

    http://www.usatoday.com/tech/news/story/2012-03-05/social-super-tuesday-

prediction/53374536/1

 

university of sheffield, nlp

methods for opinion mining

    machine learning methods
    rule-based methods

 

university of sheffield, nlp

gate (general architecture for text engineering)

    our examples mostly use gate     tool for le in development in 

sheffield since 2000.

    gate includes:

    components for language processing, e.g. parsers, machine 
learning tools, stemmers, ir tools, ie components for various 
languages...

    tools for visualising and manipulating text, annotations, 

ontologies, parse trees, etc.

    various information extraction tools
    evaluation and benchmarking tools

    more info and freely available at http://gate.ac.uk

 

university of sheffield, nlp

machine learning

 

university of sheffield, nlp

what is machine learning?

    automating the process of inferring new data from existing data
in gate, that means creating annotations or adding features to 
annotations by learning how they relate to other annotations

   

 

university of sheffield, nlp

learning a pattern

    for example, we have token annotations with string 

features and product annotations

the
token

new
token

acme
token

33

token

stinks
token

!

token

model
token
product
sentence

    ml could learn that a product close to the token    stinks    

expresses a negative sentiment, then add a 
polarity=   negative    feature to the sentence.

 

university of sheffield, nlp

how is that better than a rule-based approach?

   
   

   

   

not necessarily better, just different
people are better at writing rules for some things, ml 
algorithms are better at finding some things
with ml you don't have to create all the rules, but you have 
to manually annotate a training corpus   or get someone 
else to do it!
rule-based approaches (such as jape) and ml work well 
together; in gate, jape is often used extensively to 
prepare data for ml.

 

university of sheffield, nlp

terminology: instances

   
   
   

instances are cases that may be learned
every instance is a decision for the ml algorithm to make
to which class does this instance belong?

       california        location
       this product stinks        polarity=negative

 

university of sheffield, nlp

terminology: attributes

   

   

   

attributes are pieces of information that we already know 
about instances (sometimes called    features    in machine 
learning literature). 
these can be gate annotations, or annotation features 
that will be known before the ml algorithm is applied to 
new data
examples

    token.string ==    stinks   
    token.kind ==    punctuation   
    sentence contains product

 

university of sheffield, nlp

terminology: classes

   
   

the class is what we want to learn
suppose we want to find opinions: for every sentence 
instance, the question is    what kind of opinion does this 
express?    and the classes are positive, negative, neutral, 
and none.

 

university of sheffield, nlp

ml tasks

    gate supports 3 types of ml tasks:

    chunk recognition (id39, np chunking)
    text classification (sentiment classification, id52)
    relation annotation

    most opinion mining tasks fall under text classification

 

university of sheffield, nlp

training

   

   

   

training involves presenting data to the ml algorithm from 
which it creates a model
the training data consist of instances that have been 
annotated with correct classes as well as attributes
models are representations of decision-making processes 
that allow the ml algorithm to classify each instance based 
on its attributes

 

university of sheffield, nlp

application

   

   

   

when the ml algorithm is applied, it creates new class 
annotations on data using the model
the corpus it is applied to must contain the required 
attribute annotations
the machine learner will work best if the application data is 
similar to the training data

 

university of sheffield, nlp

evaluation

   

   

   

   

we want to know how good our machine learner is before we 
use it for a real task
therefore we apply it to some data for which we already have 
class annotations

   

the    right answers   , sometimes called    gold standard   

if the machine learner creates the same annotations as the gold 
standard, then we know it is performing well
gate's ml pr has a built-in evaluation mode that splits the 
corpus into training and test sets and cross-validates them

 

university of sheffield, nlp

id88 and paum

   

   

   

id88 is one of the oldest ml methods (invented in 
the 50s!)
like id166 (which will be covered later), it determines a 
hyperplane separator between the data points
theoretically id166 works a little better because it calculates 
the optimal separator, but in practice, however, there is 
usually little difference, and id88 is a lot faster!

 

university of sheffield, nlp
id88 algorithm with uneven margins 
(paum)

   
   

both id88 and id166 implement    uneven margins   
this means that it doesn't position the separator centred 
between the points, but more towards one side

 

university of sheffield, nlp

even margins

 

university of sheffield, nlp

why uneven margins?

   
   

   

   

   

in nlp the datasets are often very imbalanced.
if you are tagging instances of    person   , there are a few 
positive cases mixed with many words that are not 
persons.
in opinion mining, you may have a few sentences with 
opinions but mostly sentences without them.
so move the margin away from the smaller group of 
training examples.
y. li, k. bontcheva, and h. cunningham. using uneven 
margins id166 and id88 for information extraction. 
conll-2005.

 

university of sheffield, nlp

uneven margins

 

university of sheffield, nlp

support vector machines

    like id88, try to 
find a hyperplane that 
separates data

    but the goal here is to 

maximize the 
separation between 
the two classes

    wider margin = greater 

generalisation

 

university of sheffield, nlp

support vector machines

    the points near the decision boundary are the    support vectors    

(removing them would change boundary)

    the farther points are not important for decision-making
    what if you can't split the data neatly?

   
   

soft boundary methods exist for imperfect solutions
however linear separator may be completely 
unsuitable

 

university of sheffield, nlp
support vector machines

    what if there is no 

separating hyperplane?

they do not work!

 

university of sheffield, nlp

kernel trick

    map data into 
different 
dimensionality

    http://www.youtube.com/watch?v=3licbrzprza
    as shown in the 

video, due to 
polynomial kernel 
elliptical separators 
can be created 
nevertheless. 

    now the points are 

separable!

 

university of sheffield, nlp

kernel trick in gate and nlp

   

   

   

binomial kernel allows curved and elliptical separators to 
be created
these are commonly used in language processing and are 
found to be successful
in gate, linear and polynomial kernels are implemented in 
batch learning pr's id166 engine

 

university of sheffield, nlp

machine learning for id31

    ml is an effective way to classify opinionated texts
    we want to train a classifier to categorize free text according to the 

training data.

    good examples are consumers' reviews of films, products, and 

suppliers.

    sites like www.pricegrabber.co.uk show reviews and an overall 
rating for companies: these make good training and testing data
    we train the ml system on a set of reviews so it can learn good 
and bad reviews, and then test it on a new set of reviews to see 
how well it distinguishes between them

 

university of sheffield, nlp

examples of consumer reviews

 

university of sheffield, nlp
preparing the corpus
    corpus of 40 documents containing 552 company reviews.  
    each review has a 1- to 5-star rating.
    we pre-processed these in gate to label each review with a 

   

comment annotation with a rating feature (free manual annotation!)
in ml terms:
    instance = comment annotation
    class = rating feature on the comment annotation
    attributes = nlp features of the underlying text

    we will keep the spans of the comment annotations and use ml to 

classify them with the rating feature

    we develop an application that runs a set of nlp components to 

provide ml instance attributes, and train the classifier 

 

university of sheffield, nlp

annotated review

 

university of sheffield, nlp

musing ml configuration

    for this application, we used id166 (we would probably use paum 

now)

    attributes: bag of lemmatised words (unigrams of lemmata) inside 

each comment annotation

 

university of sheffield, nlp

applying the training model

    to apply the classifier to our test corpus, we need to have comment 

annotations without rating features on the default as 

    these will give us the instances to classify  
    a simple jape transducer can do this
    when the pipeline is run, the classifier will get instances (comment 

annotations) and attributes from the default as and put instances 
with classes (rating features) in the output as

    key set = user ratings
    default set =  instances with no classes
    output set = instances with ml classes

 

university of sheffield, nlp

annotation results

 

university of sheffield, nlp
evaluation: corpus qa tool in gate

 

university of sheffield, nlp

results

 

university of sheffield, nlp

cohen's kappa and confusion matrices

    we can also use the cohen's kappa measure to show a 

confusion matrix

    the confusion matrix shows how many from each manually 
annotated class were automatically classified in each of the 
classes

1
4
4
2
1
0

1
2
3
4
5

3
2
2
2
2
1

4
0
1
2
2
2

5
0
1
4
4
5

2
5
4
4
1
0

 

university of sheffield, nlp

cross-validation

    cross-validation is a standard way to    stretch    the validity of a 

manually annotated corpus, because it enables you to test on a 
larger number of documents  

    divide the corpus into 5 sub-corpora; train on abcd and test on 

e; train on abce and test on d; etc.; average the results

    the 5-fold averaged result is more meaningful than the result 
obtained by training on 80% of the corpus and testing on the 
other 20% once.
in gate, you can't use the corpus qa tool on the result, but you 
can get a detailed statistical report at the end, including p, r, & 
f1 for each class

   

 

university of sheffield, nlp

rule-based techniques

 

university of sheffield, nlp

rule-based techniques

    these rely primarily on sentiment dictionaries, plus some rules 

 to do things like attach sentiments to targets, or modify the 
sentiment scores
    examples include:

     analysis of political tweets (maynard and funk, 2011)
    analysis of opinions expressed about political events and 

rock festivals in social media (maynard, bontcheva and 
rout, 2012)

    so-cal (taboada et al, 2011) for detecting positive and 

negative sentiment of epinions reviews on the web.

 

university of sheffield, nlp

case study: rule-based opinion mining 

from political tweets in gate

 

university of sheffield, nlp

processing political tweets

    application to associate people with their political leanings, 

based on pre-election tweets
    e.g.    had the pleasure of formally proposing stuart king as 

labour candidate for putney   

    first stage is to find triple <person, opinion, political party>

    e.g. john smith is pro_labour

    usually, we will only get a single sentiment per tweet 
    later, we can collect all mentions of    john smith    that refer to 

the same person, and collate the information

    john may be equally in favour of several different parties, not 

just labour, but hates the conservatives above all else

 

university of sheffield, nlp

creating a corpus

    first step is to create a corpus of tweets
    used the twitter streaming api to suck up all the tweets over the 
pre-election period according to various criteria (e.g. use of certain 
hash tags, mention of various political parties etc.)

    collected tweets in json format and then converted these to xml 

using json-:ib library

    this gives us lots of additional twitter metadata, such as the date 

and time of the tweet, the number of followers of the person 
tweeting, the location and other information about the person 
tweeting, and so on

    this information is useful for disambiguation and for collating the 

information later

 

university of sheffield, nlp

corpus size

    raw corpus contained around 5 million tweets
    many were duplicates due to the way in which the tweets were 

collected

    added a de-duplication step during the conversion of json to xml 
    this reduced corpus size by 20% to around 4 million
    this still retains the retweets, however

 

university of sheffield, nlp

tweets with metadata

original markups set

 

university of sheffield, nlp

metadata

date

tweet

number of friends

profile info

name

location

 

university of sheffield, nlp

linguistic pre-processing

    use standard set of pre-processing resources in gate to 

identify tokens, sentences, pos tags etc., and also to perform 
ne recognition.

    slightly adapted the standard annie application (gate's 

default ie application)

 

university of sheffield, nlp

 

university of sheffield, nlp

gazetteers

    we create a flexible gazetteer to match certain useful keywords, in 

various morphological forms:
    political parties, e.g.    conservative   ,    libdem   
    concepts about winning election, e.g.    win   ,    landslide   
    words for politicians, e.g.    candidate   ,    mp   
    words for voting and supporting a party/ person, e.g.    vote   
    words indicating negation, e.g.    not   ,    never   

    we create another gazetteer containing affect/emotion words from 

id138. 
    these have a feature denoting part of speech (category) 
    keeping category information may be important, so we don't want 

a flexible gazetteer here 

 

university of sheffield, nlp

a negative sentiment list

examples of phrases following the word    go   :
    down the pan
    down the drain

to the dogs

   

    downhill
    pear-shaped

 

university of sheffield, nlp

a positive sentiment list

score=0.5
score=0.5

    awesome category=adjective
    beaming category=adjective
    becharm category=verbscore=0.5
    belonging category=noun
score=0.5
    benefic
    benevolently category=adverb score=0.5
    caring category=noun score=0.5
    charitable category=adjective
    charm category=verb  score=0.5

category=adjective

score=0.5

score=0.5

 

university of sheffield, nlp

grammar rules: creating preliminary annotations

   

identify questions or doubtful statements as opposed to "factual" 
statements in tweets, e.g. look for question marks

wont unite's victory be beneficial to labour?

    create temporary sentiment annotations if a sentiment lookup is 
found and if the category matches the pos tag on the token (this 
ensures disambiguation of the different possible categories)

   just watched video about awful days of tory rule    vs    ah good, the 
entertainment is here.   
   people like her should be shot.    vs    people like her.   

 

university of sheffield, nlp

question grammar

phase:preprocess
input: token
options: control = appelt

rule: question
(
 {token.string == "?"}
):tag
-->
:tag.question = {rule = "question"}
 

university of sheffield, nlp

phase: affect
input: affectlookup token
options: control = appelt

check category of both lookup and token
are adjectives or past participles

rule: affectadjective
(
 {affectlookup.category == adjective,token.category == vbn}|
 {affectlookup.category == adjective, token.category == jj}
):tag
-->
:tag.affect = {kind = :tag.affectlookup.kind, 
                     category = :tag.affectlookup.category, 
                     rule = "affectadjective"}

 

copy category and kind 
values from lookup to new 
affect  annotation

university of sheffield, nlp

grammar rules: finding triples

    we first create temporary annotations for person, organization, 
vote, party, negatives etc. based on gazetteer lookup, nes etc.

    we then have a set of rules to combine these into pairs or triples:

    <person, vote, party>    tory phip admits he voted libdem   .
    <party, affect>    when they get a tory government they'll be 

sorry.    

    we create an annotation    sentiment    which has the following 

features:
    kind =    pro_labour   ,    anti_libdem   , etc.
    opinion_holder =    john smith   ,    author    etc.

 

university of sheffield, nlp

identifying the opinion holder

   

   

if the opinion holder in the pattern matched is a person or 
organization, we just get the string as the value of opinion_holder
if the opinion holder in the pattern matched is a pronoun, we first 
find the value of the string of the antecedent and use this as the 
value of opinion_holder

    currently we only match opinion holders within the same sentence.

   

if no explicit opinion holder then we use "author" as the value of 
opinion_holder.

    later we can grab  the details of the twitterer instead of just using 

"author".

 

university of sheffield, nlp

grammar rules: finding antecedents

    find the antecedents of pronouns within a sentence so that we can 
refer a sentiment back to the original opinion holder or object of the 
opinion.

    first run the pronominal coreference pr
    then use a jape rule to find pronouns linked to a person or 

organization 

    we can identify these because they will have the feature  
   entity_mention_type    (created by the coreferencer)

    the co-referring pronouns all have also an antecedent_offset feature 

pointing to the proper noun antecedent

    the matching proper noun antecedent is found and its string is added 

as a feature on the relevant pronoun annotation

 

university of sheffield, nlp

implicit opinion holders

    there may not always be an explicit opinion holder

in many cases, the author of the tweet is the opinion holder 

   

i'm also going to vote tory. hello new world.

    here we can co-refer    i    with the person tweeting (using the 

metadata)

in other cases, there is no explicit opinion holder:

   

vote for labour. harry potter would.

    however, we can infer by this instruction that the author of the 

tweet shares this opinion.

   

in all these cases, we add the value    author    to the feature 
   opinion_holder   

 

university of sheffield, nlp

creating the application

    we only want to process the actual text of the tweet, not all the 

other information

    to do this, we use a segment processing pr to run the 
sentiment app over just the "text" annotation in original 
markups set.

    so, we need two applications: one containing the segment 

processing pr and one containing the actual sentiment 
application

 

university of sheffield, nlp

corpus analysis tools

    corpus analysis tools enable you to look at the results of 

processing and make sense of them manually
in gate, we have a tool called annic which lets you analyse 
annotations in context.

   

    like a kwic index but works over annotations as well as just 

strings

    enables you to search and analyse a whole corpus without 
knowing a priori what appears specifically in which document

    this is especially useful in a corpus of tweets where each 

document represents a single tweet

 

university of sheffield, nlp

 pattern examples

{party}
{affect}
{lookup.majortype == negation}  ({token})*4  {lookup.majortype 
== "vote"}{lookup.majortype == "party"}
{token.string == "i"}  ({token})*4  {lookup.majortype == "vote"}
{lookup.majortype == "party"}
{person}  ({token})*4  {lookup.majortype == "vote"}
{lookup.majortype == "party"} 
{affect}   ({token})*5   {lookup.majortype == "candidate"}
{vote} ({token})*5   {lookup.majortype == "candidate"}

   

   

   

   

   

   

   

 

university of sheffield, nlp

opinion finding in arcomem

 

university of sheffield, nlp

arcomem project

    arcomem is an eu project about storing community memories. 

   

involves detection of entities, events, topics and opinions to guide 
the crawler

    aims to answer questions such as:

    what are the opinions on crucial social events and the key 

people involved?

    how are these opinions distributed in relation to demographic 

user data?

    how have these opinions evolved?
    who are the opinion leaders?
    what is their impact and influence?

 

university of sheffield, nlp

arcomem applications

    developed a series of initial applications for opinion mining 

from social media using gate

    based on previous work identifying political opinions from 

tweets

    extended to more generic analysis about any kind of entity or 

event, in 2 domains
    greek financial crisis
    rock am ring (german rock festival)

    uses a variety of social media including twitter, facebook and 

forum posts

    based on entity and event extraction, and a rule-based 

approach

 

university of sheffield, nlp

gate application

    structural pre-processing, specific to social media types 
(such as separating the actual content of the tweet from 
the metadata)

    linguistic pre-processing (including language detection), 

ne, term and event recognition

    additional targeted gazetteer lookup
    jape grammars
    aggregation of opinions
    dynamics

 

university of sheffield, nlp

why rule-based?

    although ml applications are typically used for opinion mining, 

this task involves documents from many different text types, 
genres, languages and domains

    this is problematic for ml because it requires many 

applications trained on the different datasets, and methods to 
deal with acquisition of training material

    aim of using a rule-based system is that the bulk of it can be 

used across different kinds of texts, with only the pre-
processing and some sentiment dictionaries which are domain 
and language-specific

 

university of sheffield, nlp

linguistic pre-processing

    id46 (per sentence) using textcat
    standard tokenisation, id52 etc using gate
    ne and term recognition using modified versions of annie 

and termraider 

    event recognition using specially developed gate application 

(e.g. band performance, economic crisis, industrial strike)

 

university of sheffield, nlp

language id with textcat

 

university of sheffield, nlp

basic approach for opinion finding

    find sentiment-containing words in a linguistic relation with 

entities/events (opinion-target matching)

    use a number of linguistic sub-components to deal with issues 

such as negatives, irony, swear words etc.

    starting from basic sentiment lookup, we then adjust the 
scores and polarity of the opinions via these components

 

university of sheffield, nlp

sentiment finding components

    flexible gazetteer lookup: matches lists of affect/emotion 

words against the text, in any morphological variant

    gazetteer lookup: matches lists of affect/emotion words 
against the text only in non-variant forms, i.e. exact string 
match (mainly the case for specific phrases, swear words, 
emoticons etc.)

    sentiment grammars:  set of hand-crafted jape rules which 

annotate sentiments and link them with the relevant targets 
and opinion holders

    rdf generation: create the relevant rdf-xml for the 

annotations according to the data model (so they can be used 
by other components)

 

university of sheffield, nlp
opinion scoring
    sentiment gazetteers (developed from sentiment words in 

id138) have a starting    strength    score

    these get modified by context words, e.g. adverbs, swear 

words, negatives and so on
    the film was awesome --> the film was **** amazing.
    the film was awful --> the film was **** awful..

    swear words on their own are classified as negative, however.

    damed politicians and their lies.
    rip fergie? it's sir alex ferguson to you, carlos, you runt.

 

university of sheffield, nlp

evaluation
    very hard to measure opinion polarity beyond positive / 

negative / neutral unless you have a product review corpus

    on a small corpus of 20 facebook posts, we identified 

sentiment-containing sentences with 55% precision and 
60% recall. of these, the polarity accuracy was 82%. 

    much better results for tweets, however.
    while this is not that high, not all the subcomponents are 
complete in the system, so we would expect better results 
with improved methods for negation and sarcasm detection
    ne recognition was high on these texts: 92% precision and 
69% recall (compared with other ne evaluations on social 
media)

 

university of sheffield, nlp

comparison of opinion finding in different tasks

corpus

sentiment 
detection

polarity 
detection

target 
assignment

political tweets

78%

financial crisis facebook

55%

79%

81.8%

97.9%

32.7%

financial crisis tweets

90%

93.8%

66.7%

 

university of sheffield, nlp

using machine learning for the arcomem task

   

if we can collect enough manually annotated training data, we 
can also use an ml approach for this task

    similar to that presented earlier for musing, but modified to 
take into account what we have subsequently learned and the 
differences in the data.

    each musing product review had an opinion from 1 to 5 stars
in arcomem we classify sentences (the ml instances), many of 
which do not contain opinions

   

    so the ml classes will be positive, neutral, negative, and none 

(contains no opinion, different from a neutral opinion)

 

university of sheffield, nlp

using machine learning for the arcomem task

    we now know that paum is much faster than id166 but 

typically just as good for nlp tasks, so we will use paum 
instead

    we'll need to deal with the special issues of social media text 

(more on this later)

    for the ml attributes, we will use id165s of tokens or lemmata
    in musing, id165s with n>2 did not improve accuracy but 

slowed the ml down

     but it's worth trying 3-grams just in case they help with the 

smaller instances

 

university of sheffield, nlp

using machine learning for the arcomem task

    also worth trying other annotations such as named entities
    but these might exaggerate the effect of biased training data 

(this might not be a problem, but it's worth bearing in mind)

    for example, if most people who mention    venus williams    in 
the training data like her (or her dresses), we are training the 
ml model to expect positive opinions for that person 
annotation; the real data might or might not match

 

university of sheffield, nlp

training on tweets

    you can use hashtags as a source of classes!

    example: collect a set of tweets with the #angry tag, and a 

set without it, and delete from the second set any tweets 
that look angry

    remove the #angry tag from the text in the first set (so 

you're not just training the ml to spot the tag)

    you now have a corpus of manually annotated angry/non-

angry data!

    this approach can work well, but if you have huge datasets, 

you may not be able to do the manual deletions

    experimenting with #sarcasm is interesting (more on this 

later)

 

university of sheffield, nlp

challenges for opinion mining on social 

media

 

university of sheffield, nlp

linguistic issues

    what kinds of linguistic problems do we need to overcome?

    short sentences (problems for parsers etc)
    use of incorrect english
    negatives
    conditional statements
    use of slang/swear words
    use of irony/sarcasm
    ambiguity

 

university of sheffield, nlp

short sentences, e.g. tweets

    social media, and especially tweets, can be problematic 

because sentences are very short and/or incomplete

    typically, linguistic pre-processing tools such as pos taggers 

and parsers do badly on such texts

    even basic tools like id46 can have 

problems

    the best solution is to try not to rely too heavily on these tools
    does it matter if we get the wrong language for a sentence?
    do we actually need full parsing?
    can we use other clues when pos tags may be incorrect?

 

university of sheffield, nlp

dealing with incorrect english

    frequent problem in any nlp task involving social media

   

incorrect capitalisation, spelling, grammar, made-up words (eg 
swear words, infixes)

    backoff strategies include 

    normalisation
    using more flexible gazetteer matching
    using case-insensitive resources (but be careful)
    avoiding full parsing and using shallow techniques
    using very general grammar rules
    adding specialised gazetteer entries for common mis-spellings, 

or using co-reference techniques

 

university of sheffield, nlp

tokenisation

    splitting a text into its constituent parts
    plenty of    unusual   , but very important tokens in social media: 

    @apple     mentions of company/brand/person names
    #fail, #stevejobs     hashtags expressing sentiment, person 

or company names

    :-(, :-), :-p     emoticons (punctuation and optionally letters)
    urls 

    tokenisation key for entity recognition and opinion mining
    a study of 1.1 million tweets: 26% of english tweets have a 
url, 16.6% - a hashtag, and 54.8% - a user name mention 
[carter, 2013].  

 

university of sheffield, nlp

example

#wiredbizcon #nike vp said when @apple saw what 
http://nikeplus.com did, #stevejobs was like wow i didn't expect this at 
all.

    tokenising on white space doesn't work that well: nike and apple 

are company names, but if we have tokens such as #nike and 
@apple, this will make the entity recognition harder, as it will 
need to look at sub-token level

    tokenising on white space and punctuation characters doesn't 

work well either: urls get separated (http, nikeplus), as are 
emoticons and email addresses

 

university of sheffield, nlp

the gate twitter tokeniser

    treat rts, emoticons, and urls as 1 token each
    #nike is two tokens (# and nike) plus a separate annotation 

hashtag covering both. same for @mentions

    capitalisation is preserved, but an orthography feature is 

added: all caps, lowercase, mixcase

    date and phone number normalisation, lowercasing, and other 

such cases are optionally done later in separate modules

    consequently, tokenisation is faster and more generic

 

university of sheffield, nlp

de-duplication and spam removal

    approach from [choudhury & breslin, #msm2011]:
    remove as duplicates/spam:

    messages with only hashtags (and optional url)
    similar content, different user names and with the same 

timestamp are considered to be a case of multiple accounts

    same account, identical content are considered to be 

duplicate tweets

    same account, same content at multiple times are 

considered as spam tweets

 

university of sheffield, nlp

language detection

    there are many language detection systems readily available 
    the main challenges on tweets/facebook status updates:

    the short number of tokens (10 tokens/tweet on average)
    the noisy nature of the words (abbreviations, misspellings).

    due to the length of the text, we can make the assumption that 

one tweet is written in only one language

    most language detection tools work by building id165 

language models for each language and then assigning the 
text to the most probable language from the trained model.

 

university of sheffield, nlp

language detection for social media

    compare language detection methods [lui and baldwin, 2011]
    best results with 1-nearest-neighbour (1nn) model

    a test document is classi   ed based on the language of the 

closest training document, as determined by the cosine 
similarity metric

    character bigrams or trigrams

    we have reimplemented their best method in java, as part of 

trendminer

    https://github.com/sinjax/trendminer-

java/tree/master/text/nlp/src/main/java/org/openimaj/text/nlp 

    comes pre-trained on 97 languages and very fast

 

university of sheffield, nlp
normalisation

   

   rt @bthompson writez: @libbyabrego honored?! 
everybody knows the libster is nice with it...lol...(thankkkks a 
bunch;))   

    omg! i   m so guilty!!! sprained biibii   s leg! arghhhhhh!!!!!!
    similar to sms normalisation
    for some later components to work well (pos tagger, parser), 
it is necessary to produce a normalised version of each token
    but uppercasing, and letter and exclamation mark repetition 

often convey strong sentiment

    therefore some choose not to normalise, while others keep 

both versions of the tokens 

 

university of sheffield, nlp

syntactic normalisation [kaufmann, 2010]

    preparation: removing emoticons, 

tokenisation

    orthographic mapping: 2moro, u 
    syntactic disambiguation

    determine when @mentions and 

#tags have syntactic value and 
should be kept in the sentence, 
vs replies, retweets and topic 
tagging

    machine translation: used moses 

    trained on sms and anc 

corpora

 

university of sheffield, nlp

id30

    the snowball stemmer is already integrated in gate 
    11 european languages: danish, dutch, english, finnish, 

french, german, italian, norwegian, portuguese, russian, 
spanish and swedish

    http://snowball.tartarus.org

 

university of sheffield, nlp
ner in tweets

    performance of the stanford ner drops to 48% [liu et al, 

2011] or even 29% on another tweet corpus [ritter et al, 2011]

    pre-processing used:

    stop words, user names, and links are removed
    specially adapted/trained pos tagger [ritter et al, 2011]
    np chunker adapted to tweets [ritter et al, 2011]
    capitalisation information [ritter et al, 2011]
    syntactic normalisation [doerhmann, 2011]
    gazetteers derived from freebase [ritter et al, 2011]

 

university of sheffield, nlp

ner for tweets (2)

    performance reported on 4 entity types (per, loc, org, 

product): 80.2% f-score (81.6% p; 78.8% r) [liu et al 2011]
[doerhmann, 2011] improved on liu's results by normalising 
the tweets first

   

    ritter's scores are lower but against more freebase entity 

types: person, geo-location, company, product, 
facility, tv-show, movie, sportsteam, band, and 
other

 

university of sheffield, nlp

other challenges of social media

    strongly temporal and dynamic: temporal information (e.g. 

post timestamp) can be combined with opinion mining, to 
examine the volatility of attitudes towards topics over time (e.g. 
gay marriage).

    exploiting social context: (who is the user connected to? 
how frequently they interact). derive automatically semantic 
models of social networks, measure user authority, cluster 
similar users into groups, as well as model trust and strength 
of connection
implicit information about the user: research on 
recognising gender, location, and age of twitter users. helpful 
for generating opinion summaries by user demographics

   

 

university of sheffield, nlp

more flexible matching techniques

   

in gate, as well as the standard gazetteers, we have options 
for modified versions which allow for more flexible matching

    bwp gazetteer: uses levenshein id153 for 

approximate string matching

    extended gazetteer: has a number of parameters for matching 

 prefixes, suffixes, initial capitalisation and so on

 

university of sheffield, nlp

extended gazetteer
    part of the stringannotation plugin in gate
    has the following additional characteristics:

    gives more control over which characters are considered to 

belong to words and non-word characters

    enables matching when an initial letter of a word is 

uppercase

    matching of prefixes and suffixes
    case-insensitive matching also deals with cases (such as 

german "  " which maps to "ss") 

 

university of sheffield, nlp

case-insensitive matching
    this would seem the ideal solution, especially for gazetteer lookup, 

when people don't use case information as expected

    however, setting all prs to be case-insensitive can have undesired 

consequences
    id52 becomes unreliable (e.g.    may    vs    may   )
    back-off strategies may fail, e.g. unknown words beginning with 

a capital letter are normally assumed to be proper nouns

    gazetteer entries quickly become ambiguous (e.g. many place 

names and first names are ambiguous with common words)
    solutions include selective use of case insensitivity, removal of 
ambiguous terms from lists, additional verification (e.g. use of 
coreference)

 

university of sheffield, nlp

finding negatives

    what methods might we use for finding negatives?

    list lookup
    verb analysis
    sarcasm

 

university of sheffield, nlp

find the hidden deer...
one of the trickiest tasks in opinion mining is spotting the hidden 
meaning in a piece of text.

 

university of sheffield, nlp

irony and sarcasm

    the now abandoned hp touchpad is officially the hottest piece of 

consumer electronics on amazon.

    life's too short, so be sure to read as many articles about celebrity 

breakups as possible.

    loves being in this supah long line at the #dmv -- woo hoo

   

i had never seen snow in holland before but thanks to twitter and 
facebook i now know what it looks like. thanks guys, awesome!
    on a bright note if downing gets injured we have henderson to 

come in.

    am glad 10 day forecast calling for lots of rain/cool temps. was 

getting tired sun & dry conditions

 

university of sheffield, nlp

how do you know when someone is being 
sarcastic?

    use of hashtags in tweets such as #sarcasm
    large collections of tweets based on hashtags can be used to 

make a training set for machine learning

    but you still have to know which bit of the tweet is the sarcastic 

bit

to the hospital #fun #sarcasm
man , i hate when i get those chain letters & i don't resend them , 
then i die the next day .. #sarcasm
lol letting a baby goat walk on me probably wasn't the best idea. 
those hooves felt great. #sarcasm

 

university of sheffield, nlp

how else can you deal with it?

    look for word combinations with opposite polarity, e.g.    rain    or 

   delay    plus    brilliant   

going to the dentist on my weekend home. great. i'm totally 
pumped. #sarcasm

   

   

inclusion of world knowledge / ontologies can help (e.g. 
knowing that people typically don't like going to the dentist, or 
that people typically like weekends better than weekdays.
it's an incredibly hard problem and an area where we expect 
not to get it right that often

 

university of sheffield, nlp

ambiguity

    social media can often pose ambiguities, for a number of 

reasons

    misunderstandings: not much we can do
"i love eminem" "i like skittles better." "no, the rapper you idiot.." 
"you're the idiot! what's good about a m&m wrapper?!" 
    entity ambiguity: disambiguation techniques / linking to uri
i like how    rip fergie    is trending because of football and half the 
population of twitter think that one of the black eyed peas has 
died.
    but this is hard when there's no contextual reference...

 

university of sheffield, nlp

evaluation

    how can we evaluate opinion mining performance?
    what kind of results can we expect to get?
    what problems typically occur with evaluation?
    how can we compare existing tools and methods?

 

university of sheffield, nlp

comparing different opinion mining tools

    how do you compare different opinion mining tools, when 

there are so many out there and they all report different kinds 
of results?
it is generally accepted that tools will be 50%-70%    accurate    
out-of-the box.

   

    but what does this really mean?
    seth grimes has some pointers about this....

http://www.socialmediaexplorer.com/social-media-marketing/social-media-sentiment-competing-on-accuracy/

 

university of sheffield, nlp

1. don't compare apples with oranges

    not all tools do the same thing, even if they look the same
    document-level vs topic-level sentiment
    one tool might be good at getting the overall sentiment of a 

tweet right, but rubbish at finding the sentiment about a 
particular entity

    e.g. the following tweet is classed as being negative about the 

olympics:

skytrain seems to be having problems frequently lately. hope 
cause is upgraded and they work the kinks out before olympics. 
    the tweet is (correctly) negative overall but not specifically 

about the olympics

 

university of sheffield, nlp

2. use the same measurement scale

    positive/negative/neutral vs scalar measurement (-5 to +5)
    valence vs mood/orientation (e.g. happy, sad, angry, 

frustrated)
is reasonable emotion classification more useful to you than 
fantastic valence?

   

    how will you actually make use of the opinions generated to 

e.g. make decisions?

 

university of sheffield, nlp

3. how is accuracy defined?

    nlp tools often use precision, recall and f-measure to 

determine accuracy

    but most opinion mining tools are only measured in terms of 

accuracy (precision)

    how important is recall? 
    how important is the tradeoff between precision and recall?
    what about *contextual* relevance that incorporates 

timeliness, influence, activities, and lots of other still-fuzzy 
*social* notions?

    how trustworthy / important are the opinions? sentiment from 

a valued customer may be more important than a one-time 
buyer

 

university of sheffield, nlp

4. what's the impact of errors?

    not all inaccuracies have the same impact

   

if you're looking at aggregate statistics, a negative rating of a 
positive opinion has more impact than a neutral rating of a 
positive opinion

    how do neutral opinions affect aggregation? are they 

   

considered? should they be?
in other cases, finding any kind of sentiment (whether with 
correct polarity or not) might be more important than wrongly 
detecting no sentiment and missing important information

 

university of sheffield, nlp

creating a gold standard

    typically, we annotate a gold standard corpus manually and 

then compare the system results against that

    but have you ever tried doing manual annotation of tweets?

it's harder than it looks...

   

    you have to be very clear what you want to annotate
    you have to understand what the author intended
    you need to decide how lenient you'll be
    you may need to decide if getting something right for the 

wrong reason is still ok

 

university of sheffield, nlp

positive or negative tweets?

rt @ssssab: mariano: she used to be a very nice girl, before she 
discovered macdonalds
i'm tired after school today!
there was just a fire at work. today is looking up.
yesterday my son forgot his jacket at school.  today he 
remembered to bring home the jacket, but forgot his lunchbox.
oh no. ludo's got a new obsession with dora the explorer and 
now i find myself wondering around humming the theme tune.
i find myself sobbing at john le mesurier's beauty of soul. 
documentary about him on bbc iplayer

 

university of sheffield, nlp

opinionated or not?

the european sovereign debt crisis that   s spread from greece to 
italy and is roiling the region   s banks now has another potential 
victim: energy policy.
labour got less this time than john major did in 1997.
european leadership - where is it? 

 

university of sheffield, nlp

looking into the future

    typically, opinion mining looks at social media content to analyse 

people   s explicit opinions about a product or service

    this backwards-looking approach often aims primarily at dealing 

with problems, e.g. unflattering comments

    a forwards-looking approach aims at looking ahead to 

understanding potential new needs from consumers

    this is not just about looking at specific comments, e.g.    the 

product would be better if it had longer battery life   , but also about  
detecting non-specific sentiment

    this is achieved by understanding people's needs and interests in a 

more general way, e.g. drawing conclusions from their opinions 
about other products, services and interests.

 

university of sheffield, nlp

deep id31
    the hardest thing about getting id31 right is uncovering 

exactly what is being meant

    difference between a customer saying they merely like a brand and 

saying that they love it. 

    sentiment has many rich and nuanced dimensions that need to be teased 

apart to make it insightful.
   an old lady told me that warm dr. pepper is delicious   

   

   

is it only nice when warm? does the author share the opinion of the old 
lady?

   

    could this be a new insight for the manufacturers/advertisers?
   when i was a kid i loved smarties   .
    should smarties be targeted only at kids or do adults like them too?
    classification of sentiment according to functional, insightful, emotional 

 

etc.

university of sheffield, nlp

the ultimate question

    the book "the ultimate question" recently ranked #1 on the wall 

street journal's business best-sellers list and #1 on usa 
today's money best-sellers list.
it's all about whether a consumer likes a brand enough to 
recommend it - this is the key to a company's performance. 

   

    general sentiment detection isn't precise enough to answer this 

kind of question, because all kinds of    like    are treated equally

    growing need for id31 that can get to very fine levels 

of detail, while keeping up with the enormous (and constantly 
increasing) volume of social media. 

 

university of sheffield, nlp

the problem of sparse data

    one of the difficulties of drawing conclusions from traditional 

opinion mining techniques is the sparse data issue

    opinions tend to be based on a very specific product or service, 
e.g. a particular model of camera, but don't necessarily hold for 
every model of that brand of camera, or for every product sold by 
the company

    one solution is figuring out which statements can be generalised to 

other models/products and which are specific

    another solution is to leverage id31 from more 

generic expressions of motivation, behaviour, emotions and so on, 
e.g. what type of person buys what kind of camera?

 

university of sheffield, nlp

approaches to id31 beyond opinion 
mining

    an interesting article from seth grimes about this:

http://www.customerthink.com/article/mentions_to_meaning_a
nalytics_journey

 

university of sheffield, nlp

summary

   

introduced the concept of opinion mining and id31
    simple examples of rule-based and ml methods for creating om 

applications

    dealing with social media
    evaluation of opinion mining
    looking ahead to the future

 

university of sheffield, nlp

more information

    see the following paper for details and evaluation of a more 

complex version of the twitter application

    d. maynard and a. funk. automatic detection of political opinions in 
tweets. in proceedings of msm 2011: making sense of microposts. 
workshop at 8th extended semantic web conference (eswc 
2011). heraklion, greece. june 2011 (download pdf)

    the eu-funded arcomem and trendminer projects are dealing 

with lots of issues about opinion and trend mining from social 
media, and use gate for this.

    http://www.arcomem.eu
    http://www.trendminer-project.eu/

 

university of sheffield, nlp

references

   

t. baldwin and m. lui. id46: the long and the short of the matter. in 
proc. naacl hlt    10. http://www.aclweb.org/anthology/n10-1027.

    m. kaufmann. syntactic id172 of twitter messages. 

http://www.cs.uccs.edu/~kalita/work/reu/reufinalpapers2010/kaufmann.pdf

    s. choudhury and j. breslin. extracting semantic entities and events from sports 

tweets. proceedings of #msm2011 making sense of microposts. 2011.

    x. liu, s. zhang, f. wei, m. zhou. recognizing named entities in tweets. acl'2011.

    a. ritter, mausam, etzioni. id39 in tweets: an experimental study. 

emnlp'2011.

    doerhmann. named entity extraction from the colloquial setting of twitter. 

http://www.cs.uccs.edu/~kalita/work/reu/reu2011/finalpapers/doehermann.pdf

    s. carter, w. weerkamp, e. tsagkias. microblog id46: overcoming 

the limitations of short, unedited and idiomatic text. language resources and 
evaluation journal. 2013 (forthcoming)

   

johan bollen, huina mao, xiaojun zeng, twitter mood predicts the stock market, journal 
of computational science, volume 2, issue 1, march 2011..

 

university of sheffield, nlp

some more demos to try

    http://sentiment.christopherpotts.net/lexicon/ get sentiment 
scores for single words from a variety of sentiment lexicons

    http://sentiment.christopherpotts.net/textscores/ show how a 

variety of lexicons score novel texts

    http://sentiment.christopherpotts.net/classify/ classify tweets 

according to various probabilistic classifier models

 

university of sheffield, nlp

questions?

 

