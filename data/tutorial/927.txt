this tutorial has been given at aaai-2011, eacl-2012, and id31 symposium.

id31 and 
opinion mining

bing liu
department of computer science
university of illinois at chicago
liub@cs.uic.edu

introduction

    id31 or opinion mining

    computational study of opinions, sentiments, 

evaluations, attitudes, appraisal, affects, views, 
emotions, subjectivity, etc., expressed in text. 
    reviews, blogs, discussions, news, comments, feedback, 

or any other documents.

    terminology: 

    id31 is more widely used in industry. 
    both are widely used in academia

    but they can be used interchangeably.

bing liu, tutorial 

2

   1

why are opinions important?

       opinions    are key influencers of behaviors. 
    our beliefs and perceptions of reality are 
largely conditioned on how others see the 
world. 

    whenever we need to make a decision, we 
often seek out the opinions of others. past:
    individuals: ask opinions from friends and family
    organizations: use surveys, focus groups, opinion 

polls, consultants.

bing liu, tutorial 

3

introduction     social media + beyond

    word-of-mouth on the web

    personal experiences and opinions about anything in 

reviews, forums, blogs, twitter, micro-blogs, etc  

    comments about articles, issues, topics, reviews, etc.  
    postings at social networking sites, e.g., facebook. 
    global scale: no longer     one   s circle of friends
    organization internal data

    customer feedback from emails, call centers, etc. 

    news and reports

    opinions in news articles and commentaries 

bing liu, tutorial 

4

   2

introduction     applications

    businesses and organizations

    benchmark products and services; market intelligence. 

    businesses spend a huge amount of money to find consumer opinions 

using consultants, surveys and focus groups, etc

    individuals

    make decisions to buy products or to use services
    find public opinions about political candidates and issues  

    ads placements: place ads in the social media content

    place an ad if one praises a product. 
    place an ad from a competitor if one criticizes a product.  
    opinion retrieval: provide general search for opinions. 

bing liu, tutorial 

5

a fascinating problem!

    intellectually challenging & many applications.

    a popular research topic in nlp, id111, and web 
mining in recent years (shanahan, qu, and wiebe, 2006 (edited book); 
surveys - pang and lee 2008; liu, 2010; 2011)

    it has spread from computer science to management 

science (hu, pavlou & zhang, 2006; archak, ghose & ipeirotis, 2007; liu et al 
2007; park, lee & han, 2007; dellarocas, zhang & awad, 2007; chen & xie 2007).

    40-60 companies in usa alone 

    it touches every aspect of nlp and yet is confined.

    little research in nlp/linguistics in the past.
    potentially a major technology from nlp. 

    but it is hard.

bing liu, tutorial 

6

   3

a large problem space

    many names and tasks with somewhat different 

objectives and models
    id31
    opinion mining
    sentiment mining
    subjectivity analysis
    affect analysis
    emotion detection
    opinion spam detection
    etc.

bing liu, tutorial 

about this tutorial

    like a traditional tutorial, i will introduce the 

research in the field. 
    key topics, main ideas and approaches
    since there are a large number of papers, it is not 

possible to introduce them all, but a comprehensive 
reference list will be provided. 

    unlike many traditional tutorials, this tutorial is 
also based on my experience in working with 
clients in a startup, and in my consulting
    i focus more on practically important tasks (imho)

bing liu, tutorial 

7

8

   4

roadmap

    id31 problem
    document sentiment classification
    sentence subjectivity & sentiment classification
    aspect-based id31
    aspect-based opinion summarization
    senitment lexicon generation
    mining comparative opinions
    some other problems
    opinion spam detection 
    utility or helpfulness of reviews
    summary

bing liu, tutorial 

9

structure the unstructured (hu and liu 2004)

    structure the unstructured: natural language 

text is often regarded as unstructured data. 

    the problem definition should provide a 

structure to the unstructured problem. 
    key tasks: identify key tasks and their inter-

relationships.

    common framework: provide a common framework 

to unify different research directions. 

    understanding: help us understand the problem better.

bing liu, tutorial 

10

   5

problem statement: abstraction

    it consists of two issues
(1)  opinion definition. what is an opinion?

    can we provide a structured definition?

    if we cannot structure a problem, we probably do not 

understand the problem.
(2)  opinion summarization

    opinions are subjective. an opinion from a single 

person (unless a vip) is often not sufficient for action.

    we need opinions from many people, and thus opinion 

summarization. 

bing liu, tutorial 

11

abstraction (1): what is an opinion?

    id: abc123 on 5-1-2008    i bought an iphone a few days 

ago. it is such a nice phone. the touch screen is really 
cool. the voice quality is clear too. it is much better than 
my old blackberry, which was a terrible phone and so 
difficult to type with its tiny keys. however, my mother was 
mad with me as i did not tell her before i bought the phone. 
she also thought the phone was too expensive,        

    one can look at this review/blog at the
    document level, i.e., is this review + or -? 
    sentence level, i.e., is each sentence + or -? 
    entity and feature/aspect level 

bing liu, tutorial 

12

   6

entity and aspect/feature level

    id: abc123 on 5-1-2008    i bought an iphone a few days 

ago. it is such a nice phone. the touch screen is really 
cool. the voice quality is clear too. it is much better than 
my old blackberry, which was a terrible phone and so 
difficult to type with its tiny keys. however, my mother was 
mad with me as i did not tell her before i bought the
phone. she also thought the phone was too expensive,        

    what do we see?

    opinion targets: entities and their features/aspects
    sentiments: positive and negative
    opinion holders: persons who hold the opinions
    time: when opinions are expressed

bing liu, tutorial 

two main types of opinions 
(jindal and liu 2006; liu, 2010)
    regular opinions: sentiment/opinion 
expressions on some target entities
    direct opinions: 

       the touch screen is really cool.   

    indirect opinions: 

       after taking the drug, my pain has gone.    

    comparative opinions: comparisons of more 

than one entity. 
    e.g.,    iphone is better than blackberry.   

    we focus on regular opinions first, and just call 

them opinions. 

bing liu, tutorial 

13

14

   7

a (regular) opinion

    an opinion has the follow basic components

(gi, soijkl, hi, tl),

where 
    gj is a target
    soijl is the sentiment value of the opinion from 

opinion holder hi on target gj at time tl. soijl is 
positive, negative or neutral, or a rating score

    hi is an opinion holder. 
    tl is the time when the opinion is expressed. 

bing liu, tutorial 

15

opinion target

    in some cases, opinion target is a single entity or

topic.
       i love iphone    and    i support tax cut.   

    but in many other cases, it is more complex. 

       i bought an iphone a few days ago. it is such a nice

phone. the touch screen is really cool.   
    opinion target of the 3rd sentence is not just touch screen, but 

the    touch screen of iphone   .

       i support tax cut for the middle class, but for the rich   

    we decompose the opinion target

bing liu, tutorial 

16

   8

entity and aspect (hu and liu, 2004; liu, 2006)

    definition (entity): an entity e is a product, person, 

event, organization, or topic. e is represented as 
    a hierarchy of components, sub-components, and so on.  
    each node represents a component and is associated 

with a set of attributes of the component.

    an opinion can be expressed on any node or attribute 

of the node. 

    for simplicity, we use the term aspects (features) to 

represent both components and attributes.

bing liu, tutorial 

17

opinion definition (liu, ch. in nlp handbook, 2010)

    an opinion is a quintuple 

(ej, ajk, soijkl, hi, tl),

where 
    ej is a target entity.
    ajk is an aspect/feature of the entity ej.
    soijkl is the sentiment value of the opinion from the 
opinion holder hi on aspect ajk of entity ej at time tl. 
soijkl is +ve, -ve, or neu, or a more granular rating. 

    hi is an opinion holder. 
    tl is the time when the opinion is expressed. 

bing liu, tutorial 

18

   9

some remarks about the definition

    although introduced using a product review, the 

definition is generic
    applicable to other domains,
    e.g., politics, social events, services, topics, etc. 

    (ej, ajk) is also called the opinion target

    opinion without knowing the target is of limited use. 
    the five components in (ej, ajk, soijkl, hi, tl) must 

correspond to one another. very hard to achieve 
    the five components are essential. without any 

of them, it can be problematic in general. 

bing liu, tutorial 

19

some remarks (contd)

    of course, one can add any number of other 

components to the tuple for more analysis. e.g., 
    gender, age, web site, post-id, etc. 

    the original definition of an entity is a hierarchy 

of parts, sub-parts, and so on. 
    the simplification can result in information loss.

    e.g.,    the ink of this printer is very expensive.    
       ink    is a part of the printer and    price    (expensive) is an 

aspect of    ink    (not the printer). 

    but it is usually sufficient for practical applications. 

    it is too hard without the simplification. 

bing liu, tutorial 

20

   10

   confusing    terminologies

    entity is also called object.
    aspect is also called feature, attribute, facet, etc
    opinion holder is also called opinion source
    some researchers also use topic to mean entity

and/or aspect. 
    separating entity and aspect is preferable

    in specific applications, some specialized terms 

are also commonly used, e.g.,
    product features, political issues

bing liu, tutorial 

21

reader   s standing point

    see this sentence

       i am so happy that google price shot up today.    

    although the sentence gives an explicit sentiment, 

different readers may feel very differently. 
    if a reader sold his google shares yesterday, he will not 

be that happy. 

    if a reader bought a lot of google shares yesterday, he 

will be very happy. 

    current research either implicitly assumes a 

standing point, or ignores the issue. 

bing liu, tutorial 

22

   11

our example blog in quintuples

    id: abc123 on 5-1-2008    i bought an iphone a few days 

ago. it is such a nice phone. the touch screen is really 
cool. the voice quality is clear too. it is much better than 
my old blackberry, which was a terrible phone and so 
difficult to type with its tiny keys. however, my mother was 
mad with me as i did not tell her before i bought the phone. 
she also thought the phone was too expensive,        

    in quintuples

(iphone, general, +, abc123, 5-1-2008)
(iphone, touch_screen, +, abc123, 5-1-2008)
   .

    we will discuss comparative opinions later.

bing liu, tutorial 

23

structure the unstructured

    goal: given an opinionated document, 

    discover all quintuples (ej, ajk, soijkl, hi, tl), 
    or, solve some simpler forms of the problem
    e.g., sentiment classification at the document or 

sentence level. 

    with the quintuples, 

    unstructured text     structured data

    traditional data and visualization tools can be used to 

slice, dice and visualize the results.

    enable qualitative and quantitative analysis.  

bing liu, tutorial 

24

   12

two closely related concepts

    subjectivity and emotion. 
    sentence subjectivity: an objective 

sentence presents some factual information, 
while a subjective sentence expresses some 
personal feelings, views, emotions, or beliefs.
    emotion: emotions are people   s subjective 

feelings and thoughts.

bing liu, tutorial 

25

subjectivity
    subjective expressions come in many forms, e.g., 
opinions, allegations, desires, beliefs, suspicions, 
and speculations (wiebe, 2000; riloff et al 2005).
    a subjective sentence may contain a positive or 

negative opinion

    most opinionated sentences are subjective, but 

objective sentences can imply opinions too (liu, 2010)
       the machine stopped working in the second day   
       we brought the mattress yesterday, and a body 

impression has formed.    

       after taking the drug, there is no more pain    

bing liu, tutorial 

26

   13

emotion

    no agreed set of basic emotions of people 

among researchers. 

    based on (parrott, 2001), people have six main 

emotions, 
    love, joy, surprise, anger, sadness, and fear. 

    strengths of opinions/sentiments are related to 

certain emotions, e.g., joy, anger. 
    however, the concepts of emotions and opinions 

are not equivalent. 

bing liu, tutorial 

27

rational and emotional evaluations

    rational evaluation: many evaluation/opinion 

sentences express no emotion 
    e.g.,    the voice of this phone is clear   

    emotional evaluation
    e.g.,    i love this phone   
       the voice of this phone is crystal clear    (?)

    some emotion sentences express no 

(positive or negative) opinion/sentiment
    e.g.,    i am so surprised to see you   .

bing liu, tutorial 

28

   14

sentiment, subjectivity, and emotion

    although they are clearly related, these concepts 

are not the same
    sentiment     subjective     emotion

    sentiment is not a subset of subjectivity (without 

implied sentiments by facts, it should be)
    sentiment     subjectivity
    the following should hold

    emotion     subjectivity
    sentiment     emotion,    

bing liu, tutorial 

29

abstraction (2): opinion summary 

    with a lot of opinions, a summary is necessary.

    a id57 task

    for factual texts, summarization is to select the 

most important facts and present them in a 
sensible order while avoiding repetition
    1 fact = any number of the same fact

    but for opinion documents, it is different because 
opinions have a quantitative side & have targets
    1 opinion     a number of opinions
    aspect-based summary is more suitable

    quintuples form the basis for opinion summarization

bing liu, tutorial 

30

   15

feature based summary of 

iphone:

aspect-based opinion summary1
(hu & liu, 2004) 
      i bought an iphone a few days 
ago. it is such a nice phone. the
touch screen is really cool. the
voice quality is clear too. it is 
much better than my old 
blackberry, which was a terrible 
phone and so difficult to type 
with its tiny keys. however, my 
mother was mad with me as i did 
not tell her before i bought the
phone. she also thought the 
phone was too expensive,        

feature1: touch screen
positive: 212
    the touch screen was really cool. 
    the touch screen was so easy to 
use and can do amazing things. 

   
negative: 6
    the screen is easily scratched.
   

i have a lot of difficulty in removing 
finger marks from the touch screen. 

1.  originally called feature-based opinion 

mining and summarization

   .

bing liu, tutorial 

    
feature2: voice quality
   

note: we omit opinion holders

31

opinion observer (liu et al. 2005)

+

    summary of 
reviews of    
cell phone 1

    comparison of 

reviews of 
cell phone 1 
cell phone 2

screen

battery

size 

weight

_
voice
+

_

bing liu, tutorial 

32

   16

aspect-based opinion summary

bing liu, tutorial 

33

google product search (blair-goldensohn et al 2008 ?)

bing liu, tutorial 

34

   17

some examples from opinioneq

bing liu, tutorial 

detail opinion sentences
    click on any bar (previous slide) to see the opinion 

sentences. here are negative opinion sentences on the 
maps feature of garmin. 

bing liu, tutorial 

35

36

   18

% of +ve opinion and # of opinions

bing liu, tutorial 

aggregate opinion trend

bing liu, tutorial 

37

38

   19

live tracking of two movies (twitter)

user ratings from rotten tomatoes: captain america: 81% positive
cowboys & aliens: 60% positive

bing liu, tutorial 

july 8, 2011  to present

not just one problem

    (ej, ajk, soijkl, hi, tl),

    ej - a target entity:  named entity extraction (more)
    ajk     an aspect of ej: information extraction
    soijkl is sentiment:  sentiment identification 
    hi is an opinion holder:  information/data extraction
    tl is the time:  information/data extraction
    5 pieces of information must match

    coreference resolution
    synonym match (voice = sound quality) 
       

bing liu, tutorial 

39

40

   20

opinion mining is hard!

       this past saturday, i bought a nokia phone 

and my girlfriend bought a motorola phone 
with bluetooth. we called each other when we 
got home. the voice on my phone was not so 
clear, worse than my previous samsung 
phone. the battery life was short too. my 
girlfriend was quite happy with her phone. i 
wanted a phone with good sound quality. so 
my purchase was a real disappointment. i 
returned the phone yesterday.   

bing liu, tutorial 

easier and harder problems

    tweets from twitter are probably the easiest

    short and thus usually straight to the point

    reviews are next 

    entities are given (almost) and there is little noise 
    discussions, comments, and blogs are hard. 
    multiple entities, comparisons, noisy, sarcasm, etc 
    determining sentiments seems to be easier. 
    extracting entities and aspects is harder. 
    combining them is even harder. 

bing liu, tutorial 

41

42

   21

opinion mining in the real world

    source the data, e.g., reviews, blogs, etc

(1) crawl all data, store and search them, or
(2) crawl only the target data

    extract the right entities & aspects
    group entity and aspect expressions, 
    moto = motorola, photo = picture, etc    

    aspect-based opinion mining (id31)

    discover all quintuples 

(store the quintuples in a database)
    aspect based opinion summary

bing liu, tutorial 

roadmap

    id31 problem
    document sentiment classification
    sentence subjectivity & sentiment classification
    aspect-based id31
    aspect-based opinion summarization
    sentiment lexicon generation
    mining comparative opinions
    some other problems
    opinion spam detection 
    utility or helpfulness of reviews
    summary

bing liu, tutorial 

43

44

   22

sentiment classification
    classify a whole opinion document (e.g., a 

review) based on the overall sentiment of the 
opinion holder (pang et al 2002; turney 2002)
    classes: positive, negative (possibly neutral)
    neutral or no opinion is hard. most papers ignore it. 

    an example review: 

       i bought an iphone a few days ago. it is such a nice 

phone, although a little large. the touch screen is cool. 
the voice quality is clear too. i simply love it!   

    classification: positive or negative?

    perhaps the most widely studied problem. 

bing liu, tutorial 

45

a text classification task

    it is basically a text classification problem
    but different from topic-based text classification.
    in topic-based text classification (e.g., computer, sport, 

science), topic words are important. 

    but in sentiment classification, opinion/sentiment 
words are more important, e.g., great, excellent, 
horrible, bad, worst, etc. 

    opinion/sentiment words 

    words and phrases that express desired or undesired 

states or qualities. 

bing liu, tutorial 

46

   23

assumption and goal

    assumption: the doc is written by a single person 
and express opinion/sentiment on a single entity. 

    goal: discover  (_, _, so, _, _), 

where e, a, h, and t are ignored

    reviews usually satisfy the assumption. 

    almost all papers use reviews
    positive: 4 or 5 stars, negative: 1 or 2 stars

    many forum postings and blogs do not

    they can mention and compare multiple entities
    many such postings express no sentiments

bing liu, tutorial 

some amazon reviews

bing liu, tutorial 

47

48

   24

unsupervised classification
(turney, 2002)

    data: reviews from epinions.com on 

automobiles, banks, movies, and travel 
destinations.

    the approach: three steps
    step 1:

    part-of-speech (pos) tagging
    extracting two consecutive words (two-word 
phrases) from reviews if their tags conform to 
some given patterns, e.g., (1) jj, (2) nn.

bing liu, tutorial 

patterns of pos tags

bing liu, tutorial 

49

50

   25

    step 2: estimate the sentiment orientation 

(so) of the extracted phrases
    use pointwise mutual information
   
p

p
word
(
1
word
p
(
)
1

word
1

word

pmi

log

   

(

)

,

2

2

   
      
   

word
2
word
(

)
)

2

   
      
   

    semantic orientation (so): 

so(phrase) = pmi(phrase,    excellent   )

- pmi(phrase,    poor   )

    using altavista near operator to do search to find 

the number of hits to compute pmi and so. 

bing liu, tutorial 

    step 3: compute the average so of all 

phrases
    classify the review as positive if average so is 

positive, negative otherwise. 

    final classification accuracy:

    automobiles - 84%
    banks - 80%
    movies - 65.83 
    travel destinations - 70.53%

bing liu, tutorial 

51

52

   26

supervised learning (pang et al, 2002)

    directly apply supervised learning techniques to 

classify reviews into positive and negative. 
    like a text classification problem

    three classification techniques were tried:

    na  ve bayes
    maximum id178
    support vector machines

    pre-processing: 

    features: negation tag, unigram (single words), 

bigram, pos tag, position.

bing liu, tutorial 

supervised learning

    training and test data

    movie reviews with star ratings

    4-5 stars as positive
    1-2 stars as negative
    neutral is ignored. 
    id166 gives the best classification accuracy 

based on balance training data
    83% 
    features: unigrams (bag of individual words)

bing liu, tutorial 

53

54

   27

features for supervised learning

    the problem has been studied by numerous 

researchers subsequently
    probably the most extensive studied problem
    including domain adaption and cross-lingual, etc. 

    key: feature engineering. a large set of features 

have been tried by researchers. e.g., 
    terms frequency and different ir weighting schemes
    part of speech (pos) tags
    opinion words and phrases
    negations
    syntactic dependency

bing liu, tutorial 

55

a large number of related papers

    bickerstaffe and zukerman (2010) used a hierarchical 

multi-classifier considering inter-class similarity

    burfoot, bird and baldwin (2011) sentiment-classified 

congressional floor debates

    cui et al. (2006) evaluated some sentiment classification 

algorithms

    das and chen (2001) extracted market sentiment from 

stock message boards

    dasgupta and ng (2009) used semi-supervised learning 
    dave, lawrence & pennock (2003) designed a custom 

function for classification

    gamon (2004) classified customer feedback data

bing liu, tutorial 

56

   28

a large number of related papers

    goldberg and zhu (2006) used semi-supervised learning. 
    kim, li and lee (2009) and paltoglou and thelwall (2010) 

studied different ir term weighting schemes

    li et al (2010) made use of different polarity shifting.
    li, huang, zhou and lee (2010) used personal (i, we) and 

impersonal (they, it, this product) sentences to help

    maas et al (2011) used word vectors which are latent 

aspects of the words.

    mullen and collier (2004) used pmi, syntactic relations 

and other attributes with id166. 

    nakagawa, inui and kurohashi (2010) used dependency 

relations and crf.

bing liu, tutorial 

57

a large number of related papers

    ng, dasgupta and arifin (2006) identified reviews and 

classified sentiments of reviews

    pang and lee (2004) used minimum cuts
    qiu, zhang, hu and zhao (2009) proposed a lexicon-

based and self-supervision approach 

    tong (2001) used a set of domain specific phrases
    yessenalina, choi and cardie (2010) automatically 
generated annotator rationales to help classification

    yessenalina, yue and cardie (2010) found subjective 

sentences and then used them for model building

    zhou, chen and wang (2010) used semi-supervised and 

active learning

bing liu, tutorial 

58

   29

review rating prediction

    apart from classification of positive or negative 

sentiments, 
    research has also been done to predict the rating 
scores (e.g., 1   5 stars) of reviews (pang and lee, 
2005; liu and seneff 2009; qu, ifrim and weikum
2010; long, zhang and zhu, 2010).

    training and testing are reviews with star ratings. 

    formulation: the problem is formulated as 

regression since the rating scores are ordinal.

    again, feature engineering and model building. 

bing liu, tutorial 

59

id20 (id21)
    sentiment classification is sensitive to the domain 

of the training data. 
    a classifier trained using reviews from one domain often 

performs poorly in another domain. 
    words and even language constructs used in different 
domains for expressing opinions can be quite different. 

    same word in one domain may mean positive but negative 

in another, e.g.,    this vacuum cleaner really sucks.    

    existing research has used labeled data from one domain 

and unlabeled data from the target domain and general 
opinion words for learning (aue and gamon 2005; blitzer et al 
2007; yang et al 2006; pan et al 2010; wu, tan and cheng 2009; 
bollegala, weir and carroll 2011; he, lin and alani 2011).

bing liu, tutorial 

60

   30

cross-lingual sentiment classification

    useful in the following scenarios: 

    e.g., there are many english sentiment corpora, but for 

other languages (e.g. chinese), the annotated 
sentiment corpora may be limited. 

    utilizing english corpora for chinese sentiment 

classification can relieve the labeling burden.

    main approach: use available language corpora to train 

sentiment classifiers for the target language data. 
machine translation is typically employed 
    (banea et al 2008; wan 2009; wei and pal 2010; kim et al. 2010; 
guo et al 2010; mihalcea & wiebe 2010; boyd-graber and resnik
2010; banea et al 2010; duh, fujino & nagata 2011; lu et al 2011)

bing liu, tutorial 

61

roadmap

    id31 problem
    document sentiment classification
    sentence subjectivity & sentiment classification
    aspect-based id31
    aspect-based opinion summarization
    sentiment lexicon generation
    mining comparative opinions
    some other problems
    opinion spam detection 
    utility or helpfulness of reviews
    summary

bing liu, tutorial 

62

   31

subjectivity classification

    document-level sentiment classification is too coarse 

for most applications. 

    we now move to the sentence level. 
    much of the early work on sentence level analysis 

focuses on identifying subjective sentences.

    subjectivity classification: classify a sentence into 

one of the two classes (wiebe et al 1999)
    objective and subjective. 

    most techniques use supervised learning. 

    e.g., a na  ve bayesian classifier (wiebe et al. 1999).

bing liu, tutorial 

sentence id31

    usually consist of two steps

    subjectivity classification

    to identify subjective sentences

    sentiment classification of subjective sentences

    into two classes, positive and negative

    but bear in mind

    many objective sentences can imply sentiments
    many subjective sentences do not express 

positive or negative sentiments/opinions
    e.g.,   i believe he went home yesterday.   

bing liu, tutorial 

63

64

   32

as an intermediate step 

    we do not use the quintuple (e, a, so, h, t) to 

define the problem here because 
    sentence classification is an intermediate step. 

    knowing that some sentences have positive or 

negative opinions are not sufficient. 

    however, it helps 

    filter out sentences with no opinions (mostly)
    determine (to some extend) if sentiments about entities 

and their aspects are positive or negative.
    but not enough

bing liu, tutorial 

65

assumption

    assumption: each sentence is written by a 

single person and expresses a single positive 
or negative opinion/sentiment. 

    true for simple sentences, e.g., 

       i like this car    

    but not true for compound and    complex    

sentences, e.g., 
       i like the picture quality but battery life sucks.    
       apple is doing very well in this lousy economy.   

bing liu, tutorial 

66

   33

subjectivity classification using patterns 
(rilloff  and wiebe, 2003)

    a id64 approach.

    a high precision classifier is first used to automatically 

identify some subjective and objective sentences.
    two high precision (but low recall) classifiers are used, 

    a high precision subjective classifier
    a high precision objective classifier
    based on manually collected lexical items, single words and n-

grams, which are good subjective clues.

    a set of patterns are then learned from these identified 

subjective and objective sentences. 
    syntactic templates are provided to restrict the kinds of patterns 

to be discovered, e.g., <subj> passive-verb.

    the learned patterns are then used to extract more subject 

and objective sentences (the process can be repeated). 

bing liu, tutorial 

67

subjectivity and sentiment classification
(yu and hazivassiloglou, 2003)

    subjective sentence identification: a few methods 

were tried, e.g., 
    sentence similarity.
    na  ve bayesian classification.

    sentiment classification (positive, negative or neutral)

(also called polarity): it uses a similar method to
(turney, 2002), but 
    with more seed words (rather than two) and based on log-

likelihood ratio (llr). 

    for classification of each word, it takes the average of llr 
scores of words in the sentence and use cutoffs to decide 
positive, negative or neutral. 

bing liu, tutorial 

68

   34

segmentation and classification

    since a single sentence may contain multiple 

opinions and subjective and factual clauses

    a study of automatic clause sentiment 

classification was presented in (wilson et al 2004)
    to classify clauses of every sentence by the strength 

of opinions being expressed in individual clauses, 
down to four levels
    neutral, low, medium, and high

    clause-level may not be sufficient 

       apple is doing very well in this lousy economy.   

bing liu, tutorial 

69

some other related work

    abdul-mageed, diab and korayem (2011) carried out 
subjectivity and id31 of arabic sentences
    alm (2011) analyzed subjectivity research motivations, 

applications, characteristics, etc

    barbosa and feng (2010) and davidov, tsur and rappoport 

(2010) performed twitter subjectivity and sentiment 
classification using many features, hashtags, and smileys
    eguchi and lavrendo (2006) studied sentiment sentence 

retrieval

    gamon et al. (2005) used semi-supervised learning
    hassan, qazvinian, radev (2010) found attitude sentences 
    kim and hovy (2004) summed up orientations of opinion words 

in a sentence (or within some word window). 

    hatzivassiloglou & wiebe (2000) considered gradable adjectives

bing liu, tutorial 

70

   35

some other related work

    johansson and moschitti (2011) extracted opinion expressions 

and sentiments

    joshi and penstein-rose (2009) used dependency triples with 

   back-off    using pos tags rather than words 

    kim and hovy (2006a) automatically identified pro and con 

    kim and hovy (2006b) identified judgment opinions
    kim and hovy (2007) mined predictive opinions in election 

reasons

postings

    kim, li and lee (2010) compared subjectivity analysis tools
    mcdonald et al (2007) performed sentence to document 

sentiment classification

    mukund and srihari (2010) performed subjectivity classification 

with co-training

bing liu, tutorial 

71

some other related work

    nasukawa and yi (2003) captured favorability
    nigam and hurst (2005) classified subjective and topic 

    tackstrom & mcdonald (2011) performed sentence sentiment 

sentences

classification

    wiebe et al (2004) learned subjective language
    wiebe and riloff (2005) used semi-supervised learning with a 

initial training set identified by some strong patterns

    wiebe and mihalcea (2006) studied word sense and subjectivity
    wilson, wiebe and hwa (2006) recognized strong and weak 

opinion clauses

    wilson et al. (2004, 2005) found strength of sentiments/opinions 

in clauses

bing liu, tutorial 

72

   36

roadmap

    id31 problem
    document sentiment classification
    sentence subjectivity & sentiment classification
    aspect-based id31
    aspect-based opinion summarization
    sentiment lexicon generation
    mining comparative opinions
    some other problems
    opinion spam detection 
    utility or helpfulness of reviews
    summary

bing liu, tutorial 

73

we need to go further

    sentiment classification at both the document 
and sentence (or clause) levels are useful, but 
    they do not find what people liked and disliked.

    they do not identify the targets of opinions, i.e., 

    entities and their aspects 
    without knowing targets, opinions are of limited use. 

    we need to go to the entity and aspect level.

    aspect-based opinion mining and summarization (hu 

and liu 2004). 

    we thus need the full opinion definition.

bing liu, tutorial 

74

   37

recall an opinion is a quintuple

    an opinion is a quintuple 

(ej, ajk, soijkl, hi, tl),

where 
    ej is a target entity.
    ajk is an aspect/feature of the entity ej.
    soijkl is the sentiment value of the opinion of the 

opinion holder hi on feature ajk of entity ej at time tl. 
soijkl is +ve, -ve, or neu, or a more granular rating. 

    hi is an opinion holder. 
    tl is the time when the opinion is expressed. 

bing liu, tutorial 

75

aspect-based id31

    much of the research is based on online reviews
    for reviews, aspect-based id31   
is easier because the entity (i.e., product name) 
is usually known
    reviewers simply express positive and negative 

opinions on different aspects of the entity. 

    for blogs, forum discussions, etc., it is harder:

    both entity and aspects of entity are unknown, 
    there may also be many comparisons, and 
    there is also a lot of irrelevant information. 

bing liu, tutorial 

76

   38

find entities (entity set expansion)

    although similar, it is somewhat different from the 

traditional id39 (ner). 

    e.g., one wants to study opinions on phones

    given motorola and nokia, find all phone brands 

and models in a corpus, e.g., samsung, moto, 

    formulation: given a set q of seed entities of class 

c, and a set d of candidate entities, we wish to 
determine which of the entities in d belong to c. 
    a classification problem. it needs a binary decision for each 

entity in d (belonging to c or not)

    but it   s often solved as a ranking problem

bing liu, tutorial 

77

some methods (li et al 2010, zhang and liu, 2011)

    distributional similarity: this is the traditional 

method used in nlp. it compares the surrounding 
text of candidates using cosine or pmi. 
    it performs poorly. 

    pu learning: learning from positive and unlabeled 

examples. 
    s-em algorithm (liu et al. 2002)

    bayesian sets: we extended the method given in 

(ghahramani and heller, 2005). 

bing liu, tutorial 

78

   39

aspect extraction

    goal: given an opinion corpus, extract all aspects
    a frequency-based approach (hu and liu, 2004): 

nouns (nn) that are frequently talked about are 
likely to be true aspects (called frequent aspects) . 

    why the frequency based approach? 

    different reviewers tell different stories (irrelevant)
    when product aspects/features are discussed, the words 

they use converge. 

    they are the main aspects. 

    sequential/association pattern mining finds 

frequent nouns and noun phrases.

bing liu, tutorial 

79

an example review

great camera., jun 3, 2004 
reviewer: jprice174 from atlanta, ga.

i did a lot of research last year before i bought this
camera... it kinda hurt to leave behind my beloved
nikon 35mm slr, but i was going to italy, and i
needed something smaller, and digital.
the pictures coming out of this camera are amazing.
the 'auto' feature takes great pictures most of the
time. and with digital, you're not wasting film.    

   .

bing liu, tutorial 

80

   40

infrequent aspect extraction

    to improve recall due to loss of infrequent 

aspects. it uses opinions words to extract them

    key idea: opinions have targets, i.e., opinion 

words are used to modify aspects and entities.
       the pictures are absolutely amazing.   
       this is an amazing software.   

    the modifying relation was approximated with the 

nearest noun to the opinion word. 

    the idea was generalized to dependency in (zhuang 

et al 2006) and double propagation in (qiu et al 2009).
    it has been used in many papers and practical systems

bing liu, tutorial 

81

using part-of relationship and the web
(popescu and etzioni, 2005)

    improved (hu and liu, 2004) by removing those 

frequent noun phrases that may not be aspects: 
better precision (a small drop in recall). 

    it identifies part-of relationship

    each noun phrase is given a pointwise mutual information 

score between the phrase and part discriminators
associated with the product class, e.g., a scanner class. 
    e.g.,    of scanner   ,    scanner has   , etc, which are used to 

find parts of scanners by searching on the web:

pmi

da
,(

)

   

hits
(
hits
a
)(

da
   
hits

)
d
)(

,

bing liu, tutorial 

82

   41

extract aspects using dp (qiu et al. 2009; 2011)

    a double propagation (dp) approach proposed
    based on the definition earlier, an opinion should 

have a target, entity or aspect. 

    use dependency of opinions & aspects to extract 

both aspects & opinion words.
    knowing one helps find the other.
    e.g.,    the rooms are spacious   

    it extracts both aspects and opinion words. 

    a domain independent method. 

bing liu, tutorial 

83

the dp method

    dp is a id64 method 

    input: a set of seed opinion words, 
    no aspect seeds needed

    based on dependency grammar (tesniere 1959). 

       this phone has good screen   

bing liu, tutorial 

84

   42

rules from dependency grammar

bing liu, tutorial 

85

explicit and implicit aspects 
(hu and liu 2004)

    explicit aspects: aspects explicitly mentioned as 

nouns or noun phrases in a sentence
    the picture quality is of this phone is great. 

    implicit aspects: aspects not explicitly mentioned 

in a sentence but are implied
       this car is so expensive.   
       this phone will not easily fit in a pocket.
       included 16mb is stingy   

    not much work has been done on mining or 

mapping implicit aspects. 

bing liu, tutorial 

86

   43

implicit aspect mapping

    there are many types of implicit aspect 
expressions. adjectives and adverbs are 
perhaps the most common type. 
    most adjectives modify or describe some specific 

attributes of entities.

       expensive        aspect    price,       beautiful        aspect 

   appearance   ,    heavy        aspect    weight   

    although manual mapping is possible, in 

different contexts, the meaning can be different. 
    e.g.,    the computation is expensive   . 

bing liu, tutorial 

87

a mutual reinforcement method 
(su et al. 2009)

    it proposed an unsupervised approach which 
exploits the mutual reinforcement relationship 
between aspects and opinion words. 
    specifically, it uses the co-occurrence of aspect and 

opinion word pair in a sentence.

    the algorithm iteratively clusters the set of aspects 

and the set of opinion words separately, 
    but before id91 each set, id91 results of the 

other set is used to update the pairwise weight of the set. 

    the model is based on a bipartite graph. 

bing liu, tutorial 

88

   44

other papers on aspect extraction

we will discuss id96 based methods later. 
    carvalho et al (2011) annotated political debates with 

aspects and others.

    choi and cardie (2010) used a crf based approach. 
    jin and ho (2009) proposed a id48-based method
    jakob and gurevych (2010) used anaphora (or 

coreference) resolution to help find aspects that are 
mentioned in previous sentences but are referred to as 
pronouns in the next sentences. 
    e.g.,    i took a few pictures yesterday. they look great.    
    there is almost no improvement with id2, higher 

recall but lower precision. 

bing liu, tutorial 

89

other papers on aspect extraction

    jakob and gurevych (2010) used crf to train on review 

sentences from different domains for a more domain 
independent extraction. a set of domain independent 
features were used, e.g. tokens, pos tags, dependency, 
word distance, and opinion sentences.

    kobayashi et al (2006) extracted subject-attribute-value
    kobayashi et al (2007) extracted aspect-evaluation and 

aspect-of relations using a tree-based classifier with 
different features. 

    ku et al. (2006a, 2006b) performed the extraction from 

chinese reviews and news. 

bing liu, tutorial 

90

   45

other papers on aspect extraction

    li et al (coling-2010) integrated skip-crf and tree-crf 

to extract aspects and opinions. it was able to exploit 
structure features

    long, zhang and zhu (2010) extracted aspects (nouns) 

based on frequency and the information distance, and 
dependent words (adjectives). these words are then 
used to select reviews which discuss an aspect most. 

    ma and wan (2010) used centering theory for extraction 
in news comments. it also exploited aspects in the news 
title and contents. 

    meng and wang (2009) extracted aspects from product 

specifications, which are usually structured data. 

bing liu, tutorial 

91

other papers on aspect extraction

    scaffidi et al (2007) extracted frequent nouns and noun 
phrases but compare their frequency in a review corpus 
with their occurrence rates in generic english to identify 
true aspects

    somasundaran and wiebe (2009) also used syntactic 

dependency for aspect and opinion extraction. 

    toprak, jakob and gurevych (2010) designed a 

comprehensive annotation scheme for aspect-based 
opinion annotation. earlier annotations are partial and 
mainly for individual papers. 

    yi et al (2003) used language models to extract product 

features.  

bing liu, tutorial 

92

   46

other papers on aspect extraction

    yu et al (2011) ranked aspects by considering their 

frequency and contribution to the overall review rating
    zhu et al (cikm-2009) used a method for finding multi-

word terms, called cvalue, to find aspects. 
    the method also segments a sentence with multiple aspects.

bing liu, tutorial 

93

identify aspect synonyms (carenini et al 2005) 

    once aspect expressions are discovered, group 

them into aspect categories.
    e.g., power usage and battery life are the same. 

    it proposed a method based on some similarity 

metrics, but it needs a taxonomy of aspects. 
    the system merges each discovered aspect to a 

aspect node in the taxonomy. 

    similarity metrics: string similarity, synonyms and 

other distances measured using id138. 

    many ideas in web information integration are 

applicable.

bing liu, tutorial 

94

   47

multilevel latent categorization 
(guo et al 2009)

    this method performs multilevel latent semantic 

analysis to group aspects expressions. 
    at the first level, all the words in aspect expressions
are grouped into a set of concepts using lda. the 
results are used to build latent topic structures for 
aspect expressions, e.g., 
    touch screen: topic-1, topic-2 

    at the second level, aspect expressions are grouped 

by lda again according to 
    their latent topic structures produced from level 1 and 
    context snippets in reviews.

bing liu, tutorial 

95

group aspect synonyms (zhai et al. 2011a, b)

    a variety of information/similarities are used to 

cluster aspect expressions into aspect 
categories.
    lexical similarity based on id138
    distributional information (surrounding words context)
    syntactical constraints (sharing words, in the same sentence)
    two unsupervised learning methods were used: 

    id91: em-based.
    constrained id96: constrained-lda

    by intervening id150.

bing liu, tutorial 

96

   48

the em method

    id138 similarity

    em-based probabilistic id91 

bing liu, tutorial 

97

aspect sentiment classification

    for each aspect, identify the sentiment or opinion 

expressed on it. 

    work based on sentences, but also consider,

    a sentence can have multiple aspects with different opinions. 
    e.g., the battery life and picture quality are great (+), but the 

view founder is small (-).  

    almost all approaches make use of opinion words and

phrases. but notice:
    some opinion words have context independent orientations, 

e.g.,    good    and    bad    (almost) 

    some other words have context dependent orientations, e.g., 

   small    and    sucks    (+ve for vacuum cleaner)

bing liu, tutorial 

98

   49

some approaches
    supervised learning

    sentence level classification can be used, but    
    need to consider target and thus to segment a 

sentence (e.g., jiang et al. 2011)

    lexicon-based approach (ding, liu and yu, 2008)

    need parsing to deal with: simple sentences, compound 

sentences, comparative sentences, conditional 
sentences, questions; different verb tenses, etc.
    negation (not), contrary (but), comparisons, etc. 
    a large opinion lexicon, context dependency, etc.
    easy:    apple is doing well in this bad economy.    

bing liu, tutorial 

99

a lexicon-based method (ding, liu and yu 2008)
    input: a set of opinion words and phrases. a pair (a, s), 

where a is an aspect and s is a sentence that contains a. 

    output: whether the opinion on a in s is +ve, -ve, or neutral. 
    two steps: 

    step 1: split the sentence if needed based on but words 

(but, except that, etc). 

    step 2: work on the segment sf containing a. let the set of 
opinion words in sf be w1, .., wn. sum up their orientations 
(1, -1, 0), and assign the orientation to (a, s) accordingly. 

       

n
i

1

ow
.
i
awd
(
),

i

where wi.o is the opinion orientation of wi. d(wi, a) is the 
distance from a to wi.

bing liu, tutorial 

100

   50

sentiment shifters (e.g., polanyi and zaenen 2004)

    sentiment/opinion shifters (also called 

valence shifters are words and phrases that 
can shift or change opinion orientations. 

    negation words like not, never, cannot, etc., 

are the most common type. 

    many other words and phrases can also alter 

opinion orientations. e.g., modal auxiliary 
verbs (e.g., would, should, could, etc)
       the brake could be improved.    

bing liu, tutorial 

101

sentiment shifters (contd)

    some presuppositional items also can change 

opinions, e.g., barely and hardly
       it hardly works.    (comparing to    it works   )  
    it presupposes that better was expected. 

    words like fail, omit, neglect behave similarly, 

       this camera fails to impress me.    

    sarcasm changes orientation too 

       what a great car, it did not start the first day.   

    jia, yu and meng (2009) designed some rules 
based on parsing to find the scope of negation. 

bing liu, tutorial 

102

   51

basic rules of opinions (liu, 2010)

    opinions/sentiments are governed by many 

rules, e.g.,
    opinion word or phrase, ex:    i love this car   

p 
n 

::= 
::= 

a positive opinion word or phrase 
an negative opinion word or phrase

    desirable or undesirable facts, ex:    after my wife 

and i slept on it for two weeks, i noticed a 
mountain in the middle of the mattress    

p 
n 

::= 
::= 

desirable fact
undesirable fact

bing liu, tutorial 

103

basic rules of opinions

    high, low, increased and decreased quantity of a

positive or negative potential item, ex:    the 
battery life is long.    

po 

ne 

npi 
ppi 

::=  no, low, less or decreased quantity of npi 
|     large, larger, or increased quantity of ppi 
::=  no, low, less, or decreased quantity of ppi
|     large, larger, or increased quantity of npi
::=  a negative potential item
::=  a positive potential item

bing liu, tutorial 

104

   52

basic rules of opinions

    decreased and increased quantity of an 

opinionated item, ex:    this drug reduced my pain 
significantly.   

po 

ne 

::=   less or decreased n 
|      more or increased p
::=   less or decreased p
|      more or increased n 

    deviation from the desired value range:    this drug 

increased my blood pressure to 200.   

po 
ne 

::=  within the desired value range 
::=  above or below the desired value range 

bing liu, tutorial 

105

basic rules of opinions

    producing and consuming resources and wastes, ex: 

   this washer uses a lot of water   

po 

ne 

::=  produce a large quantity of or more resource
|     produce no, little or less waste
|     consume no, little or less resource
|     consume a large quantity of or more waste
::=  produce no, little or less resource 
|     produce some or more waste
|     consume a large quantity of or more resource
|     consume no, little or less waste

bing liu, tutorial 

106

   53

sentiment ontology tree (wei and gulla, 2010)

    recall in the definition of opinions, we simplified 
the tree structure to two levels (entity & aspects). 

    this paper uses a full tree ontology to denote 

the relationships of aspects of a product. 

bing liu, tutorial 

107

sentiment ontology tree (contd)

    the leaves of the tree are positive or negative 

sentiments. 

    it then uses a hierarchical classification model to 

learn to assign an sentiment to each node, 
which is reflected as a child leaf node. 
    hierarchical classifier is useful here because it 

considers parents when classifying children. 

    however, the ontology for each product has to 

be built manually.

bing liu, tutorial 

108

   54

aspect-sentiment statistical models
    this direction of research is mainly based on 

topic models: 
    plsa: probabilistic latent semantic analysis (hofmann 1999) 
    lda: id44 (blei, ng & jordan, 2003; 

griffiths & steyvers, 2003; 2004) 

    topic models:

    documents are mixtures of topics
    a topic is a id203 distribution over words. 

    a topic model is a document generative model
    it specifies a simple probabilistic procedure by which 

documents can be generated. 

bing liu, tutorial 

109

aspect-sentiment model (mei et al 2007)

    this model is based on plsa (hofmann, 1999). 
    it builds a topic (aspect) model, a positive 

sentiment model, and a negative sentiment 
model. 

    a training data is used to build the initial models. 

    training data: topic queries and associated positive 

and negative sentences about the topics. 

    the learned models are then used as priors to 

build the final models on the target data. 
    solution: log likelihood and em algorithm

bing liu, tutorial 

110

   55

multi-grain lda to extract aspects 
(titov and mcdonald, 2008a, 2008b)
    unlike a diverse document set used for traditional 
id96. all reviews for a product talk about 
the same topics/aspects. it makes applying plsa or 
lda in the traditional way problematic. 

    multi-grain lda (mg-lda) models global topics and 

local topics (titov and mcdonald, 2008a). 
    global topics are entities (based on reviews)
    local topics are aspects (based on local context, sliding 

windows of review sentences)

    mg-lda was extended to mas model to give aspect 

rating (titov and mcdonald, 2008b). 

bing liu, tutorial 

111

aspect-rating of short text (lu et al 2009)

    this work makes use of short phrases, head 

terms (wh) and their modifiers (wm), i.e.
    (wm, wh)
    e.g., great shipping, excellent seller

    objective: (1) extract aspects and (2) compute 

their ratings in each short comment.

    it uses plsa to extract and group aspects 
    it uses existing rating for the full post to help 

determine aspect ratings. 

bing liu, tutorial 

112

   56

aspect-rating regression 
(wang, lu, and zhai, 2010)

    in this work, some seed aspects are given. its 

first step finds more aspect words using a 
heuristic id64 method.

    its regression model makes use of the review 

rating and assumes the overall review rating is a 
linear combination of its aspect ratings. 

    the problem is model as a bayesian regression 

problem. 
    it is solved using log-likelihood and em. 

bing liu, tutorial 

113

maxent-lda hybrid (zhao et al. 2010)

bing liu, tutorial 

114

   57

graphical model (plate)

    yd,s,n indicates

    background word
    aspect word, or
    opinion word

    maxent is used to 

train a model 
using training set 
       d,s,n
    xd,s,n feature vector

    ud,s,n indicates

    general or
    aspect-specific

bing liu, tutorial 

115

topic model of snippets 
(sauper, haghighi and barzilay, 2011)

    this method works on short snippets already 

extracted from reviews. 
       battery life is the best i   ve found   

    the model is a variation of lda but with 

seeds for sentiment words as priors, 
    but it also has id48 for modeling the sequence of 
words with types (aspect word, sentiment word, or 
background word).

    id136: variational technique

bing liu, tutorial 

116

   58

considering both syntax and semantics 
(lakkaraju et al. 2011)

    this work is based the composite model of 

id48-lda of griffiths et al. (2005), which 
consider both word sequence and word-bag

    it captures both syntactic structure and semantic 

dependencies (similar to the previous paper)

    a class label is used for each word to represent 
the syntactic category of the word, whether it is 
    an aspect word, 
    a sentiment word, or 
    some other category. 

bing liu, tutorial 

117

facts model
    words: wd,1,wd,2...wd,n
    hidden variables 

    class: cd,i

   

   

   

1: appect word
2: sentiment word
others

    aspect cat.: fd,i
    sentiment cat.: sd,i
    it also has more 

sophsticated models 
    cfacts: consider 

neighboring windows
    cfacts-r: consider 

ratings

bing liu, tutorial 

118

   59

about topic model based methods

    there several other similar topic model based methods 
(e.g., brody and elhadad, 2010; lu et al. 2011; jo and 
oh, 2011; lin and he 2009; liu et al, 2007; moghaddam 
et al. 2011; mukherjee and liu, 2012; ).

    these methods tend to need a large number reviews 

(10000 and more) to make it statistically stable. they are 
hard to use for most specific products, which often have 
<100 reviews.

    they also need a lot of parameter tuning. 
    the results usually are quite coarse, not precise enough 

for practical needs. 

bing liu, tutorial 

119

roadmap

    id31 problem
    document sentiment classification
    sentence subjectivity & sentiment classification
    aspect-based id31
    aspect-based opinion summarization
    sentiment lexicon generation
    mining comparative opinions
    some other problems
    opinion spam detection 
    utility or helpfulness of reviews
    summary

bing liu, tutorial 

120

   60

aspect-based opinion summarization
    a id57 problem. 

    an opinion from a single person is usually not sufficient 

for action unless from a vip (e.g., president)

    key idea: use aspects as basis for a summary 

    not done in traditional id57. 
    we have discussed the aspect-based summary 
using quintuples earlier (hu and liu 2004; liu, 2010).
    also called: structured summary 

    similar approaches are also taken in 

    (e.g., ku et al 2006; carenini, ng and paul 2006) and 
    by most id96 based methods

bing liu, tutorial 

121

text summary of opinions

    one can also generate a summary in the 

tradition fashion, e.g., producing a short text 
summary (lerman et al 2009), by extracting 
some important sentences, etc. 
    weakness: it is only qualitative but not 

quantitative. 

    one can generate sentences based on 

aspects and opinions using some templates.
    e.g., 60% of the people like the picture quality. 

bing liu, tutorial 

122

   61

select and order sentences 
(tata and di eugenio, 2010)

    if we produce summary as a list of sentences 
for each aspect and each sentiment (+ or    ), 
it is useful to 
    select a representative sentence for each group: 

it selects a sentence that mentions fewest aspects 
(the sentence is focused). 

    order the sentences: it uses an ontology to map 

sentences to the ontology nodes (domain 
concepts).

bing liu, tutorial 

123

informativeness and readability 
(nishikawa et al. 2010)

    s* is the summary

    it summarizes by 
considering both  
informativeness and 
readability. 

    it uses frequency f(.) of 

(aspect, opinion), but it is 
more like a traditional 
summary. 

    it is not quantitative. note: 

lerman et al (2009) used 
+ve/-ve proportions.

bing liu, tutorial 

124

   62

summarization using an ontology 
(lu et al. coling-2010)

    this work uses existing online ontologies of 

entities and aspects to organize opinions
    given an entity and an online ontology of the entity
    goal: generate a structured summary of opinions.

    it performs

    aspect selection to capture major opinions
    aspect ordering that is natural for human viewing
    suggest new aspects to add to ontology

bing liu, tutorial 

125

summarization using an ontology (contd)

    aspect selection

    e.g., by frequency, by opinion coverage (no 

redundancy), or by conditional id178

    ordering aspects and their corresponding 

sentences based on their appearance in their 
original posts, called coherence

bing liu, tutorial 

126

   63

some other summarization papers

    carenini, ng and pauls (2006) evaluated different 

summarization methods using human judges. 

    huang, wan and xiao (2011) generated contrast summaries of 

news. 

    kim and zhai (2009) generated contrast opinion sentence pairs.
    lerman and mcdonald (2009) generated summaries to contrast 

opinions about two different products.

    lerman, blair-goldensohn and mcdonald (2009) designed 
three summarizers and evaluated them with human raters. 

    paul, zhai and girju (2010) found opposing views. 
    park, lee and song (2011) also found opposing views
    wang and liu (2011) generated opinion summary for 

conversations. 

bing liu, tutorial 

127

roadmap

    opinion mining problem
    document sentiment classification
    sentence subjectivity & sentiment classification
    aspect-based id31
    aspect-based opinion summarization
    sentiment lexicon generation
    mining comparative opinions
    some other problems
    opinion spam detection 
    utility or helpfulness of reviews
    summary

bing liu, tutorial 

128

   64

sentiment (or opinion) lexicon

    sentiment lexicon: lists of words and expressions  
used to express people   s subjective feelings and 
sentiments/opinions.
    not just individual words, but also phrases and idioms, 

e.g.,    cost an arm and a leg   

    they are instrumental for id31.
    there seems to be endless variety of sentiment 

bearing expressions. 
    we have compiled more than 6,700 individual words.
    there are also a large number of phrases. 

bing liu, tutorial 

129

sentiment lexicon

    sentiment words or phrases (also called polar words, 

opinion bearing words, etc). e.g., 
    positive: beautiful, wonderful, good, amazing, 
    negative: bad, poor, terrible, cost an arm and a leg. 

    many of them are context dependent, not just 

application domain dependent. 

    three main ways to compile such lists:

    manual approach: not a bad idea, only an one-time effort
    corpus-based approach
    dictionary-based approach

bing liu, tutorial 

130

   65

corpus-based approaches

    rely on syntactic patterns in large corpora. 

(hazivassiloglou and mckeown, 1997; turney, 2002; yu 
and hazivassiloglou, 2003; kanayama and nasukawa, 
2006; ding, liu and yu, 2008)
    can find domain dependent orientations (positive, negative, 

or neutral). 

    (turney, 2002) and (yu and hazivassiloglou, 2003) 

are similar. 
    assign opinion orientations (polarities) to words/phrases. 
    (yu and hazivassiloglou, 2003) is slightly different from 

(turney, 2002)
    use more seed words (rather than two) and use log-

likelihood ratio (rather than pmi). 

bing liu, tutorial 

131

corpus-based approaches (contd)

    sentiment consistency: use conventions on 

connectives to identify opinion words (hazivassiloglou 
and mckeown, 1997). e.g., 
    conjunction: conjoined adjectives usually have the 

same orientation. 
    e.g.,    this car is beautiful and spacious.    (conjunction)

    and, or, but, either-or, and neither-nor have 

similar constraints.

    learning using

    log-linear model: determine if two conjoined adjectives are of 

the same or different orientations. 

    id91: produce two sets of words: positive and negative

bing liu, tutorial 

132

   66

find domain opinion words

    a similar approach was also taken in 

(kanayama and nasukawa, 2006) but for 
japanese words:
    instead of only based on intra-sentence 

sentiment consistency, the new method also 
looks at the previous and next sentence, i.e., 
inter-sentence sentiment consistency. 

    have an initial seed lexicon of positive and 

negative words. 

bing liu, tutorial 

133

context dependent opinion

    find domain opinion words is insufficient. a word 
may indicate different opinions in same domain. 
       the battery life is long    (+) and    it takes a long time 

to focus    (-).

    ding, liu and yu (2008) and ganapathibhotla and 
liu (2008) exploited sentiment consistency (both 
inter and intra sentence) based on contexts
    it finds context dependent opinions. 
    context: (adjective, aspect), e.g., (long, battery_life)
    it assigns an opinion orientation to the pair. 

bing liu, tutorial 

134

   67

the double propagation method 
(qiu et al 2009, 2011)

    the same dp method can also use dependency 

of opinions & aspects to extract new opinion 
words.

    based on dependency relations

    knowing an aspect can find the opinion word that 

modifies it
    e.g.,    the rooms are spacious   

    knowing some opinion words can find more opinion 

words
    e.g.,    the rooms are spacious and beautiful   

bing liu, tutorial 

135

opinions implied by objective terms 
(zhang and liu, 2011)

    most opinion words are adjectives and adverbs, 

e.g., good, bad, etc
    there are also many subjective and opinion verbs and 

nouns, e.g., hate (vb), love (vb), crap (nn). 

    but objective nouns can imply opinions too.

    e.g.,    after sleeping on the mattress for one month, a 

valley/body impression has formed in the middle.   
    how to discover such nouns in a domain or 

context?

bing liu, tutorial 

136

   68

the technique

    id31 to determine whether the 

context is +ve or    ve. 
    e.g.,    i saw a valley in two days, which is terrible.   
    this is a negative context. 

    statistical test to find +ve and    ve candidates.

    pruning to move those unlikely ones though 

sentiment homogeneity. 

bing liu, tutorial 

137

pruning
    for an aspect with an implied opinion, it has a 
fixed opinion, either +ve or    ve, but not both. 
    we find two direct modification relations using 

a dependency parser. 
    type 1: o     o-dep      a

    e.g.     this tv has good picture quality.   

    type 2:  o     o-dep      h       a-dep      a

    e.g.      the springs of the mattress are bad.    

    if an aspect has mixed opinions based on the 

two dependency relations, prune it. 

bing liu, tutorial 

138

   69

opinions implied by resource usage 
(zhang and liu, 2011)

    resource usage descriptions may also imply 

opinions (as mentioned in rules of opinions)
    e.g.,    this washer uses a lot of water.   

    two key roles played by resources usage:

    an important aspect of an entity, e.g., water usage. 
    imply a positive or negative opinion

    resource usages that imply opinions can often 

be described by a triple. 

(verb, quantifier, noun_term),  

    verb: uses, quantifier:    a lot of    , noun_term: water

bing liu, tutorial 

139

the proposed technique

    the proposed method is graph-based.

    stage 1: identifying some global resource verbs 

    identify and score common resource usage verbs used 

in almost any domain, e.g.,    use    and    consume   

    stage 2: discovering resource terms in each domain 

corpus 
    use a graph-based method considering occurrence 

probabilities. 

    with resource verbs identified from stage 1 as the seeds. 
    score domain specific resource usage verbs and resource 

terms. 

bing liu, tutorial 

140

   70

dictionary-based methods

    typically use id138   s synsets and hierarchies to 

acquire opinion words
    start with a small seed set of opinion words.
    bootstrap the set to search for synonyms and antonyms in 

id138 iteratively (hu and liu, 2004; kim and hovy, 2004; 
kamps et al 2004).

    use additional information (e.g., glosses) from 

id138 (andreevskaia and bergler, 2006) and learning 
(esuti and sebastiani, 2005). (dragut et al 2010) uses a set 
of rules to infer orientations. 

bing liu, tutorial 

141

semi-supervised learning 
(esuti and sebastiani, 2005) 
    use supervised learning

    given two seed sets: positive set p, negative set n
    the two seed sets are then expanded using synonym 

and antonymy relations in an online dictionary to 
generate the expanded sets p    and n   . 

    p    and n    form the training sets. 
    using all the glosses in a dictionary for each 

term in p        n    and converting them to a vector

    build a binary classifier
    tried various learners. 

bing liu, tutorial 

142

   71

multiple runs of  id64
(andreevskaia and bergler, 2006) 

    basic id64 with given seeds sets 

(adjectives) 
    first pass: seed sets are expanded using synonym, 

antonymy, and hyponyms relations in id138. 

    second pass: it goes through all id138 glosses and 
identifies the entries that contain in their definitions the 
sentiment-bearing words from the extended seed set 
and adds these head words to the corresponding 
category (+ve, -ve, neutral)

    third pass: clean up using a pos tagger to make sure 

the words are adjectives and remove contradictions. 

bing liu, tutorial 

143

multiple runs of  id64 (contd)

    each word is then assigned a fuzzy score 

reflecting the degree of certainty that the word is 
opinionated (+ve/-ve).

    the method performs multiple runs of 

id64 using non-overlapping seed sets. 
    a net overlapping score for each word is 

computed based on how many times the word is 
discovered in the runs as +ve (or    ve)

    the score is normalized based on the fuzzy 

membership. 

bing liu, tutorial 

144

   72

which approach to use?

    both corpus and dictionary based approaches 

are needed.

    dictionary usually does not give domain or 

context dependent meaning
    corpus is needed for that

    corpus-based approach is hard to find a very 

large set of opinion words
    dictionary is good for that

    in practice, corpus, dictionary and manual 

approaches are all needed. 

bing liu, tutorial 

145

some other related papers

    choi and cardie (2009) adapting a lexicon to domain specific 

need using integer id135

    du and tan (2009) and du, tan, cheng and yun (2010) 

clustered sentiment words

    hassan and radev (2010) built a word graph based on 

synonyms and then used a number of id93 to hit 
known seed words

    hassan et al. (2011) found sentiment orientations of foreign 

words. it first created a id73 network and then did 
random walk similar to the above paper.

    jijkoun, rijke and weerkamp (2010) used target and 

sentiment word relationship. similar to that in (qiu et al 2009). 

bing liu, tutorial 

146

   73

some other related papers

    kaji and kitsuregawa (2006, 2007) and velikovich et al 

(2010) used text on the web to generate lexicons.

    lu et al (2011) dealt with the same problem as (ding et al 

2008) but used various constraints in optimization.

    mohammad, dunne, and dorr, (2009) used seeds and 

thesaurus.

    rao and ravichandran (2009) used id138 and 

openoffice thesaurus and semi-supervised learning

    wu and wen (2010) found context adjectives like large and 

small by mining the web using lexico-syntactic patterns. 
they solved the same problem as (ding et al 2008) 

bing liu, tutorial 

147

roadmap

    id31 problem
    document sentiment classification
    sentence subjectivity & sentiment classification
    aspect-based id31
    aspect-based opinion summarization
    sentiment lexicon generation
    mining comparative opinions
    some other problems
    opinion spam detection 
    utility or helpfulness of reviews
    summary

bing liu, tutorial 

148

   74

comparative opinions 
(jindal and liu, 2006)

    gradable

    non-equal gradable: relations of the type greater 

or less than
    ex:    optics of camera a is better than that of camera 

b   

    equative: relations of the type equal to

    ex:    camera a and camera b both come in 7mp   
    superlative: relations of the type greater or less 

than all others
    ex:    camera a is the cheapest in market   

bing liu, tutorial 

149

analyzing comparative opinions

    objective: given an opinionated document d, 

extract comparative opinions: 

(e1, e2, a, po, h, t), 
where e1 and e2 are the entity sets being 
compared based on their shared aspects a, po is 
the preferred entity set of the opinion holder h, 
and t is the time when the comparative opinion is 
expressed. 

    note: not positive or negative opinions. 

bing liu, tutorial 

150

   75

an example

    consider the comparative sentence 

       canon   s optics is better than those of sony and 

nikon.    

    written by john in 2010. 

    the extracted comparative opinion/relation:

    ({canon}, {sony, nikon}, {optics}, 

preferred:{canon}, john, 2010)

bing liu, tutorial 

151

common comparatives

    in english, comparatives are usually formed by 

adding -er and superlatives are formed by adding 
-est to their base adjectives and adverbs

    adjectives and adverbs with two syllables or more 

and not ending in y do not form comparatives or 
superlatives by adding -er or -est. 
    instead, more, most, less, and least are used before 

such words, e.g., more beautiful. 

    irregular comparatives and superlatives, i.e., more

most, less, least, better, best, worse, worst, etc

bing liu, tutorial 

152

   76

some techniques (jindal and liu, 2006, ding et al, 2009)

    identify comparative sentences

    using class sequential rules as attributes in the 

data, and then perform

    supervised learning

    extraction of different items

    label sequential rules
    id49

    determine preferred entities (opinions)

    parsing and opinion lexicon

    (yang and ko, 2011) is similar to (jindal and liu 206)

bing liu, tutorial 

153

analysis of comparative opinions

    gradable comparative sentences can be dealt 

with almost as normal opinion sentences.
    e.g.,    optics of camera a is better than that of 

camera b   

    positive:    optics of camera a   
    negative:    optics of camera b   

    difficulty: recognize non-standard comparatives

    e.g.,    i am so happy because my new iphone is nothing 

like my old slow ugly droid.   

    ?

bing liu, tutorial 

154

   77

identifying preferred entities 
(ganapathibhotla and liu, 2008)
    the following rules can be applied

comparative negative ::=  increasing comparative n

comparative positive 

|     decreasing comparative p  
::=  increasing comparative p 
|    decreasing comparative n

    e.g.,    coke tastes better than pepsi   
       nokia phone   s battery life is longer than moto phone   

    context-dependent comparative opinion words

    using context pair: (aspect, jj/jjr)
    deciding the polarity of (battery_life, longer) in a corpus

bing liu, tutorial 

155

some other work

    bos and nissim (2006) proposed a method to extract items 
from superlative sentences, but does not study sentiments. 

    fiszman et al (2007) tried to identify which entity has more of 

a certain property in comparisons.

    li et al (2010) finds comparative questions and compared 

entities using sequence patterns. 

    yang and ko (2009, 2011) worked on korean comparative 

sentences. 

    zhang, narayanan and choudhary (2010) found comparative 

sentences based on a set of rules, and the sentences must 
also mention at least two product names explicitly or 
implicitly (comparing with the product being reviewed).

bing liu, tutorial 

156

   78

roadmap

    id31 problem
    document sentiment classification
    sentence subjectivity & sentiment classification
    aspect-based id31
    aspect-based opinion summarization
    sentiment lexicon generation
    mining comparative opinions
    some other problems
    opinion spam detection 
    utility or helpfulness of reviews
    summary

bing liu, tutorial 

157

coreference resolution: semantic level?

    coreference resolution (ding and liu, 2010)

       i bought the sharp tv a month ago. the picture 

quality is so bad. our other sony tv is much better 
than this sharp. it is also so expensive   .
       it    means    sharp    

       i bought the sharp tv a month ago. the picture 

quality is so bad. our other sony tv is much better 
than this sharp. it is also very reliable.    
       it    means    sony

    sentiment consistency. 

bing liu, tutorial 

158

   79

coreference resolution (contd)

       the picture quality of this canon camera is very 

good. it is not expensive either.    
    does    it    mean    canon camera    or    picture quality   ?

    clearly it is canon camera because picture quality cannot 

be expensive.

    commonsense knowledge, but can be discovered.

    for coreference resolution, we actually need to 

    do id31 first, and 
    mine adjective-noun associations using dependency

    finally, use supervised learning

bing liu, tutorial 

159

some interesting sentences 

       trying out google chrome because firefox 

keeps crashing.   
    the opinion about firefox is clearly negative, but 

for google chrome, there is no opinion. 

    we need to segment the sentence into clauses to 

decide that    crashing    only applies to firefox. 

       trying out    also indicates no opinion. 

    how about this

       i changed to audi because bmw is so expensive.   

bing liu, tutorial 

160

   80

some interesting sentences (contd)

    conditional sentences are hard to deal with 

(narayanan et al. 2009)
       if i can find a good camera, i will buy it.    
    but conditional sentences can have opinions

       if you are looking for a good phone, buy nokia   

    questions may or may not have opinions

    no sentiment

       are there any great perks for employees?   

    with sentiment

       any idea how to repair this lousy sony camera?   

bing liu, tutorial 

161

some interesting sentences (contd)

    sarcastic sentences

       what a great car, it stopped working in the 

second day.   

    sarcastic sentences are very common in 

political blogs, comments and discussions. 
    they make political blogs difficult to handle
    many political aspects can also be quite complex 

and hard to extract because they cannot be 
described using one or two words.  

    some initial work by (tsur, davidov, rappoport 2010)

bing liu, tutorial 

162

   81

some interesting sentences (contd)

    see these two sentences in a medical domain:
       i come to see my doctor because of severe pain in 

my stomach   

       after taking the drug, i got severe pain in my 

stomach   

    if we are interested in opinions on a drug, the 
first sentence has no opinion, but the second 
implies negative opinion on the drug. 
    some understanding seems to be needed? 

bing liu, tutorial 

163

some interesting sentences (contd)

    the following two sentences are from reviews 

in the paint domain.
       for paint_x, one coat can cover the wood color.   
       for paint_y, we need three coats to cover the 

wood color. 

    we know that paint_x is good and paint_y is 

not, but how by a system.
    do we need commonsense knowledge and 

understanding of the text?

bing liu, tutorial 

164

   82

some more interesting/hard sentences

       my goal is to have a high quality tv with decent 

sound    

       the top of the picture was much brighter than 

the bottom.   

       google steals ideas from bing, bing steals 

market shares from google.   

       when i first got the airbed a couple of weeks 

ago it was wonderful as all new things are, 
however as the weeks progressed i liked it less 
and less.   

bing liu, tutorial 

165

roadmap

    id31 problem
    document sentiment classification
    sentence subjectivity & sentiment classification
    aspect-based id31
    aspect-based opinion summarization
    sentiment lexicon generation
    mining comparative opinions
    some other problems
    opinion spam detection 
    utility or helpfulness of reviews
    summary

bing liu, tutorial 

166

   83

incentives for opinion spamming

    opinions from social media are increasingly 

used by individuals and organizations for 
    making purchase decisions
    marketing and product design
    making choices at elections

    positive opinions often mean profits and 

fames for businesses and individuals, 
    unfortunately, this gives strong incentives for people 

to game the system by posting fake opinions and 
reviews.

bing liu, tutorial 

167

opinion spam detection
(jindal and liu, 2007, 2008)

    opinion spamming refers to people giving fake 

or untruthful opinions, e.g., 
    write undeserving positive reviews for some target 

entities in order to promote them.

    write unfair or malicious negative reviews for some 
target entities in order to damage their reputations.
    opinion spamming has become a business in 

recent years. 

    increasing number of customers are wary of 
fake reviews (biased reviews, paid reviews)

bing liu, tutorial 

168

   84

problem is wide-spread 

bing liu, tutorial 

169

an example practice of review spam
belkin international, inc
    top networking and peripherals manufacturer | sales ~ $500 million in 2008
    posted an ad for writing fake reviews on amazon.com (65 cents per review)

jan 2009

bing liu, tutorial 

170

   85

is this review fake or not?

i want to make this review in order to comment on the excellent 
service that my mother and i received on the serenade of the 
seas, a cruise line for royal caribbean. there was a lot of 
things to do in the morning and afternoon portion for the 7 days 
that we were on the ship. we went to 6 different islands and saw 
some amazing sites! it was definitely worth the effort of planning 
beforehand. the dinner service was 5 star for sure. one of our 
main waiters, muhammad was one of the nicest people i have 
ever met. however, i am not one for clubbing, drinking, or 
gambling, so the nights were pretty slow for me because there 
was not much else to do. either than that, i recommend the 
serenade to anyone who is looking for excellent service, 
excellent food, and a week full of amazing day-activities!

bing liu, tutorial 

171

what about this?

the restaurant is located inside of a hotel, but do not let that 
keep you from going! the main chef, chef chad, is absolutely 
amazing! the other waiters and waitresses are very nice and 
treat their guests very respectfully with their service (i.e. 
napkins to match the clothing colors you are wearing). we 
went to aria twice in one weekend because the food was so 
fantastic. there are so many wonderful asian flavors. from 
the plating of the food, to the unique food options, to the fresh 
and amazing nan bread and the tandoori oven that you can 
watch as the food is being cooked, all is spectacular. the 
atmosphere and the space are great as well. i just wished we 
lived closer and could dine there more frequently because it is 
quite expensive.

bing liu, tutorial 

172

   86

one more?

cameraworld is on my list of top photography/video equipment e-
tailers. their reps answer phones from early in the morning through late 
at night. the service is also first rate and the staff there is 
knowledgeable on the products they sell. prices are competitive, 
although not always the best, but they do price match should you find it 
cheaper. 
i have noticed that some of the products they carry, only a select few 
that are rare, are not listed on the website even though cameraworld 
either stocks or is willing to get for you. this is only a minor 
inconvenience, and isn't really a bother to me as i normally have other 
questions that i can get answered when calling.
they also have a "bonus bucks" program in which online purchases 
receive a percentage credit towards a future purchase. i have yet to 
make a purchase online (always phoned in orders), so no experience 
with the program.

bing liu, tutorial 

173

detecting fake review is hard

    different from web spam and email
    web spam: link spam and content spam
    email spam: mostly commercial ads

    for such spam, when you see it, you know it. 

    easy to find training data for model building
    easy to evaluate the resulting models

    fake reviews (opinion spam in general)

    no link or content spam
    almost no commercial ads

bing liu, tutorial 

174

   87

detecting fake review is hard (contd)

    fake reviews

    when you see it, you do not know it.
    can only be reliably identified by their authors!

    if one writes carefully, there is almost no way 

to identify them by their content. 

    logically impossible! 

    i write a truthful 5-star review for a good hotel.
    but i post the review to another hotel that i want 

to promote.

bing liu, tutorial 

175

a study of amazon reviews

    5.8mil reviews, 1.2mil products and 2.1mil 

    june 2006

reviewers.

    a review has 8 parts

    <product id> 
    <reviewer id> 
    <rating> 
    <date> 
    <review title> <review body> 
    <number of helpful feedbacks> <number of 

feedbacks>

bing liu, tutorial 

176

   88

log-log plot
(jindal and liu, 2008)

   fig. 1 reviews and reviewers

   fig. 2 reviews and products

bing liu, tutorial 

   fig. 3 reviews and feedbacks177

star ratings vs. percent of reviews

bing liu, tutorial 

178

   89

categorization of opinion spam 
(jindal and liu 2008)

    type 1 (fake reviews)

ex:

    type 2 (reviews on brands only)

ex:    i don   t trust hp and never bought anything from them   

    type 3 (non-reviews)

    advertisements

ex:    detailed product specs: 802.11g, imr compliant,       

      buy this product at: compuplus.com   

    other non-reviews
ex:    what port is it for   

   the other review is too funny   
   go eagles go   

bing liu, tutorial 

179

fake reviews vs. product quality

harmful regions

bing liu, tutorial 

180

   90

type of spammers 

    individual spammers: 

    the spammer does not work with anyone. he/she 
just writes fake reviews him/herself using a single 
user-id, e.g., the author of a book. 

    group spammers

    a group of spammers (persons) works in collusion 
    a single person registers multiple user-ids (called 

sock puppetting)

bing liu, tutorial 

181

type of data and clues

    review content: 

    the actual text content of each review, linguistic 

features and style features

    meta-data about each reviewer: 

    star rating, user-id, 
    time when a review was posted, and time taken to 

write/post the review, 

    host ip address and mac address 
    geo-location of the reviewer 
    sequence of clicks at the review site

bing liu, tutorial 

182

   91

type of data and clues (contd)

    product information: 

    information about the entity being reviewed, e.g.,

    the product description, 
    sales volume
    sales rank. 

    public data vs. site private (internal) data

    site private data, very useful
    but hard to get by outsiders

bing liu, tutorial 

183

spam detection (jindal and liu 2008)

    type 2 and type 3 spam reviews are 

relatively easy to detect
    supervised learning, e.g., id28
    it performs quite well, and not discuss it further. 

    type 1 spam (fake) reviews

    manual labeling is extremely hard
    propose to use duplicate and near-duplicate 

reviews as positive training data

bing liu, tutorial 

184

   92

four types of duplicates

1. same userid, same product
2. different userid, same product
3. same userid, different products
4. different userid, different products

    the last three types are very likely to be fake!

bing liu, tutorial 

185

supervised model building

    id28

    training: duplicates as spam reviews (positive) 

and the rest as non-spam reviews (negative)

    use the follow features (clues)
    review centric features (content)

    about reviews (contents (id165s), ratings, etc)

    reviewer centric features

    about reviewers (different unusual behaviors, etc)

    product centric features

    features about products reviewed (sale rank, etc)

bing liu, tutorial 

186

   93

predictive power of duplicates
    representative of all kinds of spam
    only 3% duplicates accidental
    duplicates as positive examples, rest of the reviews as 

negative examples

    reasonable predictive power
    maybe we can use duplicates as type 1 spam reviews(?)

bing liu, tutorial 

187

tentative classification results

    negative outlier reviews tend to be heavily 

spammed

    those reviews that are the only reviews of 

products are likely to be spammed

    top-ranked reviewers are more likely to be 

spammers

    spam reviews can get good helpful feedbacks 
and non-spam reviews can get bad feedbacks

       

bing liu, tutorial 

188

   94

other supervised methods

    li et al. (2011) built a model similar to that in 

(jindal and liu 2008), but 
    also use sentiment and some other features 
    manually labeled data 

    ott et al (2011) also used supervised learning. 

    use mechanical turk to write fake reviews
    use id165s as features

    yoo and gretzel (2009) also studied deceptive 

reviews. 

bing liu, tutorial 

189

finding unexpected reviewer behavior

    move    behind the scenes    

    to uncover the    secrets    of reviewers by profiling 

them based on their posted reviews and behaviors
    lim et al (2010) and nitin et al (2010) analyze 

the behavior of reviewers
    identifying unusual review patterns which may 

indicate suspicious behaviors of reviewers. 

    the problem is formulated as finding 

unexpected rules and rule groups.

bing liu, tutorial 

190

   95

spam behavior models (lim et al 2010)

    several unusual reviewer behavior models 

were identified. 
    targeting products
    targeting groups
    general rating deviation
    early rating deviation

    their scores for each reviewer are then 

combined to produce the final spam score.

    ranking and user evaluation

bing liu, tutorial 

191

finding unexpected rules (jindal, liu, lim 2010)

    for example, if a reviewer wrote all positive 

reviews on products of a brand but all negative 
reviews on a competing brand    

    finding unexpected rules, 

    data: reviewer-id, brand-id, product-id, and a class.
    mining: class association rule mining
    finding unexpected rules and rule groups, i.e., 

showing atypical behaviors of reviewers. 

rule1:   reviewer-1, brand-1 -> positive (confid=100%)
rule2:   reviewer-1, brand-2 -> negative (confid=100%)

bing liu, tutorial 

192

   96

the example (cont.)

bing liu, tutorial 

193

confidence unexpectedness

rule: reviewer-1, brand-1     positive [sup = 0.1, conf = 1]
    if we find that on average reviewers give 

brand-1 only 20% positive reviews 
(expectation), then reviewer-1 is quite 
unexpected.
cu
c

pr(

   

   

v

)

(

i

jk

i

|

v

jk

))

c
i
))

|

c

v
)
   
jk
e
(pr(
vc
pr(
|
)
i
jk
c
pr(
       
m
r

e
(pr(
c
v
|
jk
i
vc
pr(
|
)
i
gh
v
c
|
)
pr(
jk
c
pr(
)

1

r

r

r

|

v

gh

)

e

(pr(

vc
|
i

jk

,

v

gh

))

   

pr(

c

)

i

bing liu, tutorial 

194

   97

support unexpectedness

rule: reviewer-1, product-1 -> positive [sup = 5]
    each reviewer should write only one review 

on a product and give it a positive or negative 
rating (expectation). 

    this unexpectedness can detect those 
reviewers who review the same product 
multiple times, which is unexpected. 
    these reviewers are likely to be spammers.
    can be defined probabilistically as well.

bing liu, tutorial 

195

detection using review graph 
(wang et al., 2011)

    this study was based on a snapshot of all 

reviews from resellerratings.com, which were 
crawled on oct. 6th, 2010. 
    343603 reviewers, 408470 reviews, 14561 store

    form a heterogeneous review graph with 

three types of nodes, 
    reviewers, reviews and stores, 
    the graph captures their relationships and was 

used model spamming clues.

bing liu, tutorial 

196

   98

the relationships 

    three concepts were defined and computed,

    trustiness of reviewers, 
    honesty of reviews, and 
    reliability of stores. 

    a reviewer is more trustworthy if he/she has 

written more honesty reviews

    a store is more reliable if it has more positive 

reviews from trustworthy reviewers 

    a review is more honest if it is supported by 

many other honest reviews.

bing liu, tutorial 

197

definitions and equations

    trustiness of a reviewer r

    honesty of a review v

    reliability of store s

bing liu, tutorial 

198

   99

detecting group spam (mukherjee et al 2011, 2012) 

    a group of people (could be a single person with 
multiple ids) work together to promote a product 
or to demote a product. 

    such spam can be very damaging as

    they can take total control of sentiment on a product

    the algorithm has three steps

    frequent pattern mining: find groups of people who 

reviewed a number of products together.
    a set of feature indicators are identified
    ranking is performed using a relational model  

bing liu, tutorial 

199

big john   s profile

bing liu, tutorial 

200

   100

cletus    profile

bing liu, tutorial 

201

jake   s profile

bing liu, tutorial 

202

   101

finding candidate groups

    frequent itemset mining

    items     reviewer ids (rids). 
    transaction     set of rids for a product

    frequent itemsets give us 

       reviewer groups    that reviewed multiple products 

together

    using reviews of manufactured products, 

    found 7052 candidate groups 
    minimum support count = 3

bing liu, tutorial 

203

a set of clues (or features)  

    group time window (gtw)
    group deviation (gd)
    group content similarity (gcs)
    group member content similarity (gmcs)
    group early time frame (getf)
    group size ratio (gsr)
    group size (gs)
    group support count (gsup)

bing liu, tutorial 

204

   102

a relational model and algorithm

algorithm: gsrank
input: weight matrices wpg, wmp, and wgm
output: ranked list of candidate spam groups
1. initialize vg
2. iterate:

0     [0.5]|g| ; t   1;

(t-1) ; vm     wmp vp ;
t vg ;

i. vp     wpg vg
ii. vg     wgm vm ; vm     wgm
iii. vp     wmp
iv. vg
(t)     vg
(t)     vg

t vm ; vg
(t)     wpg
(t) / || vg
(t)||1 ;
(t-1) ||    <   

until || vg

t vp ;

3 output the ranked list of groups, vg*

bing liu, tutorial 

205

roadmap

    id31 problem
    document sentiment classification
    sentence subjectivity & sentiment classification
    aspect-based id31
    aspect-based opinion summarization
    sentiment lexicon generation
    mining comparative opinions
    some other problems
    opinion spam detection 
    utility or helpfulness of reviews
    summary

bing liu, tutorial 

206

   103

utility or quality of reviews

    goal: determining the helpfulness, or utility of 

each review (not necessarily fake)
    it is desirable to rank reviews based on utilities or 

qualities when showing them to users, with the 
highest quality review first. 

    many review aggregation sites have been 

practicing this, e.g., amazon.com. 
       x of y people found the following review helpful.    
    voted by user -    was the review helpful to you?   

bing liu, tutorial 

207

application motivations

    although review sites use helpfulness 

feedback to rank their reviews, 
    a review takes a long time to gather enough 

feedback.
    new reviews will not be read. 

    some sites do not provide feedback information.

    it is thus beneficial to score each review once 

it is submitted to a site.  

bing liu, tutorial 

208

   104

regression formulation 
(zhang and varadarajan, 2006;  kim et al. 2006)

    formulation: determining the utility of reviews 

is usually treated as a regression problem. 
    a set of features is engineered for model building
    the learned model assigns an utility score to each 

review, which can be used in review ranking. 
    unlike fake reviews, the ground truth data 

used for both training and testing are available
    usually the user-helpfulness feedback given to 

each review. 

bing liu, tutorial 

209

features for regression learning

    example features include 

    review length, review rating, counts of some pos tags, 
opinion words, tf-idf scores, wh-words, product aspect 
mentions, comparison with product specifications, 
timeliness, etc (zhang and varadarajan, 2006;  kim et 
al. 2006; ghose and ipeirotis 2007; liu et al 2007)

    subjectivity classification was applied in (ghose

and ipeirotis 2007).

    social context was used in (o   mahony and smyth 

2009; lu et al. 2010). 

bing liu, tutorial 

210

   105

classification formulation

    binary classification: instead of using the 

original helpfulness feedback as the target or 
dependent variable, 
    liu et al (2007) performed manual annotation of 

two classes based on whether the review 
evaluates many product aspects or not. 

    binary class classification is also used in 

(o   mahony and smyth 2009)
    classes: helpful and not helpful
    features: helpfulness, content, social, and opinion

bing liu, tutorial 

211

roadmap

    id31 problem
    document sentiment classification
    sentence subjectivity & sentiment classification
    aspect-based id31
    aspect-based opinion summarization
    sentiment lexicon generation
    mining comparative opinions
    some other problems
    opinion spam detection 
    utility or helpfulness of reviews
    summary

bing liu, tutorial 

212

   106

summary

    this tutorial presented

    the problem of id31 and opinion 

mining
    it provides a structure to the unstructured text.
    it shows that summarization is crucial.

    main research directions and their representative 

techniques. 

    by no means exhaustive, a large body of work.

    still many problems not attempted or studied. 
    none of the problem is solved. 

bing liu, tutorial 

213

summary (contd)

    a fascinating nlp or id111 problem. 

    every sub-problem is highly challenging.
    but it is also highly restricted (semantically). 

    despite the challenges, applications are 

flourishing!
    it is useful to every organization and individual.
    the general nlp is probably too hard, but 
can we solve this highly restricted problem?
    i am optimistic

bing liu, tutorial 

214

   107

references

all references are in the

new book
    bing liu. id31 
and opinion mining. morgan 
& claypool publishers. may 
2012.

    book draft: 

http://www.cs.uic.edu/~liub/fbs/senti
mentanalysis-and-opinionmining.pdf

bing liu, tutorial 

215

   108

