structured sparsity

in natural language processing:

models, algorithms, and applications

andr  e f. t. martins1,2,3 m  ario a. t. figueiredo1 noah a. smith2

1instituto de telecomunica  c  oes

instituto superior t  ecnico, lisboa, portugal

2language technologies institute, school of computer science

carnegie mellon university, pittsburgh, pa, usa

3priberam, lisboa, portugal

naacl 2012: tutorials

montr  eal, qu  ebec, june 3, 2012

slides online at http://tiny.cc/ssnlp

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

1 / 155

welcome

this tutorial is about sparsity, a topic of great relevance to nlp.

sparsity relates to feature selection, model compactness, runtime,
memory footprint, interpretability of our models.

new idea in the last 5 years: structured sparsity. this tutorial tries to
answer:

what is structured sparsity?

how do we apply it?

how has it been used so far?

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

2 / 155

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

convex analysis

batch algorithms

online algorithms

5 applications

6 conclusions

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

3 / 155

notation

many nlp problems involve mapping from one structured space to
another. notation:

input set x
for each x     x, candidate outputs are y(x)     y
mapping is hw : x     y

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

4 / 155

linear models

our predictor will take the form

hw(x) = arg max
y   y(x)

w(cid:62)f(x, y )

where:

f is a vector function that encodes all the relevant things about
(x, y ); the result of a theory, our knowledge, feature engineering, etc.
w     rd are the weights that parameterize the mapping.

nlp today: d is often in the tens or hundreds of millions.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

5 / 155

learning linear models

max ent, id88, crf, id166, even supervised generative models all    t
the linear modeling framework.

general training setup:

we observe a collection of examples {(cid:104)xn, yn(cid:105)}n
perform statistical analysis to discover w from the data.
ranges from    count and normalize    to complex optimization routines.

n=1.

optimization view:

(cid:98)w = arg min

w

1
n

(cid:124)

n(cid:88)

n=1

(cid:123)(cid:122)

l(w; xn, yn)

+    (w)

(cid:124)(cid:123)(cid:122)(cid:125)

regularizer

(cid:125)

empirical loss

this tutorial will focus on the regularizer,    .

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

6 / 155

what is sparsity?

the word    sparsity    has (at least) four related meanings in nlp!

1 data sparsity: n is too small to obtain a good estimate for w.

also known as    curse of dimensionality.   
(usually bad.)

2    id203    sparsity: i have a id203 distribution over events

(e.g., x    y), most of which receive zero id203.
(might be good or bad.)

3 sparsity in the dual: associated with id166s and other kernel-based

methods; implies that the predictor can be represented via kernel
calculations involving just a few training instances.

4 model sparsity: most dimensions of f are not needed for a good hw;

those dimensions of w can be zero, leading to a sparse w (model).

this tutorial is about sense #4: today, (model) sparsity is a good thing!

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

7 / 155

why sparsity is desirable in nlp

occam   s razor and interpretability.

the bet on sparsity (friedman et al., 2004): it   s often correct. when it
isn   t, there   s no good solution anyway!

models with just a few features are

easy to explain and implement

attractive as linguistic hypotheses

reminiscent of classical symbolic systems

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

8 / 155

a decision list from yarowsky (1995).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

9 / 155

why sparsity is desirable in nlp

computational savings.

wd = 0 is equivalent to erasing the feature from the model; smaller
e   ective d implies smaller memory footprint.

this, in turn, implies faster decoding runtime.

further, sometimes entire kinds of features can be eliminated, giving
asymptotic savings.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

10 / 155

why sparsity is desirable in nlp

generalization.

the challenge of learning is to extract from the data only what will
generalize to new examples.

forcing a learner to use few features is one way to discourage
over   tting.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

11 / 155

experimental results from kazama and tsujii (2003): f1 on two text
categorization tasks as the number of features varies.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

12 / 155

(automatic) feature selection

human nlpers are good at thinking of features.

can we automate the process of selecting which ones to keep?

three kinds of methods:

1    lters

2 wrappers

3 embedded methods (this tutorial)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

13 / 155

(automatic) feature selection

human nlpers are good at thinking of features.

can we automate the process of selecting which ones to keep?

three kinds of methods:

1    lters

2 wrappers

3 embedded methods (this tutorial)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

13 / 155

filter-based feature selection

for each candidate feature fd , apply a heuristic to determine whether to
include it. (excluding fd equates to    xing wd = 0.)

examples:

count threshold: is |{n | fd (xn, yn) > 0}| >    ?
(ignore rare features.)

mutual information or correlation between features and labels

advantage: speed!

disadvantages:

ignores the learning algorithm

thresholds require tuning

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

14 / 155

ratnaparkhi (1996), on his pos tagger:

the behavior of a feature that occurs very sparsely in the
training set is often di   cult to predict, since its statistics may
not be reliable. therefore, the model uses the heuristic that any
feature which occurs less than 10 times in the data is unreliable,
and ignores features whose counts are less than 10.1 while there
are many smoothing algorithms which use techniques more
rigorous than a simple count cuto   , they have not yet been
investigated in conjunction with this tagger.

1except for features that look only at the current word, i.e., features of the form

wi =<word> and ti = <tag>. the count of 10 was chosen by inspection of training and
development data.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

15 / 155

(automatic) feature selection

human nlpers are good at thinking of features.

can we automate the process of selecting which ones to keep?

three kinds of methods:

1    lters

2 wrappers

3 embedded methods (this tutorial)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

16 / 155

(automatic) feature selection

human nlpers are good at thinking of features.

can we automate the process of selecting which ones to keep?

three kinds of methods:

1    lters

2 wrappers

3 embedded methods (this tutorial)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

16 / 155

wrapper-based feature selection

for each subset f     {1, 2, . . . d}, learn hwf for features {fd | d     f}.
2d     1 choices; so perform a search over subsets.
cons:

np-hard problem (amaldi and kann, 1998; davis et al., 1997)

must resort to greedy methods

even those require iterative calls to a black-box learner
danger of over   tting in choosing f.
(typically use development data or cross-validate.)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

17 / 155

della pietra et al. (1997) add features one at a time. step (3) involves
re-estimating parameters:

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

18 / 155

(automatic) feature selection

human nlpers are good at thinking of features.

can we automate the process of selecting which ones to keep?

three kinds of methods:

1    lters

2 wrappers

3 embedded methods (this tutorial)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

19 / 155

(automatic) feature selection

human nlpers are good at thinking of features.

can we automate the process of selecting which ones to keep?

three kinds of methods:

1    lters

2 wrappers

3 embedded methods (this tutorial)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

19 / 155

embedded methods for feature selection

formulate the learning problem as a trade-o    between

minimizing loss (   tting the training data, achieving good accuracy on
the training data, etc.)

choosing a desirable model (e.g., one with no more features than
needed)

n(cid:88)

n=1

min
w

1
n

l(w; xn, yn) +    (w)

key advantage: declarative statements of model    desirability    often lead
to well-understood, solvable optimization problems.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

20 / 155

useful papers on feature selection and sparsity

overview of many feature selection methods:
guyon and elissee    (2003)

greedy wrapper-based method used for max ent models in nlp:
della pietra et al. (1997)

early uses of sparsity in nlp:
kazama and tsujii (2003); goodman (2004)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

21 / 155

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

convex analysis

batch algorithms

online algorithms

5 applications

6 conclusions

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

22 / 155

id168s (i)

regression (y     r) typically uses the squared error loss:

(cid:16)

(cid:17)2

lse(w; x, y ) =

1
2

y     w(cid:62)f(x)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

23 / 155

id168s (i)

regression (y     r) typically uses the squared error loss:

lse(w; x, y ) =

y     w(cid:62)f(x)

(cid:17)2

total loss:

n(cid:88)

n=1

1
2

(cid:16)

yn     w(cid:62)f(xn)

=

1
2

(cid:107)aw     y(cid:107)2

2

1
2

(cid:16)
(cid:17)2

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

23 / 155

id168s (i)

regression (y     r) typically uses the squared error loss:

lse(w; x, y ) =

y     w(cid:62)f(x)

(cid:17)2

total loss:

n(cid:88)

n=1

1
2

(cid:16)

yn     w(cid:62)f(xn)

=

1
2

(cid:107)aw     y(cid:107)2

2

1
2

(cid:16)
(cid:17)2

design matrix: a = [aij ]i=1,...,n; j=1,...,d, where aij = fj (xi ).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

23 / 155

id168s (i)

regression (y     r) typically uses the squared error loss:

lse(w; x, y ) =

y     w(cid:62)f(x)

(cid:17)2

total loss:

n(cid:88)

n=1

1
2

(cid:16)

yn     w(cid:62)f(xn)

=

1
2

(cid:107)aw     y(cid:107)2

2

1
2

(cid:16)
(cid:17)2

design matrix: a = [aij ]i=1,...,n; j=1,...,d, where aij = fj (xi ).
response vector: y = [y1, ..., yn ](cid:62).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

23 / 155

id168s (i)

regression (y     r) typically uses the squared error loss:

lse(w; x, y ) =

y     w(cid:62)f(x)

(cid:17)2

total loss:

n(cid:88)

n=1

1
2

(cid:16)

yn     w(cid:62)f(xn)

=

1
2

(cid:107)aw     y(cid:107)2

2

1
2

(cid:16)
(cid:17)2

design matrix: a = [aij ]i=1,...,n; j=1,...,d, where aij = fj (xi ).
response vector: y = [y1, ..., yn ](cid:62).
arguably, the most/best studied id168 (statistics, machine
learning, signal processing).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

23 / 155

id168s (ii)

classi   cation and id170 using id148
(id28, max ent, conditional random    elds):

llr(w; x, y ) =     log p (y|x; w)

(cid:80)

exp(w(cid:62)f (x, y ))

=     log
=     w(cid:62)f (x, y ) + log z (w, x)

y(cid:48)   y(x) exp(w(cid:62)f (x, y(cid:48)))

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

24 / 155

id168s (ii)

classi   cation and id170 using id148
(id28, max ent, conditional random    elds):

llr(w; x, y ) =     log p (y|x; w)

(cid:80)

exp(w(cid:62)f (x, y ))

=     log
=     w(cid:62)f (x, y ) + log z (w, x)

y(cid:48)   y(x) exp(w(cid:62)f (x, y(cid:48)))

partition function:

z (w, x) =

(cid:88)

y(cid:48)   y(x)

exp(w(cid:62)f (x, y(cid:48))).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

24 / 155

id168s (ii)

classi   cation and id170 using id148
(id28, max ent, conditional random    elds):

llr(w; x, y ) =     log p (y|x; w)

(cid:80)

exp(w(cid:62)f (x, y ))

=     log
=     w(cid:62)f (x, y ) + log z (w, x)

y(cid:48)   y(x) exp(w(cid:62)f (x, y(cid:48)))

partition function:

z (w, x) =

(cid:88)

y(cid:48)   y(x)

exp(w(cid:62)f (x, y(cid:48))).

related id168s: hinge loss (in id166) and the id88 loss.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

24 / 155

main id168s: summary

squared (id75)

log-linear (maxent, crf, logistic)

hinge (id166s)

id88

1
2

(cid:0)y     w(cid:62)f(x)(cid:1)2
(cid:88)
(cid:0)w(cid:62)f(x, y(cid:48)) + c(y , y(cid:48))(cid:1)

exp(w(cid:62)f(x, y(cid:48)))

y(cid:48)   y

   w(cid:62)f(x, y ) + log

   w(cid:62)f(x, y ) + max
y(cid:48)   y

   w(cid:62)f(x, y ) + max
y(cid:48)   y

w(cid:62)f(x, y(cid:48))

(in the id166 loss, c(y , y(cid:48)) is a cost function.)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

25 / 155

main id168s: summary

squared (id75)

log-linear (maxent, crf, logistic)

hinge (id166s)

id88

1
2

(cid:0)y     w(cid:62)f(x)(cid:1)2
(cid:88)
(cid:0)w(cid:62)f(x, y(cid:48)) + c(y , y(cid:48))(cid:1)

exp(w(cid:62)f(x, y(cid:48)))

y(cid:48)   y

   w(cid:62)f(x, y ) + log

   w(cid:62)f(x, y ) + max
y(cid:48)   y

   w(cid:62)f(x, y ) + max
y(cid:48)   y

w(cid:62)f(x, y(cid:48))

(in the id166 loss, c(y , y(cid:48)) is a cost function.)
all these losses are particular cases of general family (martins et al., 2010).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

25 / 155

id173

regularized parameter estimate:

(cid:98)w = arg min

w

n(cid:88)
(cid:124)

n=1

   (w) +

l(w; xn, yn)

(cid:123)(cid:122)

(cid:125)

total loss

where    (w)     0 and

lim(cid:107)w(cid:107)          (w) =     (coercive function).

why regularize?

improve generalization by avoiding over-   tting.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

26 / 155

id173

regularized parameter estimate:

(cid:98)w = arg min

w

n(cid:88)
(cid:124)

n=1

   (w) +

l(w; xn, yn)

(cid:123)(cid:122)

(cid:125)

total loss

where    (w)     0 and

lim(cid:107)w(cid:107)          (w) =     (coercive function).

why regularize?

improve generalization by avoiding over-   tting.

the total loss may not be coercive (e.g., logistic loss on separable
data), thus having no minima.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

26 / 155

id173

regularized parameter estimate:

(cid:98)w = arg min

w

n(cid:88)
(cid:124)

n=1

   (w) +

l(w; xn, yn)

(cid:123)(cid:122)

(cid:125)

total loss

where    (w)     0 and

lim(cid:107)w(cid:107)          (w) =     (coercive function).

why regularize?

improve generalization by avoiding over-   tting.

the total loss may not be coercive (e.g., logistic loss on separable
data), thus having no minima.
express prior knowledge about w.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

26 / 155

id173

regularized parameter estimate:

(cid:98)w = arg min

w

n(cid:88)
(cid:124)

n=1

   (w) +

l(w; xn, yn)

(cid:123)(cid:122)

(cid:125)

total loss

where    (w)     0 and

lim(cid:107)w(cid:107)          (w) =     (coercive function).

why regularize?

improve generalization by avoiding over-   tting.

the total loss may not be coercive (e.g., logistic loss on separable
data), thus having no minima.
express prior knowledge about w.
select relevant features (via sparsity-inducing id173).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

26 / 155

id173

regularized parameter estimate:

(cid:98)w = arg min

w

n(cid:88)
(cid:124)

n=1

   (w) +

l(w; xn, yn)

(cid:123)(cid:122)

(cid:125)

total loss

where    (w)     0 and

lim(cid:107)w(cid:107)          (w) =     (coercive function).

why regularize?

improve generalization by avoiding over-   tting.

the total loss may not be coercive (e.g., logistic loss on separable
data), thus having no minima.
express prior knowledge about w.
select relevant features (via sparsity-inducing id173).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

26 / 155

id173 formulations

(cid:98)w = arg min

w

n(cid:88)

n=1

tikhonov id173:

        (w) +

l(w; xn, yn)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

27 / 155

tikhonov id173:

        (w) +

l(w; xn, yn)

ivanov id173

id173 formulations

n(cid:88)

n=1

(cid:98)w = arg min
n(cid:88)

w

(cid:98)w = arg min

w

l(w; xn, yn)

n=1

subject to    (w)       

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

27 / 155

tikhonov id173:

        (w) +

l(w; xn, yn)

ivanov id173

id173 formulations

n(cid:88)

n=1

(cid:98)w = arg min
n(cid:88)

w

(cid:98)w = arg min

w

l(w; xn, yn)

n=1

subject to    (w)       

morozov id173

(cid:98)w = arg min

w

   (w)

n(cid:88)

subject to

l(w; xn, yn)       

n=1

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

27 / 155

tikhonov id173:

        (w) +

l(w; xn, yn)

ivanov id173

id173 formulations

n(cid:88)

n=1

(cid:98)w = arg min
n(cid:88)

w

(cid:98)w = arg min

w

l(w; xn, yn)

n=1

subject to    (w)       

morozov id173

(cid:98)w = arg min

w

   (w)

n(cid:88)

subject to

l(w; xn, yn)       

equivalent, under mild conditions (namely convexity).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

27 / 155

n=1

id173 vs. bayesian estimation

n(cid:88)

n=1

   (w) +

l(w; xn, yn)

regularized parameter estimate: (cid:98)w = arg min
n(cid:89)
(cid:124)

(cid:98)w = arg max

exp (      (w))

prior p(w)

(cid:123)(cid:122)

(cid:124)

(cid:125)

n=1

w

w

...interpretable as bayesian maximum a posteriori (map) estimate:

exp (   l(w; xn, yn))

.

(cid:123)(cid:122)

(cid:125)

likelihood (i.i.d. data)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

28 / 155

id173 vs. bayesian estimation

n(cid:88)

n=1

   (w) +

l(w; xn, yn)

regularized parameter estimate: (cid:98)w = arg min
n(cid:89)
(cid:124)

(cid:98)w = arg max

exp (      (w))

prior p(w)

(cid:123)(cid:122)

(cid:124)

(cid:125)

n=1

w

w

...interpretable as bayesian maximum a posteriori (map) estimate:

exp (   l(w; xn, yn))

.

(cid:123)(cid:122)

(cid:125)

likelihood (i.i.d. data)

this interpretation underlies the id28 (lr) loss:
llr(w; xn, yn) =     log p (yn|xn; w).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

28 / 155

id173 vs. bayesian estimation

n(cid:88)

n=1

   (w) +

l(w; xn, yn)

regularized parameter estimate: (cid:98)w = arg min
n(cid:89)
(cid:124)

(cid:98)w = arg max

exp (      (w))

prior p(w)

(cid:123)(cid:122)

(cid:124)

(cid:125)

n=1

w

w

...interpretable as bayesian maximum a posteriori (map) estimate:

exp (   l(w; xn, yn))

.

(cid:123)(cid:122)

(cid:125)

likelihood (i.i.d. data)

this interpretation underlies the id28 (lr) loss:
llr(w; xn, yn) =     log p (yn|xn; w).

same is true for the squared error (se) loss:
lse(w; xn, yn) = 1
2

(cid:0)y     w(cid:62)f(x)(cid:1)2 =     log n(y|w(cid:62)f(x), 1)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

28 / 155

norms: a quick review

before focusing on regularizers, a quick review about norms.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

29 / 155

norms: a quick review

before focusing on regularizers, a quick review about norms.
some function p : r     r is a norm if if satis   es:
p(  w) = |  |p(w), for any w (homogeneity);

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

29 / 155

norms: a quick review

before focusing on regularizers, a quick review about norms.
some function p : r     r is a norm if if satis   es:
p(  w) = |  |p(w), for any w (homogeneity);
p(w + w(cid:48))     p(w) + p(w(cid:48)), for any w, w(cid:48) (triangle inequality);

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

29 / 155

norms: a quick review

before focusing on regularizers, a quick review about norms.
some function p : r     r is a norm if if satis   es:
p(  w) = |  |p(w), for any w (homogeneity);
p(w + w(cid:48))     p(w) + p(w(cid:48)), for any w, w(cid:48) (triangle inequality);
p(w) = 0 if and only if w = 0.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

29 / 155

norms: a quick review

before focusing on regularizers, a quick review about norms.
some function p : r     r is a norm if if satis   es:
p(  w) = |  |p(w), for any w (homogeneity);
p(w + w(cid:48))     p(w) + p(w(cid:48)), for any w, w(cid:48) (triangle inequality);
p(w) = 0 if and only if w = 0.

examples of norms:

(cid:32)(cid:88)

(cid:33)1/p

(cid:107)w(cid:107)p =

(wi )p

(called (cid:96)p norm, for p     1).

i

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

29 / 155

norms: a quick review

before focusing on regularizers, a quick review about norms.
some function p : r     r is a norm if if satis   es:
p(  w) = |  |p(w), for any w (homogeneity);
p(w + w(cid:48))     p(w) + p(w(cid:48)), for any w, w(cid:48) (triangle inequality);
p(w) = 0 if and only if w = 0.

examples of norms:

(cid:33)1/p

(cid:32)(cid:88)
p      (cid:107)w(cid:107)p = max{|wi|, i = 1, ..., d}

(wi )p

(called (cid:96)p norm, for p     1).

(cid:107)w(cid:107)p =
(cid:107)w(cid:107)    = lim

i

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

29 / 155

norms: a quick review

before focusing on regularizers, a quick review about norms.
some function p : r     r is a norm if if satis   es:
p(  w) = |  |p(w), for any w (homogeneity);
p(w + w(cid:48))     p(w) + p(w(cid:48)), for any w, w(cid:48) (triangle inequality);
p(w) = 0 if and only if w = 0.

examples of norms:

(cid:33)1/p

(cid:32)(cid:88)
p      (cid:107)w(cid:107)p = max{|wi|, i = 1, ..., d}

(wi )p

(called (cid:96)p norm, for p     1).

(cid:107)w(cid:107)p =
(cid:107)w(cid:107)    = lim

i

fact: all norms are convex.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

29 / 155

norms: a quick review

before focusing on regularizers, a quick review about norms.
some function p : r     r is a norm if if satis   es:
p(  w) = |  |p(w), for any w (homogeneity);
p(w + w(cid:48))     p(w) + p(w(cid:48)), for any w, w(cid:48) (triangle inequality);
p(w) = 0 if and only if w = 0.

examples of norms:

(cid:33)1/p

(cid:32)(cid:88)
p      (cid:107)w(cid:107)p = max{|wi|, i = 1, ..., d}

(wi )p

(called (cid:96)p norm, for p     1).

(cid:107)w(cid:107)p =
(cid:107)w(cid:107)    = lim

i

fact: all norms are convex.
also important (but not a norm): (cid:107)w(cid:107)0 = lim
p   0

(cid:107)w(cid:107)p

p = |{i : wi

(cid:54)= 0}|

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

29 / 155

classical regularizers: ridge

n(cid:88)

regularized parameter estimate: (cid:98)w = arg min
corresponds to zero-mean gaussian prior p(w)     exp(cid:0)     (cid:107)w(cid:107)2

arguably, the most classical choice: squared (cid:96)2 norm:    (w) =

l(w; xn, yn) +    (w)

(cid:1)

  
2

(cid:107)w(cid:107)2

n=1

w

2

2

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

30 / 155

classical regularizers: ridge

n(cid:88)

regularized parameter estimate: (cid:98)w = arg min
corresponds to zero-mean gaussian prior p(w)     exp(cid:0)     (cid:107)w(cid:107)2

arguably, the most classical choice: squared (cid:96)2 norm:    (w) =

l(w; xn, yn) +    (w)

(cid:1)

  
2

(cid:107)w(cid:107)2

n=1

w

2

2

ridge regression (se loss): hoerl and kennard (1962 and 1970).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

30 / 155

classical regularizers: ridge

n(cid:88)

regularized parameter estimate: (cid:98)w = arg min
corresponds to zero-mean gaussian prior p(w)     exp(cid:0)     (cid:107)w(cid:107)2

arguably, the most classical choice: squared (cid:96)2 norm:    (w) =

l(w; xn, yn) +    (w)

(cid:1)

  
2

(cid:107)w(cid:107)2

n=1

w

2

2

ridge regression (se loss): hoerl and kennard (1962 and 1970).
ridge id28: schaefer et al. (1984), cessie and
houwelingen (1992); in nlp: chen and rosenfeld (1999).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

30 / 155

classical regularizers: ridge

n(cid:88)

regularized parameter estimate: (cid:98)w = arg min
corresponds to zero-mean gaussian prior p(w)     exp(cid:0)     (cid:107)w(cid:107)2

arguably, the most classical choice: squared (cid:96)2 norm:    (w) =

l(w; xn, yn) +    (w)

(cid:1)

  
2

(cid:107)w(cid:107)2

n=1

w

2

2

ridge regression (se loss): hoerl and kennard (1962 and 1970).
ridge id28: schaefer et al. (1984), cessie and
houwelingen (1992); in nlp: chen and rosenfeld (1999).

closely related to tikhonov (1943) and wiener (1949).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

30 / 155

classical regularizers: ridge

n(cid:88)

regularized parameter estimate: (cid:98)w = arg min
corresponds to zero-mean gaussian prior p(w)     exp(cid:0)     (cid:107)w(cid:107)2

arguably, the most classical choice: squared (cid:96)2 norm:    (w) =

l(w; xn, yn) +    (w)

(cid:1)

  
2

(cid:107)w(cid:107)2

n=1

w

2

2

ridge regression (se loss): hoerl and kennard (1962 and 1970).
ridge id28: schaefer et al. (1984), cessie and
houwelingen (1992); in nlp: chen and rosenfeld (1999).

closely related to tikhonov (1943) and wiener (1949).
pros: smooth and convex, thus benign for optimization.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

30 / 155

classical regularizers: ridge

n(cid:88)

regularized parameter estimate: (cid:98)w = arg min
corresponds to zero-mean gaussian prior p(w)     exp(cid:0)     (cid:107)w(cid:107)2

arguably, the most classical choice: squared (cid:96)2 norm:    (w) =

l(w; xn, yn) +    (w)

(cid:1)

  
2

(cid:107)w(cid:107)2

n=1

w

2

2

ridge regression (se loss): hoerl and kennard (1962 and 1970).
ridge id28: schaefer et al. (1984), cessie and
houwelingen (1992); in nlp: chen and rosenfeld (1999).

closely related to tikhonov (1943) and wiener (1949).
pros: smooth and convex, thus benign for optimization.
cons: doesn   t promote sparsity (no explicit feature selection).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

30 / 155

n(cid:88)

n=1

d(cid:88)

classical regularizers: lasso

regularized parameter estimate: (cid:98)w = arg min

w

l(w; xn, yn) +    (w)

the new classic is the (cid:96)1 norm:    (w) =   (cid:107)w(cid:107)1 =   

|wi|.

corresponds to zero-mean laplacian prior p(wi )     exp (     |wi|)

i=1

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

31 / 155

n(cid:88)

n=1

d(cid:88)

classical regularizers: lasso

regularized parameter estimate: (cid:98)w = arg min

w

l(w; xn, yn) +    (w)

the new classic is the (cid:96)1 norm:    (w) =   (cid:107)w(cid:107)1 =   

|wi|.

corresponds to zero-mean laplacian prior p(wi )     exp (     |wi|)
best known as: least absolute shrinkage and selection operator
(lasso) (tibshirani, 1996).

i=1

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

31 / 155

n(cid:88)

n=1

d(cid:88)

classical regularizers: lasso

regularized parameter estimate: (cid:98)w = arg min

w

l(w; xn, yn) +    (w)

the new classic is the (cid:96)1 norm:    (w) =   (cid:107)w(cid:107)1 =   

|wi|.

i=1

corresponds to zero-mean laplacian prior p(wi )     exp (     |wi|)
best known as: least absolute shrinkage and selection operator
(lasso) (tibshirani, 1996).
used earlier in signal processing (claerbout and muir, 1973; taylor
et al., 1979) neural networks (williams, 1995),...

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

31 / 155

n(cid:88)

n=1

d(cid:88)

classical regularizers: lasso

regularized parameter estimate: (cid:98)w = arg min

w

l(w; xn, yn) +    (w)

the new classic is the (cid:96)1 norm:    (w) =   (cid:107)w(cid:107)1 =   

|wi|.

i=1

corresponds to zero-mean laplacian prior p(wi )     exp (     |wi|)
best known as: least absolute shrinkage and selection operator
(lasso) (tibshirani, 1996).
used earlier in signal processing (claerbout and muir, 1973; taylor
et al., 1979) neural networks (williams, 1995),...

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

31 / 155

n(cid:88)

n=1

d(cid:88)

classical regularizers: lasso

regularized parameter estimate: (cid:98)w = arg min

w

l(w; xn, yn) +    (w)

the new classic is the (cid:96)1 norm:    (w) =   (cid:107)w(cid:107)1 =   

|wi|.

i=1

corresponds to zero-mean laplacian prior p(wi )     exp (     |wi|)
best known as: least absolute shrinkage and selection operator
(lasso) (tibshirani, 1996).
used earlier in signal processing (claerbout and muir, 1973; taylor
et al., 1979) neural networks (williams, 1995),...
in nlp: kazama and tsujii (2003); goodman (2004).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

31 / 155

n(cid:88)

n=1

d(cid:88)

classical regularizers: lasso

regularized parameter estimate: (cid:98)w = arg min

w

l(w; xn, yn) +    (w)

the new classic is the (cid:96)1 norm:    (w) =   (cid:107)w(cid:107)1 =   

|wi|.

i=1

corresponds to zero-mean laplacian prior p(wi )     exp (     |wi|)
best known as: least absolute shrinkage and selection operator
(lasso) (tibshirani, 1996).
used earlier in signal processing (claerbout and muir, 1973; taylor
et al., 1979) neural networks (williams, 1995),...
in nlp: kazama and tsujii (2003); goodman (2004).
pros: encourages sparsity: embedded feature selection.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

31 / 155

n(cid:88)

n=1

d(cid:88)

classical regularizers: lasso

regularized parameter estimate: (cid:98)w = arg min

w

l(w; xn, yn) +    (w)

the new classic is the (cid:96)1 norm:    (w) =   (cid:107)w(cid:107)1 =   

|wi|.

i=1

corresponds to zero-mean laplacian prior p(wi )     exp (     |wi|)
best known as: least absolute shrinkage and selection operator
(lasso) (tibshirani, 1996).
used earlier in signal processing (claerbout and muir, 1973; taylor
et al., 1979) neural networks (williams, 1995),...
in nlp: kazama and tsujii (2003); goodman (2004).
pros: encourages sparsity: embedded feature selection.
cons: convex, but non-smooth: challenging optimization.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

31 / 155

the lasso and sparsity

why does the lasso yield sparsity?

the simplest case:

(cid:98)w = arg min

w

(w     y )2 +   |w| = soft(y ,   ) =

1
2

          y            y >   

    |y|       
0
y +        y <      

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

32 / 155

the lasso and sparsity

why does the lasso yield sparsity?

the simplest case:

(cid:98)w = arg min

w

(w     y )2 +   |w| = soft(y ,   ) =

1
2

          y            y >   

    |y|       
0
y +        y <      

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

32 / 155

the lasso and sparsity

why does the lasso yield sparsity?

the simplest case:

(cid:98)w = arg min

w

(w     y )2 +   |w| = soft(y ,   ) =

1
2

          y            y >   

    |y|       
0
y +        y <      

contrast with the squared (cid:96)2 (ridge) regularizer (linear scaling):

(cid:98)w = arg min

w

(w     y )2 +

1
2

  
2

w 2 =

1

1 +   

y

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

32 / 155

the lasso and sparsity (ii)

why does the lasso yield sparsity?

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

33 / 155

the lasso and sparsity (ii)

why does the lasso yield sparsity?

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

33 / 155

relationship between (cid:96)1 and (cid:96)0

the (cid:96)0    norm    (number of non-zeros): (cid:107)w(cid:107)0 = |{i : wi
not convex, but...

(cid:98)w = arg min

w

(w     y )2 +   |w|0 = hard(y ,

1
2

(cid:54)= 0}|.

(cid:26) y     |y| >
   
0     |y|        

2  
2  

   

2  ) =

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

34 / 155

relationship between (cid:96)1 and (cid:96)0

the (cid:96)0    norm    (number of non-zeros): (cid:107)w(cid:107)0 = |{i : wi
not convex, but...

(cid:98)w = arg min

w

(w     y )2 +   |w|0 = hard(y ,

1
2

(cid:54)= 0}|.

(cid:26) y     |y| >
   
0     |y|        

2  
2  

   

2  ) =

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

34 / 155

relationship between (cid:96)1 and (cid:96)0

the (cid:96)0    norm    (number of non-zeros): (cid:107)w(cid:107)0 = |{i : wi
not convex, but...

(cid:98)w = arg min

w

(w     y )2 +   |w|0 = hard(y ,

1
2

(cid:54)= 0}|.

(cid:26) y     |y| >
   
0     |y|        

2  
2  

   

2  ) =

the    ideal    feature selection criterion (best subset):

(cid:98)w = arg min

w

n(cid:88)

l(w; xn, yn)

n=1

subject to (cid:107)w(cid:107)0       

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

34 / 155

(limit the number of features)

relationship between (cid:96)1 and (cid:96)0 (ii)

the best subset selection problem

(cid:98)w = arg min

w

n(cid:88)

l(w; xn, yn)

n=1

subject to (cid:107)w(cid:107)0       

.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

35 / 155

relationship between (cid:96)1 and (cid:96)0 (ii)
the best subset selection problem is np-hard amaldi and kann
(1998)(davis et al., 1997).

(cid:98)w = arg min

w

n(cid:88)

l(w; xn, yn)

n=1

subject to (cid:107)w(cid:107)0       

.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

35 / 155

relationship between (cid:96)1 and (cid:96)0 (ii)
the best subset selection problem is np-hard amaldi and kann
(1998)(davis et al., 1997).

(cid:98)w = arg min

w

n(cid:88)

l(w; xn, yn)

n=1

subject to (cid:107)w(cid:107)0       

a closely related problem,

(cid:98)w = arg min

w

(cid:107)w(cid:107)0

n(cid:88)

subject to

l(w; xn, yn)       

n=1

.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

35 / 155

relationship between (cid:96)1 and (cid:96)0 (ii)
the best subset selection problem is np-hard amaldi and kann
(1998)(davis et al., 1997).

(cid:98)w = arg min

w

n(cid:88)

l(w; xn, yn)

n=1

subject to (cid:107)w(cid:107)0       

a closely related problem, also np-hard (muthukrishnan, 2005).

(cid:98)w = arg min

w

(cid:107)w(cid:107)0

n(cid:88)

subject to

l(w; xn, yn)       

n=1

.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

35 / 155

relationship between (cid:96)1 and (cid:96)0 (ii)
the best subset selection problem is np-hard amaldi and kann
(1998)(davis et al., 1997).

(cid:98)w = arg min

w

n(cid:88)

l(w; xn, yn)

n=1

subject to (cid:107)w(cid:107)0       

a closely related problem, also np-hard (muthukrishnan, 2005).

(cid:98)w = arg min

w

(cid:107)w(cid:107)0

n(cid:88)

subject to

l(w; xn, yn)       

n=1

in some cases, one may replace (cid:96)0 with (cid:96)1 and obtain    similar    results:
central issue in compressive sensing (cs) (cand`es et al., 2006a; donoho,
2006).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

35 / 155

compressive sensing in one slide

even in the noiseless case, it seems impossible to recover w from y

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

36 / 155

compressive sensing in one slide

even in the noiseless case, it seems impossible to recover w from y
...unless, w is sparse and a has some properties.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

36 / 155

compressive sensing in one slide

even in the noiseless case, it seems impossible to recover w from y
...unless, w is sparse and a has some properties.

if w is sparse enough and a has certain properties, then w is stably
recovered via (haupt and nowak, 2006)

(cid:98)w = arg min

w

(cid:107)w(cid:107)0

subject to (cid:107)aw     y(cid:107)       

np-hard!

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

36 / 155

...ok, in two slides

under some conditions on a (e.g., the restricted isometry property (rip)),
(cid:96)0 can be replaced with (cid:96)1 (cand`es et al., 2006b):

(cid:98)w = arg min

w

(cid:107)w(cid:107)1

subject to (cid:107)aw     y(cid:107)       

convex problem

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

37 / 155

...ok, in two slides

under some conditions on a (e.g., the restricted isometry property (rip)),
(cid:96)0 can be replaced with (cid:96)1 (cand`es et al., 2006b):

(cid:98)w = arg min

w

(cid:107)w(cid:107)1

subject to (cid:107)aw     y(cid:107)       

convex problem

matrix a satis   es the rip of order k, with constant   k     (0, 1), if
2     (cid:107)aw(cid:107)     (1 +   k )(cid:107)w(cid:107)2

(cid:107)w(cid:107)0     k     (1       k )(cid:107)w(cid:107)2

2

...i.e., for k-sparse vectors, a is approximately an isometry.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

37 / 155

...ok, in two slides

under some conditions on a (e.g., the restricted isometry property (rip)),
(cid:96)0 can be replaced with (cid:96)1 (cand`es et al., 2006b):

(cid:98)w = arg min

w

(cid:107)w(cid:107)1

subject to (cid:107)aw     y(cid:107)       

convex problem

matrix a satis   es the rip of order k, with constant   k     (0, 1), if
2     (cid:107)aw(cid:107)     (1 +   k )(cid:107)w(cid:107)2

(cid:107)w(cid:107)0     k     (1       k )(cid:107)w(cid:107)2

2

...i.e., for k-sparse vectors, a is approximately an isometry.

other properties (spark and null space property (nsp)) can be used;
checking rip, nsp, spark is np-hard (tillmann and pfetsch, 2012).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

37 / 155

relevant theory?

are sparsity-related compressed sensing (cs) results relevant for nlp?

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

38 / 155

relevant theory?

are sparsity-related compressed sensing (cs) results relevant for nlp?

nearly all cs results assume linear observations y = aw + noise;
recent exceptions: blumensath (2012); plan and vershynin (2012).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

38 / 155

relevant theory?

are sparsity-related compressed sensing (cs) results relevant for nlp?

nearly all cs results assume linear observations y = aw + noise;
recent exceptions: blumensath (2012); plan and vershynin (2012).

   good    matrices with rip or nsp are randomly constructed.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

38 / 155

relevant theory?

are sparsity-related compressed sensing (cs) results relevant for nlp?

nearly all cs results assume linear observations y = aw + noise;
recent exceptions: blumensath (2012); plan and vershynin (2012).

   good    matrices with rip or nsp are randomly constructed.

what is missing: results for (multinomial) logistic loss, not based on
rip or nsp.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

38 / 155

relevant theory?

are sparsity-related compressed sensing (cs) results relevant for nlp?

nearly all cs results assume linear observations y = aw + noise;
recent exceptions: blumensath (2012); plan and vershynin (2012).

   good    matrices with rip or nsp are randomly constructed.

what is missing: results for (multinomial) logistic loss, not based on
rip or nsp.

other types of results for (cid:96)1-regularized id28:

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

38 / 155

relevant theory?

are sparsity-related compressed sensing (cs) results relevant for nlp?

nearly all cs results assume linear observations y = aw + noise;
recent exceptions: blumensath (2012); plan and vershynin (2012).

   good    matrices with rip or nsp are randomly constructed.

what is missing: results for (multinomial) logistic loss, not based on
rip or nsp.

other types of results for (cid:96)1-regularized id28:

pac-bayesian bounds (generalization improves with sparsity):
krishnapuram et al. (2005)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

38 / 155

relevant theory?

are sparsity-related compressed sensing (cs) results relevant for nlp?

nearly all cs results assume linear observations y = aw + noise;
recent exceptions: blumensath (2012); plan and vershynin (2012).

   good    matrices with rip or nsp are randomly constructed.

what is missing: results for (multinomial) logistic loss, not based on
rip or nsp.

other types of results for (cid:96)1-regularized id28:

pac-bayesian bounds (generalization improves with sparsity):
krishnapuram et al. (2005)

oracle (van de geer, 2008) and consistency (negahban et al., 2012)
results.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

38 / 155

take-home messages

sparsity is desirable for interpretability, computational savings, and
generalization
(cid:96)1-id173 gives an embedded method for feature selection
another view of (cid:96)1: a convex surrogate for direct penalization of
cardinality ((cid:96)0)
under some conditions, (cid:96)1 guarantees exact support recovery
however: the currently known su   cient conditions are too strong and
not met in typical nlp problems

yet: a lot of theory is still missing

there are compelling algorithmic reasons for using convex surrogates
like (cid:96)1

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

39 / 155

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

convex analysis

batch algorithms

online algorithms

5 applications

6 conclusions

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

40 / 155

models

(cid:96)1 id173 promotes sparse models

a very simple sparsity pattern: prefer models with small cardinality

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

41 / 155

models

(cid:96)1 id173 promotes sparse models

a very simple sparsity pattern: prefer models with small cardinality

our main question: how can we promote less trivial sparsity patterns?

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

41 / 155

models

(cid:96)1 id173 promotes sparse models

a very simple sparsity pattern: prefer models with small cardinality

our main question: how can we promote less trivial sparsity patterns?

we   ll talk about structured sparsity and group-lasso id173.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

41 / 155

structured sparsity and groups

main goal: promote structural patterns, not just penalize cardinality

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

42 / 155

structured sparsity and groups

main goal: promote structural patterns, not just penalize cardinality

group sparsity: discard entire groups of features

density inside each group
sparsity with respect to the groups which are selected
choice of groups: prior knowledge about the intended sparsity patterns

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

42 / 155

structured sparsity and groups

main goal: promote structural patterns, not just penalize cardinality

group sparsity: discard entire groups of features

density inside each group
sparsity with respect to the groups which are selected
choice of groups: prior knowledge about the intended sparsity patterns

leads to statistical gains if the prior assumptions are correct (stojnic
et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

42 / 155

tons of uses

feature template selection (martins et al., 2011b)

id72 (caruana, 1997; obozinski et al., 2010)

multiple kernel learning (lanckriet et al., 2004)

learning the structure of id114 (schmidt and murphy,
2010)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

43 / 155

   grid    sparsity

for feature spaces that can be arranged as a grid (examples next)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

44 / 155

   grid    sparsity

for feature spaces that can be arranged as a grid (examples next)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

44 / 155

   grid    sparsity

for feature spaces that can be arranged as a grid (examples next)

goal: push entire columns to have zero weights

the groups are the columns of the grid

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

44 / 155

example 1: sparsity with multiple classes

assume the feature map decomposes as f(x, y ) = f(x)     ey
in words: we   re conjoining each input feature with each output class

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

45 / 155

input featureslabelsexample 1: sparsity with multiple classes

assume the feature map decomposes as f(x, y ) = f(x)     ey
in words: we   re conjoining each input feature with each output class

   standard    sparsity is wasteful   we still need to hash all the input features

what we want: discard some input features, along with each class they
conjoin with

solution: one group per input feature

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

45 / 155

input featureslabelsexample 2: id72

(caruana, 1997; obozinski et al., 2010)

same thing, except now rows are tasks and columns are features

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

46 / 155

shared featurestasksexample 2: id72

(caruana, 1997; obozinski et al., 2010)

same thing, except now rows are tasks and columns are features

what we want: discard features that are irrelevant for all tasks

solution: one group per feature

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

46 / 155

shared featurestasksexample 3: multiple kernel learning

same thing, except now columns are id81s {km}m

m=1

(lanckriet et al., 2004)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

47 / 155

kernelsimplicit featuresexample 3: multiple kernel learning

same thing, except now columns are id81s {km}m

m=1

(lanckriet et al., 2004)

m(cid:88)

goal: a new kernel which is a sparse combination of the given kernels

k ((x, y ), (x(cid:48), y(cid:48))) =

  mkm((x, y ), (x(cid:48), y(cid:48))),

   is sparse

m=1

solution: make each group be a kernel kj

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

47 / 155

kernelsimplicit featuresgroup sparsity

d features

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

48 / 155

group sparsity

d features
m groups g1, . . . , gm , each
gm     {1, . . . , d}
parameter subvectors w1, . . . , wm

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

48 / 155

group sparsity

d features
m groups g1, . . . , gm , each
gm     {1, . . . , d}
parameter subvectors w1, . . . , wm

group-lasso (bakin, 1999; yuan and lin, 2006):

   (w) =(cid:80)m

m=1 (cid:107)wm(cid:107)2

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

48 / 155

group sparsity

d features
m groups g1, . . . , gm , each
gm     {1, . . . , d}
parameter subvectors w1, . . . , wm

group-lasso (bakin, 1999; yuan and lin, 2006):

   (w) =(cid:80)m

m=1 (cid:107)wm(cid:107)2

intuitively: the (cid:96)1 norm of the (cid:96)2 norms
technically, still a norm (called a mixed norm, denoted (cid:96)2,1)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

48 / 155

group sparsity

d features
m groups g1, . . . , gm , each
gm     {1, . . . , d}
parameter subvectors w1, . . . , wm

group-lasso (bakin, 1999; yuan and lin, 2006):

   (w) =(cid:80)m

m=1   m(cid:107)wm(cid:107)2

intuitively: the (cid:96)1 norm of the (cid:96)2 norms
technically, still a norm (called a mixed norm, denoted (cid:96)2,1)
  m: prior weight for group gm (di   erent groups have di   erent sizes)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

48 / 155

id173 formulations (reminder)

tikhonov id173:

   (w) +

l(w; xn, yn)

n(cid:88)

n=1

(cid:98)w = arg min
n(cid:88)

w

(cid:98)w = arg min

w

l(w; xn, yn)

n=1

subject to    (w)       

ivanov id173

morozov id173

(cid:98)w = arg min

w

   (w)

n(cid:88)

subject to

l(w; xn, yn)       

equivalent, under mild conditions (namely convexity).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

49 / 155

n=1

lasso versus group-lasso

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

50 / 155

lasso versus group-lasso

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

50 / 155

other names, other norms

statisticians call these composite absolute penalties (zhao et al., 2009)
in general: the (weighted) (cid:96)r -norm of the (cid:96)q-norms (r     1, q     1), called
the mixed (cid:96)q,r norm

(cid:16)(cid:80)m

   (w) =

m=1  m(cid:107)wm(cid:107)r

q

(cid:17)1/r

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

51 / 155

other names, other norms

statisticians call these composite absolute penalties (zhao et al., 2009)
in general: the (weighted) (cid:96)r -norm of the (cid:96)q-norms (r     1, q     1), called
the mixed (cid:96)q,r norm

(cid:16)(cid:80)m

   (w) =

m=1  m(cid:107)wm(cid:107)r

q

(cid:17)1/r

group sparsity corresponds to r = 1

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

51 / 155

other names, other norms

statisticians call these composite absolute penalties (zhao et al., 2009)
in general: the (weighted) (cid:96)r -norm of the (cid:96)q-norms (r     1, q     1), called
the mixed (cid:96)q,r norm

(cid:16)(cid:80)m

   (w) =

m=1  m(cid:107)wm(cid:107)r

q

(cid:17)1/r

group sparsity corresponds to r = 1

this talk: q = 2
however q =     is also popular (quattoni et al., 2009; gra  ca et al., 2009;
wright et al., 2009; eisenstein et al., 2011)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

51 / 155

three scenarios

non-overlapping groups

tree-structured groups

graph-structured groups

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

52 / 155

three scenarios

non-overlapping groups

tree-structured groups

graph-structured groups

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

52 / 155

non-overlapping groups

assume g1, . . . , gm are disjoint
    each feature belongs to exactly one group

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

53 / 155

non-overlapping groups

assume g1, . . . , gm are disjoint
    each feature belongs to exactly one group

   (w) =(cid:80)m

m=1   m(cid:107)wm(cid:107)2

trivial choices of groups recover unstructured regularizers:

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

53 / 155

non-overlapping groups

assume g1, . . . , gm are disjoint
    each feature belongs to exactly one group

   (w) =(cid:80)m

m=1   m(cid:107)wm(cid:107)2

trivial choices of groups recover unstructured regularizers:

(cid:96)2-id173: one large group g1 = {1, . . . , d}
(cid:96)1-id173: d singleton groups gd = {d}

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

53 / 155

non-overlapping groups

assume g1, . . . , gm are disjoint
    each feature belongs to exactly one group

   (w) =(cid:80)m

m=1   m(cid:107)wm(cid:107)2

trivial choices of groups recover unstructured regularizers:

(cid:96)2-id173: one large group g1 = {1, . . . , d}
(cid:96)1-id173: d singleton groups gd = {d}

examples of non-trivial groups:

label-based groups (groups are columns of a matrix)

template-based groups (next)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

53 / 155

example: feature template selection

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

54 / 155

example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

54 / 155

example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

54 / 155

example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

(cid:53)

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

54 / 155

example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

(cid:53)

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

54 / 155

example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

(cid:53)
the
dt
b-np

explore

vb
i-vp

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

54 / 155

"the feature""explore the"example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

54 / 155

"the feature""explore the"example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

(cid:53)

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

54 / 155

"the feature""explore the"example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

(cid:53)
the
dt
b-np

explore

vb
i-vp

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

54 / 155

"the feature""explore the""dt nn nn""vb dt nn"example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates
    make each group correspond to a feature template

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

54 / 155

"the feature""explore the""dt nn nn""vb dt nn"example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates
    make each group correspond to a feature template

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

54 / 155

"dt nn nn""vb dt nn"example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates
    make each group correspond to a feature template

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

54 / 155

example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates
    make each group correspond to a feature template

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

54 / 155

example: feature template selection

input: we
prp

to
to
output: b-np b-vp i-vp

want
vbp

explore

vb
i-vp

the
dt
b-np

feature

nn
i-np

space
nn
i-np

goal: select relevant feature templates
    make each group correspond to a feature template

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

54 / 155

three scenarios

non-overlapping groups

tree-structured groups

graph-structured groups

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

55 / 155

three scenarios

non-overlapping groups

tree-structured groups

graph-structured groups

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

55 / 155

tree-structured groups

assumption: if two groups overlap, one is contained in the other
    hierarchical structure (kim and xing, 2010; mairal et al., 2010)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

56 / 155

tree-structured groups

assumption: if two groups overlap, one is contained in the other
    hierarchical structure (kim and xing, 2010; mairal et al., 2010)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

56 / 155

tree-structured groups

assumption: if two groups overlap, one is contained in the other
    hierarchical structure (kim and xing, 2010; mairal et al., 2010)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

56 / 155

tree-structured groups

assumption: if two groups overlap, one is contained in the other
    hierarchical structure (kim and xing, 2010; mairal et al., 2010)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

56 / 155

tree-structured groups

assumption: if two groups overlap, one is contained in the other
    hierarchical structure (kim and xing, 2010; mairal et al., 2010)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

56 / 155

tree-structured groups

assumption: if two groups overlap, one is contained in the other
    hierarchical structure (kim and xing, 2010; mairal et al., 2010)

what is the sparsity pattern?

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

56 / 155

tree-structured groups

assumption: if two groups overlap, one is contained in the other
    hierarchical structure (kim and xing, 2010; mairal et al., 2010)

what is the sparsity pattern?
if a group is discarded, all its descendants are also discarded

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

56 / 155

plate notation

typically used for id114, but also works here for representing
the hasse diagram of tree-structured groups

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

57 / 155

three scenarios

non-overlapping groups

tree-structured groups

graph-structured groups

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

58 / 155

three scenarios

non-overlapping groups

tree-structured groups

graph-structured groups

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

58 / 155

graph-structured groups

in general: groups can be represented as a directed acyclic graph

set inclusion induces a partial order on groups (jenatton et al., 2009)
feature space becomes a poset
sparsity patterns: given by this poset

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

59 / 155

example: coarse-to-   ne id173

1 de   ne a partial order between basic feature templates (e.g., p0 (cid:22) w0)
2 extend this partial order to all templates by lexicographic closure:

p0 (cid:22) p0p1 (cid:22) w0w1

goal: only include    ner features if coarser ones are also in the model

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

60 / 155

things to keep in mind

structured sparsity cares about the structure of the feature space
group-lasso id173 generalizes (cid:96)1 and it   s still convex

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

61 / 155

things to keep in mind

structured sparsity cares about the structure of the feature space
group-lasso id173 generalizes (cid:96)1 and it   s still convex
choice of groups: problem dependent, opportunity to use prior
knowledge to favour certain structural patterns

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

61 / 155

things to keep in mind

structured sparsity cares about the structure of the feature space
group-lasso id173 generalizes (cid:96)1 and it   s still convex
choice of groups: problem dependent, opportunity to use prior
knowledge to favour certain structural patterns
next: algorithms
we   ll see that optimization is easier with non-overlapping or
tree-structured groups than with arbitrary overlaps

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

61 / 155

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

convex analysis

batch algorithms

online algorithms

5 applications

6 conclusions

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

62 / 155

learning the model

recall that learning involves solving

min
w

(cid:124)(cid:123)(cid:122)(cid:125)

   (w)

regularizer

+

1
n

(cid:124)

n(cid:88)

i=1

(cid:123)(cid:122)

l(w, xi , yi )

,

(cid:125)

total loss

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

63 / 155

learning the model

recall that learning involves solving

min
w

(cid:124)(cid:123)(cid:122)(cid:125)

   (w)

regularizer

+

1
n

(cid:124)

n(cid:88)

i=1

(cid:123)(cid:122)

l(w, xi , yi )

,

(cid:125)

total loss

we   ll address two kinds of optimization algorithms:

batch algorithms (attacks the complete problem);

online algorithms (uses the training examples one by one)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

63 / 155

learning the model

recall that learning involves solving

min
w

(cid:124)(cid:123)(cid:122)(cid:125)

   (w)

regularizer

+

1
n

(cid:124)

n(cid:88)

i=1

(cid:123)(cid:122)

l(w, xi , yi )

,

(cid:125)

total loss

we   ll address two kinds of optimization algorithms:

batch algorithms (attacks the complete problem);

online algorithms (uses the training examples one by one)

before that: we   ll review some key concepts of convex analysis

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

63 / 155

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

convex analysis

batch algorithms

online algorithms

5 applications

6 conclusions

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

64 / 155

key concepts in convex analysis: convex sets

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

65 / 155

key concepts in convex analysis: convex functions

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

66 / 155

key concepts in convex analysis: minimizers

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

67 / 155

key concepts in convex analysis: subgradients

convexity     continuity; convexity (cid:54)    di   erentiability (e.g., f (w) = (cid:107)w(cid:107)1).
subgradients generalize gradients for (maybe non-di   .) convex functions:

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

68 / 155

key concepts in convex analysis: subgradients

convexity     continuity; convexity (cid:54)    di   erentiability (e.g., f (w) = (cid:107)w(cid:107)1).
subgradients generalize gradients for (maybe non-di   .) convex functions:

v is a subgradient of f at x if f (x(cid:48))     f (x) + v(cid:62)(x(cid:48)     x)

subdi   erential:    f (x) = {v : v is a subgradient of f at x}

linear lower bound

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

68 / 155

key concepts in convex analysis: subgradients

convexity     continuity; convexity (cid:54)    di   erentiability (e.g., f (w) = (cid:107)w(cid:107)1).
subgradients generalize gradients for (maybe non-di   .) convex functions:

v is a subgradient of f at x if f (x(cid:48))     f (x) + v(cid:62)(x(cid:48)     x)

subdi   erential:    f (x) = {v : v is a subgradient of f at x}
if f is di   erentiable,    f (x) = {   f (x)}

linear lower bound

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

68 / 155

key concepts in convex analysis: subgradients

convexity     continuity; convexity (cid:54)    di   erentiability (e.g., f (w) = (cid:107)w(cid:107)1).
subgradients generalize gradients for (maybe non-di   .) convex functions:

v is a subgradient of f at x if f (x(cid:48))     f (x) + v(cid:62)(x(cid:48)     x)

subdi   erential:    f (x) = {v : v is a subgradient of f at x}
if f is di   erentiable,    f (x) = {   f (x)}

linear lower bound

non-di   erentiable case

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

68 / 155

key concepts in convex analysis: subgradients

convexity     continuity; convexity (cid:54)    di   erentiability (e.g., f (w) = (cid:107)w(cid:107)1).
subgradients generalize gradients for (maybe non-di   .) convex functions:

v is a subgradient of f at x if f (x(cid:48))     f (x) + v(cid:62)(x(cid:48)     x)

subdi   erential:    f (x) = {v : v is a subgradient of f at x}
if f is di   erentiable,    f (x) = {   f (x)}

linear lower bound

non-di   erentiable case

notation:      f (x) is a subgradient of f at x

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

68 / 155

key concepts in convex analysis: strong convexity
recall the de   nition of convex function:           [0, 1],

f (  x + (1       )x(cid:48))       f (x) + (1       )f (x(cid:48))

convexity

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

69 / 155

key concepts in convex analysis: strong convexity
recall the de   nition of convex function:           [0, 1],

f (  x + (1       )x(cid:48))       f (x) + (1       )f (x(cid:48))

a      strongly convex function satis   es a stronger condition:           [0, 1]
  (1       )(cid:107)x     x(cid:48)(cid:107)2

f (  x + (1       )x(cid:48))       f (x) + (1       )f (x(cid:48))       
2

2

convexity

strong convexity

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

69 / 155

key concepts in convex analysis: strong convexity
recall the de   nition of convex function:           [0, 1],

f (  x + (1       )x(cid:48))       f (x) + (1       )f (x(cid:48))

a      strongly convex function satis   es a stronger condition:           [0, 1]
  (1       )(cid:107)x     x(cid:48)(cid:107)2

f (  x + (1       )x(cid:48))       f (x) + (1       )f (x(cid:48))       
2

2

convexity

strong convexity

strong convexity

   
(cid:54)    strict convexity.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

69 / 155

let     : rd       r be a convex function.

proximity operators

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

70 / 155

proximity operators

let     : rd       r be a convex function.
the    -proximity operator is the following rd     rd map:

w (cid:55)    prox   (w) = arg min

u

1
2

(cid:107)u     w(cid:107)2

2 +    (u)

...always well de   ned, because (cid:107)u     w(cid:107)2

2 is strictly convex.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

70 / 155

proximity operators

let     : rd       r be a convex function.
the    -proximity operator is the following rd     rd map:

w (cid:55)    prox   (w) = arg min

u

1
2

(cid:107)u     w(cid:107)2

2 +    (u)

...always well de   ned, because (cid:107)u     w(cid:107)2
classical examples:

2 is strictly convex.

squared (cid:96)2 id173,    (w) =   

2: scaling operation

2(cid:107)w(cid:107)2

prox   (w) =

1

1 +   

w

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

70 / 155

proximity operators

let     : rd       r be a convex function.
the    -proximity operator is the following rd     rd map:

w (cid:55)    prox   (w) = arg min

u

1
2

(cid:107)u     w(cid:107)2

2 +    (u)

...always well de   ned, because (cid:107)u     w(cid:107)2
classical examples:

2 is strictly convex.

squared (cid:96)2 id173,    (w) =   

2: scaling operation

2(cid:107)w(cid:107)2

prox   (w) =

1

w

1 +   

(cid:96)1 id173,    (w) =   (cid:107)w(cid:107)1: soft-thresholding;

prox   (w) = soft(w,   )

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

70 / 155

proximity operators (ii)

prox    (w) = arg min
u

(cid:107)u     w(cid:107)2

2 +    (u)

1
2

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

71 / 155

proximity operators (ii)

prox    (w) = arg min
u

(cid:107)u     w(cid:107)2

2 +    (u)

1
2

(cid:96)2 id173,    (w) =   (cid:107)w(cid:107)2: vector soft thresholding

(cid:26) 0
    (cid:107)w(cid:107)       
w(cid:107)w(cid:107) ((cid:107)w(cid:107)       )     (cid:107)w(cid:107) >   

prox   (w) =

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

71 / 155

proximity operators (ii)

prox    (w) = arg min
u

(cid:107)u     w(cid:107)2

2 +    (u)

1
2

(cid:96)2 id173,    (w) =   (cid:107)w(cid:107)2: vector soft thresholding

prox   (w) =

(cid:26) 0
    (cid:107)w(cid:107)       
w(cid:107)w(cid:107) ((cid:107)w(cid:107)       )     (cid:107)w(cid:107) >   
    w     s
+        w (cid:54)    s

(cid:26) 0

indicator function,    (w) =   s(w) =

prox   (w) = ps(w)

euclidean projection

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

71 / 155

proximity operators (iii)

m(cid:88)

   j (wgm)

group regularizers:    (w) =
groups: gm     {1, 2, ..., d}.

m=1

wgm is a sub-vector of w.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

72 / 155

proximity operators (iii)

m(cid:88)

   j (wgm)

group regularizers:    (w) =
groups: gm     {1, 2, ..., d}.

m=1

wgm is a sub-vector of w.

non-overlapping groups (gm     gn =    , for m (cid:54)= n): separable prox
operator

[prox   (w)]gm

= prox   m(wgm)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

72 / 155

proximity operators (iii)

m(cid:88)

   j (wgm)

group regularizers:    (w) =
groups: gm     {1, 2, ..., d}.

m=1

wgm is a sub-vector of w.

non-overlapping groups (gm     gn =    , for m (cid:54)= n): separable prox
operator

[prox   (w)]gm

= prox   m(wgm)

tree-structured groups: (two groups are either non-overlapping or
one contais the other) prox    can be computed recursively (jenatton
et al., 2011).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

72 / 155

proximity operators (iii)

m(cid:88)

   j (wgm)

group regularizers:    (w) =
groups: gm     {1, 2, ..., d}.

m=1

wgm is a sub-vector of w.

non-overlapping groups (gm     gn =    , for m (cid:54)= n): separable prox
operator

[prox   (w)]gm

= prox   m(wgm)

tree-structured groups: (two groups are either non-overlapping or
one contais the other) prox    can be computed recursively (jenatton
et al., 2011).
arbitrary groups:

for    j (wgm ) = (cid:107)wgm(cid:107)2: solved via convex smooth optimization (yuan
et al., 2011).
sequential proximity steps (martins et al., 2011a) (more later).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

72 / 155

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

convex analysis

batch algorithms

online algorithms

5 applications

6 conclusions

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

73 / 155

subgradient methods

(cid:80)n

minw    (w) +   (w), where   (w) = 1
n

i=1 l(w, xi , yi ) (loss)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

74 / 155

subgradient methods

(cid:80)n

minw    (w) +   (w), where   (w) = 1
n

i=1 l(w, xi , yi ) (loss)

subgradient methods were invented by shor in the 1970   s (shor, 1985):

input: stepsize sequence (  t)t
initialize w
for t = 1, 2, . . . do

t=1

(sub-)gradient step: w     w       t

(cid:0)         (w) +        (w)(cid:1)

end for

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

74 / 155

subgradient methods

(cid:80)n

minw    (w) +   (w), where   (w) = 1
n

i=1 l(w, xi , yi ) (loss)

subgradient methods were invented by shor in the 1970   s (shor, 1985):

input: stepsize sequence (  t)t
initialize w
for t = 1, 2, . . . do

t=1

(sub-)gradient step: w     w       t

(cid:0)         (w) +        (w)(cid:1)

end for

key disadvantages:

the step size   t needs to be annealed for convergence: very slow!
doesn   t explicitly capture the sparsity promoted by (cid:96)1 regularizers.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

74 / 155

(block-)coordinate descent

(cid:80)n

minw    (w) +   (w), where   (w) = 1
n

i=1 l(w, xi , yi ) (loss)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

75 / 155

(block-)coordinate descent

(cid:80)n

minw    (w) +   (w), where   (w) = 1
n

i=1 l(w, xi , yi ) (loss)

update one (block of) component(s) of w at a time:

i     arg min
w new

wi

   ([w1, ..., wi , ...wd]) +   ([w1, ..., wi , ...wd])

(genkin et al., 2007; krishnapuram et al., 2005; liu et al., 2009; shevade and
keerthi, 2003; tseng and yun, 2009; yun and toh, 2011)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

75 / 155

(block-)coordinate descent

(cid:80)n

minw    (w) +   (w), where   (w) = 1
n

i=1 l(w, xi , yi ) (loss)

update one (block of) component(s) of w at a time:

i     arg min
w new

wi

   ([w1, ..., wi , ...wd]) +   ([w1, ..., wi , ...wd])

(genkin et al., 2007; krishnapuram et al., 2005; liu et al., 2009; shevade and
keerthi, 2003; tseng and yun, 2009; yun and toh, 2011)

squared error loss: closed-form solution. other losses (e.g., logistic):

solve numerically, e.g., newton steps (shevade and keerthi, 2003).
use local quadratic approximation/bound (krishnapuram et al., 2005;
tseng and yun, 2009; yun and toh, 2011).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

75 / 155

(block-)coordinate descent

(cid:80)n

minw    (w) +   (w), where   (w) = 1
n

i=1 l(w, xi , yi ) (loss)

update one (block of) component(s) of w at a time:

i     arg min
w new

wi

   ([w1, ..., wi , ...wd]) +   ([w1, ..., wi , ...wd])

(genkin et al., 2007; krishnapuram et al., 2005; liu et al., 2009; shevade and
keerthi, 2003; tseng and yun, 2009; yun and toh, 2011)

squared error loss: closed-form solution. other losses (e.g., logistic):

solve numerically, e.g., newton steps (shevade and keerthi, 2003).
use local quadratic approximation/bound (krishnapuram et al., 2005;
tseng and yun, 2009; yun and toh, 2011).

shown to converge; competitive with state-of-the-art (yun and toh, 2011).

has been used in nlp: sokolovska et al. (2010); lavergne et al. (2010).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

75 / 155

instead of min

w

   (w) +   (w) , tackle min
w

  (w) subject to    (w)        .

projected gradient

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

76 / 155

projected gradient

  (w) subject to    (w)        .

instead of min

w

   (w) +   (w) , tackle min
w

building blocks:

loss gradient        (w)
euclidean projection ps(w), where s = {w :    (w)       }

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

76 / 155

projected gradient

  (w) subject to    (w)        .

instead of min

w

   (w) +   (w) , tackle min
w

building blocks:

loss gradient        (w)
euclidean projection ps(w), where s = {w :    (w)       }

w     ps(w               (w))

...maybe using line search to adjust the step length.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

76 / 155

projected gradient

  (w) subject to    (w)        .

instead of min

w

   (w) +   (w) , tackle min
w

building blocks:

loss gradient        (w)
euclidean projection ps(w), where s = {w :    (w)       }

w     ps(w               (w))

...maybe using line search to adjust the step length.
example: for s = {w : (cid:107)w(cid:107)1       }, projection ps(w) has o(d log d) cost
(duchi et al., 2008).

viable and competitive alternative, which has been used in machine
learning and nlp (duchi et al., 2008; quattoni et al., 2009).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

76 / 155

projected gradient

  (w) subject to    (w)        .

instead of min

w

   (w) +   (w) , tackle min
w

building blocks:

loss gradient        (w)
euclidean projection ps(w), where s = {w :    (w)       }

w     ps(w               (w))

...maybe using line search to adjust the step length.
example: for s = {w : (cid:107)w(cid:107)1       }, projection ps(w) has o(d log d) cost
(duchi et al., 2008).

viable and competitive alternative, which has been used in machine
learning and nlp (duchi et al., 2008; quattoni et al., 2009).

shown later: projected gradient is a particular instance of the more
general proximal gradient methods.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

76 / 155

from gradient to hessian: newton   s method

assume f (w) =    (w) +   (w) is twice-di   erentiable.
second order (quadratic) taylor expansion around w(cid:48):

(cid:62)
f (w)     f (w(cid:48)) +    f (w(cid:48))

(w     w(cid:48)) +

1
2

(w     w(cid:48))(cid:62) h(w(cid:48))

(w     w(cid:48))

(cid:124) (cid:123)(cid:122) (cid:125)

gradient

(cid:124) (cid:123)(cid:122) (cid:125)

hessian:

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

77 / 155

from gradient to hessian: newton   s method

assume f (w) =    (w) +   (w) is twice-di   erentiable.
second order (quadratic) taylor expansion around w(cid:48):

(cid:62)
f (w)     f (w(cid:48)) +    f (w(cid:48))

(w     w(cid:48)) +

1
2

(w     w(cid:48))(cid:62) h(w(cid:48))

(w     w(cid:48))

(cid:124) (cid:123)(cid:122) (cid:125)

gradient

(cid:124) (cid:123)(cid:122) (cid:125)

hessian:

use the direction that minimizes this quadratic approximation:

w     w       (h(w))   1   f (w)

with stepsize    usually determined by line search.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

77 / 155

from gradient to hessian: newton   s method

assume f (w) =    (w) +   (w) is twice-di   erentiable.
second order (quadratic) taylor expansion around w(cid:48):

(cid:62)
f (w)     f (w(cid:48)) +    f (w(cid:48))

(w     w(cid:48)) +

1
2

(w     w(cid:48))(cid:62) h(w(cid:48))

(w     w(cid:48))

(cid:124) (cid:123)(cid:122) (cid:125)

gradient

(cid:124) (cid:123)(cid:122) (cid:125)

hessian:

use the direction that minimizes this quadratic approximation:

w     w       (h(w))   1   f (w)

with stepsize    usually determined by line search.

drawback: may be costly (or impossible) to compute and invert the
hessian! o(d 3) for a na    ve approach.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

77 / 155

from gradient to hessian: newton   s method

assume f (w) =    (w) +   (w) is twice-di   erentiable.
second order (quadratic) taylor expansion around w(cid:48):

(cid:62)
f (w)     f (w(cid:48)) +    f (w(cid:48))

(w     w(cid:48)) +

1
2

(w     w(cid:48))(cid:62) h(w(cid:48))

(w     w(cid:48))

(cid:124) (cid:123)(cid:122) (cid:125)

gradient

(cid:124) (cid:123)(cid:122) (cid:125)

hessian:

use the direction that minimizes this quadratic approximation:

w     w       (h(w))   1   f (w)

with stepsize    usually determined by line search.

drawback: may be costly (or impossible) to compute and invert the
hessian! o(d 3) for a na    ve approach.

quasi-id77s, namely l-bfgs, approximate the inverse
hessian directly from past gradient information.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

77 / 155

orthant-wise limited-memory quasi newton

owl-qn: clever adaptation of l-bfgs to (cid:96)1-id173 (andrew and
gao, 2007; gao et al., 2007)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

78 / 155

orthant-wise limited-memory quasi newton

owl-qn: clever adaptation of l-bfgs to (cid:96)1-id173 (andrew and
gao, 2007; gao et al., 2007)

input: stepsize sequence (  t)t
initialize w = 0
for t = 1, 2, . . . do

t=1

compute a particular subgradient gt :=         (w) +        (w)
compute inverse hessian approximation st    a la l-bfgs   
compute descent direction dt =    (st) gt
do line search for   , and update w     w +   dt
clip w if necessary to stay in the same orthant

end for

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

78 / 155

orthant-wise limited-memory quasi newton

owl-qn: clever adaptation of l-bfgs to (cid:96)1-id173 (andrew and
gao, 2007; gao et al., 2007)

input: stepsize sequence (  t)t
initialize w = 0
for t = 1, 2, . . . do

t=1

compute a particular subgradient gt :=         (w) +        (w)
compute inverse hessian approximation st    a la l-bfgs   
compute descent direction dt =    (st) gt
do line search for   , and update w     w +   dt
clip w if necessary to stay in the same orthant

end for

pros: provably convergent; updates are sparse due to the clipping.
cons: not applicable to group-regularizers.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

78 / 155

proximal gradient

recall the problem: min

w

   (w) +   (w)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

79 / 155

proximal gradient

recall the problem: min
key assumptions:      (w) and prox       easy   .

   (w) +   (w)

w

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

79 / 155

proximal gradient

recall the problem: min
key assumptions:      (w) and prox       easy   .

   (w) +   (w)

w

wt+1     prox  t     (wt       t     (wt))

key feature: each steps decouples the loss and the regularizer.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

79 / 155

proximal gradient

recall the problem: min
key assumptions:      (w) and prox       easy   .

   (w) +   (w)

w

wt+1     prox  t     (wt       t     (wt))

key feature: each steps decouples the loss and the regularizer.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

79 / 155

proximal gradient

recall the problem: min
key assumptions:      (w) and prox       easy   .

   (w) +   (w)

w

wt+1     prox  t     (wt       t     (wt))

key feature: each steps decouples the loss and the regularizer.

projected gradient is a particular case, for prox    = ps.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

79 / 155

proximal gradient

recall the problem: min
key assumptions:      (w) and prox       easy   .

   (w) +   (w)

w

wt+1     prox  t     (wt       t     (wt))

key feature: each steps decouples the loss and the regularizer.

projected gradient is a particular case, for prox    = ps.
can be derived with di   erent tools:

expectation-maximization (em) (figueiredo and nowak, 2003);

majorization-minimization (daubechies et al., 2004);

forward-backward splitting (combettes and wajs, 2006);

separable approximation (wright et al., 2009).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

79 / 155

majorization-minimization derivation

assume   (w) has l-lipschitz gradient: (cid:107)     (w)          (w(cid:48))(cid:107)     l(cid:107)w     w(cid:48)(cid:107).
separable 2nd order approximation of   (w) around wt

  (w(cid:48)) + (w     wt)

(cid:62)     (w(cid:48)) +

(cid:107)w     w(cid:48)(cid:107)2 = q(w, wt)

1
2  t

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

80 / 155

majorization-minimization derivation

assume   (w) has l-lipschitz gradient: (cid:107)     (w)          (w(cid:48))(cid:107)     l(cid:107)w     w(cid:48)(cid:107).
separable 2nd order approximation of   (w) around wt

  (w(cid:48)) + (w     wt)

1
2  t
if   t     1/l, with equality for w = wt.

(cid:62)     (w(cid:48)) +

(cid:107)w     w(cid:48)(cid:107)2 = q(w, wt)       (w)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

80 / 155

majorization-minimization derivation

assume   (w) has l-lipschitz gradient: (cid:107)     (w)          (w(cid:48))(cid:107)     l(cid:107)w     w(cid:48)(cid:107).
separable 2nd order approximation of   (w) around wt

(cid:107)w     w(cid:48)(cid:107)2 = q(w, wt)       (w)

(cid:62)     (w(cid:48)) +

  (w(cid:48)) + (w     wt)

1
2  t
if   t     1/l, with equality for w = wt.
consequently, if wt+1 = arg minw q(w, wt) +    (w),
  (wt+1) +    (wt+1)     q(wt+1, wt) +    (wt+1)

    q(wt, wt) +    (w1) =   (wt) +    (wt)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

80 / 155

majorization-minimization derivation

assume   (w) has l-lipschitz gradient: (cid:107)     (w)          (w(cid:48))(cid:107)     l(cid:107)w     w(cid:48)(cid:107).
separable 2nd order approximation of   (w) around wt

(cid:107)w     w(cid:48)(cid:107)2 = q(w, wt)       (w)

(cid:62)     (w(cid:48)) +

  (w(cid:48)) + (w     wt)

1
2  t
if   t     1/l, with equality for w = wt.
consequently, if wt+1 = arg minw q(w, wt) +    (w),
  (wt+1) +    (wt+1)     q(wt+1, wt) +    (wt+1)

    q(wt, wt) +    (w1) =   (wt) +    (wt)

easy to show that

wt+1 = arg min
w

q(w, wt) +    (w) = prox  t     (wt       t     (wt)) .

thus, with   t     1/l: objective monotonically decreases.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

80 / 155

monotonicity and convergence

proximal gradient, a.k.a., iterative shrinkage thresholding (ist):

wt+1     prox  t     (wt       t     (wt)) .

monotonicity: if   t     1/l, then   (wt+1) +    (wt+1)       (wt) +    (wt).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

81 / 155

monotonicity and convergence

proximal gradient, a.k.a., iterative shrinkage thresholding (ist):

wt+1     prox  t     (wt       t     (wt)) .

monotonicity: if   t     1/l, then   (wt+1) +    (wt+1)       (wt) +    (wt).
convergence of objective value (beck and teboulle, 2009)

(cid:0)  (wt) +    (wt)(cid:1)    (cid:0)  (w   ) +    (w   )(cid:1) = o

(cid:18) 1

(cid:19)

t

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

81 / 155

monotonicity and convergence

proximal gradient, a.k.a., iterative shrinkage thresholding (ist):

wt+1     prox  t     (wt       t     (wt)) .

monotonicity: if   t     1/l, then   (wt+1) +    (wt+1)       (wt) +    (wt).
convergence of objective value (beck and teboulle, 2009)

(cid:0)  (wt) +    (wt)(cid:1)    (cid:0)  (w   ) +    (w   )(cid:1) = o

(cid:18) 1

(cid:19)

t

important: monotonicity doesn   t imply convergence of w1, w2, ..., wt, ....
convergence (even with inexact steps) proved for   t     2/l (combettes
and wajs, 2006).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

81 / 155

accelerating ist: sparsa

the step sizes   t     2/l (guarantees convergence) and   t     1/l
(monotonicity) are too conservative.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

82 / 155

accelerating ist: sparsa

the step sizes   t     2/l (guarantees convergence) and   t     1/l
(monotonicity) are too conservative.

a bolder choice: let   t mimic a newton step (barzilai and borwein, 1988),

i     h(wt)

1
  t

(hessian)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

82 / 155

accelerating ist: sparsa

the step sizes   t     2/l (guarantees convergence) and   t     1/l
(monotonicity) are too conservative.

a bolder choice: let   t mimic a newton step (barzilai and borwein, 1988),

i     h(wt)

1
  t

(hessian)

approximation in the mean squared sense over the previous step:
(cid:107)  (wt     wt   1)     (     (wt)          (wt   1))(cid:107)2

= arg min

1
  t

  

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

82 / 155

accelerating ist: sparsa

the step sizes   t     2/l (guarantees convergence) and   t     1/l
(monotonicity) are too conservative.

a bolder choice: let   t mimic a newton step (barzilai and borwein, 1988),

i     h(wt)

1
  t

(hessian)

approximation in the mean squared sense over the previous step:
(cid:107)  (wt     wt   1)     (     (wt)          (wt   1))(cid:107)2

= arg min

1
  t

  

resulting algorithm: sparsa (sparse reconstruction by separable
approximation); shown to converge (with a safeguard) and to be fast
wright et al. (2009).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

82 / 155

accelerating ist: fista

idea: compute wt+1 based, not only on wt, but also on wt   1.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

83 / 155

accelerating ist: fista

idea: compute wt+1 based, not only on wt, but also on wt   1.
fast ist algorithm (fista) (beck and teboulle, 2009):

   
bt+1 = 1+
z
wt+1 = prox      (z            (z))

= wt + bt   1

1+4 b2
t
2

(wt     wt   1)

bt+1

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

83 / 155

accelerating ist: fista

idea: compute wt+1 based, not only on wt, but also on wt   1.
fast ist algorithm (fista) (beck and teboulle, 2009):

(wt     wt   1)

bt+1

1+4 b2
t
2

= wt + bt   1

   
bt+1 = 1+
z
wt+1 = prox      (z            (z))
(cid:19)

(cid:18) 1

convergence of objective value (beck and teboulle, 2009)

(cid:0)  (wt) +    (wt)(cid:1)    (cid:0)  (w   ) +    (w   )(cid:1) = o

(vs o(1/t) for ist)

t2

convergence of iterates has not been shown.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

83 / 155

accelerating ist: fista

idea: compute wt+1 based, not only on wt, but also on wt   1.
fast ist algorithm (fista) (beck and teboulle, 2009):

(wt     wt   1)

bt+1

1+4 b2
t
2

= wt + bt   1

   
bt+1 = 1+
z
wt+1 = prox      (z            (z))
(cid:19)

(cid:18) 1

convergence of objective value (beck and teboulle, 2009)

(cid:0)  (wt) +    (wt)(cid:1)    (cid:0)  (w   ) +    (w   )(cid:1) = o

(vs o(1/t) for ist)

t2

convergence of iterates has not been shown.

another two-step method: twist (two-step ist) (bioucas-dias and
figueiredo, 2007).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

83 / 155

least angle regression (lars)
  (cid:107)w(cid:107)1 + (cid:107)aw     y(cid:107)2

lars only applies to   w(  ) = arg min
w

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

84 / 155

least angle regression (lars)
  (cid:107)w(cid:107)1 + (cid:107)aw     y(cid:107)2

lars only applies to   w(  ) = arg min
w

key ideas (efron et al., 2004; osborne et al., 2000)

   id173 path      w(  ) is piecewise linear (markowitz, 1952);
the cusps can be identi   ed in closed form;
simply jump from one cusp to the next.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

84 / 155

least angle regression (lars)
  (cid:107)w(cid:107)1 + (cid:107)aw     y(cid:107)2

lars only applies to   w(  ) = arg min
w

key ideas (efron et al., 2004; osborne et al., 2000)

   id173 path      w(  ) is piecewise linear (markowitz, 1952);
the cusps can be identi   ed in closed form;
simply jump from one cusp to the next.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

84 / 155

least angle regression (lars)
  (cid:107)w(cid:107)1 + (cid:107)aw     y(cid:107)2

lars only applies to   w(  ) = arg min
w

key ideas (efron et al., 2004; osborne et al., 2000)

   id173 path      w(  ) is piecewise linear (markowitz, 1952);
the cusps can be identi   ed in closed form;
simply jump from one cusp to the next.

cons: doesn   t apply to group regularizers; exponential worst case
complexity (mairal and yu, 2012).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

84 / 155

homotopy/continuation methods

lars is related to a more general family: homotopy/continuation
methods.

consider   w(  ) = arg min
w

        (w) +   (w)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

85 / 155

homotopy/continuation methods

lars is related to a more general family: homotopy/continuation
methods.

consider   w(  ) = arg min
w

        (w) +   (w)

key ideas

start with high value of   , such that   w(  ) is easy (e.g., zero);
slowly decrease    while    tracking    the solution;
   tracking    means: use the previous   w(  ) to    warm start    the solver
for the next problem.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

85 / 155

homotopy/continuation methods

lars is related to a more general family: homotopy/continuation
methods.

consider   w(  ) = arg min
w

        (w) +   (w)

key ideas

start with high value of   , such that   w(  ) is easy (e.g., zero);
slowly decrease    while    tracking    the solution;
   tracking    means: use the previous   w(  ) to    warm start    the solver
for the next problem.

it   s a meta-algorithm of general applicability when using    warm startable   
solvers (figueiredo et al., 2007; hale et al., 2008; osborne et al., 2000).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

85 / 155

some stu    we didn   t talk about

shooting method (fu, 1998);

grafting (perkins et al., 2003) and grafting-light (zhu et al., 2010);

forward stagewise regression (hastie et al., 2007);

alternating direction method of multipliers (admm) (figueiredo and
bioucas-dias, 2011).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

86 / 155

some stu    we didn   t talk about

shooting method (fu, 1998);

grafting (perkins et al., 2003) and grafting-light (zhu et al., 2010);

forward stagewise regression (hastie et al., 2007);

alternating direction method of multipliers (admm) (figueiredo and
bioucas-dias, 2011).

next: we   ll talk about online algorithms.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

86 / 155

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

convex analysis

batch algorithms

online algorithms

5 applications

6 conclusions

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

87 / 155

why online?

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

88 / 155

why online?

1 suitable for large datasets

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

88 / 155

why online?

1 suitable for large datasets

2 suitable for id170

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

88 / 155

why online?

1 suitable for large datasets

2 suitable for id170

3 faster to approach a near-optimal region

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

88 / 155

why online?

1 suitable for large datasets

2 suitable for id170

3 faster to approach a near-optimal region
4 slower convergence, but this is    ne in machine learning

cf.    the tradeo   s of large scale learning    (bottou and bousquet, 2007)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

88 / 155

why online?

1 suitable for large datasets

2 suitable for id170

3 faster to approach a near-optimal region
4 slower convergence, but this is    ne in machine learning

cf.    the tradeo   s of large scale learning    (bottou and bousquet, 2007)

what we will say can be straighforwardly extended to the mini-batch case.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

88 / 155

plain stochastic (sub-)id119

min
w

(cid:124)(cid:123)(cid:122)(cid:125)

   (w)

regularizer

+

1
n

(cid:124)

n(cid:88)

i=1

(cid:123)(cid:122)

empirical loss

l(w, xi , yi )

,

(cid:125)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

89 / 155

plain stochastic (sub-)id119

min
w

(cid:124)(cid:123)(cid:122)(cid:125)

   (w)

regularizer

+

1
n

(cid:124)

n(cid:88)

i=1

(cid:123)(cid:122)

empirical loss

l(w, xi , yi )

,

(cid:125)

input: stepsize sequence (  t)t
initialize w = 0
for t = 1, 2, . . . do

t=1

take training pair (xt, yt)
(sub-)gradient step: w     w       t

end for

(cid:0)         (w) +      l(w; xt, yt)(cid:1)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

89 / 155

what   s the problem with sgd?

(cid:0)         (w) +      l(w; xt, yt)(cid:1)

(sub-)gradient step: w     w       t

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

90 / 155

what   s the problem with sgd?

(cid:0)         (w) +      l(w; xt, yt)(cid:1)

(sub-)gradient step: w     w       t

(cid:96)2-id173    (w) =   

2(cid:107)w(cid:107)2
(cid:123)(cid:122)
w     (1       t  )w

2 =            (w) =   w
(cid:125)
      t      l(w; xt, yt)

(cid:124)

scaling

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

90 / 155

what   s the problem with sgd?

(cid:0)         (w) +      l(w; xt, yt)(cid:1)

(sub-)gradient step: w     w       t

(cid:96)2-id173    (w) =   

2(cid:107)w(cid:107)2
(cid:123)(cid:122)
w     (1       t  )w

2 =            (w) =   w
(cid:125)
      t      l(w; xt, yt)

(cid:124)

scaling

(cid:96)1-id173    (w) =   (cid:107)w(cid:107)1 =            (w) =   sign(w)

w     w       t  sign(w)

      t      l(w; xt, yt)

(cid:124)

(cid:123)(cid:122)

(cid:125)

constant penalty

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

90 / 155

what   s the problem with sgd?

(cid:0)         (w) +      l(w; xt, yt)(cid:1)

(sub-)gradient step: w     w       t

(cid:96)2-id173    (w) =   

2(cid:107)w(cid:107)2
(cid:123)(cid:122)
w     (1       t  )w

2 =            (w) =   w
(cid:125)
      t      l(w; xt, yt)

(cid:124)

scaling

(cid:96)1-id173    (w) =   (cid:107)w(cid:107)1 =            (w) =   sign(w)

w     w       t  sign(w)

      t      l(w; xt, yt)

(cid:124)

(cid:123)(cid:122)

(cid:125)

constant penalty

problem: iterates are never sparse!

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

90 / 155

plain sgd with (cid:96)2-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

91 / 155

plain sgd with (cid:96)2-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

91 / 155

plain sgd with (cid:96)2-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

91 / 155

plain sgd with (cid:96)2-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

91 / 155

plain sgd with (cid:96)2-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

91 / 155

plain sgd with (cid:96)2-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

91 / 155

plain sgd with (cid:96)2-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

91 / 155

plain sgd with (cid:96)2-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

91 / 155

plain sgd with (cid:96)2-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

91 / 155

plain sgd with (cid:96)2-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

91 / 155

plain sgd with (cid:96)1-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

92 / 155

plain sgd with (cid:96)1-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

92 / 155

plain sgd with (cid:96)1-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

92 / 155

plain sgd with (cid:96)1-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

92 / 155

plain sgd with (cid:96)1-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

92 / 155

plain sgd with (cid:96)1-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

92 / 155

plain sgd with (cid:96)1-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

92 / 155

plain sgd with (cid:96)1-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

92 / 155

plain sgd with (cid:96)1-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

92 / 155

plain sgd with (cid:96)1-id173

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

92 / 155

   sparse    online algorithms

sgd with cumulative penalty (tsuruoka et al., 2009)

truncated gradient (langford et al., 2009)

online forward-backward splitting (duchi and singer, 2009)

regularized dual averaging (xiao, 2010)

online proximal gradient (martins et al., 2011a)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

93 / 155

   sparse    online algorithms

sgd with cumulative penalty (tsuruoka et al., 2009)

truncated gradient (langford et al., 2009)

online forward-backward splitting (duchi and singer, 2009)

regularized dual averaging (xiao, 2010)

online proximal gradient (martins et al., 2011a)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

93 / 155

cumulative penalties (tsuruoka et al., 2009)

an attempt to reconcile sgd and (cid:96)1 regularizarion, maintaining
algorithmic e   ciency
computational trick: accumulates the penalties, and applies them
all at once when a feature    res (due to carpenter (2008))
clipping: if the total penalty is greater than the magnitude of the
feature weight wj , clip wj to zero

but store the amount of clipping for future use.

leads to very sparse models
however: no proof of convergence

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

94 / 155

cumulative penalties (tsuruoka et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

95 / 155

cumulative penalties (tsuruoka et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

95 / 155

cumulative penalties (tsuruoka et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

95 / 155

cumulative penalties (tsuruoka et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

95 / 155

cumulative penalties (tsuruoka et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

95 / 155

cumulative penalties (tsuruoka et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

95 / 155

cumulative penalties (tsuruoka et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

95 / 155

cumulative penalties (tsuruoka et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

95 / 155

cumulative penalties (tsuruoka et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

95 / 155

cumulative penalties (tsuruoka et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

95 / 155

   sparse    online algorithms

sgd with cumulative penalty (tsuruoka et al., 2009)

truncated gradient (langford et al., 2009)

online forward-backward splitting (duchi and singer, 2009)

regularized dual averaging (xiao, 2010)

online proximal gradient (martins et al., 2011a)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

96 / 155

truncated gradient (langford et al., 2009)

input: laziness coe   cient k , stepsize sequence (  t)t
initialize w = 0
for t = 1, 2, . . . do

t=1

take training pair (xt, yt)
(sub-)gradient step: w     w       t      l(  ; xt, yt)
if t/k is integer then

truncation step: w     w     sign(w) (|w|       tk    )

(cid:123)(cid:122)

soft-thresholding

(cid:125)

(cid:124)

end if
end for

take gradients only with respect to the loss
every k rounds: a    lazy    soft-thresholding step
langford et al. (2009) also suggest other forms of truncation
converges to  -accurate objective after o(1/ 2) iterations

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

97 / 155

truncated gradient (langford et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

98 / 155

truncated gradient (langford et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

98 / 155

truncated gradient (langford et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

98 / 155

truncated gradient (langford et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

98 / 155

truncated gradient (langford et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

98 / 155

truncated gradient (langford et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

98 / 155

truncated gradient (langford et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

98 / 155

truncated gradient (langford et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

98 / 155

truncated gradient (langford et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

98 / 155

truncated gradient (langford et al., 2009)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

98 / 155

   sparse    online algorithms

sgd with cumulative penalty (tsuruoka et al., 2009)

truncated gradient (langford et al., 2009)

online forward-backward splitting (duchi and singer, 2009)

regularized dual averaging (xiao, 2010)

online proximal gradient (martins et al., 2011a)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

99 / 155

online forward-backward splitting (duchi and

singer, 2009)

input: stepsize sequences (  t)t
initialize w = 0
for t = 1, 2, . . . do

t=1, (  t)t

t=1

take training pair (xt, yt)
gradient step: w     w       t   l(w; xt, yt)
proximal step: w     prox  t    (w)

end for

generalizes truncated gradient to arbitrary regularizers    

can tackle non-overlapping or hierarchical group-lasso, but arbitrary
overlaps are di   cult to handle (more later)

practical drawback: without a laziness parameter, iterates are
usually not very sparse
converges to  -accurate objective after o(1/ 2) iterations

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

100 / 155

   sparse    online algorithms

sgd with cumulative penalty (tsuruoka et al., 2009)

truncated gradient (langford et al., 2009)

online forward-backward splitting (duchi and singer, 2009)

regularized dual averaging (xiao, 2010)

online proximal gradient (martins et al., 2011a)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

101 / 155

regularized dual averaging (xiao, 2010)

input: coe   cient   0
initialize w = 0
for t = 1, 2, . . . do

take training pair (xt, yt)
gradient step: s     s +    l(w; xt, yt)
proximal step: w       0

t    prox   (   s/t)

   

end for

based on the dual averaging technique (nesterov, 2009)
in practice: quite e   ective at getting sparse iterates (the proximal
steps are not vanishing)
o(c1/ 2 + c2/
and c2 is the variance of the stochastic gradients
drawback: requires storing two vectors (w and s), and s is not sparse

 ) convergence, where c1 is a lipschitz constant,

   

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

102 / 155

what about group sparsity?

both online forward-backward splitting (duchi and singer, 2009) and
regularized dual averaging (xiao, 2010) can handle groups

all that is necessary is to compute prox   (w)

easy for non-overlapping and tree-structured groups
but what about general overlapping groups?

martins et al. (2011a): a prox-grad algorithm that can handle arbitrary
overlapping groups

decompose    (w) =(cid:80)j

j=1    j (w) where each    j is non-overlapping

then apply prox   j sequentially
still convergent (martins et al., 2011a)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

103 / 155

   sparse    online algorithms

sgd with cumulative penalty (tsuruoka et al., 2009)

truncated gradient (langford et al., 2009)

online forward-backward splitting (duchi and singer, 2009)

regularized dual averaging (xiao, 2010)

online proximal gradient (martins et al., 2011a)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

104 / 155

online proximal gradient (martins et al., 2011a)

input: gravity sequence (  t)t
initialize w = 0
for t = 1, 2, . . . do

t=1, stepsize sequence (  t)t

t=1

take training pair (xt, yt)
gradient step: w     w       t   l(  ; xt, yt)
sequential proximal steps:
for j = 1, 2, . . . do

w     prox  t   t    j (w)

end for

end for

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

105 / 155

online proximal gradient (martins et al., 2011a)

input: gravity sequence (  t)t
initialize w = 0
for t = 1, 2, . . . do

t=1, stepsize sequence (  t)t

t=1

take training pair (xt, yt)
gradient step: w     w       t   l(  ; xt, yt)
sequential proximal steps:
for j = 1, 2, . . . do

w     prox  t   t    j (w)

end for

end for

pac convergence.  -accurate solution after t     o(1/ 2) rounds
computational e   ciency. each gradient step is linear in the
number of features that    re.
each proximal step is linear in the number of groups m.
both are independent of d.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

105 / 155

implementation tricks (martins et al., 2011b)

budget driven shrinkage. instead of a id173 constant,
specify a budget on the number of selected groups. each proximal
step sets   t to meet this target.
sparseptron. let l(w) = w(cid:62)(f(x,   y )     f(x, y )) be the id88
loss. the algorithm becomes id88 with shrinkage.

debiasing. run a few iterations of sparseptron to identify the
relevant groups. then run a unregularized learner at a second stage.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

106 / 155

0510150246x 106# epochs# features  mirasparceptron + mira (b=30)implementation tricks (martins et al., 2011b)

budget driven shrinkage. instead of a id173 constant,
specify a budget on the number of selected groups. each proximal
step sets   t to meet this target.
sparseptron. let l(w) = w(cid:62)(f(x,   y )     f(x, y )) be the id88
loss. the algorithm becomes id88 with shrinkage.

debiasing. run a few iterations of sparseptron to identify the
relevant groups. then run a unregularized learner at a second stage.

memory e   ciency. only a
small active set of features need
to be maintained. entire groups
can be deleted after each
proximal step.
many irrelevant features are
never instantiated.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

106 / 155

0510150246x 106# epochs# features  mirasparceptron + mira (b=30)summary of algorithms

sparse? groups? overlaps?

coordinate descent
prox-grad (ist)
owl-qn
sparsa
fista
admm (c-salsa)
online subgradient
cumulative penalty
truncated gradient
fobos
rda
online prox-grad

converges?

(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)
?
(cid:88)
(cid:88)
(cid:88)
(cid:88)

rate?

?

o(1/ )

?

   
o(1/ )
o(1/
?

o(1/ 2)

?

o(1/ 2)
o(1/ 2)
o(1/ 2)
o(1/ 2)

(cid:88)

yes/no
yes/no
yes/no
 ) yes/no

no
no
(cid:88)
(cid:88)

sort of

(cid:88)
(cid:88)

maybe

(cid:88)
no
(cid:88)
(cid:88)
(cid:88)
(cid:88)
no
no
(cid:88)
(cid:88)
(cid:88)

no

not easy

no

not easy
not easy

(cid:88)
no
no
no

not easy
not easy

(cid:88)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

107 / 155

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

convex analysis

batch algorithms

online algorithms

5 applications

6 conclusions

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

108 / 155

applications of structured sparsity in nlp

relatively few to date (but this list may not be exhaustive).

1 martins et al. (2011b):

phrase chunking
id39
id33

2 semisupervised lexicon expansion (das and smith, 2012)

3 unsupervised tagging (gra  ca et al., 2009)

4 sociolinguistic association discovery (eisenstein et al., 2011)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

109 / 155

applications of structured sparsity in nlp

relatively few to date (but this list may not be exhaustive).

1 martins et al. (2011b):

phrase chunking
id39
id33

2 semisupervised lexicon expansion (das and smith, 2012)

3 unsupervised tagging (gra  ca et al., 2009)

4 sociolinguistic association discovery (eisenstein et al., 2011)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

109 / 155

martins et al. (2011b): group by template

feature templates provide a straightforward way to de   ne non-overlapping
groups.

to achieve group sparsity, we optimize:

n(cid:88)

n=1

(cid:123)(cid:122)

min
w

1
n

(cid:124)

l(w; xn, yn)

+    (w)

(cid:124)(cid:123)(cid:122)(cid:125)

regularizer

(cid:125)

empirical loss

where we use the (cid:96)2,1 norm:

   (w) =   

m(cid:88)

m=1

dm(cid:107)wm(cid:107)2

for m groups/templates.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

110 / 155

chunking

conll 2000 shared task (sang and buchholz, 2000)

unigram features: 96 feature templates using pos tags, words, and
word shapes, with various context sizes

bigram features: 1 template indicating the label bigram
baseline: l2-regularized mira, 15 epochs, all features,
cross-validation to choose id173 strength
template-based group lasso: 5 epochs of sparseptron + 10 of
mira

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

111 / 155

chunking experiments

# templates
model size

f1 (%)

baseline

96

template-based group lasso
40

20

30

10

5,300,396

93.10

71,075
92.99

158,844
93.28

389,065
93.59

662,018
93.42

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

112 / 155

memory requirement of sparseptron is < 7.5% of that of the baseline.
(oscillations are due to proximal steps after every 1,000 instances.)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

113 / 155

0510150246x 106# epochs# features  mirasparceptron + mira (b=30)applications of structured sparsity in nlp

relatively few to date (but this list may not be exhaustive).

1 martins et al. (2011b):

phrase chunking
id39
id33

2 semisupervised lexicon expansion (das and smith, 2012)

3 unsupervised tagging (gra  ca et al., 2009)

4 sociolinguistic association discovery (eisenstein et al., 2011)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

114 / 155

id39

conll 2002/2003 shared tasks (sang, 2002; sang and de meulder,
2003): spanish, dutch, and english

unigram features: 452 feature templates using pos tags, words, word
shapes, pre   xes, su   xes, and other string features, all with various
context sizes

bigram features: 1 template indicating the label bigram
baselines:

l2-regularized mira, 15 epochs, all features, cross-validation to choose
id173 strength
sparseptron with lasso, di   erent values of c

template-based group lasso: 5 epochs of sparseptron + 10 of
mira

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

115 / 155

named entity models: number of features. (lasso c = 1/  n.)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

116 / 155

0 1000000 2000000 3000000 4000000 5000000 6000000 7000000 8000000 9000000 10000000 spanish dutch english mira lasso (0.1) lasso (0.5) lasso (1) group lasso (100) group lasso (200) group lasso (300) named entity models: f1 accuracy on the test set. (lasso c = 1/  n.)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

117 / 155

60 65 70 75 80 85 spanish dutch english mira lasso (0.1) lasso (0.5) lasso (1) group lasso (100) group lasso (200) group lasso (300) applications of structured sparsity in nlp

relatively few to date (but this list may not be exhaustive).

1 martins et al. (2011b):

phrase chunking
id39
id33

2 semisupervised lexicon expansion (das and smith, 2012)

3 unsupervised tagging (gra  ca et al., 2009)

4 sociolinguistic association discovery (eisenstein et al., 2011)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

118 / 155

non-projective id33

conll-x shared task (buchholz and marsi, 2006): arabic, danish,
dutch, japanese, slovene, and spanish

arc-factored models (mcdonald et al., 2005)

684 feature templates by conjoining words, shapes, lemmas, and pos
of the head and the modi   er, contextual pos, distance and
attachment direction
baselines:

mira with all features
   lter-based template selection (information gain)
standard lasso

our methods: template-based group lasso; coarse-to-   ne
id173

budget sizes: 200, 300, and 400

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

119 / 155

non-projective id33 (c   ed)

template-based group lasso is better at selecting feature templates than
the ig criterion, and slightly better than coarse-to-   ne.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

120 / 155

24681012x 10676.57777.57878.5number of featuresuas (%)arabic  051015x 1068989.289.489.689.890danish02468x 1069292.59393.5japanese0246810x 10681828384slovene00.511.52x 1078282.58383.584spanish051015x 1067474.57575.576turkish  group   lassogroup   lasso (c2f)lassofilter   based (ig)which features get selected?

qualitative analysis of selected templates:

arabic danish

japanese

slovene

spanish

turkish

bilexical
lex.     pos
pos     lex.
pos     pos
middle pos
shape
direction
distance

++
+
++

++
++

++

+

+

++
++
+
+

+
+
++
++
++
+
+

+
++
++
+
+

+

+

++

+
+

+

++

+
+

(empty: none or very few templates selected; +: some templates
selected; ++: most or all templates selected.)

morphologically-rich languages with small datasets (turkish and
slovene) avoid lexical features.

in japanese, contextual pos appear to be especially relevant.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

121 / 155

which features get selected?

qualitative analysis of selected templates:

arabic danish

japanese

slovene

spanish

turkish

bilexical
lex.     pos
pos     lex.
pos     pos
middle pos
shape
direction
distance

++
+
++

++
++

++

+

+

++
++
+
+

+
+
++
++
++
+
+

+
++
++
+
+

+

+

++

+
+

+

++

+
+

(empty: none or very few templates selected; +: some templates
selected; ++: most or all templates selected.)

morphologically-rich languages with small datasets (turkish and
slovene) avoid lexical features.

in japanese, contextual pos appear to be especially relevant.
take this with a grain of salt: some patterns may be properties of
the datasets, not the languages!

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

121 / 155

applications of structured sparsity in nlp

relatively few to date (but this list may not be exhaustive).

1 martins et al. (2011b):

phrase chunking
id39
id33

2 semisupervised lexicon expansion (das and smith, 2012)

3 unsupervised tagging (gra  ca et al., 2009)

4 sociolinguistic association discovery (eisenstein et al., 2011)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

122 / 155

lexicon expansion (das and smith, 2012)

desired: mapping from words (types) to categories (e.g., pos or
semantic predicates)

allow ambiguity, but not too much!

given some words    mappings and a large corpus

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

123 / 155

lexicon expansion (das and smith, 2012)

approach:

1 calculate distributional vectors for words

2 construct a graph with words as vertices; edges from a word to the

k-most similar distributional vectors

3    link    known words to empirical distributions

4    propagate    label distributions throughout the graph (corduneanu

and jaakkola, 2003; zhu et al., 2003; subramanya and bilmes, 2008,
2009; talukdar and crammer, 2009)

known as graph-based semisupervised learning.

see noah   s talk about this work on wednesday!

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

124 / 155

example (das and smith, 2011)

green words are observed in framenet data, each with a single frame
(category); other words come from a larger, unlabeled corpus.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

125 / 155

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              graph-based ssl

here we reason about types, not tokens (instances).

regularized empirical risk minimization doesn   t quite describe this
setting.

instead, think of maximum a posteriori id136 in a factor graph
g = (v, f, e):

p({v}v   v) =

1
z

  f ({v}v   v:(v ,f )   e)

(cid:89)

f    f

where v are random variables, f are factors, and e are edges.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

126 / 155

factor graph representation of graph-based ssl

shaded variables   x1 and   x4 take the values of empirical distributions over
categories for words 1 and 4. shaded factors encourage inferred
distributions x1 and x4 to be similar to them. solid white factors
encourage smoothness across the graph, and dashed unary factors can be
used to encourage sparsity.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

127 / 155

x1x4x3x2(cid:19)2

qn(y )     1
|y|

(cid:18)
(cid:88)

(cid:88)

n

y

unary factors

here, qn(y ) is an unnormalized distribution over categories given the word
associated with the nth vertex.

three unary factor conditions:

uniform squared (cid:96)2:      

(cid:88)

(cid:88)

n

y

used in past work (subramanya et al., 2010); with quadratic pairwise
penalties and normalized q, generalizes zhu et al. (2003)

lasso ((cid:96)1) for global sparsity:      

|qn(y )|

elitist lasso (squared (cid:96)1,2; kowalski and torr  esani, 2009) for
per-vertex sparsity):

(cid:32)(cid:88)

(cid:88)

     

|qn(y )|

(cid:33)2

n

y

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

128 / 155

experimental results: expanding the framenet

lexicon

vertices: lemmatized, coarse pos-tagged word types
each qn(  ) is a(n unnormalized) distribution over 877 semantic frames
9,263 vertices with labels, 55,217 unlabeled

accuracy is for unknown predicates, partial match score (semeval
2007)

supervised
das and smith (2011)
(normalized qn, squared (cid:96)2-uniform)
squared (cid:96)2-uniform
(cid:96)1
squared (cid:96)1,2

accuracy

46.62

lexicon size
   

62.35
62.81
62.43
65.28

128,960
128,232
128,771
45,554

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

129 / 155

applications of structured sparsity in nlp

relatively few to date (but this list may not be exhaustive).

1 martins et al. (2011b):

phrase chunking
id39
id33

2 semisupervised lexicon expansion (das and smith, 2012)

3 unsupervised tagging (gra  ca et al., 2009)

4 sociolinguistic association discovery (eisenstein et al., 2011)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

130 / 155

unsupervised tagging (gra  ca et al., 2009)

posterior id173 (ganchev et al., 2010): penalize
probabilistic models based on properties of their posterior
distributions.
one such property: for each token, the number of labels with nonzero
id203.

related idea: ravi and knight (2009) directly minimized the number
of tag bigram types allowed by the model (using ilp).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

131 / 155

understanding posterior id173

begin with a generative model pw(x , y ). we are unsupervised here,
so y is always hidden.

l(w; xn) =     log(cid:80)

y   y pw(xn, y ).

this leads to log marginal likelihood as loss:

for a given x, de   ne a set of distributions
qx,   = {q(y | x) | eq[f(x, y )]     b       }.
for a model distribution pw(x , y ), de   ne a regularizer:

   (w,   ) =   (cid:107)  (cid:107)   +

kl(q(  )(cid:107)pw(   | xn))

min
q   qxn,  

n(cid:88)

n=1

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

132 / 155

optimization for posterior id173

an iterative em-like algorithm can be used to locally optimize the
regularized loss:

e-step: calculate posteriors pw(y | xn),   n. (hinges on local
factorization of pw.)
project onto qxn: for each n:

   y , q(y | xn)     pw(y | xn)e        

n

(cid:62)f(xn,y )

where       are the solution to the dual of the optimization problem
inside    . (hinges on local factorization of f.)
m-step: solve quasi-supervised problem using q to    ll in the
distribution over y :

n(cid:88)

(cid:88)

n=1

y

min
w

   q(y | xn) log pw(xn, y ))

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

133 / 155

de   ne indicator features:

posterior sparsity (gra  ca et al., 2009)

          1 if xn contains the ith occurrence of w at position j

and y [j] = t

0 otherwise

fi,w ,t(xn, y ) =

bi,w ,t = 0

regularize with the (cid:96)   ,1 norm (   is solved for and substituted):

   (w) =   

epw[fi,w ,t]

+

kl(q(  )(cid:107)pw(   | xn))

min
q   qxn

(cid:88)
(cid:124)

w ,t

(cid:124)

max

i

(cid:123)(cid:122)
(cid:123)(cid:122)

(cid:96)   

(cid:96)1

n(cid:88)

n=1

(cid:125)
(cid:125)

the dual form of the optimization problem is solvable with projected
id119.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

134 / 155

from ganchev et al. (2010),    gure 11. (left) initial tag distributions for
20 instances of a word. (middle) optimal dual variables   ; each row sums
to    = 20. (right) q concentrates posteriors for all instances on the nn
tag, reducing the (cid:96)   ,1 norm from     4 to     1.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

135 / 155

unsupervised tagging results (gra  ca et al., 2009)

average accuracy (standard deviation in parentheses) over 10 di   erent
runs (random seeds identical across models) for 200 iterations. sparse pr
constraint strength is give in parentheses.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

136 / 155

applications of structured sparsity in nlp

relatively few to date (but this list may not be exhaustive).

1 martins et al. (2011b):

phrase chunking
id39
id33

2 semisupervised lexicon expansion (das and smith, 2012)

3 unsupervised tagging (gra  ca et al., 2009)

4 sociolinguistic association discovery (eisenstein et al., 2011)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

137 / 155

sociolinguistic association discovery

dataset:

geotagged tweets from 9,250 authors
mapping of locations to the u.s. census    zip code tabulation areas
(zctas)
a ten-dimensional vector of statistics on demographic attributes

can we learn a compact set of terms used on twitter that associate
with demographics?

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

138 / 155

sociolinguistic association discovery (eisenstein

et al., 2011)

setup: multi-output regression.

xn is a p-dimensional vector of independent variables; matrix is
x     rn  p
yn is a t -dimensional vector of dependent variables; matrix is
y     rn  t
wp,t is the regression coe   cient for the pth variable in the tth task;
matrix is w     rp  t
regularized objective with squared error loss typical for regression:

   (w) + (cid:107)y     xw(cid:107)2

f

min
w

regressions are run in both directions.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

139 / 155

structured sparsity with (cid:96)   ,1

drive entire rows of w to zero (turlach et al., 2005):    some
predictors are useless for any task   

   (w) =   (cid:80)t

t=1 maxp wp,t

optimization with blockwise coordinate ascent (liu et al., 2009) and
some tricks to maintain sparsity:

scale x to achieve variance 1 for each predictor
precompute c = x(cid:62)y     n  x(cid:62)  y, where   x and   y are mean row vectors
for x, y, respectively
precompute d = x(cid:62)x     n  x(cid:62)  x
more regression tricks in eisenstein et al. (2011)

related work: duh et al. (2010) used multitask regression and (cid:96)2,1 to
select features useful for reranking across many instances (application
in machine translation).

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

140 / 155

predicting demographics from text (eisenstein

et al., 2011)

predict 10-dimensional zcta characterization from words tweeted in
that region (vocabulary is p = 5, 418)
measure pearson   s correlation between prediction and correct value
(average over tasks, cross-validated test sets)
compare with truncated svd, greatest variance across authors, most
frequent words

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

141 / 155

1021030.160.180.20.220.240.260.28number of featuresaverage correlation  multi   output lassosvdhighest variancemost frequentpredictive words (eisenstein et al., 2011)

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

142 / 155

predicting text from demographics (eisenstein

et al., 2011)

embed the model in a feature induction outer loop:    screen and
clean    (wu et al., 2010)
compare language model perplexity of models with no demographic
features, raw demographic features (10), and 37 discovered
conjunctive features.

signi   cant reduction compared to both baselines.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

143 / 155

predictive demographic features (eisenstein et al.,

2011)

selected demographic features and words with high and low log-odds
associated with each.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

144 / 155

outline

1 introduction

2 id168s and sparsity

3 structured sparsity

4 algorithms

convex analysis

batch algorithms

online algorithms

5 applications

6 conclusions

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

145 / 155

summary

sparsity is desirable in nlp: feature selection, runtime, memory
footprint, interpretability
beyond plain sparsity: structured sparsity can be promoted through
group-lasso id173

choice of groups re   ects prior knowledge about the desired sparsity
patterns.

we have seen examples for feature template selection, grid sparsity,
and elite discovery, but many more are possible!

small/medium scale: many batch algorithms available, with fast
convergence (ist, fista, sparsa, ...)

large scale: online proximal-gradient algorithms suitable to explore
large feature spaces

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

146 / 155

thank you!

questions?

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

147 / 155

acknowledgments

national science foundation (usa), career grant iis-1054319

funda  c  ao para a ci  encia e tecnologia (portugal), grant
pest-oe/eei/la0008/2011.

funda  c  ao para a ci  encia e tecnologia and information and
communication technologies institute (portugal/usa), through the
cmu-portugal program.

priberam: qren/por lisboa (portugal), eu/feder programme,
discooperio project, contract 2011/18501.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

148 / 155

references i

amaldi, e. and kann, v. (1998). on the approximation of minimizing non zero variables or unsatis   ed relations in linear

systems. theoretical computer science, 209:237   260.

andrew, g. and gao, j. (2007). scalable training of l1-regularized id148. in proc. of icml. acm.

bakin, s. (1999). adaptive regression and model selection in data mining problems. phd thesis, australian national university.

barzilai, j. and borwein, j. (1988). two point step size gradient methods. ima journal of numerical analysis, 8:141   148.

beck, a. and teboulle, m. (2009). a fast iterative shrinkage-thresholding algorithm for linear inverse problems. siam journal

on imaging sciences, 2(1):183   202.

bioucas-dias, j. and figueiredo, m. (2007). a new twist: two-step iterativeshrinkage/thresholding algorithms for image

restoration. ieee transactions on image processing, 16:2992   3004.

blumensath, t. (2012). compressed sensing with nonlinear observations and related nonlinear optimisation problems. technical

report, arxiv/1205.1650.

bottou, l. and bousquet, o. (2007). the tradeo   s of large scale learning. nips, 20.

buchholz, s. and marsi, e. (2006). conll-x shared task on multilingual id33. in proc. of conll.

cand`es, e., romberg, j., and tao, t. (2006a). robust uncertainty principles: exact signal reconstruction from highly

incomplete frequency information. ieee transactions on id205, 52:489   509.

cand`es, e., romberg, j., and tao, t. (2006b). stable signal recovery from incomplete and inaccurate measurements.

communications in pure and applied mathematics, 59:1207   1223.

carpenter, b. (2008). lazy sparse stochastic id119 for regularized multinomial id28. technical report,

technical report, alias-i.

caruana, r. (1997). multitask learning. machine learning, 28(1):41   75.

cessie, s. l. and houwelingen, j. c. v. (1992). ridge estimators in id28. journal of the royal statistical society;

series c, 41:191   201.

chen, s. and rosenfeld, r. (1999). a gaussian prior for smoothing maximum id178 models. technical report,

cmu-cs-99-108.

claerbout, j. and muir, f. (1973). robust modelling of erratic data. geophysics, 38:826   844.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

149 / 155

references ii

combettes, p. and wajs, v. (2006). signal recovery by proximal forward-backward splitting. multiscale modeling and

simulation, 4:1168   1200.

corduneanu, a. and jaakkola, t. (2003). on information id173. in proc. of uai.

das, d. and smith, n. a. (2011). semi-supervised frame-id29 for unknown predicates. in proc. of acl.

das, d. and smith, n. a. (2012). graph-based lexicon expansion with sparsity-inducing penalties. in proceedings of naacl.

daubechies, i., defrise, m., and de mol, c. (2004). an iterative thresholding algorithm for linear inverse problems with a

sparsity constraint. communications on pure and applied mathematics, 11:1413   1457.

davis, g., mallat, s., and avellaneda, m. (1997). greedy adaptive approximation. journal of constructive approximation,

13:57   98.

della pietra, s., della pietra, v., and la   erty, j. (1997). inducing features of random    elds. ieee transactions on pattern

analysis and machine intelligence, 19:380   393.

donoho, d. (2006). compressed sensing. ieee transactions on id205, 52:1289   1306.

duchi, j., shalev-shwartz, s., singer, y., and chandra, t. (2008). e   cient projections onto the l1-ball for learning in high

dimensions. in icml.

duchi, j. and singer, y. (2009). e   cient online and batch learning using forward backward splitting. jmlr, 10:2873   2908.

duh, k., sudoh, k., tsukada, h., isozaki, h., and nagata, m. (2010). n-best reranking by multitask learning. in proceedings of

the joint fifth workshop on id151 and metrics.

efron, b., hastie, t., johnstone, i., and tibshirani, r. (2004). least angle regression. annals of statistics, 32:407   499.

eisenstein, j., smith, n. a., and xing, e. p. (2011). discovering sociolinguistic associations with structured sparsity. in proc. of

acl.

figueiredo, m. and bioucas-dias, j. (2011). an alternating direction algorithm for (overlapping) group id173. in signal

processing with adaptive sparse structured representations   spars11. edinburgh, uk.

figueiredo, m. and nowak, r. (2003). an em algorithm for wavelet-based image restoration. ieee transactions on image

processing, 12:986   916.

figueiredo, m., nowak, r., and wright, s. (2007). gradient projection for sparse reconstruction: application to compressed

sensing and other inverse problems. ieee journal of selected topics in signal processing: special issue on convex
optimization methods for signal processing, 1:586   598.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

150 / 155

references iii

friedman, j., hastie, t., rosset, s., tibshirani, r., and zhu, j. (2004). discussion of three boosting papers. annals of

statistics, 32(1):102   107.

fu, w. (1998). penalized regressions: the bridge versus the lasso. journal of computational and graphical statistics, pages

397   416.

ganchev, k., gra  ca, j., gillenwater, j., and taskar, b. (2010). posterior id173 for structured latent variable models.

jmlr, 11:2001   2049.

gao, j., andrew, g., johnson, m., and toutanova, k. (2007). a comparative study of parameter estimation methods for

statistical natural language processing. in proc. of acl.

genkin, a., lewis, d., and madigan, d. (2007). large-scale bayesian id28 for text categorization. technometrics,

49:291   304.

goodman, j. (2004). exponential priors for maximum id178 models. in proc. of naacl.

gra  ca, j., ganchev, k., taskar, b., and pereira, f. (2009). posterior vs. parameter sparsity in latent variable models. advances

in neural information processing systems.

guyon, i. and elissee   , a. (2003). an introduction to variable and feature selection. journal of machine learning research,

3:1157   1182.

hale, e., yin, w., and zhang, y. (2008). fixed-point continuation for l1-minimization: methodology and convergence. siam

journal on optimization, 19:1107   1130.

hastie, t., taylor, j., tibshirani, r., and walther, g. (2007). forward stagewise regression and the monotone lasso. electronic

journal of statistics, 1:1   29.

haupt, j. and nowak, r. (2006). signal reconstruction from noisy random projections. ieee transactions on information

theory, 52:4036   4048.

jenatton, r., audibert, j.-y., and bach, f. (2009). structured variable selection with sparsity-inducing norms. technical report,

arxiv:0904.3523.

jenatton, r., mairal, j., obozinski, g., and bach, f. (2011). proximal methods for hierarchical sparse coding. journal of

machine learning research, 12:2297   2334.

kazama, j. and tsujii, j. (2003). evaluation and extension of maximum id178 models with inequality constraints. in proc. of

emnlp.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

151 / 155

references iv

kim, s. and xing, e. (2010). tree-guided group lasso for multi-task regression with structured sparsity. in proc. of icml.

kowalski, m. and torr  esani, b. (2009). sparsity and persistence: mixed norms provide simple signal models with dependent

coe   cients. signal, image and video processing, 3(3):251   264.

krishnapuram, b., carin, l., figueiredo, m., and hartemink, a. (2005). sparse multinomial id28: fast algorithms

and generalization bounds. pattern analysis and machine intelligence, ieee transactions on, 27:957   968.

lanckriet, g. r. g., cristianini, n., bartlett, p., ghaoui, l. e., and jordan, m. i. (2004). learning the kernel matrix with

semide   nite programming. jmlr, 5:27   72.

langford, j., li, l., and zhang, t. (2009). sparse online learning via truncated gradient. jmlr, 10:777   801.

lavergne, t., capp  e, o., and yvon, f. (2010). practical very large scale crfs. in proc. of acl.

liu, h., palatucci, m., and zhang, j. (2009). blockwise coordinate descent procedures for the multi-task lasso, with applications
to neural semantic basis discovery. in proceedings of the 26th annual international conference on machine learning, pages
649   656. acm.

mairal, j., jenatton, r., obozinski, g., and bach, f. (2010). network    ow algorithms for structured sparsity. in advances in

neural information processing systems.

mairal, j. and yu, b. (2012). complexity analysis of the lasso id173 path. technical report, arxiv:1205.0079.

markowitz, h. (1952). portfolio selection. journal of finance, 7:77   91.

martins, a. f. t., figueiredo, m. a. t., aguiar, p. m. q., smith, n. a., and xing, e. p. (2011a). online learning of structured

predictors with multiple kernels. in proc. of aistats.

martins, a. f. t., smith, n. a., aguiar, p. m. q., and figueiredo, m. a. t. (2011b). structured sparsity in structured

prediction. in proc. of empirical methods for natural language processing.

martins, a. f. t., smith, n. a., xing, e. p., aguiar, p. m. q., and figueiredo, m. a. t. (2010). turbo parsers: dependency

parsing by approximate variational id136. in proc. of emnlp.

mcdonald, r. t., pereira, f., ribarov, k., and hajic, j. (2005). non-projective id33 using spanning tree

algorithms. in proc. of hlt-emnlp.

muthukrishnan, s. (2005). data streams: algorithms and applications. now publishers, boston, ma.

negahban, s., ravikumar, p., wainwright, m., and yu, b. (2012). a uni   ed framework for high-dimensional analysis of

m-estimators with decomposable regularizers. technical report, department of eecs, uc berkeley.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

152 / 155

references v

nesterov, y. (2009). primal-dual subgradient methods for convex problems. mathematical programming, 120(1):221   259.

obozinski, g., taskar, b., and jordan, m. (2010). joint covariate selection and joint subspace selection for multiple

classi   cation problems. statistics and computing, 20(2):231   252.

osborne, m., presnell, b., and turlach, b. (2000). a new approach to variable selection in least squares problems. ima journal

of numerical analysis, 20:389   403.

perkins, s., lacker, k., and theiler, j. (2003). grafting: fast, incremental feature selection by id119 in function

space. journal of machine learning research, 3:1333   1356.

plan, y. and vershynin, r. (2012). robust 1-bit compressed sensing and sparse id28: a convex programming

approach. technical report, arxiv/1202.1212.

quattoni, a., carreras, x., collins, m., and darrell, t. (2009). an e   cient projection for l1,    id173. in proc. of icml.
ratnaparkhi, a. (1996). a maximum id178 model for part-of-speech tagging. in proc. of emnlp.

ravi, s. and knight, k. (2009). minimized models for unsupervised part-of-speech tagging. in proc. of acl.

sang, e. (2002). introduction to the conll-2002 shared task: language-independent id39. in proc. of

conll.

sang, e. and buchholz, s. (2000). introduction to the conll-2000 shared task: chunking. in proceedings of conll-2000 and

lll-2000.

sang, e. and de meulder, f. (2003). introduction to the conll-2003 shared task: language-independent named entity

recognition. in proc. of conll.

schaefer, r., roi, l., and wolfe, r. (1984). a ridge logistic estimator. communications in statistical theory and methods,

13:99   113.

schmidt, m. and murphy, k. (2010). convex structure learning in id148: beyond pairwise potentials. in proc. of

aistats.

shevade, s. and keerthi, s. (2003). a simple and e   cient algorithm for gene selection using sparse id28.

bioinformatics, 19:2246   2253.

shor, n. (1985). minimization methods for non-di   erentiable functions. springer.

sokolovska, n., lavergne, t., capp  e, o., and yvon, f. (2010). e   cient learning of sparse conditional random    elds for

supervised sequence labelling. ieee journal of selected topics in signal processing, 4(6):953   964.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

153 / 155

references vi

stojnic, m., parvaresh, f., and hassibi, b. (2009). on the reconstruction of block-sparse signals with an optimal number of

measurements. signal processing, ieee transactions on, 57(8):3075   3085.

subramanya, a. and bilmes, j. (2008). soft-supervised learning for text classi   cation. in proc. of emnlp.

subramanya, a. and bilmes, j. (2009). entropic graph id173 in non-parametric semi-supervised classi   cation. in proc.

of nips.

subramanya, a., petrov, s., and pereira, f. (2010). e   cient graph-based semi-supervised learning of structured tagging models.

in proc. of emnlp.

talukdar, p. p. and crammer, k. (2009). new regularized algorithms for transductive learning. in proc. of the ecml-pkdd.

taylor, h., bank, s., and mccoy, j. (1979). deconvolution with the (cid:96)1 norm. geophysics, 44:39   52.
tibshirani, r. (1996). regression shrinkage and selection via the lasso. journal of the royal statistical society b., pages

267   288.

tikhonov, a. (1943). on the stability of inverse problems. in dokl. akad. nauk sssr, volume 39, pages 195   198.

tillmann, a. and pfetsch, m. (2012). the computational complexity of rip, nsp, and related concepts in compressed sensing.

technical report, arxiv/1205.2081.

tseng, p. and yun, s. (2009). a coordinate id119 method nonsmooth seperable approximation. mathematical

programmin (series b), 117:387   423.

tsuruoka, y., tsujii, j., and ananiadou, s. (2009). stochastic id119 training for l1-regularized id148 with

cumulative penalty. in proc. of acl.

turlach, b. a., venables, w. n., and wright, s. j. (2005). simultaneous variable selection. technometrics, 47(3):349   363.

van de geer, s. (2008). high-dimensional generalized linear models and the lasso. the annals of statistics, 36:614   645.

wiener, n. (1949). extrapolation, interpolation, and smoothing of stationary time series. wiley, new york.

williams, p. (1995). bayesian id173 and pruning using a laplace prior. neural computation, 7:117   143.

wright, s., nowak, r., and figueiredo, m. (2009). sparse reconstruction by separable approximation. ieee transactions on

signal processing, 57:2479   2493.

wu, j., devlin, b., ringquist, s., trucco, m., and roeder, k. (2010). screen and clean: a tool for identifying interactions in

genome-wide association studies. genetic epidemiology, 34(3):275   285.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

154 / 155

references vii

xiao, l. (2010). dual averaging methods for regularized stochastic learning and online optimization. journal of machine

learning research, 11:2543   2596.

yarowsky, d. (1995). unsupervised id51 rivaling supervised methods. in proc. of acl.

yuan, l., liu, j., and ye, j. (2011). e   cient methods for overlapping group lasso. in advances in neural information

processing systems 24, pages 352   360.

yuan, m. and lin, y. (2006). model selection and estimation in regression with grouped variables. journal of the royal

statistical society (b), 68(1):49.

yun, s. and toh, k.-c. (2011). a coordinate id119 method for l1-regularized convex minimization. computational

optimization and applications, 48:273   307.

zhao, p., rocha, g., and yu, b. (2009). grouped and hierarchical model selection through composite absolute penalties. annals

of statistics, 37(6a):3468   3497.

zhu, j., lao, n., and xing, e. (2010). grafting-light: fast, incremental feature selection and structure learning of markov

random    elds. in proc. of international conference on knowledge discovery and data mining, pages 303   312.

zhu, x., ghahramani, z., and la   erty, j. d. (2003). semi-supervised learning using gaussian    elds and id94. in

proc. of icml.

martins, figueiredo, smith (ist, cmu)

structured sparsity in nlp

http://tiny.cc/ssnlp

155 / 155

