foundations and trends r(cid:1) in
information retrieval
vol. 5, nos. 4   5 (2011) 235   422
c(cid:1) 2012 m. larson and g. j. f. jones
doi: 10.1561/1500000020

spoken content retrieval:

a survey of techniques and technologies

by martha larson and gareth j. f. jones

contents

1 introduction

1.1 de   nition of spoken content retrieval (scr)
1.2 relationship of scr to information retrieval (ir)
1.3 relationship of scr to id103
1.4 scr and other    searching speech    tasks
1.5 a brief overview of scr research

2 overview of spoken content

indexing and retrieval

information retrieval for scr

2.1 general architecture of an scr system
2.2
2.3 evaluation
2.4 challenges for scr

3 automatic id103 (asr)

3.1 the nature of human speech
3.2 the hidden markov model framework
3.3 aspects of asr critical for scr
3.4 considerations for the combination

of asr and ir

237

240
242
245
246
250

256

256
259
271
273

277

278
286
292

299

4 exploiting automatic speech

recognition output

4.1 going beyond 1-best asr output
4.2 con   dence scores
4.3 representing speech with subwords
4.4 subwords as indexing features
4.5 hybrid approaches for scr
4.6 techniques for detecting spoken terms

5 spoken content retrieval
beyond asr transcripts

5.1 motivation for moving beyond asr
5.2 augmenting asr and spoken content
5.3 structuring and representing spoken content

6 accessing information in spoken content

6.1 query entry
6.2 display of results for selection
6.3 review and playback of results

7 conclusion and outlook

references

308

310
317
324
327
341
346

353

353
359
368

376

377
379
384

395

399

foundations and trends r(cid:1) in
information retrieval
vol. 5, nos. 4   5 (2011) 235   422
c(cid:1) 2012 m. larson and g. j. f. jones
doi: 10.1561/1500000020

spoken content retrieval:

a survey of techniques and technologies

martha larson1 and gareth j. f. jones2

1 faculty of electrical engineering, mathematics and computer science,
multimedia information retrieval lab, delft university of technology,
delft, the netherlands, m.a.larson@tudelft.nl

2 centre for next generation localisation, school of computing, dublin

city university, dublin, ireland, gjones@computing.dcu.ie

abstract

speech media, that is, digital audio and video containing spoken con-
tent, has blossomed in recent years. large collections are accruing
on the internet as well as in private and enterprise settings. this
growth has motivated extensive research on techniques and technologies
that facilitate reliable indexing and retrieval. spoken content retrieval
(scr) requires the combination of audio and speech processing tech-
nologies with methods from information retrieval (ir). scr research
initially investigated planned speech structured in document-like units,
but has subsequently shifted focus to more informal spoken content
produced spontaneously, outside of the studio and in conversational
settings. this survey provides an overview of the    eld of scr encom-
passing component technologies, the relationship of scr to text ir
and automatic id103 and user interaction issues. it is

aimed at researchers with backgrounds in speech technology or ir
who are seeking deeper insight on how these    elds are integrated to
support research and development, thus addressing the core challenges
of scr.

1

introduction

spoken content retrieval (scr) provides users with access to digi-
tized audio-visual content with a spoken language component. in recent
years, the phenomenon of    speech media,    media involving the spoken
word, has developed in four important respects.

first, and perhaps most often noted, is the unprecedented volume
of stored digital spoken content that has accumulated online and in
institutional, enterprise and other private contexts. speech media col-
lections contain valuable information, but their sheer volume makes
this information useless unless spoken audio can be e   ectively browsed
and searched.

second, the form taken by speech media has grown progressively
diverse. most obviously, speech media includes spoken-word audio col-
lections and collections of video containing spoken content. however,
a speech track can accompany an increasingly broad range of media.
for example, speech annotation can be associated with images cap-
tured with smartphones. current developments are characterized by
dramatic growth in the volume of spoken content that is spontaneous
and is recorded outside of the studio, often in conversational settings.

237

238

introduction

third, the di   erent functions ful   lled by speech media have
increased in variety. the spoken word can be used as a medium for com-
municating factual information. examples of this function range from
material that has been scripted and produced explicitly as video, such
as television documentaries, to material produced for a live audience
and then recorded, such as lectures. the spoken word can be used as
a historical record. examples include speech media that records events
directly, such as meetings, as well as speech media that captures events
that are recounted, such as interviews. the spoken word can also be
used as a form of entertainment. the importance of the entertainment
function is re   ected in creative e   orts ranging from professional    lm to
user-generated video on the internet.

fourth, user attitudes towards speech media and the use of speech
media have evolved greatly. although privacy concerns dominate, the
acceptance of the creation of speech recordings, for example, of call cen-
ter conversations, has recently grown. also, users are becoming increas-
ingly acquainted with the concept of the spoken word as a basis on
which media can be searched and browsed. the expectation has arisen
that access to speech media should be as intuitive, reliable and com-
fortable as access to conventional text media.

the convergence of these four developments has served to change
the playing    eld. as a result, the present time is one of unprecedented
potential for innovative new applications for scr that will bring bene   t
to a broad range of users. search engines and retrieval systems that
make use of scr are better able to connect users with multimedia
items that match their needs for information and content.

this survey is motivated by the recognition of the recent growth in
the potential of scr and by our aim to contribute to the realization
of that potential. it provides an integrated overview of the techniques
and technologies that are available to design and develop state-of-the-
art scr systems. we bring together information from other overviews
on the subject of searching speech [5, 25, 38, 76, 91, 148, 166, 180,
193] as well as from a large number of individual research papers. our
survey di   ers from other overviews in that it encompasses a broad range
of application domains and is organized in terms of the overarching
challenges that face scr.

239

the basic technology used for scr is automatic speech recog-
nition (asr), which generates text transcripts from spoken audio.
na    vely formulated, scr can be considered the application of infor-
mation retrieval (ir) techniques to asr transcripts. the overarching
challenges of scr present themselves di   erently in di   erent applica-
tion domains. this survey takes the position that an scr system for a
particular application domain will be more e   ective if careful consid-
eration is given to the integration of asr and ir. the survey provides
information on when and how to move beyond a na    ve combination of
asr and ir to address the challenges of scr. undeniably, asr has
made considerable progress in recent years. however, developing raw
technologies and computational power alone will not achieve the aim
of making large volumes of speech media content searchable. rather,
it is necessary to understand the nature of the spoken word, spoken
word collections and the interplay between asr and ir technologies,
in order to achieve this goal.

reading the survey. this survey is aimed at the reader with a back-
ground in speech technologies or ir who seeks to better understand the
challenges of developing algorithms and designing systems that search
spoken media. it provides a review of the component technologies and
the issues that arise when combining them. finally, it includes a brief
review of user interaction issues, which are key to truly useful scr
systems.

the survey can be read sequentially from beginning to end, but is
structured in modules, making it possible to read parts of the survey
selectively:

    the present introduction de   nes scr as it is used in this sur-
vey, and di   erentiates it from related tasks that fall outside
of the survey   s scope. further, it provides a brief overview of
scr research, including a summary of the two-decade his-
tory of the    eld of scr.
    overview of spoken content indexing and retrieval begins
with the presentation of a general scr architecture. for
completeness, a high-level overview of ir techniques is

240

introduction

provided. then, in subsection 2.4,    challenges for scr,    we
set out a list of the key challenges faced in designing and
implementing scr systems. the presentation of scr tech-
niques and technologies in the remainder of the survey is
motivated by the need to address these key challenges.
    automatic id103 presents, for completeness, a
high-level overview of human speech and of asr technology.
then, issues speci   c to scr are addressed in subsection 3.3,
   aspects of asr critical for scr,    and in subsection 3.4,
   considerations for the combination of asr and ir.    these
subsections focus on speci   c aspects of asr and its integra-
tion with ir and introduce issues that are covered in greater
depth in the rest of the survey.
    exploiting automatic id103 output presents
techniques used to exploit asr within an scr system,
including making use of multiple asr hypotheses and sub-
word units.
    spoken content retrieval beyond asr transcripts discusses
how asr output can be supplemented to improve scr,
including issues related to extending asr transcripts e   ec-
tively and also to structuring and representing speech media.
    accessing information in spoken content addresses issues
involving user interaction with speech media and the presen-
tation of search results to users.
    conclusion and outlook summarizes the major themes from
a high-level perspective and presents an outlook to the future.

a particularly important feature of this survey is its extensive bib-
liography, including over 300 references. the bibliography was selected
with the goal of providing a comprehensive selection of entry points
into the literature that would allow further exploration of the issues
covered by this survey.

1.1 de   nition of spoken content retrieval (scr)

in the broad sense, scr encompasses any approach that aims to
provide users with access to speech media. however, in the narrow

1.1 de   nition of spoken content retrieval (scr)

241

sense, its goal is much more speci   c. in scr,    retrieval    is used as
it is in ir, namely, to designate the task of automatically returning
content with the aim of satisfying a user information need, expressed
by a user query. scr involves an interpretation of the user need and a
matching of that need to the speech media. we formalize this concept
with the following de   nition:

spoken content retrieval is the task of returning
speech media results that are relevant to an information
need expressed as a user query.

since the emergence of research related to search of speech media,
a number of terms have been used to refer to various tasks and tech-
niques. it is worthwhile highlighting their similarities and di   erences
here. the term    speech retrieval    (sr) was used in the    rst ir paper to
treat scr [87], which explored search of radio news. this form of scr
soon became generally known as    spoken document retrieval    (sdr).
this term is used to refer to retrieval techniques for collections having
pre-de   ned document structure, such as stories in broadcast news. as
the    eld has matured, it has become clear that for many tasks, there
is no pre-de   ned or natural de   nition of documents and that the term
sdr is not always appropriate. the term    speech retrieval    [205] was
re-adopted as an umbrella designation for search in collections with and
without document boundaries.

at the same time, the    eld of    voice search    or    voice retrieval    has
emerged, which is focused on returning results (which may be textual)
to queries that have been spoken by users [285]. in order to clearly
distinguish searching speech tasks from spoken-query tasks, the desig-
nation    speech-based information retrieval    is used. this designation
also serves to emphasize that the results returned to the user may
actually have other modalities alongside of spoken content, such as the
visual channel in video [202]. our choice of    spoken content retrieval   
encompasses both sdr and sr, while keeping the focus clearly on the
spoken word as content, not query, and including not just audio-only
speech content, but rather speech media in its wide array of di   erent
forms, including video.

242

introduction

1.2 relationship of scr to information retrieval (ir)

scr is often characterized as ir performed over text transcripts gen-
erated by an asr system. this survey takes the position that this
characterization is too na    ve to be useful in every situation. the extent
to which it is possible to create an scr system by indexing the output
of an out-of-the-box asr system using an out-of-the-box ir system
will ultimately depend on the domain of application and the use case,
including the user tasks, the complexity and content of the data, the
types of queries that users issue to the system and the form of results
that they expect to receive in return. in this subsection, we discuss
scr issues from the ir perspective.

1.2.1 di   erences between scr and ir

generally speaking, there are several di   erences between scr and text
ir that vary to di   ering degrees depending on the situation. the most
often cited di   erence between scr and text ir is the fact that tran-
scriptions generated by asr systems generally contain errors. this can
mean that an scr system will often need to make a collection search-
able using asr transcripts that have a high average error rate, some-
times as high as 50%. under such conditions, scr cannot be treated as
merely a text ir task since this level of noise in the    text    will impact
on ir e   ectiveness.

an additional di   erence is that spoken audio, unlike text, is rarely
structured into logical units such as paragraphs, or even sentences,
meaning that some form of segmentation into retrieval units is often
required prior to entering the data into the retrieval system. also,
speech is a temporal medium, meaning that a speech signal extends
over a    xed length of time. as a result, accessing raw spoken content is
time consuming and ine   cient, meaning that scr systems must pro-
vide visualizations of spoken content in results lists and in playback
interfaces. such visualizations allow users to scan and access spoken
material e   ciently, faster than in real time.

further, it is important not to overlook the fact that asr technol-
ogy can generate information that is not included in standard text

1.2 relationship of scr to information retrieval (ir)

243

media. this information can be exploited by the scr system and
generally comes in several forms. first, each recognized word is accom-
panied by a time code indicating its position within the speech media.
second, the asr system generates acoustic information re   ecting the
closeness of the match between a given word and the speech signal
at a particular position. third, the asr system generates information
about words that were potentially spoken within the speech signal, but
were not found by the system to be in the most likely transcription of
the signal (so-called multiple hypotheses). also, when combined with
additional audio analysis technology, an asr system is able to generate
rich transcripts that contain more information than text. for exam-
ple, encoding speaker characteristics such as speaker change points,
male/female speaker and identifying the speaker or audio events, such
as applause and laughter. we return to issues relevant to the di   er-
ence between scr and ir in exploiting automatic id103
output and spoken content retrieval beyond asr transcripts.

1.2.2 user information needs for scr

an information need can be de   ned as the reason for which the user
turns to a search engine [57]. in our case, the information need is the
reason why the user turns to an scr system. the information need can
be thought of as the set of characteristics that an item must possess in
order for it to satisfy the requirement that motivated the user to engage
in a search activity. in general, the sorts of characteristics desired by
users determine the approaches that are best deployed by the scr
system. assumptions about the nature of user needs inform the design
process of an scr system. the more explicit these assumptions can
be made, the more likely the scr system will succeed in ful   lling user
needs. for example, if it is safe to assume that users will be satis   ed
with segments of audio in which a speaker has pronounced the query
term or terms, then the scr system should be implemented as a system
that detects the location of mentions of speci   c spoken terms. from this
most basic       nding mention    type of speech search, systems should
grow more complex, only to the extent that it is necessary in order to
meet the user needs.

244

introduction

in [298], it is noted that speech retrieval systems have conventionally
paid little attention to user requirements. here, we mention a handful
of examples of research papers on systems, which give a clear state-
ment of the nature of the user need that the systems are designed
to handle. an early example is the voice message routing application
in [231], which speci   es that the system is intended to sort voice mes-
sages or route incoming customer telephone calls to customer service
areas. in [94], the design of an scr system for a large oral history
archive is described. user requirement studies were performed that
made use of actual requests that had been submitted to the archives
and also of the literature concerning how historians work with oral his-
tory transcripts. in [21], a user study is conducted for the domain of
podcasts, and    ve di   erent user goals in podcast search are identi   ed
and used as the basis for evaluation of an scr system.

the reasons that motivate users to turn to speech search are diverse.
it is arguable that the range of user search goals for scr is larger
than for traditional text-based ir. consider the example query, taxes
lipreading. two possible information needs behind this query are:
   find results discussing george bush   s famous quote, read my lips, no
new taxes    and    find items discussing recent decisions by the federal
communications commission to impose a fee on video relay service
for the deaf.    it is clear that the query either under-speci   es or mis-
speci   es the information need and that the ir system will have a serious
burden of query interpretation. however, the possibilities are multiplied
if the collection to be searched contains speech media rather than text.
in addition to these two information needs, the following could also be
possible,    find items in which a speaker pronounces the phrase read
my lips, no new taxes    and    find a recording of the original speech in
which bush said read my lips, no new taxes.   

in order to satisfy user information needs, an scr system must
also ful   ll user interaction requirements. in general, it is not su   cient
that the scr system returns items that are good matches to the user
information need. rather, the system must also present an item in a
way that also convinces users that it is a good match. users do not
examine all results in detail, and are very likely to skip over results
that, at the    rst glance, look like they will not be useful. the e   ect is

1.3 relationship of scr to id103

245

particularly egregious in the case of scr, due to the time that it takes
to    listen-in    to particular spoken content hits or view individual seg-
ments of video. we will return to these issues in more detail in spoken
content retrieval beyond asr transcripts and accessing information
in spoken content.

1.3 relationship of scr to id103

in this subsection, we discuss scr issues from the asr perspective.
id103 research naturally falls into two main branches. the
   rst branch, called speech understanding (su), is devoted to develop-
ing dialogue systems capable of carrying on conversations with humans
for the purpose of, for example, providing train schedule information.
the second branch is arguably the more closely related to scr and has
performed research in the    listening typewriter    speech transcription
paradigm. under this paradigm, given a stream of speech, the goal of
the asr system is to generate a transcript of the words spoken, equiv-
alent to one that would be made by a human sitting at a typewriter.
in this paradigm, the asr system should operate as independently
as possible from the domain or the topic of speech. by contrast, su
systems typically operate in highly constrained domains and involve
complex models intended to capture and exploit the semantic intent of
the speaker.

recently, the    eld of asr has been moving away from the    listening
typewriter    paradigm and towards forms of speech output that are
speci   cally designed to provide indexing terms (words and phrases)
that can be used as the basis of scr. early systems used a    xed set
of keywords and identi   ed spoken instances of these keywords in the
speech stream, a task referred to as    wordspotting    [301]. further devel-
opment in this area was devoted to dropping the restriction that the
keywords must be speci   ed in advance [122]. more recently, the key-
word spotting paradigm has attracted renewed interest dedicated to
creating e   cient systems capable of handling large amounts of spoken
content. for such systems, the designation    spoken term detection   
(std) is generally applied [199]. the std task returns instances of
particular words being pronounced within the speech stream. a related

246

introduction

task, spoken utterance retrieval (sur), involves returning short doc-
uments in which speci   c words are pronounced. if std or sur is used
to search for particular query words, it can be considered a form of
speech search, or even retrieval. however, std and sur are, in and of
themselves, blind to larger meaning. in other words, systems designed
to carry out these tasks make no attempt to match results with an
underlying need for a speci   c sort of content (e.g., content on a partic-
ular topic) expressed by the user query. in order to match speech media
and user needs, scr is necessary. in the next subsection, we develop a
systematic comparison between tasks closely related to asr and those
that are from core scr tasks.

1.4 scr and other    searching speech    tasks

it is possible to identify a large range of    searching speech    tasks that
are similar to scr in that they can be characterized by the same surface
form (i.e., matching a string to speech content) and also make use of
the same underlying technology (i.e., asr). these tasks are related to
scr, but are distinct from the core case of scr that is the topic of
this survey. we distinguish between four di   erent tasks, summarized
in table 1.1, that have the same surface form as scr and make use of
asr technology.

the four tasks are broken down along two dimensions. the    rst
dimension involves how the system addresses the user need, that is,
the criteria by which the match between the user query and the spoken
content items is determined. in a       nding mentions    type task, the

table 1.1. asr-based search takes the form of four tasks, involving two dimensions.

system addresses need by    nding
mentions

(words or phrases)

system addresses need by    nding
relevant content

(documents, segments, entry points)

user need known
at indexing time

wordspotting

user need known
at search time

spoken term

detection (std)

classi   cation

   ltering

spoken content

retrieval (scr)

1.4 scr and other    searching speech    tasks

247

user inputs a query and the system returns occurrences of the query
string found within the asr transcript. this type of task includes
wordspotting, std and sur. in a       nding mentions    task, a hit is
considered to be a successful match to the query if it contains the
words or the query string pronounced in the speech stream. a mention
can be returned as a result to the user in the form of either a time-
point (i.e., for std) or a larger item containing that string (i.e., for
sur). in a       nding content    task, the user inputs a query and the
system returns items that either treat the topic speci   ed by that query
or    t the description of that query. we consider the core case of scr
to be       nding content    tasks. the importance of the       nding content   
scr task is also emphasized in [38], which refers to it as    evaluating
performance from a document retrieval point of view    (p. 42).

it is important to recognize that for a       nding mentions    task and
for a       nding content    task the input string (i.e., the query) can be
identical. the di   erence lies in how the retrieval system interprets the
information need behind this query. a simple example illustrates the
di   erence. under an scr scenario, the retrieval system would respond
to the query volcanic ash, by providing results that explain the prop-
erties, causes and e   ects of volcanic ash. if a speaker utters the sen-
tence,    the organizers put together a diverse and interesting program
and the future internet assembly was a great success, despite air
travel interruption due to volcanic ash,    the appearance of the phrase
   volcanic ash    in that utterance would not necessarily be su   cient to
constitute relevance for an scr result. the topic of this utterance is
the future internet assembly, and it is likely to be more directly rele-
vant to queries concerning this event. under an std scenario, however,
this phrase would clearly be relevant to the query volcanic ash since
it contains the spoken phrase    volcanic ash.    if the system failed to
return this occurrence as a result, the std system would be consid-
ered to have failed to retrieve a relevant result.

the second dimension involves prior availability of the information
concerning the requests to which the system is expected to provide a
response. the    rst category under this dimension comprises tasks that
have information about the user need at indexing time, that is, at the
moment at which indexing features for the spoken content items are

248

introduction

generated. early wordspotting systems are       nding mention    systems,
which fall into this category. here, the information need is    xed in the
form of a list of terms that must be found in the spoken content stream.
as noted earlier, for such wordspotting systems, this list must be known
at    asr-time,    that is, the moment at which the asr transcripts are
produced. spoken content classi   cation and    ltering systems also fall
into this category. here, the information need is constituted by a topic
class and the system is provided in advance with a list of classes it is
expected to identify. the classi   cation system judges the content of the
speech media items and makes a decision on whether or not each item
belongs to a class. note that spoken content classi   cation is a       nd-
ing content    type task, thus mention of the name of the class (e.g.,
   cooking   ) in the speech media item is not enough to guarantee mem-
bership in that class. the item must actually treat subject material that
belongs to that topical class. typically, labeled training data are used to
train classi   ers that are able to separate in-class from out-of-class items.
the second category under this dimension comprises tasks for
which no information about the user need or query is available until
search time. early wordspotting systems quickly evolved into keyword
spotting systems that required no advance knowledge of the query.
currently, keyword spotting techniques are researched in the context
of either std or sur. the core case of scr is a       nding content    task
in which there is no information available in advance. in sum, although
scr is clearly related to other    searching speech    tasks, it is distinct
in that it involves responding to the information need, i.e., the topical
speci   cation or the item description, represented by an ad hoc query
posed by the user.

   searching speech    tasks also di   er according to whether the spoken
content collection is treated as static, or relatively static, or whether
it involves a steady stream of incoming spoken content. thus far, we
have discussed tasks that involve a static collection, one that does not
grow over time. in another scenario, the collection is dynamic, that is,
new speech content is constantly arriving and the goal of the system is
to make a judgment about the incoming stream. such a task is referred
to as information    ltering or media monitoring. the information need
can consist of    nding mention, or it can consist of identifying topics.

1.4 scr and other    searching speech    tasks

249

if new topics must be discovered within the stream, the task is often
referred to as topic detection and tracking (tdt) [4].

it is important to note that although the tasks in table 1.1 cannot
all be considered core cases of scr, they all make an important
contribution to scr. as has been noted, these tasks are all    searching
speech    tasks, that is, they are related via their surface form and their
use of asr. however, there is a further connection that motivates
us to include discussion of these tasks in this survey: these tasks can
be used as sub-components of an scr system whose function it is to
extract indexing features that will be used for the purposes of retrieval.
we will return to mention these tasks again in exploiting automatic
id103 output and spoken content retrieval beyond asr
transcripts.

1.4.1 other tasks related to scr

we now proceed to brie   y treat two other tasks that are often men-
tioned in the context of searching speech, but which do not fall into
the scope of this survey.

spoken queries/query by example. spoken queries can be used
to query either a text collection or a speech media collection. in
either case, if the query is short, a word error in the query can be
di   cult to compensate for. systems that accept spoken queries are
often referred to as    voice search    systems. work on spoken queries
includes [15, 151, 285]. research comparing spoken to written queries
is described in [56, 191]. finally, a technique that bears a   nity with
spoken query techniques is query by example [188, 262]. here the
information need of the user is speci   ed with a sample of the types of
documents that are relevant and the system returns documents that
match these samples on the basis of spoken content. techniques in
which the user need is expressed as speech are clearly relevant for scr,
but will not be treated as part of the material covered in this survey.

id53. the task of id53 (qa) involves
extracting the answer to a user   s question from an information source.
there has been very extensive work on qa from text sources in recent

250

introduction

years. however, there is also interest in developing qa for spoken
data. for example qa for lectures and meetings has been reported
in [55, 233], while [310] describes research on video news qa. qa for
spoken data can utilize many of the methods developed for text qa.
however, as with the application of any natural language processing
techniques to speech, the noise in asr transcripts must be taken into
account. this may require methods to be simpli   ed for application to
speech data. in terms of answer presentation, this could simply make
use of the asr transcript. alternatively a portion of the audio could be
played back. in the case of the latter option, the potential need to pro-
vide context to enable the user to understand what is being said must be
taken into account. id53 is also quite evidently related
to scr.

1.5 a brief overview of scr research

research in scr and its underlying technologies has been ongoing for
more than twenty years. during this time many techniques have been
proposed and explored for di   erent tasks and datasets. this subsection
begins with a brief chronological history of scr research from its birth
to the present. we then o   er an overview of some application areas and
a brief discussion of scr research for di   erent languages of the world.
our objective here is both to present a historical perspective of the
development of scr and to highlight the key technological innovations
at each point.

1.5.1 the history of scr research

the history of scr research falls relatively neatly into four di   erent
eras. each new era brought new tasks, new algorithms and new initia-
tives to strengthen the scr research community.

the    rst era can be thought of as proto-scr and its heyday was in
the early 1990s. key examples of work conducted in this era are [230,
231] from mit lincoln labs and [301] from xerox parc. modern
large-vocabulary continuous id103 (lvcsr) had not yet
emerged onto the scene, and systems addressed the task of    ltering

1.5 a brief overview of scr research

251

voice messages by using wordspotting techniques, which recognized a
small set of words within the speech stream. in [230, 231], the task
is referred to in the literature as    information retrieval,    but it di   ers
from the concept of ir as understood by the ir research community. in
this work, topics or speech message classes were de   ned ahead of time
and not at the time at which the system was queried. instead, this task
is more akin to    information    ltering    (cf. subsection 1.4) than scr.
we call the second era the dawn of scr. this era arguably began
with the 1992 publication of [87], a description of a prototype    system
for retrieving speech documents    at eth z  urich. the prototype
made use of subword indexing features and, critically, information
about the queries or the information needs of the users did not have to
be available to the system in advance. other systems dating from this
era also accepted ad hoc queries from users. the year 1994 saw the
publication of [122], which proposed a wordspotting approach based
on phonetic lattices that made it possible to carry out vocabulary-
independent wordspotting after recognition. if an lvcsr system alone
is used to transcribe the spoken content, the index of the scr system
will be limited to containing those words occurring in the vocabulary
of the recognizer. phone lattice spotting (pls) made possible
vocabulary independent scr and was exploited by subsequent work
at cambridge university [27, 120, 121]. an important result to emerge
was that the vocabulary independence of pls could be combined with
the robustness of lvcsr to obtain improved scr results [132]. this
era was characterized by research conducted in isolation at individual
research sites. during this era, the    rst systems for broadcast news
retrieval were an important development, especially the informedia
system at carnegie mellon university (cmu) [105]. the informedia
project established the    rst large scale digital video search system,
with its search driven by a combination of manually-generated closed
captions and lvcsr transcriptions.

the scr research scene changed dramatically with the beginning of
what we refer to as the rise of the scr benchmark. this era dates from
1997, the year the text retrieval conference (trec) [277] o   ered the
   rst sdr task. research sites emerged from isolation as they began

252

introduction

working on the same data sets and tasks within the framework of
benchmark initiatives. this focus made it possible to compare results
across algorithms and across sites. the trec tasks provoked a variety
of research into methods to improve scr e   ectiveness, notably the
value of id183 [306] for scr and an exploration of docu-
ment expansion [251]. this era drew to a close with the publication in
2000 of [82], which broadly concluded that the problems of scr, as
de   ned in terms of retrieval of spoken documents, were either solved or
su   ciently well characterized to be addressed without signi   cant fur-
ther research e   ort. remaining challenges were identi   ed as involving
more complex tasks, such as id53 or spoken queries, or
extending the environment to multimedia video search.

the present era can be characterized as the era of spontaneous, con-
versational speech. it can be considered to have begun in 2001, with a
workshop entitled    information retrieval techniques for speech appli-
cations    [53] organized at the acm sigir (special interest group on
information retrieval) conference, at which the keynote speaker [3]
pointed out that trec sdr had focused on long documents and
long queries, in contrast to the shorter queries or shorter documents
characterizing many of the new scr use scenarios. in such scenar-
ios, the importance of id103 error could rise enormously.
arguably, however, the era of spontaneous, conversational speech did
not get under way until there was also a spontaneous, conversational
benchmark task available to provide researchers with material to exper-
iment and compare results. in 2005, a spoken retrieval track organized
within the cross-language evaluation forum (clef) used a large,
challenging corpus of interview data [205]. in 2008, a video retrieval
track was founded within clef, which later developed into an inde-
pendent benchmark called mediaeval. this benchmark o   ers tasks that
make use of user-contributed speech media collected from the inter-
net [156, 160]. the trec video retrieval evaulation (trecvid)
benchmark [255] has conventionally focused on the visual relevance
of video to user queries, but makes use of asr transcripts and has
recently expanded the notions of relevance that it explores. a fur-
ther evaluation involving search of informal speech was introduced in
2011 at the 9th ntcir: nii testbeds and community for information

1.5 a brief overview of scr research

253

access research evaluation workshop, where the spokendoc track
had std and scr tasks focused on searching a corpus of japanese
lectures [1].

1.5.2 use scenarios

scr has been applied in a range of di   erent application areas. ini-
tially, the dominant application was access to broadcast news data.
this research    rst investigated radio news [87, 121] and later television
news in the informedia project [105]. it formed the basis of the trec
sdr datasets [82]. broadcast media reports involve a combination of
scripted and unscripted material, however, they are well-behaved in
the sense that the topical scope is limited and they have an underlying
structure that is readily identi   able.

another area that received considerable attention in the early
phases of scr research was voice mail. both the scanmail
project
[297, 298] and the video mail retrieval using voice
project [27, 132]
focused on search of spoken mail messages. of
particular note are the studies at at&t that explored users    inter-
action with audio content from a cognitive perspective. these studies
investigated, for example, people   s poor ability in recalling details
in spoken content, such as answering machine messages [112, 113].
understanding how people actually interact most e   ectively without
audio material is crucial to the success of scr systems.

other application areas involve less planned, more spontaneous
speech or speech that is produced within the context of a conversation
or other less formal settings. search of this less well-planned content
has formed the basis of more recent work in scr. examples include
search of meetings, [23, 150], call center recordings [176], collections
of interviews [33, 61], historical archives [100], lectures [86], podcasts
[207], and political speeches [2].

in [38], it is noted that the best method for indexing audio data can
di   er according to the goal of the retrieval system. for this reason, a
good understanding of the underlying use scenario will translate into
a more highly e   ective scr system. di   erences between use scenarios
encompass both di   erences between user needs and di   erences between

254

introduction

the underlying spoken content collection, in terms of language, speaking
style, topic, stability and structure.

1.5.3 languages of the world

with the notable exception of the work at eth z  urich [87, 88],
the early work on scr was devoted to english language spoken
content. retrieval of english language content is relatively simple,
since features to be searched in the form of words are readily available.
some pre-processing in the form of id30, see overview of spoken
content indexing and retrieval, can be used to match di   erent word
forms, but the features themselves are easily identi   ed. this is not the
case for many other languages. for example compounding languages
(e.g., german and dutch) express semantically complex concepts using
single lexical words. these must often be de-compounded to constitute
simpler words for search. still more challenging are agglutinative lan-
guages (e.g., turkish and finnish), which have enormous vocabularies
resulting from the combination of a relatively small set of morphemes
with a vocabulary of stems. extracting suitable search features for
these languages can be a complex process. in the case of languages
like chinese, where whitespace is not used to delimit words in written
language, segmentation methods are required. generally, a separate
asr system must be deployed for every language that is to be included
in a spoken content index. an asr system for a new language involves
a large implementation e   ort and in some cases an optimization of the
basic design of the asr system. these issues are addressed in greater
depth in exploiting automatic id103 output.

although much of the research discussed in this survey has been
carried out for english-language spoken content, we would like to
emphasize the importance of considering the full scope and variety
of human languages for research and development in scr. research
on scr for non-english languages is gaining in volume. coverage
for a number of languages has been relatively strong. work on non-
english scr includes: chinese [283], german [155, 241], italian [71],
french [84], dutch [212], finnish [153], czech [200], japanese [167], and
turkish [8].

1.5 a brief overview of scr research

255

cross-language speech retrieval combines scr with machine trans-
lation techniques in order to give users querying in one language access
to speech collections in another language. such a system is helpful for
users who have passive knowledge of a language, and would be able
to derive bene   t from listening to or watching speech media in that
language, but whose knowledge is not advanced enough to allow them
to formulate queries. as noted by [133], scenarios for cross-language
speech retrieval include cases in which the collection contains multiple
languages or accepts queries formulated in multiple languages. early
work in the area of cross-language scr includes [216], which describes a
system that accepts a textual query in french and returns spoken ger-
man broadcast news stories. much of the work on cross-language speech
retrieval has been carried out within the clef [217]. other important
work includes that on mandarin chinese/english cross-language speech
retrieval [183].

now we turn to a more detailed overview of spoken content indexing
and retrieval, including a high-level overview of ir techniques, which
will allow us to formulate a list of the key challenges faced when design-
ing and implementing scr systems.

2

overview of spoken content

indexing and retrieval

in this module, we o   er a high-level perspective on scr that sets the
stage for discussion of speci   c component technologies and issues in the
material that follows. in subsection 2.1, we present an overview of the
general architecture for a basic scr system and brie   y describe its var-
ious components. the overview provides a context for our presentation
of background information on information retrieval (ir) techniques in
subsection 2.2. then, subsection 2.3 covers evaluation of ir systems.
finally, subsection 2.4 describes in detail those aspects of scr that
make it di   erent from text-based ir, identifying a list of key challenges
that are particular to scr and are faced when designing and imple-
menting an scr system.

2.1 general architecture of an scr system

although scr systems are implemented di   erently depending on the
deployment domain and the use scenario, the underlying architecture
consists of a set of conventional components that remain more or less
stable. this architecture is presented schematically in figure 2.1, in

256

2.1 general architecture of an scr system 257

fig. 2.1 block diagram depicting an abstraction of a typical spoken content retrieval
system.

order to present an initial impression of the technologies    under the
hood    of a typical scr system.

the query depicted on the left represents the user input to the sys-
tem. we emphasize that the query is not the actual information need of
the user, but rather an attempt of the user to express this information
need. often the query is a highly under-speci   ed representation of the
information need, and part of the goal of the system will be to auto-
matically enhance this speci   cation in order to return useful results to
the user.

the retrieval system has the function of matching the query with
the items in the collection. this matching takes place using one of the
many ir frameworks that have been developed for text-based applica-
tions. these will be discussed further in subsection 2.2.

the retrieval system consults the index, here labeled timed
media index, which contains features that represent the items in the
collections and, in general, also time-codes indicating the time points
within each item associated with occurrences of these features. an
index is a representation of a collection as indexing features, together
with information that associates those features with particular items.

258 overview of spoken content indexing and retrieval

the process of indexing involves generation of an index, and can be
de   ned as follows:

spoken content indexing is the task of generating
representations of spoken content for use in a retrieval
system. these representations include indexing features
(i.e., terms consisting of words and phrases derived from
the spoken content and also terms describing the spo-
ken content, such as speaker identities), weights for the
indexing terms and also time codes indicating the time
points within the spoken content associated with the
indexing terms.

the exact form of the index depends on the domain and the applica-
tion. for example, for some applications, speech media is pre-segmented
into documents and only entire items are returned to the user in the
results list. in this case, the time code information returned by the
speech recognizer may be discarded, and the index may contain only
the information regarding which indexing term is associated with which
document. in other cases, the system might return a time point within
an item as a result. in this case, time code information must be retained
in the index. the structure of the index is optimized for e   cient cal-
culation of matches between the query and the speech media item.
since indexing techniques are shared by scr and ir, we will not treat
them here, but rather refer the reader to the ir literature, for example,
[57, 178, 305].

indexing features are generated by the id103 system
that processes the material in the speech collection at indexing time.
the major source of features in the index is asr, that is, the process
that transcribes the spoken word to text. however, other sources of
information, such as metadata and rich transcripts that include labels
indicating who spoke when, are also important. the parts of this sur-
vey that are relevant to the processes of spoken content indexing are
automatic id103, exploiting automatic speech recogni-
tion output, and spoken content retrieval beyond asr transcripts,
which covers ways in which speech recognizer output can be exploited
for indexing.

2.2 information retrieval for scr 259

as the output of the scr system, the user receives from the sys-
tem a list of ranked results, a set of results ordered in terms of their
likelihood of potential relevance to the query. a result can take the
form of a spoken content item, a segment of the speech stream whose
scope is dynamically customized to the query (sometimes referred to as
a    relevance interval   ) or a time point at which the user should start
viewing/listening to the content (a so-called    listen-in point    or    jump-
in point   ). the choice of the form of results depends on the domain
and on the use scenario. in any case, it is important that the results list
contain proper surrogates, representations of each result that allow the
user to make an initial judgment of whether or not the result is a good
match to the information need without having to initiate playback.

finally, the scr system must o   er the user a means of visualiza-
tion and playback of the individual results. result visualization in a
playback interface is necessary for the same reason as surrogates are
important for the results list: users must be able to judge the relevance
of speech media results without listening to or viewing long swaths of
audio or video content, a very time-consuming process. time can also
be saved by providing the user with an intelligent multimedia player,
which makes it possible to jump directly to certain points within a
speech media result, for example the beginning of a particular speaker
turn. issues concerning the user interface will be discussed in greater
detail in accessing information in spoken content.

2.2

information retrieval for scr

the retrieval system module depicted in figure 2.1 is most often taken
to be a standard ir system, implementing one of the standard ir frame-
works. each of these frameworks matches items to queries, and outputs
a ranked list of results. the di   erence between the frameworks lies in
their choice of ranking function, which computes a ranking score (rs)
for each of the result items. the rs is sometimes called the retrieval
status value (rsv). the rs is generated on the basis of indexing fea-
tures that have been extracted from items. the standard ir frameworks
all apply an independence assumption, also known as the bag of words
approach, which means that the system disregards information about

260 overview of spoken content indexing and retrieval

order or co-occurrence of features in a particular item and treats them
as occurring independently of each other.

this survey does not review ir issues in depth. instead, we o   er
an overview of existing ir models and refer the reader to the ir lit-
erature for full details of their theory, implementation, and evaluation
[14, 57, 89, 178, 305]. the exposition is ordered in terms of the his-
torical timeline along which the ir models were developed, which also
re   ects their relative complexity.

in the boolean search framework, an item is
boolean search.
returned as potentially relevant if its contents match a possibly very
complex query constructed using boolean operators. since boolean
search does not generate a relevance score for each item, the set of
returned results is not ordered by potential relevance. the searcher
must either make use of other information,
for example, date of
creation, or abbreviated representation of their content (i.e., surrogates)
to decide which items to inspect further.

a basic form of boolean search query is a simple and construction
requiring all features in the query to present in the item in order for
it to be retrieved. taking a simple and query, the boolean search
framework basically addresses a       nding mentions    task, looking for
the presence of certain features in the items to be retrieved.

vector space model. the vector space model (vsm) makes use
of a vector representation containing indexing features, (cid:1)v (i). one such
representation is made for each item in the collection of available items.
the elements of the vector contain the weights of the individual index-
ing features (i.e., terms). each weight represents the importance of that
feature for the item.

the vsm is based on the assumption that the closeness of vectors
within the vector space re   ects the semantic similarity of the items
that they represent. within the vsm, the ranking score of an item,
with respect to the query, is calculated as the similarity between a vec-
tor representing the item and a vector representing the query. query-
document similarity in the vsm is calculated using the dot product
between the vectors. this similarity can be used either without length

2.2 information retrieval for scr 261

id172, as in equation 2.1, or with the id172, as in
equation 2.2.

rsvsmdot(q, i) = simdot(q, i) = (cid:1)v (q)    (cid:1)v (i)
(cid:1)v (q)    (cid:1)v (i)
|(cid:1)v (q)||(cid:1)v (i)|

rsvsmcos(q, i) = simcos(q, i) =

(2.1)

(2.2)

there are several alternative schemes that can be used for calculat-
ing the weights of each term. conventionally, these schemes make use
of statistics calculated on the basis of term occurrences in the collec-
tion. the underlying principle that should be followed when choosing
a weighting scheme is quite simple: the weighted terms for a certain
item should be representative (i.e., capture the content of that item)
and discriminative (i.e., capture properties of that item that make it
di   erent from other items).

a popular weighting scheme, referred to as tf-idf uses tf, term fre-
quency (the number of occurrences of a given feature in the item), as
the representative component and idf, the inverse document frequency
(the inverse of the number of documents in the collection containing the
given term), as the discriminative component. a detailed explanation
of the vsm and presentation of alternative tf and idf component func-
tions is given in [235]; this account is extended to incorporate a more
e   ective method of query and document length id172 than in
the standard cosine function in [249].

the vsm was the    rst widely used ranked ir framework, and has
been used in a number of scr studies. scr work that makes use of
the vsm for retrieval includes [196, 197, 241, 288, 304].

probabilistic retrieval. probabilistic retrieval (pr) is based on
the id203 ranking principle, which states that the most e   ec-
tive retrieval system given the available data is the system that ranks
items according to the id203 of their relevance to the user   s infor-
mation need. in order to calculate the id203 of an item being
relevant to the information need, a particular model must be adopted.
the common approach is to treat relevance as binary (i.e., an item is
either relevant or it is not) and to apply bayes    decision rule. under
this approach, an item is considered relevant if the id203 that it

262 overview of spoken content indexing and retrieval

belongs to the relevant class of items is larger than the id203 that
it belongs to the non-relevant class of items.

analysis from this starting point, under the assumption that terms
occur independently of each other, leads to the binary independence
model (bim) [273]. operationalizing this model for a particular query
further leads to the robertson/sp  arck jones relevance weight rw(t) of
each term t [226] calculated as follows:

rw(t) = log

(rt + 0.5)(n     r     nt + rt + 0.5)

(r     rt + 0.5)(nt     rt + 0.5)

,

(2.3)

where r is the number of relevant documents for this query, rt is the
number of relevant documents containing term t, n is the total num-
ber of documents in the collection, and n is the total number of doc-
uments containing t. the addition of 0.5 facilitates estimation using
small amounts of data     speci   cally, by avoiding the assumption that
if a term has not appeared in any relevant documents so far, it will
never appear in any.

in the absence of relevance information, rw(i) reduces to the collec-

tion frequency weight (cfw(t)),

cfw(i) = log

n     nt

.

nt

if, as is generally the case, n (cid:1) nt, this equation is a very close approx-
imation to the standard inverse document frequency weight.

idf(t) = log

n
nt

query-document similarity under the bim is calculated as

rsbim(q, i) =

cfw(t).

(2.4)

(cid:1)

t   query

this formula makes clear the relationship between the ranking score
as calculated by the bim for pr and the ranking score as calculated
by the vsm in equation 2.1. when idf term weights are used, the
ranking score produced by a vsm making use of dot-product similarity
is expressed as:

rsvsm =

(cid:1)

t   query

log

n
nt

.

(2.5)

2.2 information retrieval for scr 263

both models use a sum over individual term contributions because they
both impose the assumption that terms are distributed independently
of each other in items. further, both models capture the way in which
term occurrence serves to discriminate between documents.

the similarity between equations 2.4 and 2.5 makes evident both
the advantage and disadvantage of the bim. the advantage is that the
probabilistic framework achieves the same basic model as the vsm,
known to work well in practice, but uses more principled means based
on id203 theory and a notion of relevance. the disadvantage is
that the bim captures no information about term frequencies. in other
words, equation 2.4 contains no term that is analogous to tf in the tf   
idf weighting scheme for the vsm, which captures how representative
a given term is for a given item. the lack of information about term
frequencies compromises the performance of the bim and led to the
development of the extension of the bim referred to as the okapi or
bm25 model [227, 228]. this extension introduces a sensitivity to term
frequency and document length into the original bim and has been
demonstrated to yield good performance for many ir tasks including
scr [47, 137].

id38 framework. the most recently introduced
of the major ir frameworks is the id38 (lm) frame-
work [111, 218]. this framework takes a very di   erent approach to the
generation of a ranked list of potentially relevant items in response to
a query. the model essentially estimates the id203 of a language
model associated with each item generating the query, or, alternatively,
the query generating the item. this value is returned by the system
as the ranking score. conventionally, this problem is expressed using
bayes    theorem and then simpli   ed, as shown in equation 2.6.

rslm = p (i|q) =

p (q|i)p (i)

p (q)

    p (q|i)

(2.6)

the simpli   cation involves imposing the assumption that the prior
id203 of all items (i.e., the id203 of items before the query is
known) is the same, and noting that the prior id203 of the query
is the same for all documents so does not impact on the ranking of doc-
uments with respect to the ranking score. the model in equation 2.6

264 overview of spoken content indexing and retrieval

is referred to as the query likelihood model since the language model
used here is one that returns the likelihood of a query given a par-
ticular item. this likelihood is calculated via a language model of the
document, as shown in equation 2.7.
(cid:1)

rslm =

w   q

log p (w|i)

(2.7)

smoothing is used to prevent equation 2.7 from returning a zero value if
a query word is not contained in the document. a method often applied
is to use a linear interpolation language model, as discussed by [178],
that uses a linear combination between a multinomial distribution esti-
mated for the item p (w|c) and a multinomial distribution estimated
over the background collection p (w|c), as shown in equation 2.8.

rslm =

  log p (w|i) + 1       log p (w|c)

(2.8)

(cid:1)

w   q

estimation of the weighting parameter    is important and is often
carried out using a development data set containing queries and refer-
ence documents that have been hand labeled as relevant. as with the
vsm and bim discussed previously, this model makes use of the inde-
pendence assumption. since terms are assumed to have independent
distributions, the contributions can be combined in a straightforward
manner. in equation 2.7, the contributions from terms are represented
in the log domain to prevent under   ow and they are combined using
a simple sum. scr work that has made use of the id38
framework includes [47].

relevance feedback and pseudo-relevance feedback. relevance
feedback in ir is a technique that aims to enrich the user   s original
query with further information concerning the user   s underlying need.
if the user identi   es relevant items for the system, for example, by
making a selection of items from the initial results list returned by the
system, information from these items can be used as feedback to expand
the query and revise parameters of the retrieval system. user-based
relevance feedback can be approximated by making the assumption
that the top ranked items in the initial results list are relevant and
using these items as the feedback set. in this case, the technique is

2.2 information retrieval for scr 265

referred to as    pseudo-relevance feedback    (prf) or    blind relevance
feedback.   

for text retrieval, the motivation to perform prf is to create a
query that gives a better speci   cation of the user   s information need
with the aim of improving retrieval e   ectiveness. in the scr setting,
prf has the potential to serve two purposes: expansion of the query
to better describe the underlying information need, as in the case of
text retrieval, but also to assist in addressing a particular problem of
scr. as mentioned in the introduction, scr e   ectiveness is impacted
by the presence of errors in the asr transcripts. in the scr setting,
prf can expand the query to include terms that are well recognized by
the asr system, which can help to address retrieval problems arising
when query terms are either missing from the vocabulary of the asr
system or are not well recognized by the asr system.

relevance feedback is implemented in di   erent ways into the vsm,
pr and lm ir frameworks. we give a brief description of each method
here, and, again, make reference to ir texts for more detail.

within the vsm framework, the general strategy is to adjust the
vector representing the query away from non-relevant items within the
vector space and towards relevant items. to this end, the original query
vector is expanded using rocchio   s algorithm, which we give here in the
formulation used by [178].

(cid:1)qm =    (cid:1)q0 +    1
|dr|

(cid:1)dj       

1
|dnr|

(cid:1)

(cid:1)dj   dr

(cid:1)

(cid:1)dj   dnr

(cid:1)dj

(2.9)

the new query, (cid:1)qm, consists of the original query, (cid:1)q0, weighted by a
factor of    and modi   ed with a positive contribution from the set of
relevant documents, dr, and a negative contribution from the set of
non-relevant documents, dnr. the contribution of each document set,
weighted with a factor (   and   ), consists of the sum over the con-
tributions of the individual vectors, (cid:1)dj, representing the documents in
that set.

within the pr framework, the approach is to seek to select expan-
sion terms which when added to the query have the greatest utility in
improving the rank of relevant documents in the ranked list. in order to
do this, all terms appearing in relevant or assumed relevant documents

266 overview of spoken content indexing and retrieval

are ranked using an o   er weight ow(t) (sometimes referred to as the
robertson selection value rsv(t)). the derivation of ow(t) is described
in [225], and is calculated as follows:

ow(t) = rt    rw(t),

(2.10)

where rt and rw(t) have the same de   nitions used in the calculation of
the pr term weights. note that ow(t) represents a trade-o    between
the speci   city of terms via rw(t) and the presence of a term in known
relevant documents via rt. thus, terms achieving high ow(t) values are
those with strong speci   city, which occur in multiple relevant docu-
ments. the number of terms that should be added to the query must
be determined empirically for a speci   c task.

within the lm framework, prf is implemented using a query
model called a relevance model, which is a language model estimated
to provide an enriched representation of the information need behind
the query. the general strategy is to make use of information about
which items are relevant in order to estimate a highly accurate rele-
vance model that serves in place of the original query. here, we present
the formulation given by [57]. first, documents are ranked using the
query likelihood score in equation 2.7 and a set c of top-ranked docu-
ments is selected to form the relevance set, the basis for the relevance
models. next, for every word occurring in the relevance set, a relevance
model id203 p (w|r) is calculated. finally, new ranking scores
are calculated for the documents using the negative cross-id178 of
the relevance model and the original query likelihood model.

p (w|r)log p (w|d)

rsrm =

(2.11)
for details of the estimation of p (w|r), we refer to [57, 164]. relevance
feedback has been shown to be an e   ective technique for improving scr
in many studies for a wide range of tasks. further details on the use of
relevance feedback in scr are given in subsection 5.2.4.

w

(cid:1)

representing term dependence. the models that we have dis-
cussed thusfar have made use of the    bag of words    approach, that is,
the assumption that terms within items are distributed independently
of each other. the use of this assumption is widespread because it

2.2 information retrieval for scr 267

forms the basis for simple, yet e   ective systems. the utility of such
systems is perhaps surprising due to the fact that the independence
assumption represents a gross over-simpli   cation of the reality of
speech and language use. there are two factors that constrain the
co-occurrence of words in human language:    rst, the requirements of
syntactic structure, and second, the association between particular
words and the concepts that humans speak about. the second factor is
particularly important for ir, which regards words as the basic units
that bear meaning within language. certain meanings are expressed by
using words in combination, and are absent if the same words are used
individually.

the classic example is a phrasal noun such as    ground truth.    when
used together, the words provide strong evidence that a document
is about a topic related to experimental evaluation. the evidence is
stronger than twice the evidence contribution that would be made by
an occurrence of    ground    without    truth    or an occurrence of    truth   
without    ground.   

a second example illustrates a more challenging case. words
whose co-occurrence is associated with speci   c topics are not neces-
sarily adjacent within documents, rather they can be separated by
an arbitrary number of intervening words. for example, the words
   polish,       nails    and       ling    taken together constitute strong evidence
that a particular document treats the topic of manicures. the com-
bined evidence is clearly stronger that the individual contributions.
a document containing the word    polish    could just as well concern
gemstones, one containing    nails    could just as well be about carpentry
and one containing the word       ling    could just as well be related to
o   ce work.

ir systems handle term dependence with various approaches.
a straightforward approach to integrating proximity information is to
use phrase terms (contiguous words) and proximity terms (words sepa-
rated by a constrained number of intervening terms) alongside of con-
ventional word-level terms [178]. refer to [187] for details, in particular
how weights are estimated for such terms for use in the vsm. a similar
approach is the dependence model, which makes use of proximity terms
within a framework that combines id38 and an id136

268 overview of spoken content indexing and retrieval

network [57, 186]. scr research that has taken phrasal nouns and term
proximity into account includes [38, 137].

integrating additional information. commercial search engines
incorporate a wide range of features into the computation that gener-
ates their search results [57]. the features reach beyond the content
of the results themselves. perhaps the best known examples are tech-
niques that exploit the hyperlinked structure of the collection, such as
id95 [24].

research work on text-based ir provides us with various methods
of integrating additional information into the retrieval framework. the
simplest method is data fusion of retrieval results via linear combi-
nation of scores (lcs) of retrieved item lists. this method can be
used when more than one ir system or independent content index is
available, each of which ranks items with respect to a di   erent repre-
sentation, for example, asr transcript and metadata.

rslcs =

wsrss(q, i)

(2.12)

(cid:1)

s   systems

for a given document-query pair, the retrieval scores of each system
rss(q, i) are weighted with an empirically set weighting factor ws
and combined with a simple sum. lcs generally bene   ts retrieval
e   ectiveness when the separate lists are of similar quality, but bring
additional information to the retrieval ranking process [19, 276]. an
early example of combining retrieval
lists from multiple indexing
sources is described in [132].

another case arises when one of the information sources is basically
query independent, such as, recency of the creation of the document.
this feature can be integrated into the id38 framework.
in this case, the simpli   cation in equation 2.6 is made retaining the
prior id203, i.e.,

rslm = p (i|q) =

p (q|i)p (i)

p (q)

    p (q|i)p (i).

(2.13)

the additional source of information can then be appropriately scaled
and integrated into the retrieval score calculation as p (i).

other possibilities for integrating additional

information into a
retrieval system involve applying discriminative learning techniques

2.2 information retrieval for scr 269

to the ranking problem (   learning-to-rank   ), applying reranking tech-
niques, exploiting graph-based integration and making use of bayesian
networks. here, our intent is to point out the wide range of available
approaches. we refer the reader to the ir literature for further informa-
tion and recommend [57, 178] as starting points from which to explore
the related literature.

pre-processing. pre-processing refers to the steps that are taken to
modify the original text in order to make it more suited for indexing.
pre-processing for scr follows similar methods to that for regular text
retrieval, as explored in detail in most ir textbooks. for completeness,
we brie   y review common pre-precessing methods here, highlighting
signi   cant relevant points for scr.

all pre-processing methods involve some degree of id172,
mapping lexical items (i.e., words) that are equivalent onto character
strings that are completely identical. for example, words that are cap-
italized because they occur at the beginning of a sentence are often
mapped to their lower case forms, so that both versions correspond to
only a single term in the index. some systems discard case altogether,
which is an easy approach, but may eliminate information about valu-
able distinctions from the system, for example, between common and
proper nouns. in some cases, documents will use di   erent character
encodings so it is important to make sure that the encoding is standard-
ized and characters are not dropped or rendered unreadable in the pro-
cess. id172 can also involve standardizing spellings (e.g., u.k.
vs. u.s. spelling standard for english) and dealing with spelling errors.
some systems map acronyms onto their full forms, (e.g.,    f.c.c.   
is mapped to    federal communications commission   ). id172
needs to be considered carefully for scr. the output of asr systems
often does not provide casing as used in standard written texts, and
spoken expressions are in some cases not of the same form as their
written versions. the combination of asr transcripts with metadata
sources would require that their potentially rather di   erent linguis-
tic forms are normalized for consistency. however, it is clear that this
mapping must be done correctly in order to avoid introducing errors
into the text. for this reason, acronyms can also be treated elsewhere

270 overview of spoken content indexing and retrieval

in an ir system on par with synonyms (e.g.,    car    and    automobile   ),
for example during a id183 step.

often, an ir system applies some form of con   ation of di   erent
forms of semantically related words. for english, these usually take
the form of id30 [178]. as outlined in the introduction, other lan-
guages often require di   erent language speci   c processing, for example
noun decompounding in german. id30 is the process of mapping
words onto their word stems or base forms. for example    tests,       test,   
   testing    and    tested    would all be mapped to the base form    test.   
id30 is useful because it collapses words that are semantically
related, but slightly di   erent in form, onto a single category. it should
however, be applied with caution. for example, it is reported that users
typically mean quite di   erent things when entering the queries    apple   
and    apples    into a search engine, the former referring to the company
and the latter to the fruit.

in addition to id30, stopping or stopword removal can also be
applied. stopwords are generally function words, such as the english
words    the,       a,       her,       that,       for.    these words are removed since
they have a syntactic function, but do not directly express mean-
ing in the way that other words (known as content words) do. their
removal can produce signi   cant computational savings in an ir sys-
tem. although the practice is common for ir systems, like id30,
it must be applied with discretion. stopwords can contain important
clues to the style or genre of the document (e.g., a personal testimony
can be expected to have a high incidence of the terms    we    and    i   ).
for some applications, these factors will contribute to relevance.
for scr, it is not always clear if applying id30 and stopping will
enhance system e   ectiveness. id30 could potentially have either a
positive or negative e   ect. if the word in the original audio has the same
base form as a word that the asr system mis-recognizes in its place,
then id30 can be bene   cial. for example, if the original audio
contained the sequence    test in    and the asr system mis-recognized
this sequence as    testing,    then id30 the word    testing    to its
base form    test    would help to ameliorate the impact of the error.
however, if the word in the original audio does not have the same
base form as the mis-recognized word that replaces it, id30 could

2.3 evaluation

271

potentially con   ate mis-recognized word forms, and possibly exacerbate
the e   ect of word errors. for example, if    test in    in the original audio is
recognized as    nesting,    then id30 the word    nesting    to its base
form    nest    could increase the impact of the word error on the scr
system. in some cases, stopword removal may be harmful. stopwords
in the asr transcripts might not re   ect actual spoken stopwords, but
rather may be substituted for the actual spoken content words, which
are either poorly articulated or outside the vocabulary of the asr
system. in this case, stopword patterns may o   er a useful clue to the
original spoken content and discarding them may remove potentially
useful information from the scr system.

finally, we would like to mention the importance of id121,
the process of mapping the characters occurring in the document to
the words that will be used for indexing. id121 presents well
understood challenges for written text content. while it is straightfor-
ward for a language like english, which uses white space to separate
words, it is more challenging for a language like chinese where text
segmentation methods must be applied to identify word units. related
segmentation issues are raised in free compounding languages such as
german, which, as mentioned previously, must often be split to iden-
tify their constituent words for indexing. asr transcripts can present
di   erent id121 challenges. for example, the recognizer can rec-
ognize the word    notebook    as the words    note    and    book.    thus,
it can actually be useful to consider the mapping of some elements of
asr transcripts to compounds. if such confusions are frequent enough,
as is likely the case with asr systems that make use of vocabularies
including sub words, recombining words can introduce a further source
of error. in such cases, subwords may be used directly, as further dis-
cussed in exploiting automatic id103 output.

2.3 evaluation

the success of an ir system lies in its ability to satisfy user informa-
tion needs. since it is not always practical to carry out user studies
to evaluate systems, evaluation is often accomplished using a system-
oriented approach. under such an approach, systems are evaluated by

272 overview of spoken content indexing and retrieval

using a list of queries for which the relevant documents have been
annotated by human assessors (called    the ground truth   ) and the
system output is compared to this list. the accessor   s annotation pro-
cess serves to generate a representation of user perceptions of whether
an item satis   es a particular information need. an advantage of the
systems-oriented approach is that it creates highly controlled evalua-
tion conditions that make it possible to compare performance across
algorithms, systems and research sites.

in the    eld of ir, a variety of metrics are used to evaluate retrieval
performance. the most familiar are precision and recall. precision is
de   ned as the proportion of retrieved documents that are relevant to
the query. recall is de   ned as the proportion of relevant documents
that are retrieved. these measures are often reported for results lists of
lengths n = 5,10,20, in which case they are labeled p @n and r@n.
often, the f-measure, the harmonic mean of precision and recall, is
also reported.

in order to express the quality of the overall results list (i.e., not only
the top-n), mean average precision (map) is commonly used. map is
literally the average average precision (ap)     where ap is averaged
across all queries in the query set. ap is calculated by moving down the
results list for a given query and calculating precision at each rank n,
where a document has been correctly retrieved. ap is then calculated
by averaging p @n over all n at which it has been calculated.

another method of presenting a complete picture is to produce a
precision-recall graph, which plots precision against recall for every
point in the retrieved results list. related is the roc curve, which plots
the precision with respect to the positive class against the precision with
respect to the negative class.

an important dimension to ir evaluation is statistical signi   -
cance, which measures the reliability in the comparison of experimental
results. this topic is very important in the reporting of ir experimental
results. an introduction to signi   cance testing in ir is contained in [32].
note that these measures are binary relevance judgments. in other
words, a document is either judged relevant or not relevant. we refer
the reader to the ir literature (e.g.,
[57, 178]) for more informa-
tion on evaluation, and for an explanation of normalized discounted

2.4 challenges for scr 273

cumulative gain (ndcg), which is used when relevance judgments
are non-binary, that is, they have been made at a number of levels.

in the case of scr, an evaluation metric adopted directly from ir
is not always suitable. in particular, in cases in which spoken content
is unstructured, there is no fundamental notion of a document over
which precision or recall could be evaluated. an scr system that
returns    listen-in points    rather than items can be evaluated using the
generalized average precision (gap) introduced by [168]. gap is an
extension of average precision that incorporates weights based on the
distance between the    listen-in points    returned by the scr system and
the reference start parts (i.e., the manually labeled ground truth) [205].

2.4 challenges for scr

in the introduction, we presented an initial overview of the basic dif-
ferences between scr and ir. now that we have covered the basic ir
frameworks, we shall revisit the issues in more detail. in this subsection,
we discuss a list of areas in which scr presents particular challenges,
above and beyond the challenges of text ir. in order to design and
develop an scr system, it is important to go beyond a na    ve combina-
tion of asr and ir and to address these areas explicitly.

the challenge of handling uncertainty. the asr systems that
produce speech transcripts are also capable of generating some form
of con   dence score, values that re   ect the level at which the recog-
nizer is certain that the recognized word is indeed the word that was
spoken in the speech signal. conventionally, a con   dence score re   ects
an acoustic match between the signal and the transcribed word. in
text-based ir there is no equivalent of a con   dence score for individ-
ual words     rather, all words deterministically either occur or do not
occur. the challenge for scr is to determine the appropriate way to
allow uncertain evidence regarding the presence of the word in an item
to contribute to a useful representation of the spoken content item.

further, during the process of generating the asr transcript, the
asr system generates information about alternate words that provide
good matches for the speech signal. alternate asr output represents

274 overview of spoken content indexing and retrieval

a rich source of information that can be exploited to improve scr.
special care must be taken to deal appropriately with the high level of
uncertainty associated with the asr lattices that are used to encode a
range of alternate recognizer hypotheses.

the challenge of covering all possible words. the vocabu-
lary of a conventional large vocabulary continuous id103
(lvscr) system is    nite, and this vocabulary limits the terms that
appear in the speech transcripts and thus the indexing terms that can
be used for retrieval. words that do not occur in the vocabulary of
the recognizer are called out of vocabulary (oov) words. the oov
problem is a perennial challenge for scr. a larger vocabulary for the
asr system is not the solution, since language is in constant growth
and new words enter the vocabulary steadily. in [197], an analysis of
news text is presented that demonstrates that vocabulary sizes continue
to grow as a data set gets larger. in other words, it is not possible to
create a single large vocabulary that will eliminate the oov problem.
further, under certain conditions, adding more words can compromise
the recognition performance of words already in the vocabulary. as
pointed out by [174, 197], the oov problem is particularly disruptive
for scr since many of the new vocabulary words are proper names,
which are important for ir. according to [174], up to 10% of all query
words can be oov words, in a typical application that uses a word-
based recognizer with a large vocabulary (i.e., lvcsr). of course it is
possible to update the vocabulary of the asr system by adding new
words to the language model. however, as noted by [174], it can be
di   cult to obtain enough training data to train the language model
for new words. additionally, for most application scenarios, it is not
feasible to re-recognize spoken content once the initial transcripts have
been generated, due to the high computation costs of the asr process
and the huge sizes of current-day spoken content collections. for these
reasons, the oov problem is a formidable one.

the challenge of context. speech media is not fully represented
by its transcript. other aspects of the media will impact whether or
not a certain result is relevant to the user information need. in the case

2.4 challenges for scr 275

of video, it is clear that the user may also require the visual channel to
ful   ll certain speci   cations. however, even with respect to the audio-
channel only, certain aspects might make a particular result more or
less suited to the user   s needs. the user might require a certain speaker,
a certain speaking style, a result of a certain length, speech media of a
certain quality (no background noise) or format. licensing conditions,
as with all intellectual property, might also be important. the time-
liness of media is important, for example in a news search, the user
might want only recent results, or also might want to look back at his-
torical developments. this challenge is also related to how the results
are depicted within the user interface. many of these aspects have bear-
ing on whether or not a user will    nd a particular result suitable and
thus must be represented in the surrogate. some ambiguities can be
handled by the system by simply o   ering users results of multiple cat-
egories in the results list, for example, for the query    guillermo del
toro,    the results might be clips from his    lms, from interviews and of
information about his life.

the challenge of structuring spoken content. although some
speech media is produced in a manner that lends itself to segmentation
(think of a news broadcast composed of individual reports), much of it
does not have a well-de   ned inherent structure. it is important not to
assume that there exists a prede   ned notion of a    spoken document   
for an scr system. thus, the system must either carry out a topi-
cal segmentation or it must determine the temporal boundaries of the
result that will be returned to the user at query time. the structure
is important, since within the retrieval system it is necessary to know
which indexing features should be taken into account for the calcula-
tion of the ranking score. also, the results need to be presented to the
user in an accessible form.

the challenge of visualization and playback. one of the appeal-
ing aspects of full text search involves user assessment of results. a key
feature of a successful scr system is that the user can quickly review
speech media results and decide whether or not they are worthy of
further attention. in the case of a boolean search, the result is required

276 overview of spoken content indexing and retrieval

to contain the terms that appear in the query. if the user hears a query
word very quickly upon initiating playback of a result, it serves as a
shorthand con   rmation of the relevance of the result. users are seldom
willing to listen to long stretches of audio in order to con   rm result rel-
evance. for this reason, intelligent players that allow users to directly
con   rm the presence of their query words are helpful. notice that such
con   rmation is not only important for the purposes of user result assess-
ment, but also to build user con   dence in the system. intelligent players
can also contain information about speaker segmentation     such infor-
mation allows users to gain an impression of the properties and content
of the entire result without listening to it from end to end.

the remainder of this survey is devoted to discussing techniques
and technologies that allow scr systems to address these challenges.
along the way, we will refer back to this list of challenges in order
to make clear which approaches are helpful for addressing the speci   c
challenges.

3

automatic id103 (asr)

automatic id103 (asr) is the technology that tran-
scribes a speech signal into a textual representation. it is also called
speech-to-text (stt) technology, especially in contexts such as
dialogue systems. the designation stt emphasizes that asr is essen-
tially the inverse of text-to-speech (tts), which is also known as
id133. asr systems range from isolated word recognition
systems used, for example, in command-and-control applications, to
large vocabulary continuous id103 (lvcsr) systems
that transcribe human speech in unconstrained human-to-human com-
munication.    large vocabulary    id103 aims to provide
signi   cant coverage of the large and diverse range of word forms used
by humans.    continuous    id103 recognizes words in the
natural stream of language, where they are generally unseparated by
pauses or other cues that could signal a word boundary.

currently,

it is rarely ambiguous whether    id103   
means human recognition of speech or automatic recognition of speech.
we retain the designation automatic id103 not only due
to the currency of the acronym asr, but also as a reminder that human

277

278 automatic id103 (asr)

beings are the original and the best speech recognizers. as good as
asr systems are or may become in the future, human transcription-
ists will remain a highly viable method of generating speech transcripts
or spoken content annotations. ultimately, any fully automatic speech
recognition system must compete in speed, robustness, and e   ciency
with humans or with computer-assisted human annotators.

the material that follows provides background information on the
nature of speech and then describes how the id103 task
is standardly approached, namely, using a technique for probabilistic
modeling of time series data called the hidden markov model (id48)
framework.

many resources exist that can provide a deeper or broader under-
standing of asr that goes beyond the basic material presented here.
the sections on asr in jurafsky and martin   s popular textbook on
speech and language [139] are a useful entry point into the    eld. other
indispensable books in the area of asr include [90, 117, 124]. the
classic reference for id48s, in the context of id103, is
rabiner   s tutorial [222]. rabiner and juang   s book [221], however, cov-
ers additional useful topics. a more recent treatment of id48s for
id103 that provides an in-depth discussion of implementa-
tion issues is [81]. the present exposition is intended to provide a basic
overview that is su   cient for understanding the discussion of asr in
the rest of the survey.

3.1 the nature of human speech

we often produce speech with little conscious thought, but, in fact,
human speech is a complex signal with an intricate structure. in order
to grasp the complex nature of speech, it is helpful to think of the
speech stream as decomposable into progressively smaller units. real-
world speech cannot be cleanly split into elements belonging to such
neat hierarchical categories, but this conceptualization is elegant and
captures the basic principles. additionally, it turns out that represen-
tations of speech as layers of structure provide the basis for robust and
e   ective id103 architectures.

3.1 the nature of human speech

279

the largest unit of speech is the utterance, a string of words pro-

duced in a single speaking burst. for example:

   this union may never be perfect, but generation after
generation has shown that it can always be perfected.   1

although this example happens to be a well-formed sentence, an utter-
ance need not be well-formed or even a sentence at all. unlike written
language, spoken language is produced as a string of sentences only in
a very limited number of formal domains. human-to-human communi-
cation is dynamic and speakers change direction mid-sentence. utter-
ances can also be sentence fragments, a speaker turn in a dialogue, or
a run-on. the following is an example of an utterance taken from the
ami corpus [34], a dataset of transcribed meeting recordings used for
multimodal research.

   there there are time stamps um for, well, segments,
um and for th. . . um segments is for example
when when you look at the data, what is displayed in
one line.   

this example is more typical of an utterance produced during conver-
sational speech.

utterances are composed of words. the conversational speech exam-
ple above shows that utterances may also contain vocal non-lexical
events such as those the speaker produces when hesitating. as a rule of
thumb, a speaking rate of more than 240 words per minute is perceived
as fast and one of less that 160 words per second as slow [163]. words
are the prototypical bearers of meaning in the language and are the
smallest units that can be assigned to lexical categories, also referred
to as part of speech (pos) categories. the basic inventory of lexical
categories is generally agreed to include eight members: noun, verb,
adjective, adverb, pronoun, preposition, conjunction and interjec-
tion. inventories that are larger or have slightly di   erent composition
are also used, cf. e.g., [7, 139]. the phrase is the basic unit of syntactic

1 barack obama   s speech on race, march 18, 2009

280 automatic id103 (asr)

structure and consists either of a single word, or more typically a group
of words. phrases belong to syntactic categories such as noun phrase
(np) and verb phrase (vp).

language processing devotes a great deal of attention to named
entities (nes). nes are words or phrases that designate a restricted
set of referents. in the most common case, this list is de   ned to include
the names of persons, locations, and organizations. nes are    named   
in the sense that they are proper nouns or noun phrases conventionally
used to designate a speci   c entity, for example,    knut    rather than
   a polar bear.    unlike nps, which are syntactically de   ned, nes are
motivated by exigency. the list of nes can be modi   ed or extended
according to the needs of the application [139].

a distinction is often drawn between function words and meaning-
bearing words. function words are words that play a role in the syntax
of the language, but do not have inherent meaning, i.e., it is di   cult to
provide a classical dictionary-type de   nition for these words. function
words include articles (e.g.,    a    and    the   ), prepositions (e.g.,    on    and
   at   ) pronouns (e.g.,    they    and    she   ), and conjunctions (e.g.,    and   
and    but   ). function words tend to be short, often monosyllabic. they
are among the most frequently occurring words, with    the    being the
single most frequent word in the english language [139]. function words
tend to belong to the closed lexical classes, classes that do not readily
develop new members during the natural course of language evolution.
stopword lists used in ir generally target function words. they may
also include a variety of other forms that are relatively meaningless due
to their frequency in a particular collection.

meaning-bearing words are classically nouns and verbs. if it is easy
to provide a dictionary-type de   nition or a synonym for a word, chances
are it should be considered to be meaning-bearing. languages demon-
strate high levels of linguistic productivity for meaning-bearing words.
new words that are coined or introduced from other languages enter
into a language in one of the open lexical classes, for example, noun,
verb, adjective, cf. [110]. open classes acquire new members during
language growth via processes of borrowing or coinage. vocabulary
growth is a basic characteristic of a healthy language. the reality of lan-
guage change is one of the underpinnings of modern linguistics, cf. the

3.1 the nature of human speech

281

explicit assumption of [67],    every language, and every dialect within
a language, is always in a state of change    (p. 2).

below the word level, units that cannot stand alone and, for this
reason, are generally never used in isolation, can be distinguished. the
morpheme, like the word, is a meaning-bearing unit. it is the smallest
meaning-bearing unit of a language. an example of a morpheme is    dis   
in dis   uency, which conveys the meaning of    not.   

the syllable is the smallest unit of speech that can be pronounced
independently. it consists of a core nucleus and, optionally, an onset
and/or coda. the nuclei of syllables, typically vowels, are usually salient
as energy peaks in the speech signal. nuclei are responsible for the
characteristic energy and pitch contours of human speech, which are
usually 150   250 ms long [90]. a more detailed discussion of syllable
rate is included in [163]. words and morphemes are units de   ned with
respect to meaning     when we look at these units from the perspective
of form, they may turn out to be single syllables such as the morpheme
   dis    or the word    speech.   

id144 refers to variations that do not impact the identity of
words, but create meaning or mood nonetheless. id144 manifests
itself acoustically as pitch, lengthening, and loudness. an example is
the rising pitch contour that marks a question in english and many
other languages. note that in tonal languages pitch is not prosodic,
but rather serves to di   erentiate the meanings of words.

the phoneme, an individual speech sound, is the basic acoustic
building block of speech. phonemes are represented by phonetic alpha-
bets. the international phonetic alphabet (ipa)2 is used by pho-
neticians. speech and language systems generally use some variant
of ipa, for example, sampa (speech assessment methods phonetic
alphabet).3 other examples include the timit phone set cf. [90] and
for american english, the arpabet cf. [139].

linguistics distinguishes between phonemes and phones. a phoneme
is an abstract sound category: change a phoneme in a word and
the word is either destroyed or has changed its identity. a phone

2 http://www.langsci.ucl.ac.uk/ipa
3 http://www.phon.ucl.ac.uk/home/sampa

282 automatic id103 (asr)

is a speech sound as it is instantiated in a speci   c speech signal.
when a id103 system is designed, a compromise is made
between modeling phonemes (i.e., abstract sound categories) and mod-
eling phones (i.e., speci   c sounds). this compromise serves to explain
why id103 scientists often use the designation    phone   
and    phoneme    interchangeably, despite their distinct
linguistic
de   nitions.

phones are produced when a fundamental frequency generated by
an airstream interrupted at quick regular intervals by the vocal cords
resonates in the cavities of the vocal tract. the relative size and shape
of these cavities are changed by the motion of the articulators (e.g.,
tongue and lips), giving a speech sound its distinguishing qualities.
when the air   ow does not encounter any signi   cant obstruction while
moving through the vocal tract, a vowel sound is produced. vowels
di   er with respect to the con   guration of the tongue, jaw and lips,
which control the size and shape of the resonating chambers in the
vocal tract, for example, the mouth cavity. when the air   ow undergoes
signi   cant blockage, either in the form of major constriction or complete
interruption, a consonant is produced. for example, the phoneme [b] is
produced by blocking the nasal cavity and by interrupting the    ow of
air through the mouth using the lips, cf. e.g., [51, 117, 139] for a more
detailed description of phoneme production.

the speech signal changes over time, with the midpoints of
phonemes, particular vowels, being the most stable stretches. the tran-
sition from one phoneme to the next corresponds to the movement of
the lips, tongue and other articulators, which can be visualized as a
gesture aimed at achieving a target con   guration su   ciently charac-
teristic of a given phoneme to allow humans to distinguish it from con-
trasting sounds. the fact that articulatory gestures are fundamentally
freeform in nature means that it is not possible to de   ne a time point
at which one phoneme ends and the next begins. the natural blurring
of phonemes one into the other is the source of co-articulation e   ects
in speech. the enormous variability during sound transitions gives rise
to a signi   cant challenge for recognizing human speech.

a visualization of a spoken utterance helps to illustrate the
magnitude of this challenge. figure 3.1 is a spectrogram of the speech

3.1 the nature of human speech

283

fig. 3.1    it rains a lot in portland.   : spectrogram of the speech signal.

signal of a human speaker pronouncing the sentence    it rains a lot in
portland.   4

a spectrogram shows the variation of the speech signal with time.
the vertical axis represents frequency, and the strength of particular
frequencies are represented by the relative darkness of their depiction.
the horizontal access represents time. it can be seen that the phonemes
are not distinct from each other, but rather    ow from one into the other.
a single phoneme can be pronounced in di   erent ways, meaning
that a single phoneme is associated with multiple phones. these phones
are called variants or allophones. which allophone is used to realize a
particular phoneme is dependent on the neighboring phones or some
other feature of its context [51].

3.1.1 the scope and variability of human speech

in addition to its rich structure, human speech is characterized by a
high level of variability. physiological diversity, individual style di   er-
ences, speaker origin and the heterogenous and dynamic nature of what
we as humans wish to express, all conspire to make the speech signal
highly variable and id103 a di   cult task. a useful inven-
tory of the sources of variability in speech is presented by [20]. this
work identi   es gender and age as sources of variability due to physio-
logical variation. social diversity gives rise to non-native accents, which
tend to be highly dependent on the individual speaker and also a   ected
by native tongue and pro   ciency level. it is also the source of regional
accents, which tend to be stable over speaker populations.

4 credit: aaron parecki (http://aaronparecki.com)

284 automatic id103 (asr)

the social origins of diversity in human speech style make it
clear why human speech is inherently and necessarily highly variable.
a speech variety can be de   ned as the type of speech used by a par-
ticular speech community or for a particular function. sociolinguists
de   ne a speech community as a group that uses the same variety of a
language and that agrees on appropriate language usage in their social
context.

a classic typology of speech varieties is given by [7], which iden-
ti   es four types of varieties in speech, standard speech, social speech
varieties, regional speech varieties and registers. standard speech is a
variety that is accepted as socially outranking other varieties and is con-
ventionally used in government, broadcast communication media, and
educational settings. standard speech is highly conventionalized, apply-
ing rigid norms for pronunciation and syntax, and broadly overlaps
with written language. some language communities establish organiza-
tions charged with the de   nition and enforcement of standard speech, a
prime example being the acad  emie fran  caise, the o   cial authority for
the french language. although prescriptivist control of standard speech
without doubt has a large impact on speech production, much speech
is produced    in the wild    so to speak, compliant with usage convention
and dynamic social norms rather than a set of    xed rules. social speech
varieties include types of speech used in groups that share a socio-
economic status, a gender, an ethnic background, an age background,
an occupation, or any other set of common interests or circumstances.
social speech evolves with the dual function of enabling communica-
tion between members of the group and di   erentiating the group from
other groups. regional speech varieties arise under the constraints of
geographical proximity and speech registers are varieties of speech spe-
ci   c to certain contexts or functions. further information on language
variation and change can be found in [36].

in the speech signal, variability in speech can be observed to follow
certain patterns. for example, articulation di   erences among words
show a dependency on word identity. function words are generally
reduced and content words are generally stressed, although these ten-
dencies can be reversed in spontaneous speech [31]. however, variabil-
ity remains very di   cult to predict e   ectively. a fast speaking rate can

3.1 the nature of human speech

285

result from multiple strategies, either speeding articulation or deleting
phonemes, depending on style and context [291]. the e   ect of mood
change on speech production is deemed    considerable    by [20]. results
of experiments reported by [291] suggest that the impact of speaking
style on id103 performance is great.

asr research distinguishes among di   erent types of speech and
de   nes di   erent tasks. over the course of the development of the    eld,
tasks of increasing di   culty have been formulated. the earliest asr
systems recognized isolated words only. isolated word recognition is
applied in command-and-control applications such as controlling a car
stereo. research on recognition of continuous speech, that is, words
spoken one after another, began in the 1990s. e   orts in the early 1990s
focused on recordings of speakers reading text aloud, so-called    read
speech.    the national institute of standards and technology (nist)
sponsors a series of benchmark tests that have been central to the devel-
opment of id103 technology.5 read speech corpora provide
good reference material for quantitative evaluation, but do not involve
spontaneous e   ects and also lack the variety in recording conditions
(i.e., background noise) characterizing real-world situations [214].

speech as we deploy it in daily use, which is often spontaneous
speech, has radically di   erent characteristics from read speech. upon
   rst consideration, these di   erences may not be obvious, and in [54]
it is noted that speakers are often surprised when presented with a
literal transcription of what they have spoken. particularly striking
is the fact that interruptions and corrections are a natural part of
conversation. spontaneous speech e   ects include variable articulation,
variable sentence lengths, dis   uencies such as    ller words, false-starts,
self corrections, and breathing [31, 54]. the description of spontaneous
speech in [54] includes unconventional usage of words, agreement mis-
matches, run-on sentences, hesitations, and restarts that give rise to
partial words and phrases. non-verbal communication such as eye con-
tact and nodding behavior is also considered by [54] as part of sponta-
neous speech     although this information is not available to a speech
recognition system that only processes audio input.

5 http://nist.gov/itl/iad/mig/

286 automatic id103 (asr)

recently, progress has been achieved in a number of particular
domains of asr including telephone speech [96], children   s speech [220],
noisy environments [58], speech emotion recognition [244] and meeting
speech [23]. in the next subsection, we turn to the details of how an
asr system is built.

3.2 the hidden markov model framework

is

this

text

into text.
asr is the process of transcribing an acoustic signal
in its most basic form,
the sequence of words
w = w1, w2, w3, . . . , wn comprising the spoken content of the sig-
nal. the fundamental task of the recognizer is to determine which
word sequence w best matches the sequence of acoustic observations
o = o1, o2, o3, . . . , on. the id48 approach was    rst applied to speech
recognition in the 1970s [16, 223]. in the 1980s, statistical approaches
to the task of id103 came into their own and the id48
approach emerged as the dominant approach to lvcsr [138]. these
systems were groundbreaking in the area of recognizing words spoken
in a natural speaking    ow by an arbitrary speaker. id48s provide
a uni   ed statistical modeling framework that exploits the structure
of speech and at the same time captures its variability and temporal
dynamics. the system processes a speech signal with the goal of
making an optimal decision concerning the sequence of spoken words.
probabilistically, id103 is expressed as    nding the word
sequence   w that is most probable given the sequence of acoustic
observations o that compose the original signal.
p (w|o)

(3.1)

  w = arg max
all w

  w is the recognizer output and is often referred to as the recognition
hypothesis. this expression is expanded using bayes    law to yield,

  w = arg max
all w

p(o|w)p (w)

p (o)

.

(3.2)

p (w) is the prior id203 of a sequence of words occurring and is
provided by the language model. p(o|w) is the acoustic likelihood of
a string of observations giving a sequence of underlying words and is
provided by the acoustic models, which are trained at the phone level

3.2 the hidden markov model framework

287

and then linked into words using the lexicon. the operation involves
comparing the hypothesis word strings only in terms of their relative
scores. for this reason, the denominator p (o), which remains the
same for any given sequence of acoustic observations, can be dropped
when calculating the recognizer output, yielding,
p(o|w)p (w).

(3.3)

  w = arg max
all w

the elements of equation 3.3 correspond directly to components
of the id48 id103 system. figure 3.2 summarizes in a
block diagram the id48 framework for a typical id103
system. in the following discussion, each module of this diagram will
be treated in turn.

3.2.1 the language model

the language model encodes the id203 of one word being spoken
after another. in [279], the challenge of id38 is described
as    nding a balance between maximum constraint of the search space
and maximum freedom of the input. the standard language model-
ing approach for lvcsr is to use the so-called id165 model. the
language model is trained using large amounts of text, and encodes
the id203 of the occurrence of a given word, based on the words
before it. the list of language model innovations that have been used
to improve id103 e   cacy includes higher order id165s,

fig. 3.2 block diagram of a typical id48-based id103 system.

288 automatic id103 (asr)

for example 5-gram language models in which each word is conditioned
on the context of the previous four words, class-based language models
and multi-gram models. language models are smoothed by introducing
corrections to the estimates of term occurence frequency. smoothing is
an important issue for language models and we refer the reader to the
literature [43] for more detail.

collateral text can also be used to extend the vocabulary of the
speech recognizer and to adapt the language model [13, 224]. in this
process, text can be collected from suitable available sources such as
news feeds, domain-speci   c publications or internet sources. the mate-
rial is analyzed to identify signi   cant new words, which, if they are
deemed important enough, can be added to the vocabulary of the rec-
ognizer. in order for a new word to be added, a pronunciation must be
generated and a id203 must be estimated for the language model.
word pronunciations can either be researched in a suitable dictionary
or potentially generated using grapheme to phoneme conversion [272].
some asr systems use multiple pronunciations for individual words
and pronunciation modeling has long been recognized as a key issue for
asr [261]. similarly, language model probabilities can be checked in a
table of counts from the existing language model training data, if the
word was present in the training set but not selected for inclusion in
the vocabulary, or otherwise using some estimation method.

another technique for improving the language model with addi-
tional information is the    web 2.0    approach to id103
introduced in [93, 207]. here, asr transcripts and speech media are
made available in parallel on the internet and users are encouraged
to correct the mistakes in the transcripts. the transcripts are then
used to re-train the speech recognizer. id104 in this way
has great potential to provide hugely expanded training resources for
language-based applications at reasonable cost.

3.2.2 the lexicon

the lexicon encodes the pronunciations of the words in the speech
recognizer vocabularies. the pronunciation of a word is represented by
a string of phonemes. a single word can be associated with multiple

3.2 the hidden markov model framework

289

pronunciations. if a word is not in the lexicon, it cannot be recognized
by the speech recognizer and is designated as an out-of-vocabulary
word or oov word. oov words can never occur in speech recog-
nizer output, but instead will be substituted by whatever word or word
sequences from within the available lexicon that yield the highest prob-
ability output during the recognition process.

3.2.3 acoustic modeling

each speech sound is represented by an acoustic model. if a language
can be covered by a relatively small inventory of syllables, as is the
case for chinese and japanese, syllables and not phones are used as
the basic unit for acoustic modeling [117]. in other cases, however,
acoustic models are trained at the phoneme level. the phone inventory
for english id103 usually contains around 50 phones [117].
three rules for choosing acoustic units for id103 are
outlined in [117]. the unit should be accurate (able to represent the
sound in its interchangeable contexts), trainable (it should be possible
to collect su   cient training data), and generalizable (the sound should
play a part in providing coverage for new words that are introduced into
the vocabulary). in practice, if an asr system is implemented using
an existing lexicon, the acoustic model inventory will be limited by
the transcription alphabet used by the lexicon. because the phoneme
inventory and the realization of particular phonemes varies from lan-
guage to language, it is usually necessary to train a new set of acoustic
models for each new language that is to be transcribed by the speech
recognizer. however, situations exist in which multilingual phone sets
are appropriate [147, 198].

a typical id103 system represents each phoneme with a
set of triphone models. a triphone is a phone together with the context
provided by its right and left neighbors.

triphone inventories are kept compact by    state tying   , which iden-
ti   es similar neighborhoods to con   ate for the purpose of parameter
estimation. acoustic models are trained by using large amounts of
acoustic data that have been transcribed. a typical amount of training
data would be 300 hours and upwards. an important consideration is

290 automatic id103 (asr)

fig. 3.3 acoustic model of a phone.

whether the speech data used for training is well matched with the
speech data that will be transcribed by the asr system. for example,
training data should re   ect the types of speakers and channels condi-
tions anticipated in the data to be recognised.

some statistics of phone lengths in the switchboard corpus (a
dataset of recorded spontaneous telephone conversations) are given
by [139]. the statistics reveal phonemes to be highly variable in length,
with short lengths as little as 7 ms and long lengths as much as
1.3 seconds. the basic acoustic model represents phones in a length-
independent fashion. as a result, it does a good job of generalizing
over phonemes pronounced at di   erent speeds, but may also sacri   ce
important information encoded in the length as a result.

3.2.4 feature extraction

the speech signal is transformed into a sequence of acoustic feature
vectors before it is processed by the id103 system. the
vectors contain spectral features, which encode how much energy
is present at di   erent frequencies in the speech signal [139]. these
vectors are extracted from overlapping windows of the speech signal.
typically, each second of speech is represented by 100 spectral feature
vectors. each vector is extracted from a signal window that is small
enough to support the assumption that the speech signal is stationary
(non-changing) in its duration. a typical window is 25 ms in length.
for each vector, the window is shifted forward by, for example, 10 ms.
this overlap seeks to ensure that rapid changes in the input signal
are captured in the feature vectors. in general, acoustic vectors are 39
components in length. commonly, the components are mel-frequency

3.2 the hidden markov model framework

291

cepstral coe   cients (mfccs), the change of the mfccs between
windows, and the change of the change of the mfccs. mel frequencies
are frequency bands warped to approximate the sensitivity of the
human ear. perceptual linear prediction is also commonly used for
spectral vectors     this is a linear prediction method that retains
information in the signal that is relevant for human perception.

3.2.5 training

training is the process of estimating the parameters of the id48. a cor-
pus of transcribed speech is necessary for training. a special case of
expectation maximization called the baum   welch algorithm is used to
estimate emission probabilities of individual states and transition prob-
abilities between states. emission probabilities are commonly modeled
as mixtures of gaussians. it is possible to bootstrap acoustic models by
performing a rough recognition and by using the resulting transcripts
to feed model training     cf. the discussion of lightly supervised and
unsupervised training in [81]. if speaker-speci   c data are available or
become available, acoustic models can be adapted to speakers [81].

3.2.6 recognition

recognition is the process of determining the most likely sequence of
words spoken given an observed speech signal. typically, the    noisy
channel model    is used to conceptualize the recognition process. under
this model, the underlying sequence of words spoken is taken to be the
original signal and this sequence is considered to have been distorted by
noise during the generation of the speech signal. the goal of recognition
is to recover this original signal, and, for this reason, recognition is often
referred to as    decoding.    the recognition process involves searching
through the large space of word strings that could be possibly hypoth-
esized by the speech recognizer in order to    nd that string that best
matches the input. this process is referred to in the id103
community as    search    and the output of the speech recognizer as the
   hypothesis.    in the area of speech retrieval, the word    search    is gen-
erally reserved for the process of retrieving results in response to user
queries and, less frequently, for the process of id103.

292 automatic id103 (asr)

decoding strategies fall

into two categories, time-synchronous
and time-asynchronous. the viterbi algorithm is the main example
of time-synchronous decoding. time-synchronous decoding applies a
breadth-   rst strategy for exploring the search space, considering each
hypothesis possible at a given point of time before moving on to the
next point of time. time-synchronous methods make use of dynamic
programming algorithms, which reduce the computational complexity
of a solution of the overall problem by breaking it in to incrementally
solvable sub-problems. time-asynchronous decoding pursues a depth-
   rst strategy, in which the best hypotheses are explored forward in time
before being compared to their competitors. the distinction between
time-synchronous and time-asynchronous decoding is not absolute [12].
in practice, time-synchronous decoders limit the number of hypothe-
ses under examination at any given point using a process called beam
pruning, which eliminates unpromising hypotheses from consideration.
asynchronous stack decoding attempts to expand the best hypotheses
   rst rather than pursuing a purely depth-   rst strategy.

in the next subsection, we turn to discuss some aspects of asr
systems that are particularly relevant when asr is used in order to
realize scr.

3.3 aspects of asr critical for scr

in this subsection, we discuss forms of recognizer output beyond the
1-best transcript, the use of subword units for asr and also the nature
of error in asr transcripts. these topics provide more speci   c back-
ground material for the discussion of    considerations for the combi-
nation of asr and ir    in subsection 3.4 and also the discussion of the
exploitation of asr output in exploiting automatic id103
output.

3.3.1 recognizer output beyond the 1-best transcript

the output produced by a speech recognizer can be represented in dif-
ferent forms. the choice of forms is dependent on what the output will
be used for. the simplest form is called the    rst best hypothesis (1-best
transcript) and consists of a single string of words, for example, using

3.3 aspects of asr critical for scr 293

text factors for search. this string represents the word sequence
identi   ed by the recognizer as being the most likely one pronounced
in the input speech signal. the recognizer can also be set so that it
also outputs hypotheses that are less likely than the    rst best, but
nonetheless likely. such output is called an n-best list. a typical value
for n is 10 and an example of a 10-best list is illustrated in figure 3.4.
a lattice, pictured in figure 3.5, is a network that encodes likely
hypotheses of the recognizer. the horizontal positions of the nodes

using text factors for search
using text vectors for search
using text vectors for surge
using tags vectors for surge
using tags factors for surge
use syntax vectors for search
use syntax factors for search
use syntax factors for surge
using text fact or surge
using tags fact or surge

fig. 3.4 illustration of an n-best list with n = 10.

fig. 3.5 illustration of a word lattice. hypotheses with di   erent start and end points are
represented as di   erent nodes.

294 automatic id103 (asr)

encode time information. each node corresponds to the start time of a
hypothesized spoken unit. in the general case, the recognized units are
words, but they can be any base unit used by the language model of
the asr system, for example, syllables or phones. the distinguishing
feature of the lattice is that it preserves the time information generated
by the speech recognizer. in other words, if the top-n list contains the
same word form recognized with two di   erent starting times, the word
is represented as two nodes in the lattice. each possible path through
the lattice corresponds to a recognizer hypothesis. the lattice makes it
possible to store a large number of speech recognizer hypotheses using
a reduced amount of memory.

a confusion network, pictured in figure 3.6, is a lossy represen-
tation of a lattice. it is constructed such that it contains a path for
every path that existed in the original lattice, but it discards time
information generated by the recognizer. instead,
it retains only
information about the relative position of recognized units and their
competitors. it is not necessarily the case that all the alternatives in
the set compete on exactly the same time segment within the speech
signal. however, the word confusion network (wcn) is formed so
that the confusion sets of words are close enough to be regarded
as competing and the confusion network is normalized so that the
probabilities of all competing words sum to one [81].

the structuring of words into a series of sets gives the confusion net-
work a distinct form, which is often thought of as reminiscent of a string
of sausage links. wcns are usually created with an algorithm, origi-
nally proposed by [177], that bears the nickname    sausages.    indexing
lattices and wcns for scr is discussed in greater depth in exploiting
automatic id103 output.

fig. 3.6 illustration of a word confusion network.

3.3 aspects of asr critical for scr 295

id103 transcripts can be improved by combining the
output of multiple recognizers. a prime example is recognizer output
voting error reduction or rover. combination techniques such
as rover [73] are dependent on the fact that di   erent recognizers
produce di   erent kinds of errors. id145 alignments
create a word transition network and a rescoring or    voting    process is
used. within the resulting word transition network there are correspon-
dence sets. voting between the alternatives in the correspondence set is
carried out on the basis of frequency of occurrence, average con   dence
scores and maximum con   dence scores.

3.3.2 subword recognition

in the case of an lvcsr system, the vocabulary of the recognizer con-
sists of lexical words. other sorts of asr systems, however, can make
use of base units other than lexical words. two challenges must be
faced in order to use a base unit other than an orthographic word. first,
the training text for the language model must be decomposed into
the alternate units so that the id165 probabilities can be estimated.
second, appropriate pronunciations must be assigned to the alternate
units.

a particularly attractive choice of base unit is a morpheme,
the smallest unit of meaning in the language. a morpheme-based
recognizer for the english language would, for example, contain the
units    develop,       test,       ing,       ed    and    ment    and would be able
to combine these units to recognize the forms    developing,       testing,   
   developed,       tested    and    development.    the advantage is that
a small number of units covers a larger word-level vocabulary. the
disadvantage is that the language model bears a much larger burden
of preventing illicit combinations from being recognized, for example,
   testment.    also, the pronunciation of morphemes is sometimes
context dependent (cf. the pronunciation of    ed    in    developed    vs. in
   tested   ). for this reason, it is sometimes advantageous to retain the
association with the base words.

there are two basic strategies for splitting words. first, they can be
split according to statistical occurrences of phone strings, an approach

296 automatic id103 (asr)

chosen by [296], which refers to such strings as particles. second, they
can be split according to linguistic rules.

two types of linguistic splitting can be distinguished. the    rst is
compound decomposition, that is, decomposing    keyboard    into its
component pieces    key    and    board.    compound splitting results in
word fragments (i.e., compound components) that can be considered
to be content bearing, in the sense that they can be assigned a meaning.
in english, compounds generally split into stand-alone words, as is the
case with the example of    keyboard.    in other languages, such as ger-
man and dutch, compounding is a highly productive form of word gen-
eration, meaning that new compounds are created freely    on the    y   
rather than being subject to particular rules or conventions. in such
languages, compound components are often special combining forms
that cannot be used as stand-alone words. for example, the german
compound spracherkennung,    id103,    decomposes into
sprach,    speech,    and erkennung,    recognition.    the stand-alone form
of    speech,    however, is    sprache    and not    sprach.   

the second kind of linguistic splitting breaks up a base form and
additional morphemes. words can also be split into a base form plus
derivational morphology (e.g.,    utterance    =    utter    +    ance   ), in
which case the base form has a di   erent part of speech than the original
word. in other cases, they are split into a base form plus in   ectional
morphology (   recognizes    =    recognize    +    s   ), in which case the part
of speech does not change. in some languages, such as english, there is a
strict limit on how many morphemes can be added to a base form. other
languages are agglutinating, meaning that long strings of morphemes
can be added to base forms. examples are turkish and finnish.

the choice of base unit for the language model is often determined
by the use to which the asr transcripts will be put afterwards. using
smaller units in the language model makes it possible to provide better
coverage for the input signal     the recognizer will encounter less speech
content that it cannot cover with its language model. the disadvantage
of using smaller units is that the system gives up much of the constraint
on the search space. when the search space can contain only words,
which are relatively long, possible transitions from unit to unit take
place less frequently. adding compound components to the recognizer

3.3 aspects of asr critical for scr 297

vocabulary means that the recognizer is capable of producing non-
words, which could be disturbing to a human reader, but yet may be
useful in the context of a speech indexing application. we return to
discuss extracting indexing features from subword asr transcripts in
exploiting automatic id103 output (subsection 4.4).

3.3.3 asr error

asr output is usually evaluated in terms of word error rate (wer).
wer is calculated by calculating the number of word-level mistakes in
the speech transcripts. word-level mistakes fall into three categories:
insertions, substitutions, and deletions. examples, again taken from
the ami corpus of meeting recordings [34], illustrate each type of
error. an insertion error occurs when the asr system hypothesizes a
word for which there is no corresponding word spoken in the speech
signal. here, the recognizer has inserted the word    so   :
that   s. . .    
spoken:
recognized:

   that   s at
that   s that   s the end so that   s

the end.

a substitution error occurs when the word hypothesized by the
recognizer does not match the one spoken in the speech signal. in the
previous example,    at    was substituted by the recognizer with    that   s.   

the following example illustrates a deletion error:

spoken:
recognized:

those who need it   

   to all
to all those need it

here, the recognizer has dropped the word    who,    which does not
appear in the transcript.

in order to calculate word error rate, the hypothesis string generated
by the asr system is compared to the reference string containing the
words actually spoken in the speech signal, using the minimum edit
distance. word error rate is a combination of insertions, substitutions,
and deletions.

wer = 100    insertions + substitutions + deletions
total correct words in transcript

(3.4)

a id145 method is used to create an alignment of the
reference transcripts with the speech recognizer output.

298 automatic id103 (asr)

errors can be roughly divided into two categories: errors that occur
because a word spoken in the speech signal is not present in the rec-
ognizer vocabulary (so-called oov errors) and errors that occur due
to a mismatch between the acoustics produced by the speakers and
those expected by the recognizer. the exact source of error cannot
be explicitly determined, but human listeners can usually make a
good attribution to a particular category. for example, in the error
analysis in [31] the following categories are used: the acoustic model,
the language model, the articulation quality of the segment and other
e   ects such as breaths, out-of-vocabuary words or extraneous noise.
the authors of [31] conclude that non-dis   uent spontaneous speech has
the same error rate as read speech. here, dis   uencies include pause-
   llers, word fragments, overly lengthened or overly stressed function
words, self-edits, mispronunciations and overly long pauses. in [31] it
is recommended that systems explicitly model these events in the rec-
ognizer   s lexicon.

these examples illustrate dis   uencies, which can give rise to asr
errors. the    rst contains a self-edit and the second contains a    ller
word, which is correctly recognized, but associated with an error in its
vicinity.
spoken:
recognized: everything let   s to work as as tight time stamp

   everything that   s a word has a st. . .

time stamp.   

stamps um for, well, segments   
spoken:
recognized: that there   s times sense um for low segments

   there there are time

note that the word    stamp    is recognized correctly in the    rst utter-
ance but that    stamps    is mis-recognized as    sense    in the second. as
discussed in detail later, words that are repeated in the speech signal
help to compensate for recognition errors because the asr system
has multiple    opportunities    to get them right. if these two utter-
ances were contained in the same spoken content item, and if id30
were applied to pre-process the transcript for indexing, the confusion
between    stamps    and    senses    would lower the count of the stem
stamp in the index, but the item would still be able to match a query
that contained the word    stamp.   

3.4 considerations for the combination of asr and ir 299

it cannot be expected that the length of the words or the number
of syllables will match up in the spoken utterance and the recognized
utterances. most implementations of id48 recognition do not model
length explicitly and, as such, spoken words can be contracted during
substitution error.
spoken:
recognized: beyond the front so

   be on utterance

level   

again, item length is an issue. if items are long enough, the num-
ber of words in the transcripts provides a good approximation of the
number of words spoken in the speech signal. these examples of asr
error provide a lead-in for the next subsection, which begins with a
discussion of the impact of asr error on scr performance.

3.4 considerations for the combination of asr and ir

the issue of how to combine asr and ir optimally has occupied
researchers quite intensively. we argue that consideration of the com-
bination of asr and ir will allow an scr system to outperform one
in which asr and ir have been combined na    vely. in this subsection,
we    rst look at the implications that asr error has for scr. then we
turn to discussion of design decisions during implementation which are
important for the integration of asr and ir.

3.4.1 impact of asr errors on scr

the relationship between the quality of the transcripts produced by an
asr system and the performance of a spoken content retrieval (scr)
system that makes use of these transcripts is a complex one. conven-
tionally, transcript quality is measured in terms of word error rate
(wer), de   ned in subsection 3.3.3. the wer is dependent on the
domain. in [221], a textbook published in 1993, the state-of-the-art
in asr performance was reported for    uent id103 on the
naval resource management corpus. the wer was on the order of 4%,
which is roughly the same level of error made by humans carrying out
manual transcriptions of naturally produced speech. this bright picture
comes nowhere near the reality of the speech data in the domains for

300 automatic id103 (asr)

which scr systems are currently developed. the vocabulary of the
naval resource management corpus was slightly less than 1000 words
and the recording conditions were highly controlled. as a result, this
low error level is not typical for real world applications.

interestingly, and possibly unexpectedly, experience has shown that
a scr system can make use of transcripts with a relatively high wer
without compromising retrieval performance. the robustness of scr to
asr error was studied in the literature as early as 1994 [240]. in [104],
experiments were carried out with simulated wers where it was shown
that wer of up to 50% still provided 85%   90% scr accuracy. in [5],
e   ectiveness of retrieval is reported to fall less than 10% even with
wer of around 40%.

the large corpus of broadcast news used for the trec spoken
document retrieval track in 1999 and 2000 (i.e., sdr at trec-8 and
trec-9) was recognized with wers that ranged from 10% to 20%.
in [38], it is noted that trec sdr shows that retrieval performance
was not impacted by these error levels, but that wers of 30%   50%
are typical for more challenging domains. in [148], wers of 10%   20%
are cited for spoken news and of 20%   40% for voicemail and conver-
sational speech. in [39], a wer of 50% is reported for state-of-the-art
recognizers on recorded lectures used for scr. wers of ca. 50% are
faced by [176], in the case of spoken document retrieval from call-center
recordings, and by [47], in the case of retrieval from conversational
telephone speech. in short, high asr error rates are characteristic of
many of the domains for which scr systems are developed and systems
that assume very high quality speech transcripts may be of limited
applicability in real world scr settings.

3.4.2 suitability of wer for predicting scr performance

in practice, wer may not be the best measure of quality of speech
transcripts and their usefulness in a retrieval setting. in [256], it is
stated that wer is not particularly appropriate for use in the retrieval
context because it treats all words equally. in the area of spoken lan-
guage understanding, it is shown that language models that produce
better understanding accuracy actually su   er from higher wers [284].

3.4 considerations for the combination of asr and ir 301

in [83], it is found that the correlation between wer and scr per-
formance was non-linear for the 1998 (trec-7) trec sdr data
collection. the literature contains several proposals for what is referred
to by [82] as    ir-customized asr scoring,    in other words, metrics that
capture those aspects of asr transcripts that make them well-suited
for scr. in [83], several alternatives were explored. filtering stopwords
from the asr transcripts before calculating wer did not succeed in
achieving a better correlation between wer and scr performance.
the wer calculated on the basis of named entities (people, loca-
tions, and organizations), however, did show a better correlation with
scr performance than wer alone.

3.4.3 interaction of asr error and ir

although standard wer might not be the best predictor of scr per-
formance, what is clear is that asr error does impact scr. in [237], it
is shown that asr transcripts that provide good matches with queries
tend to have lower wers than asr transcripts that match the query
less well. analysis showed that documents that contained a broader
range of query words tended to have lower wers. the authors con-
clude that it is the process of retrieval itself that is identifying doc-
uments with lower wers. this is not a surprising result. if a topic
can be considered to be characterized by a certain pattern of word use,
then it can be expected that the pattern will characterize both a human
generated query related to that topic and also human speech on that
topic. the pattern is better represented if the asr wer is low and
the query can be more easily matched with the content item.

the importance of topical word patterns, leads us to expect that
errors generated by asr have a good chance of having minimal impact
on scr. wer patterns are in   uenced by factors (such as acoustics)
that are independent of the human process of meaning production.
because of this independence, the distribution of word errors will not
readily fall into characteristic patterns for topics. in other words, it is
not obvious that mis-recognized words could easily appear in a pattern
that    accidentally    resembles a topic. it is an open question whether,
and under which conditions, the distribution of word errors in asr

302 automatic id103 (asr)

transcripts will mimic topic patterns. it is reported in [5] that auto-
matically expanding trec sdr queries using asr transcripts seems
to add helpful recognition errors that are correlated with query top-
ics     these helpful words are e   ectively representing in the query a
spoken word that the asr system consistently misrecognizes. in [162],
an analysis of turn-sized segments of meeting speech from the ami
corpus is carried out. the results suggest that    habitual    errors of
the asr system (when a word is consistently substituted for another
word) have less of an e   ect on the overall semantic relatedness between
items in the collection than    infrequent    errors,
i.e., substitution
errors the asr system makes no more than three times in the entire
collection.

the literature analyzing the statistics of mis-recogized words in
asr transcripts is restricted to relatively few references. in [240], it
is noted that errors produced by the recognizer and well-recognized
query features have di   erent distributions and that ir models are able
to exploit this di   erence. in [135], human-generated transcripts and
asr transcripts from the 1999 trec sdr (trec-8) were compared
with respect to word statistics. it was found that the human transcripts
contained roughly four times more unique word types than the asr
transcripts. the asr transcripts contained a radically smaller num-
ber of singularities (word types that only occur once in the collection).
the authors point out that misspellings in the human transcripts can
impact this number, but the e   ect remains marked. the picture that
emerges is that the impact of wer on the distribution of word types
in asr transcripts involves a large number of types that are deleted or
substituted out and a rather smaller number of    favored    types from
the asr system   s vocabulary that are inserted or substituted in.

word errors potentially have unexpected interaction with ir
models. recall from the overview of ir models in subsection 2.2 that
methods for calculating term weights and term probabilities typi-
cally take account of term frequencies and document lengths. these
term weights are frequently integrated into ir models using non-linear
functions. the implication is that the    rst occurrence of a word in a doc-
ument can be very important, but subsequent occurrences may make
only a very marginal di   erence in the ranking function. word types

3.4 considerations for the combination of asr and ir 303

that are infrequent and are substituted out by the speech recognizer,
possibly because they are not in the vocabulary, pose a clearer threat
to scr performance. words missing in the asr transcript destroy the
possibility of creating a match between queries and items.

for longer queries and documents, the impact on ranking is likely
to be small due to redundancy e   ects. essentially, longer queries and
documents provide richer expressions of topical word patterns and
more reliable matches. in [5], the challenge of short queries and short
documents is pointed out. the mismatch issue is likely to be most sig-
ni   cant for short queries consisting of only two or three words. if even
one of these is absent from the transcript then it can have a signi   cant
impact on the matching score, reducing the relative score of the item
and its rank. similarly, a single match with an error in the transcript
could cause an irrelevant item to be ranked high in the list. this e   ect is
similar to what is observed for standard text ir in cases in which there
is mismatch between query terms and terms used in the document    
although in the case of scr noise in the form of errors, the transcript
will probably cause an even greater negative e   ect than in the
text case.

in sum, asr error leads to reduction in retrieval e   ectiveness in
scr. the underlying reasons for this arise from a complex interac-
tion of query matching issues and distortion of term weights. these
issues can to some extent be overcome by a range of techniques, includ-
ing external text collections to provide better collection frequency
estimates, and document and id183 techniques to amelio-
rate matching problems. these techniques will be discussed further in
spoken content retrieval beyond asr transcripts.

3.4.4 strategies for combining asr and ir

implementing an scr system requires making a decision concerning the
level of integration between the asr component (that transcribes the
speech signal to text) and the indexing component (that derives useful
indexing terms from the text) used for ir. in the following discussion,
we brie   y sketch the arguments for isolating and then for integrating
asr and indexing in an scr system.

304 automatic id103 (asr)

isolating asr and ir. a system that isolates asr and retrieval,
does not allow information concerning the retrieval process to inform
the generation of the id103 transcripts. the primary
motivation for isolating asr from retrieval in an scr system is modu-
larity. the speech recognizer can be treated as a black box: it produces
a certain output and this output is then passed to the indexer. the
indexer extracts terms from the transcripts and creates the index that
will be used for search.

maintaining modularity greatly simpli   es the implementation and
maintenance of an scr system. it enables, for example, the use of
an o   -the-shelf asr system, which is easy to use, but may o   er only a
limited amount of control over the language model, the acoustic models
and the system parameters. when asr and indexing are fully isolated,
it is possible to substitute one asr system for another with a minimum
of e   ort. this    exibility could be an advantage if new and better asr
systems or systems versions become available.

isolation has another practical advantage. if the asr system is
isolated from the indexing process, it is possible to optimize it just
once, and from that point leave it to do its job, representing lower
maintenance costs. lvcsr is an incredibly complex process. in [12],
a    trilogy    of factors is identi   ed as being responsible for the best-
performing lvcsr systems: combination of multiple algorithms, clever
design cooperative with the hardware, careful parameter tuning. if any
of these factors goes awry during the process of integrating the asr
and indexing system, sub-optimal performance could result.

it can be argued that there exists a precedent for maintaining
modularity. the success of the id48 framework for asr is built upon
a high level of separation between individual components. the acoustic
models are separated from the language model and both can be trained
and optimized separately. they are brought together (via the pronun-
ciation lexicon) during the transcription process. the dependency of
phoneme pronunciation on word identity or sentence structure is mod-
eled by using context-dependent triphones, but otherwise neglected.
historically, this separation has proved e   cient and productive for
id103, suggesting that judicious modularization is also
the appropriate tactic to pursue in scr systems.

3.4 considerations for the combination of asr and ir 305

finally, it is important to consider the needs of the user interface.
the interface might require transcript information, in addition to the
terms that are used for indexing. for example, full transcripts or snip-
pets might be needed for display to the user. in this case, the asr
system needs to generate a classic transcript, closely resembling writ-
ten text. if the asr system and the indexing system are integrated, it
is necessary to ensure that the proper text material is also generated
for representing spoken results in the interface.

integrating asr and ir. the primary motivation for integrat-
ing asr and indexing in an scr system is information exploitation.
additional information made available during the transcription process
serves to restrict the search space of the speech recognizer, leading to
a better recognition result. this principle has made an important con-
tribution to the success of id103 in the area of spoken
dialogue systems. in general, dialogue systems do not carry out lvcsr,
but rather use a highly speci   ed model of what a human can possibly
say during that section of the dialogue, making heavy use of wordspot-
ting and grammars. the success of dialogue systems can be used to
support the argument that scr systems should strive to identify and
extract the semantics of spoken content during the asr process, rather
than waiting for the indexing process.

proponents of a high level of integration between asr and indexing
remind us that id103 has a critical dependency on speech
understanding. indeed, as humans we are well acquainted with the dif-
   culty of following a conversation on a topic that we know nothing
about. alex hauptman has pointed out that id103 cannot
be perfect without understanding and that understanding is consid-
ered to be    ai-complete,    namely, as di   cult to solve as the problem
of arti   cial intelligence itself [102]. if a system uses natural language
processing for another task within the video library, it should be tightly
coupled with the id103 component. the exigencies of sys-
tem design often prevent tight integration of asr and information
retrieval within a system.

asr is a resource intensive process and the extreme case is clear:
the collection must be transcribed in advance     asr cannot run

306 automatic id103 (asr)

at query time. however, approaches that generate subword lattices
using a    rst recognition pass and subsequently re-score these lattices
as additional information becomes available are computationally viable
approaches. these are important approaches to be aware of and will
be treated in more detail in exploiting automatic speech recogni-
tion output. such approaches make it possible to address the chal-
lenge of covering all possible words caused largely by oov words,
which are not in the lexicon of the asr system and therefore can
never be recognized. they also address the challenge of handling
uncertainty     the variability of speech means that there is never
a perfect match between the speech stream and the asr systems
models.

design of an scr system that integrates asr and ir requires a
solid knowledge of both components. if a linguistic resource already
used during the asr process is to be used again during the ir pro-
cess, it must be used in a di   erent manner. if not, the same informa-
tion will have simply been added to the system twice and performance
will not improve. information should be added to the system where
it can be most helpful. if a resource used by the ir component could
be more e   ectively exploited by the asr component, systems should
be designed to use it there. in particular, the trend towards produc-
ing lattice representations and also to integrating long range syntactic
dependencies into the decoding process, as mentioned by [12], is an
important one.

historically, asr research has pursued the vision of    the listening
typewriter,    a machine that would produce a transcript comparable to
that of a human transcriptionist using a typewriter. spoken content,
however, is not structured in the same way as text and does not lend
itself to representation in a conventional transcript, unless it was dic-
tated for that purpose. although asr error rates have been pushed to
dramatically low levels for speaker-dependent applications, it is only
user groups with specialized needs who rely on them heavily for tran-
scription. these groups include doctors, who transcribe large volumes
of information with specialized vocabulary and conventionalized struc-
ture, and users, who cannot manipulate manual interfaces or who need
to save hands and wrists from undue strain. in only a small number

3.4 considerations for the combination of asr and ir 307

of cases, does asr actually serve the function of    the listening type-
writer.   

despite the limited success of this initial vision, lvcsr shows
promise of yet living up to its potential as a revolutionary technology.
scr has become an important application area for lvcsr. in 2003,
ken church predicted that search as a form of consumption would
replace dictation as a speech and language processing priority [50]. if
asr research is increasingly pursued with the goal of scr, a tighter
integration between asr and indexing will become a natural develop-
ment in the future.

in sum, a system that isolates
achieving an optimal balance.
asr from indexing should strive to carry out the best, most    exible
asr transcription when the spoken content is initially processed. the
asr transcripts should retain as much information as possible (e.g.,
lattices, which encode alternative recognizer hypotheses) so that, as
further information becomes available, it is still possible to make use of
detailed information from the speech signal. the transcription should
be suitable both for indexing purposes and for the generation of repre-
sentations of documents and results in the user interface. the optimal
balance between asr and ir will depend on the use scenario in which
the system is to be deployed. if, for example, the speech is recorded
under ideal conditions and oov is not a problem, then it may be the
case that the    rst-best word-level output of an lvcsr system may
be entirely suitable to provide users with the speech content that they
are looking for and further optimization aimed towards integration of
asr and ir is not necessary. this example is exaggerated since most
domains in which scr systems are needed are far from being highly
ideal. however, it serves to illustrate the danger of seeking a na    ve or
a    one size    ts all    solution for the integration of asr and ir in an
scr system.

we now turn to examine speci   c techniques and technologies that
can be drawn upon for developing scr systems. speci   cally, we next
treat in greater detail the issue of extracting indexing features from
spoken content and using these features for scr in a way that achieves
an optimal balance between asr and ir.

4

exploiting automatic speech

recognition output

here, we cover techniques for exploiting the output of an automatic
id103 (asr) system in a spoken content retrieval (scr)
system. we focus on the indexing of asr transcripts. indexing is the
process of creating representations of content items that capture their
key characteristics and make it possible to access them using a search
system. the representation of an item consists of a set of indexing
features, which in the case of spoken content includes words or word-
like units, and, importantly for scr, information on where these units
appear (i.e., their order or time codes). as in text-based ir, such index-
ing features are called    terms.    a term can be considered a string of
characters used to represent a content item. ir systems do not treat
all terms equally, rather terms are associated with di   erent weights or
probabilities in the system. scr makes use of corollaries of term counts
and other term-level statistics that play a central role in classic text
ir frameworks (cf. subsection 2.2). in the scr setting, calculation of
these quantities additionally can involve taking into account the infor-
mation contained in the asr output that is able to re   ect uncertainty
concerning the correspondence of this output to the original spoken
input (i.e., the words spoken in the original speech signal).

308

309

the indexing techniques presented in the material that follows make
it possible to adapt and apply text ir indexing approaches to asr tran-
scripts. as a    rst step to understanding the indexing challenges in the
scr setting, it is helpful to re   ect on the appropriateness of the word
   transcript    to designate the output of an asr system.    transcript   
implies a transcription of spoken content that respects the conventions
of written language, that is, it is a human-readable text. although
asr output is, in many cases, human readable (i.e., consists of lexical
words), in a large number of cases it is not. in subsection 3.3.1, we saw
that asr output can di   er from conventional text, in that it is charac-
terized by noise (both word errors and segment boundary errors) and it
contains timing information. di   erences between conventional text and
asr output correspond to additional information sources that could
and should be exploited for scr. we focus our discussion on three
sources of information often included in asr output and their use in
scr:

    the inclusion of multiple recognizer hypotheses
    the association of each unit with a con   dence score that rep-
resents its level of match to the speech signal
    the use of subwords as the basic unit of recognition

in subsection 3.4, we arrived at the conclusion that it was necessary to
strive for a balance between asr and ir that is speci   c for a particu-
lar scr system and its use scenario. these three information sources
represent three opportunities for achieving an e   ective balance of inte-
gration between the output of the asr system and the index used for
scr. in the following discussion, the usage of these information sources
is covered and their untapped potential is highlighted.

this material addresses the    rst two challenges for scr that were
listed in subsection 2.4. we begin by discussing techniques that have
been developed in response to the    rst challenge, the challenge of
handling uncertainty. subsection 4.1    going beyond the 1-best asr
output,    deals with techniques for indexing n-best lists, and lattices
and confusion networks, structures that encode multiple recognizer
hypotheses. subsection 4.2    exploiting con   dence scores,    presents
the use of information on the reliability of the asr output for indexing.

310 exploiting automatic id103 output

the remaining subsections discuss techniques that are concerned
with representing uncertainty, but which also address the second
challenge, namely, the challenge of covering all possible words.
subsection 4.3,    representing speech with subwords,    gives a general
introduction to sub-lexical units that can be used for vocabulary inde-
pendent representation of spoken content. subsection 4.4,    subwords
as indexing features,    covers methods of extracting subwords from
speech, presenting a survey of techniques that use subwords and combi-
nations of subwords as the basic indexing unit for scr. subsection 4.5,
   hybrid approaches for scr,    describes how subword-based and word-
based approaches can be combined for improving scr performance.

in overview of spoken content indexing and retrieval, a distinc-
tion was made between scr, which focuses on meaning-based rele-
vance ranking, and other    searching speech    tasks that aim to    nd
mentions of query terms, for example, spoken term detection (std).
approaches that    nd mentions are, however, in the scr setting for
the purpose of extracting indexing features. the    nal subsection 4.6,
   techniques for detecting spoken terms,    provides a brief overview
covering these approaches.

the techniques presented next can be used to compensate for the
limitations and also to exploit the potential of the information produced
by the asr system during the id103 process, thus poten-
tially improving scr performance. other sources of indexing features
that can be used for scr, such as metadata and social annotations,
will be examined in spoken content retrieval beyond asr transcripts.

4.1 going beyond 1-best asr output

as described in subsection 3.3.1, asr is capable of producing several
forms of recognition output. the recognition process involves determin-
ing the 1-best hypothesis, that is, the most likely word string spoken
in the audio signal. during this process the asr system generates
information about    runners-up,    other highly competitive hypotheses.
information about the most likely hypotheses can be output in the form
of a list (n-best list), but also in the form of a lattice or a confusion
network. the motivation to use information derived from hypotheses

4.1 going beyond 1-best asr output

311

beyond the 1-best output of the asr system is straightforward: words
spoken in the speech signal not contained in the top-one most likely
hypothesis might indeed be contained in other highly likely hypotheses.
in [247], it is noted that for a given utterance, there are generally a small
number of top hypotheses produced by the speech recognizer that are
close in likelihood. the 1-best hypothesis is, of course, the most likely
of these, but is typically not strikingly more likely that the other top
hypotheses, pointing to the potential high value of the    runners-up.   
in [39], a set of lecture recordings is investigated for which the asr
system achieves a 1-best word error rate (wer) of 45%. the authors
calculate the wer of the lowest wer path through the lattices gener-
ated by the asr system to be 26%. they note that this ratio might not
generalize to other systems or content, but conclude that it suggests
that lattices do have enormous potential to improve scr recall. the
use of lattices is advocated by [238], which notes that using the 1-best
hypothesis is probably su   cient for tasks with moderate error rates
(about 20%), but that tasks with higher error rates require multiple
asr hypotheses. whether or not multiple hypotheses can yield perfor-
mance increases will also be dependent on the domain and on the task,
for example, whether it involves retrieval of short speech segments or
longer documents.

in this subsection, we survey methods that have been applied to
exploit multiple hypotheses of the asr system for scr, looking in
turn at n-best lists, lattices and confusion networks. the subsection
covers only those methods that use word-level indexing features and
exact matching techniques. discussion of methods that make use of
multiple hypotheses consisting of subwords and of fuzzy matching is
delayed until after we have treated the use of subwords as indexing
units in subsection 4.4. note also that the techniques discussed in this
subsection are used for meaning-oriented scr. techniques for mention-
oriented search, such as std, also make use of lattices, and these will
be discussed in subsection 4.6.

4.1.1 indexing n-best lists

using n-best lists is a simple approach to exploiting multiple recognizer
hypotheses that has been reported to work well in the literature. work

312 exploiting automatic id103 output

by siegler et al. at cmu [245] showed that using the top 50 hypotheses
of the recognizer dramatically improved the performance of their spo-
ken document retrieval in the 1997 trec-6 sdr task, which involved
a corpus of english-language broadcast news containing 1500 stories
(ca. 50 hours of spoken data) and a set of 50 queries for known-items
(i.e., only one document in the collection was considered relevant to the
query). the 1-best wer of the asr system was approximately 35%.
inverse average inverse rank (iair) of the topmost relevant doc-
uments in the lists returned by their vector space model (vsm)
retrieval system improved from 1.37 to 1.32. siegler   s dissertation [247]
reported on experiments on the 1997 trec-6 task in which extremely
long n-best lists were used. the task involved 23 queries and a corpus
containing ca. 2800 documents (ca. 75 hours of spoken data). the use of
long n-best lists yielded scr performance that was improved 64% over
that achieved when only the 1-best hypothesis was used. the approach
assumes that all hypotheses in the n-best list are equally likely, but also
imposes the assumption that the n hypotheses are independent of each
other and the individual words in the hypotheses are also independent.
weights for the vsm are calculated simply by concatenating the top-n
hypotheses for each spoken document and counting the terms accord-
ing to a standard text-ir procedure. the approach is an appealing one,
since it provides a very simple estimate of term frequencies, obviating
the need for the more complex models needed to derive term frequencies
from lattices or confusion matrixes. these experiments were carried
out on a relatively small corpus of broadcast news pre-segmented into
stories. the gains of this method may or may not transfer to the larger,
less well-behaved corpora currently used for scr research. however,
the simplicity of using n-best lists makes it worthwhile to investigate, or
at least consider, this approach in any new scr use scenario.

4.1.2 indexing lattices

lattices provide a compact representation of top-hypotheses of the asr
system. recall from automatic id103 (cf. figure 3.5) that
a lattice is a directed acyclic graph structure that stores a speech rec-
ognizer search space that has been pruned to contain the most likely

4.1 going beyond 1-best asr output

313

output hypotheses. the    rst lattices used in scr were actually phone-
lattices, and their use dates back to the earliest scr systems developed
in the mid-1990s [120, 132]. we will return to discuss these techniques in
subsection 4.6 after we have also covered subword units such as phones.
the lattice contains the most likely hypotheses of the speech recog-
nizer and in this way resembles an n-best list. unlike the n-best list,
however, it contains timing information     in other words, the time
point at which the recognizer hypothesized each word is encoded in
the lattice. in [37, 38, 39], a 17%   26% improvement in mean average
precision (map) is reported compared with indexing the 1-best recog-
nizer output. the corpus used contains 169 lectures of approximately
one hour each. the 1-best wer of the asr transcripts was ca. 45%.
the authors proposed a    soft hits    approach that explicitly addresses
the fact that existing schemes for indexing lattices are not designed to
take into account information concerning the proximity of words.

they point out that there are two aspects of uncertainty about a
word in a lattice:    rst, whether or not that word was actually spoken
and, second, the position at which it appeared (including its ordering
relative to other words). on individual paths through a lattice, position
indexes can be assigned to words in the order in which they occur. the
uncertainty arises in calculating which position should be considered
the overall position of the word within the lattice for the purposes
of applying information retrieval (ir) approaches that make use of
word proximity. proximity information is needed to search for phrases,
for example, or to integrate nearness constraints into multiple word
queries.

the    soft hits    approach [37, 38, 39] involves reducing the lattice
generated by the asr system into a    position speci   c posterior
lattice    (pspl), which is smaller than the original lattice and also
makes possible indexing of word position. the challenge of deriving
word-position information from a lattice can be best understood by
noting that on any given path through the lattice, the order of the
words on that path is uniquely de   ned. however, given one word in
the lattice, it is not possible to assign it an overall position, since it
may have di   erent positions on di   erent paths. the pspl lattice bins
the words in the original lattice and stores the id203 that a word

314 exploiting automatic id103 output

occurs in a particular position. a soft hit consists of an integer position
in the speech segment and a posterior id203. the score of a word is
calculated by summing its id203 across all positions. these scores
are then used as weights in a vsm retrieval scheme. a major advance
of pspl is that existing implementations of phrase matching can be
applied to the pspl index without modi   cation.

an alternative to pspl called    time-based merging for indexing
(tmi)    is proposed by [315]. recall that pspl retains information
about the relative order of words, but discards timing information.
the tmi approach is designed with an eye to applications requiring
navigation and previewing, in which the time stamp of individual words
is important. like pspl, tmi lattices achieve dramatic improvement
in map over approaches using 1-best. on the same lecture recording
data set described above, 1-best achieves a map of ca. 0.53 and tmi
of 0.62. tmi reduces the lattice by merging word hypotheses that have
nearly identical start and end times. tmi outperforms pspl in terms
of the trade-o    between the index size and accuracy.

there are two speci   c characteristics of the lecture recording set
used to evaluate pspl and tmi that should be noted. first, the speech
style of the speakers falls between planned and spontaneous, meaning
that it is a challenging corpus, but that a completely spontaneous cor-
pus would prove more challenging. second, the soft hits were calculated
on the basis of sentence-length segments that were produced by align-
ing the speech signal with a transcript containing human generated
segment boundaries. the performance of the approach under less con-
strained conditions remains an empirical question.

in [47], a lattice-based approach is proposed and tested using
the bm25 model and the id38 framework. this work
includes a careful analysis of the impact of lattice pruning on lattice-
based scr. the optimal pruning setting is a trade o    between
insertions and deletions of the correct word in the lattice. in order to
achieve good scr results, it is important to carefully select the degree
to which the asr output lattices are pruned before indexing. the
methods were tested on a corpus of english-language telephone speech
containing nearly 2000 hours of data divided into approximately
12000 conversation-sized documents of up to 10 minutes in length.

4.1 going beyond 1-best asr output

315

the 1-best wer was ca. 50%. the highest performance was achieved
using lattices: with lattices, a map of about 0.77 was reported and
could be achieved using both the bm25 model and the language
modeling framework. the added advantage of using lattices is quite
slim, however. applied to 1-best transcripts, the id38
approach already achieves a map of 0.76. these results suggest that
the advantages of using lattices are dependent on the use scenario,
including the task, the data set and the asr system.

4.1.3 indexing word confusion networks

word confusion networks (wcns) are simpli   cations of lattices that
allow direct access to information about competing word hypotheses.
recall from automatic id103 (figure 3.6) that the nodes
in a confusion network do not correspond to points in time, as the nodes
in a lattice do. instead, they correspond to starting and ending points
of words and their hypothesized alternatives. each arc in a wcn is
associated with the posterior id203 of a word. each path in the
original lattice has a corresponding path in the wcn. approaches that
index wcns for scr build on [177], which proposes an algorithm for
deriving confusion networks from lattices. the wcn is, in essence, a
multiple alignment of all the di   erent paths contained in the lattice.
the    rst step in creating a wcn is initializing the equivalence classes.
the initialization is done by creating classes from all words that are
the same and have the same starting and ending times. the id91
algorithm then merges equivalence classes, taking identity and phonetic
similarity into account. the posterior word probabilities within the
wcn are then scaled and normalized to sum to one, over all hypotheses.
for an alternate approach that aligns all other paths to the 1-best path
through the lattice refer to [98]. the wcn is very similar to the tmi
structure discussed in the previous subsection. a practical di   erence
is that wcns can contain null transitions [177], which makes them
a less natural choice for phrase matching (i.e., matching of adjacent
terms).

previous to their application to scr, wcns were shown to
yield performance improvement in the area of spoken language

316 exploiting automatic id103 output

understanding [97]. in [176], wcns were applied to scr and exper-
iments were conducted using a large corpus of english-language call
center data. experiments were carried out using the vsm framework
and results were reported for wer levels in the range of 30%   65%.
the data set contained around 2000 calls of approximately 10 minutes
each. results were evaluated by comparing the retrieval performance
on the asr transcripts with retrieval performance on corresponding
human-generated transcripts. these experiments demonstrate a rela-
tionship between higher wer and deteriorating scr results. using
wcns decreases precision over the use of 1-best transcripts, but over-
all map is increased when weights derived from wcn are boosted
with information on their relative rank within their equivalence class.
further exploration of this approach, including analysis of the impact
of pruning on the weights used for wcn indexing, would help to shed
light on whether, in practice, the use of wcns is worth the computa-
tion involved to extract them.

in sum, the relative merits of using word lattices have strong depen-
dencies on the domain, the asr system, lattice pruning, and the way
in which lattices are processed for indexing. it remains di   cult to build
a system that takes advantage of multiple recognizer hypotheses that is
better-principled and outperforms that achieved with n-best lists. intu-
itively, lattices and wcns make it possible to incorporate information
about word hypotheses that are in direct competition with each other.
however, it was pointed out in [247] that a word is contained in many
good paths through the lattice, then its direct competitors will be con-
tained in relatively fewer good paths in the lattice. in this way, scores
calculated on top-n lists do capture to some extent the competition
between di   erent words in the same part of the speech signal. any
indexing of lattices or wcns must incorporate a substantially better
manner of encoding the uncertainty of individual words if they are to
lead to overall scr improvement.

an important aspect of the approach proposed in [176] involves
taking into account the con   dence level of each word. because of the
potential for con   dence in support of scr, we turn in the next sub-
section to a brief overview of the area of con   dence score computation
for id103 transcripts.

4.2 con   dence scores

317

4.2 con   dence scores

a con   dence measure is a score that encodes the asr system   s assess-
ment of the reliability of its output [125]. word-level con   dence scores
re   ect whether a word actually occurred in the spoken content where
it was recognized. the scores express the reliability of the hypotheses
or their id203 of correctness. in a provocatively-titled 1996 paper,
   towards increasing id103 error rates    [22], a case is made
for    knowing what we do not know.    this paper called for more e   ort
to be invested in methods for estimating when the speech recognizer
goes wrong, rather than in just making errors less frequent. this call is
motivated by the observation that the success of asr technology is in
task completion. in the scr setting, task completion involves providing
a set of items that satis   es the user   s information needs. fifteen years
later, we have at our disposal an array of interesting techniques for
estimating word-level con   dence in asr output, but no single widely-
used framework that guides the integration of this information into the
scr system in such a way that guarantees improvement. this subsec-
tion provides a compact overview of methods for generating con   dence
scores, in order to provide an introduction to the topic for researchers
who are interested in using these scores to improve the performance of
spoken content retrieval (scr). we also note the existence of helpful
surveys on con   dence scores, including [125, 294].

in general, con   dence scores fall into two categories, depending on
the kind of information used to calculate them: scores using information
from the asr system and scores integrating information from external
sources. in the discussion that follows, we describe each method in turn,
and cover techniques that have been used for incorporating con   dence
scores into scr systems.

4.2.1 information from the speech recognizer

probabilities derived from the asr system are a widely-used source
of con   dence information. recall from subsection 3.2 that the speech
recognition process involves searching for the most likely path through a
search space encoding all possible word strings known to the recognizer.
the string that is output by the recognizer (i.e., the 1-best hypothesis)

318 exploiting automatic id103 output

is more likely than any other hypothesis, but the recognizer does not
calculate its likelihood in absolute terms. early work on asr con   dence
scores [311] characterized the information derived from the asr system
with the remark,    we know which utterance is most likely, but we don   t
really know how good of a match it is.   

the process of looking for the relatively most likely string corre-
sponds to the simpli   ed version of bayes    rule in which the denominator
is dropped (cf. equation 3.2 vs. equation 3.3). during the recognition
process, di   erent word-string hypotheses are compared with respect
to a given segment of the speech signal, that is, with respect to the
same acoustic observations. for this reason, it makes sense for an asr
system to calculate only the relative likelihood of the hypotheses    
the id172 introduced by the denominator is not necessary. the
situation changes, however, when we want to generate a con   dence
score. a con   dence score should re   ect the overall id203 of a
word having been correctly recognized and not ignore dependencies on
characteristics of the speech signal. the un-normalized scores generated
by the asr system are not suited for comparing word hypotheses that
were generated by the recognizer as    tting di   erent acoustics [294].
instead, we need a normalized score, in other words a true poste-
rior id203 of the recognized words. the normalizing factor is the
prior id203 of the acoustic signal p (o), whose calculation is made
tractable by applying a method for approximation.

methods of generating con   dence scores from speech recognizer
output generally di   er with respect to the approximation that they
choose. in [311], id172 is accomplished by carrying out a second
recognition of the speech signal using a phone-based recognizer and
using the score derived in this way as an approximation for p (o). such
approaches are motivated by the following reasoning: p (o) can be cal-
culated by summing over p (o|w ) for every possible hypothesis w .
this sum can be approximated by determining p (o|w ) for the most
closely competing hypotheses, in this case those of the second phone-
based recognizer.

other researchers have focused on approaches in which only one
asr system is necessary. a basic form of id172 can be accom-

4.2 con   dence scores

319

plished using the n-best list. in [290], a word score is generated by
summing the likelihood of all hypotheses containing the word and divid-
ing by the total likelihood of all hypotheses. a related approach, used
by [294], makes use of a posterior word id203 calculated on a
word lattice. here, the id203 for a word in the lattice, i.e., a
single lattice arc, is calculated by combining a sum of all the proba-
bilities of all possible histories of the arc and all possible futures of
the arc, normalized with respect to the total id203 mass in the
lattice. note that if the language model id203 and the acous-
tic probabilities are all set to one in the lattice, then this approach
reduces to counting the number of paths through the lattice that pass
through the word, normalized by the total number of paths through the
lattice [294].

one of the challenges of lattice-based scores involves making a
decision about which words will be considered to compete within the
lattice. a word with few competitors should have a higher con   dence
than a word with many competitors. in the work of [142], a simple
density-based approach is applied. the approach examines the speech
signal spanned by each word in the 1-best asr output. at each frame
within the word, the number of competing links within the lattice is
counted. this count is equivalent to the number of links that intersect
a vertical line drawn through the lattice at a time point corresponding
to a particular frame. scores are calculated using various combinations
of these statistics.

another basic challenge is determining which hypothesized words
should be treated as a realization of the same underlying spoken
word [290]. in essence, the use of lattices in con   dence score generation
requires overcoming the same issues confronting the use of lattices for
indexing, addressed in the previous subsection. in [294], word probabil-
ity scores are accumulated for words in the lattice that are determined
to correspond to the same word spoken in the signal.

the issue of con   dence scores is addressed both in the literature
on large-vocabulary continuous id103 (lvcsr) and in
the literature on std. recently [280, 281], a discriminative approach
to estimating the con   dence of words recognized by a std system has

320 exploiting automatic id103 output

emerged. the discriminative approach is motivated by the conjecture
that acoustic and language models perform poorly at modeling oov
terms and that generative approaches to con   dence are sub-optimal
in these cases. the strong performance delivered by the discriminative
approach on oov terms supports this conjecture.

4.2.2 information from other sources

information from sources external to the recognizer can be inte-
grated for the purposes of con   dence estimation. we provide a sur-
vey of the nature and the diversity of these information sources by
overviewing selected work of authors who have contributed in this
area. early research was carried out within the context of detecting
mis-recognitions in dialogue systems. in [311], semantic, pragmatic
and discourse-based information sources are combined with acoustic
information to build a con   dence measure for this purpose. in [68],
word-level acoustic scores were combined with context-independent and
context-dependent features. the context-independent features included
the number of phones per word, the frequency of word occurrence in
the language model and acoustic training data sets, and statistics on
the occurrence of the phones in the word in the acoustic training set.
the context-dependent features included the length-normalized likeli-
hood of the sentence according to the language model, the estimated
signal-to-noise ratio, and the speaking rate.

an extensive inventory of information sources for the purposes
of estimating con   dence was evaluated by [239]. it includes features
already mentioned, as well as scores re   ecting additional characteris-
tics, such as language model backo   f during decoding, the number of
active    nal word states in the asr system search space, the number
of pronunciation variants and the frame-wise average id178 of
the phone-level acoustic models. in [302], a computationally cheap
score derived from the language model
is proposed and combined
with acoustic measures. finally, in [179], information about language
model backo    is combined with acoustic information and applied not
only for the purpose of detecting well-recognized words, but also of
well-recognized segments.

4.2 con   dence scores

321

4.2.3 integrating con   dence measures into ir models

scr makes use of con   dence scores by integrating them into ir models,
as introduced in subsection 2.2. we now turn to discuss the approaches
that have been taken to achieve this integration. it should be noted that
since con   dence scores are often derived from information contained in
lattices and confusion networks, it is not possible to draw a sharp line
between techniques that integrate con   dence measures into ir models
and techniques that index lattices and confusion networks (i.e., tech-
niques already discussed in subsection 4.1), and some overlap in issues
is to be expected.

there are two basic categories of method that have been applied
to integrate con   dence scores into ir frameworks. in the    rst, an
attempt is made to change the counts of terms that provide the input
for the statistics (weights and probabilities) for the ir model. this
approach often amounts to a computation of    e   ective frequencies   
that then serve as a replacement of term frequency and document fre-
quency counts. in the second, a hard threshold is applied and only
words with con   dence scores above this threshold are considered to
occur in the spoken content. this approach then uses standard word
statistics, as applied in text ir, to calculate counts. in general, both
approaches require tuning using additional labeled data, in order to
optimize performance.

the use of expected occurrences for the purposes of scr has its
roots in early tasks involving    ltering speech messages. at the begin-
ning of the 1990s, an early system [230, 231] classi   ed speech messages
according to topic class. keyword likelihoods, weighted by learned sig-
moid functions, were used as input to the message classi   er. in [181], an
   expected number of occurrences    of keywords in messages is obtained
by summing the scores of keyword hypotheses that are generated by
a word spotter. thresholding is a common practice for keyword spot-
ting systems and also for word features derived from lattice scans as
in [122, 132]. the particular setting of the threshold is important for
maximizing scr performance, cf. e.g., [77, 131]. in [288, 289], a collec-
tion of spoken documents is transcribed using a recognizer that gener-
ates phoneme sequences and    query features    (words in the query) are

322 exploiting automatic id103 output

extracted from these transcripts at query time. individual occurrences
of query features are called slots. a hard threshold is imposed on slots,
by ranking all the detected slots in the collection by their probabilities
and discarding all by the top-n most probable. this procedure amounts
to a hard cuto    threshold. the use of keyword spotting and std to
extract features for scr will be touched on again in subsection 4.6.

the choice of whether to impose a hard threshold or whether to
integrate a con   dence score into the retrieval model depends on the
use scenario for the system and the domain of application. if the use
scenario requires a very high precision list, admitting items containing
words recognized with low con   dence might hurt system performance
and a tight, hard cut at a high threshold might be helpful. if recall
is important, such items might help to ensure that as many relevant
documents as possible are retrieved from the collection. additionally, if
the domain of application is structured such that the units of retrieval
are short sentences rather than longer spoken documents, redundant
use of words within one document will be limited. it is then not pos-
sible to rely on there being other words in the document that would
compensate for 1-best errors. in this case, it might be useful not to
impose a high threshold. in the discussion that follows, we overview
techniques that have been used to integrate con   dence scores into ir
models for scr, especially those based on acoustic information. the
picture that emerges is that exploitation of con   dence scores is di   cult
and that there is probably not one overall solution that is suitable for
every situation.

in [245], an oracle experiment is performed, which assumes access to
perfect con   dence information, that is, direct knowledge about whether
a word was correctly or incorrectly recognized. all words in the speech
transcripts not occurring in the reference transcripts are removed. this
leads to a small improvement in scr performance: the inverse aver-
age inverse rank (iair) of the topmost relevant documents in the lists
returned by a vsm retrieval system decreases from 1.38 to 1.33. this
experiment suggests that, in general, con   dence scores should be help-
ful. however, the authors are unable to achieve the same performance
when automatic estimates of con   dence are used. other contemporary
authors also report di   culties in e   ective exploitation of con   dence

4.2 con   dence scores

323

scores. in [236], the authors report failure of acoustic information,
re   ecting the value of recognition con   dence of words to improve
retrieval, but conclude that their approach has run afoul of a length
id172 issue.

a more formal analysis of the incorporation of con   dence measures
in term weights is described in [128]. this work examines the impact on
idf and tf functions for the vsm and bm25 that arises from replac-
ing binary 0/1 presence indicators with normalized con   dence mea-
sures. the analysis suggests that the impact of con   dence measures
on term weights will generally be very small, and often outweighed by
the e   ect of id103 errors. modi   cations of the standard
idf and tf functions were derived and evaluated for the bm25 weight-
ing. the results showed that term weighting using only the modi   ed
idf
function, i.e., a standard binary independence weight, produced
a small improvement in retrieval e   ectiveness; however, this improve-
ment disappeared when it was integrated into the full bm25 model.
experiments showed the tf
function to have a detrimental e   ect on
retrieval. these results are generally consistent with other attempts to
incorporate con   dence measures in term weighting for scr.

the literature also contains instances of successful attempts to
exploit con   dence scores. in [246], word lattices are used to calculate
a    lattice occupation density    (lod) score, similar to the lattice
density measure previously mentioned above. known-item-retrieval
experiments were carried out. a training set was used to derive a model
of word id203 from the lod score. the model generated probabil-
ities for each word in the 1-best hypothesis, which were then used in a
vsm with probabilistic weights. the word id203 model improved
retrieval performance on asr transcripts measured against retrieval
performance on reference text transcripts.

con   dence information derived from wcns has been exploited for
the purposes of retrieval. the authors of [176] report that for retrieval
using 1-best transcripts, methods incorporating information on the con-
   dence level outperform term frequencies. the improvement is slim, but
does appear to increase for higher levels of error rate. a question that
remains open is whether a better optimization of lattice pruning might
have led to better performance of the term frequencies approach.

324 exploiting automatic id103 output

in [208], an approach is proposed for estimating the frequencies
of vocabulary-independent terms for spoken content indexing. the
method uses a discriminative technique to estimate frequencies for
phone sequences and is evaluated by comparison to phone sequence
counts derived from reference transcripts. since this approach is a
vocabulary-independent approach, it also provides an appropriate lead-
in for the following subsection. we turn from approaches that focus on
the challenge of handling uncertainty to approaches that also address
the challenge of covering all possible words.

4.3 representing speech with subwords

the challenge of covering all possible words is generally referred to,
more prosaically, as    the oov problem.    recall that, in order to
recognize a word, an asr system must have that word included in its
vocabulary. for many scr use scenarios, it is not possible to assume
that all necessary words can be known in advance. for this reason,
scr systems su   er under the problem of oov words     words that
are encountered in the speech signal, but are not contained in the vocab-
ulary. in order to address the oov problem, words are not recognized
directly, but rather in smaller building blocks, called subwords. this
subsection presents an introduction to subwords that will provide the
necessary background to the discussion in the material that follows.

4.3.1 introduction to subwords

a subword is a unit that is smaller than an orthographic word. it may
or may not correspond to a linguistic unit (cf. subsection 3.1), such as a
phoneme, syllable or morpheme, but it may also be any small unit that
is used by an scr system. the main motivation for scr systems to
use subword indexing units is to address the oov problem. a subword
inventory represents the speech stream with a smaller, more    exible set
of building blocks, which gives greater coverage of the spoken content
than a prede   ned vocabulary. this subsection provides a background
on the principle of subword units.

subword indexing units also have the potential of addressing the
general problem of error. although oov is a major contributor to

4.3 representing speech with subwords

325

asr error, acoustic mismatches and sub-optimal pronunciation mod-
eling can lead to the recognizer mis-recognizing an in-vocabulary word,
substituting another word or word string in its place. the substituted
word string represents a    best    t    with the signal and for this reason
stands to share a large degree of similarity with the correct word.
subwords make it possible for the retrieval system to exploit partial
matches (also referred to as    inexact matches    or    fuzzy matches   )
within speech transcripts. these partial matches are particularly use-
ful in situations where unexpected pronunciations or channel conditions
cause a recognition error in the asr transcripts.

the creation of a subword inventory for the representation of spo-
ken content follows one of two basic strategies: either subwords are
based on the orthographic forms of words or on word phonemizations.
orthographic subwords are derived from the written forms of words
and consist of a sequence of graphemes. for example, under the ortho-
graphic subword approach    knowledge    would be represented as know
ledge. the advantage of this method is that a text corpus can be eas-
ily decomposed into subwords for the purposes of training a subword
language model for the speech recognizer. component orthographic
subwords are similar for words with similar spellings. in this example,
   knowledge    (know ledge) shares a common subword with    knowing   
(know ing) and with    acknowledging    (ac know ledg ing). in the
case of orthographic subwords, it is necessary to provide their pro-
nunciations to the asr system in the lexicon. a single subword can
have multiple pronunciations. for example, orthographic subword know
is pronounced di   erently in    knowledge    and    knowing.   

phonetic subwords are derived from word pronunciations. here,
subwords are represented as strings of phonemes. for example,
under the phonetic subword approach    knowledge    would be rep-
resented as n a l i dz .
in this case, component orthographic
subwords are similar for words with similar pronunciations. under
the phonetic subword approach,    knowledge    (n a l i dz ) does
not share a common subword with    knowing    (n o w i n )1 notice,

1 the underscore,        , in the phonetic representation is used for readability, but also can be
used to di   erentiate word-internal from word-initial and -   nal forms in the vocabulary.

326 exploiting automatic id103 output

however, it shares two common subwords with    acknowledging    (i k
n a l i dz i n ). under the phonemic subword approach there is
only a single pronunciation per subword. the number of di   erent
orthographic subwords that map to a phonetic subword with a single
pronunciation varies from language to language. another important
language-dependent e   ect is the number of shared subwords resulting
when semantically related words are decomposed.

the error compensation potential of subword units can be best illus-
trated by considering an example. we choose to examine the ortho-
graphic subword decomposition of the word    shenandoah.    the same
principles apply to other words and to phonetic subwords. two possible
subword decompositions of    shenandoah    are syllables (she nan do
ah) and overlapping grapheme strings (shen hena enan nand ando
ndoa doah). systems that make use of subwords are attempting to
leverage two e   ects.

first, if a word is not in the vocabulary of the asr system, it can be
reconstructed from a series of subword units. for example, the original
audio could be recognized using an asr system with a syllable vocab-
ulary. if this vocabulary contained the units she nan do ah, it would
be able to recognize the string shenandoah without explicit knowledge
of the existence of the word    shenandoah.   

second, if the asr system makes an error with a particular word,
subwords can help to provide a partial match. take again the example
of the asr system with a syllable vocabulary. if an error occurs, for
instance, because the word is spoken in the speech signal with a pronun-
ciation di   ering slightly to that included in the asr system   s lexicon,
the following string might result she nen do ah. in this case, three out
of the four syllables are recognized correctly. the possibility to make
use of a partial match during the ir process is left open. if the asr
system had a word-based vocabulary, it would output a word-level error
for the misrecognized word, for example, crescendo. this word-level
error is di   cult to match with the original spoken word,    shenandoah.   
it is also possible to take this partial match one step further. note
that the syllables in our mis-recognized strings she nen do ah (output
of the syllable-level recognizer) and cre scen do (syllabi   ed output of
the word-level) contain syllables that are very similar to the correct

4.4 subwords as indexing features

327

syllabi   cation of the word she nan do ah. speci   cally, nen is not far
from nan and scen bears a resemblance to she. some subword-based
systems attempt to use matches on multiple levels, in order to create
the most reliable inexact match possible between the target word and
its realization in the asr transcripts.

it is not necessary to have a recognizer with a subword language
model
in order to exploit subword matching e   ects. a word-level
transcript containing crescendo in place of shenandoah could be
decomposed. a syllabi   cation (syllable-based decomposition) of the
substitution error word would yield the syllable sequence cre scen do.
here, one out of four syllables matches a syllable in the original spoken
word. this match is rather distant, but could still prove useful to the
retrieval system. using subword units, however they are generated, thus
implements a partial match between words.

4.4 subwords as indexing features

the choice of indexing units is critical to the performance of an asr
system. strong statements of their importance were made early on:
in 1995, [241] asserted    . . . that indexing vocabulary is the main factor
determining the retrieval e   ectiveness, and the recognition performance
is a minor factor once it achieves a certain quality    (p. 11) and in
1997, [304] identi   ed oov as the    . . . single largest source of retrieval
error    (p. 31). in this subsection, we examine indexing features that are
extracted from asr transcripts and used to retrieve speech content. we
look    rst at word-level features and then at subword-level features. in
each case, we discuss in detail the potential for these features to address
the challenge of covering all possible words.

in automatic id103, we introduced the linguistic
units that are used for id103. in a conventional english-
language asr system, phone-based units (e.g., triphones) are used
for acoustic modeling and lexical words are used as base units of the
language model. however, subsection 3.3.2 introduced the notion that
other choices of linguistic units may be appropriate for asr systems
and highlighted the advantages of using id38 units that
are smaller than words.

328 exploiting automatic id103 output

the trade-o    between the advantages and disadvantages of using
subword units is quite similar, whether they are used as the base-units
of the language model in asr or as indexing features in an scr system.
subwords are short and modular, meaning that due to the structure
of language, a    nite, or highly limited set of subwords provides the
building blocks to generate all possible larger units. for these reasons,
subwords make possible a better    t with the spoken content in a speech
signal. the high coverage provided by subwords means that a greater
proportion of the information occurring in the original speech signal is
retained in the asr transcript. the preserved information creates an
opportunity to confront the oov problem, that is, the challenge of
covering all possible words.

features of all shapes and sizes, from phonemes through morphemes
to word phrases, have been investigated at some point in the literature,
and the most useful cases will be reviewed in the material that follows.
however, it is important to keep the disadvantages of subwords    rmly
in mind. a major disadvantage of subword units is that recognition
rates fall as units get shorter, since context is modeled less robustly.
another disadvantage is the ambiguity that they introduce. ambiguity
is a classic problem that must be confronted in text-based ir. exam-
ples are simple english-language examples including double-meaning
words like       y    (insect vs. airborne movement),    mouse    (rodent vs.
computer pointing device), and    cambridge    (uk vs. massachusetts).
if these words are to be considered in terms of subwords, for exam-
ple, cam+bridge, additional confusion is introduced, namely, confusion
between other forms containing the same components, for example,
   drawbridge,    which decomposes as draw+bridge. the success of sub-
word indexing features for improving scr depends on the ability of
the system to exploit the greater coverage while compensating for the
additional ambiguity.

subwords that are appropriate to serve as a set of indexing fea-
tures for scr must satisfy several requirements. we formulate a list of
requirements that is loosely based on the criteria laid out in [87]:

    representative. the units should be able to adequately
represent the speech stream. the inventory should provide
adequate coverage.

4.4 subwords as indexing features

329

    discriminative. the units should allow for semantic discrim-
ination of di   erent parts of the speech stream (or between
di   erent speech documents, if documents are de   ned within
the collection).
    accurate. the speech recognizer must be able to reliably
extract the indexing units from the speech stream.
    e   cient. the representation should require a reasonable
amount of computation at asr-time and be very fast at
search time.

in fact, this requirements list is not particularly speci   c to scr
or to subword units. indexing units for text-based ir should, quite
obviously, also be representative and discriminative. when words are
used as indexing features for scr, they must clearly also be accu-
rate and e   cient. we include the list here because it is particularly
important to consider each of these points when designing a subword
indexing feature inventory for scr. the lexical word is well entrenched
as the default indexing unit     it is not only convenient, but it has
also established its usefulness in practice. the lexical word is a    safe   
choice since it is always quite reasonable to assume that the seman-
tics of a speech stream is largely conveyed by the speaker   s choice of
lexical words. also, lexical words are distributed in length and fre-
quency in such a way that many of them can be modeled relatively
readily and thus can be more easily recognized by an asr system.
moving away from using words as the basic indexing unit, especially
in languages like english that are not syllable based, means adopting a
model of speech whose architecture is less constrained and less clearly
intuitive.

during the design of an scr system, it is important to keep in mind
the basic reliability of word-level features and not to integrate subword
information to an extent greater than that which is necessary to con-
front the oov problem. subword units should be used in a way that
exploits their ability to represent the speech signal, without sacri   cing
discrimination, accuracy or e   ciency.

there are di   erent methods for using asr to obtain subword index-
ing units from the speech signal. the most straightforward manner is

330 exploiting automatic id103 output

to make use of the units that are also used by the asr system:

    acoustic units. using the indexing unit as the acoustic mod-
eling unit in the asr system
    lexicon units. using the basic vocabulary item in the lexicon
(dictionary) of the asr system as the unit

phones are examples of acoustic units (in the case of a recognizer that
uses the phone as its basic acoustic unit and also a phone-based lan-
guage model) and words are examples of lexicon units (in the case of
lvcsr). additionally, as mentioned previously it is possible to derive
units from post-processing (i.e., re-tokenizing) the output of an asr
system. here, there are two main approaches:

units derived from post-processing asr output

    units via agglomeration. deriving the unit by merging units
in the asr transcripts during a post-processing step (e.g.,
extracting phoneme sequences from phoneme-level tran-
scripts)
    units via decomposition. deriving the units by decomposing
units in the asr transcripts during a post-processing step
(e.g., syllabifying word-level transcripts to derive syllable-
level indexing features)

in the remainder of this section, we discuss the di   erent levels of sub-
word indexing units that have been used for scr.

4.4.1 phone-sequence indexing features

at    rst blush, it seems rather surprising that short phone sequences
(phone id165s) are capable of capturing su   cient meaning in order
to support scr. generally, semantics in human language is consid-
ered to be situated at the word level and higher. however, recall from
automatic id103 that the morpheme is the basic unit of
meaning in speech, and that this unit is (often) smaller than a word.
single phones cannot be expected to encode much, if any, semantic
information, but phone sequences of length two (phone bigrams) and

4.4 subwords as indexing features

331

especially of length three (phone trigrams) and higher grow close to
morphemes in length and for this reason it is also plausible that such
units can be used to capture semantic information.

indexing features consisting of sequences of phones were initially
proposed by [241] and tested on a small corpus of german-language
radio news. the phone transcripts are generated by a phone-based asr
system that uses a phone-bigram language model. the phone sequences
extracted from the phone-based asr transcripts to be used as indexing
features are maximally overlapping phone sequences, 3   6 phones in
length. the sequences are chosen by a method that eliminates both very
frequent and very infrequent sequences. to perform retrieval, the query
is    rst mapped to phone sequences and the vsm is used to compare
the query and the documents in the collection. the system described
in [254] was an early prototype that made use of this approach, adopting
triphone indexing features.

in [304], phone sequences are created by decomposing word-level
transcripts generated by a word-based asr system into phone-based
transcripts with the help of a phonetic dictionary. all phone sequences
of 3   6 phones in length are used as indexing features. queries are also
converted into phone strings via the dictionary. it is important to point
out that the dictionary used for this conversion process is larger than
the lexicon of the asr system. if it were not, the phone-based method
would not help to compensate for oov. again, the vsm is used for
retrieval. the method was tested on a collection of english-language
news stories. the use of phone-strings in [304] is intended to emulate the
e   ect of wordspotting in phoneme lattices, which is computationally a
more expensive technique. in [174], the method was shown to perform
well for english-language broadcast news retrieval, which used phone
5-grams that overlapped by four phones.

in [195], di   erent methods for extracting overlapping phone-
sequence indexing features for scr are explored in detail. this arti-
cle arrives at the general conclusion that phone-based retrieval is not
as e   ective as word-based retrieval, but there are certain situations
where it is appropriate. speci   cally, phone-based retrieval is e   ective
for addressing the oov problem. further, if id103 must
be performed on a platform with limited capacity (i.e., a hand-held

332 exploiting automatic id103 output

device), then a small language model, such as a phoneme bigram model,
makes the asr system lightweight and compact. the authors of [195]
   nd that in terms of phone-sequence-based indexing features, a combi-
nation of phone 3-grams and 4-grams proved most e   ective. this result
con   rms the    ndings of [304] that phone-based features derived from
word-level transcripts are able to help compensate for word-level error.
further, [195] shows that ignoring word boundaries when extracting
phone-based features does not a   ect retrieval performance signi   cantly.
similar results are achieved by [197], which investigates a wide
variety of di   erent subword indexing terms derived from speech tran-
scripts produced by a recognizer with phoneme-level acoustic models
and a phoneme-bigram language model. retrieval experiments were
performed using the vsm and 50 topical queries. overlapping phone
trigrams yielded the best retrieval performance. the authors conclude
that the overlap of the strings is important because it provides more
opportunities for a partial match to be made between the query and
the asr transcript.

experiments reported in [197] start with phoneme monograms and
gradually increase the length of the phoneme sequences. these reveal
that retrieval performance    rst increases and then falls o   . this behav-
ior clearly demonstrates the importance of using indexing features that
are speci   c enough to be representative, but not overly speci   c to the
point where they fail to generalize.

we close the discussion on phone sequence indexing units by men-
tioning work that makes use of sequences of units that are approx-
imately phones. in [87], specialized indexing features are used that
are de   ned as    the maximum sequence of consonants enclosed by two
maximum sequences of vowels at both ends.    note that these sequences
do not overlap, but rather the speech signal is cut in the middle of
a vowel. a vowel is easily identi   able within the speech signal and
also more stable than a phoneme transition, making this choice of
segmentation boundary a natural and robust one. the indexing fea-
tures are extracted from the speech signal using a keyword spotting
system. in subsequent work, [241], the authors point out that using
phone-sequence features instead of specialized phone-like sequences is
bene   cial because it greatly increases the ease of feature extraction.

4.4 subwords as indexing features

333

a similar observation has been established in the area of spoken con-
tent classi   cation. in [286], two topic classi   cation systems are tested
that use phone-sized units as their indexing features. the    rst uses
codebook class sequences (ccs), which are sequences of phone-sized
(80 ms) units that have been created by an automatic vector quan-
tization process. the second uses phones generated by a phone-based
recognizer. both feature sets are demonstrated to be suitable for topic
classi   cation and in both cases sequences of three units were the top
performers.

in [192], experiments are presented that demonstrate the relative
bene   t of extracting phone-string features from lattices rather than
from 1-best phone-level recognizer output. this approach bears a   nity
to the techniques for detecting spoken terms that will be discussed in
subsection 4.6.

4.4.2 syllable and morpheme indexing features

the classic application of syllable-level indexing features for scr is in
syllable-based languages. chinese is a typical syllable-based language.
an inventory of 1,345    tonal syllables    covers all the pronunciations
for mandarin chinese [282]. chinese is written as a string of charac-
ters and, in contrast to western languages, does not use whitespace
to delimit words. each character is monosyllabic and can correspond
to di   erent syllables depending on its word-level context. if a chinese
asr system uses a syllable-level language model it can easily achieve
complete coverage     there is no oov problem for chinese syllables.
in [15], a collection of 1,000 voice messages are indexed using a syllable
spotting approach. each message is represented by a vector of syllable-
bigram features     each component consists of a weight with an acoustic
score component and an inverse document frequency (idf) component.
retrieval is carried out with the vsm.

in [282], an asr system with syllable language models and acous-
tic models based on half-syllables is used to generate syllable-level
asr transcripts for a collection of mandarin chinese broadcast news.
two types of syllable-based features are extracted from the transcripts
and are used to index the collection. the    rst is single syllables and

334 exploiting automatic id103 output

the second is overlapping syllable bigrams. the authors point out the
   homophone problem    with chinese syllables. the same phonetic syl-
lable can correspond to multiple chinese characters and the characters
can play a part in multiple chinese words. syllable bigrams help to
compensate for this problem by retaining some of the context of the
syllable. they consider the overlapping syllable bigrams to be a special
case of overlapping phone-sequence features used by [241].

in [41], the discriminative ability of the syllable is explored in detail.
subword-based approaches are shown to be better than word-based
approaches for mandarin scr. a very practical reason for preferring
to use syllables as indexing features is that it is possible to circumvent
the need to tokenize chinese text into words in order to train a word-
level language model for the asr system. however, there are other
reasons why subword features perform so well. as pointed out by [41],
syllables or characters in mandarin are capable of covering cases that
are di   cult for word-level features. the    exibility of subword features
aids matching in the case of expressions for the same concept containing
slightly di   erent characters. such cases include id68s of for-
eign words, which may choose related but not identical representations
for the same string of sounds, and abbreviations, which make use of a
subset of characters in the full character sequence used to represent the
concept. in the case of perfect transcripts, character monograms per-
form better than syllable monograms, since they are semantically more
speci   c. however, when error is involved, syllables come into their own.
in all cases, subword monograms are to be preferred to word mono-
grams. the best syllable-level indexing features were sequences of 1   3
units in length. the authors of [41] point out that this set of indexing
features includes much word-level information, since words are often
two syllables in length.

the approach presented in [215] brings together techniques for
exploiting multiple hypotheses and subword techniques by proposing
subword-based approaches building on word-based approaches that
use confusion networks and pspl lattices. experiments on mandarin
chinese show the ability of these approaches to improve over word-
based baselines and also examine trade-o   s between index size and
performance.

4.4 subwords as indexing features

335

the use of syllable-based indexing features for chinese scr has
been an important source of inspiration for a wide range of feature
extraction techniques that eschew the lexical word as the basic feature
and opt for smaller subword features. as research into subword-based
scr matured, it became clear that it is not necessary for a language
to be syllable based in order to carry a su   cient amount of semantic
information on the subword level for the support of scr systems.

an interesting case is german, which is not considered to be a
syllable-based language. the largest possible german syllable is eight
phones in length, theoretically admitting the possible existence of
millions of syllables. although the vast majority of possible phone com-
binations are not used as syllables, the net result is that the syllable
inventory for german is for all practical purposes in   nite. however, a
relatively small syllable inventory still manages to give good coverage.
in [155], the same (syllable-level) asr performance is achieved using a
5,000 syllable trigram language model or a 91k word bigram language
model. the syllable recognizer uses phones as the acoustic unit and an
inventory of phonetic syllables. retrieval experiments are performed
on syllable transcripts generated by the syllable recognizer and also
created by decomposing the word output of the 91k lvcsr system
into syllables. in the experiments, syllable bigrams outperform words
as indexing features, demonstrating the ability of syllables to provide
a    exible    t between the speech signal and the query. slightly bet-
ter performance is achieved by using syllable features derived from the
word-level transcripts rather than syllables derived from the transcripts
generated by the syllable recognizer.

the usefulness of syllable-based features for german-language scr
has a very intuitive explanation. as previously mentioned, german is a
compounding language and new compounds can be created, for exam-
ple the word silbenspracherkennung2 is composed of silben (compound-
ing form of silbe,    syllable   ) + sprach (compounding form of sprache,
   speech   ) + erkennung (   recognition   ) and would not be contained
in a standard dictionary of the german language, but is very natu-
ral to use in the context of discussing syllable-based methods for asr

2 nouns are capitalized consistent with german spelling convention.

336 exploiting automatic id103 output

and scr. the vocabulary of a syllable-based asr system contains
the components necessary to compose this word    on the    y.    even a
lvcsr recognition could come reasonably close by recognizing silbe
+ spracherkennung or silbe + sprache + erkennung. when decom-
posed, these sequences would share many syllables in common with
queries containing the words silben and spracherkennung.

this explanation for the usefulness of syllables, does not readily
extend to english, which is not a compounding language. however,
as noted by [197], syllables capture constraints on the combinations
of phones that can occur within words (i.e., so-called phonotactic
constraints). the syllable level, located between the phone-level and
the word-level, provides a balance between capturing incidental pat-
terns (a danger of phone sequence indexing features) and capturing
only overly speci   c features (a danger of word-level indexing features).
in [197], syllable indexing features were outperformed by overlapping
phone sequence features. however, since longer syllable strings were not
taken into consideration, this approach cannot be considered to have
exhausted the potential of syllable-level indexing features for english-
language scr.

researchers have also investigated the usefulness of indexing units
that are syllable-like, but not strictly speaking syllables. such units
hold an intuitive appeal, since they strive to combine the strength of
phone-sequence indexing features with features that are motivated by
frequent occurrence patterns within language. in [197], phone multi-
grams, de   ned as non-overlapping, variable-length phone sequences, are
compared to other indexing features. like syllables, they are outper-
formed by overlapping phone sequence features. their intuitive appeal
does not translate into system performance gains. a similar result is
achieved by [174], in which experiments with an inventory of    particles   
are conducted.    particles    are within-word sequences of phones that are
automatically derived from a phonetic decomposition of the language
model training corpus. the selection of particles involves a greedy pro-
cess that seeks to maximize the id203 of the resulting particle-
language model generating withheld text data. the use of particles as
indexing features achieves good performance, but is outperformed by
the retrieval system that used phone 5-grams as indexing features.

4.4 subwords as indexing features

337

recent work on syllable-like units is reported in [308]. the authors
note that an asr system might mis-recognize a linguistic unit spo-
ken in the speech signal, but still generate a characteristic pattern in
the n-best list. by matching fragments of n-best hypotheses, identi-
cal speech segments can be found, without requiring a correct    rst
best asr system output. vectors of alternative syllable hypotheses are
used to represent each recognized unit, which are then represented in
a transformed space where they are quantized to produce a codeword.
the approach is evaluated on mandarin chinese speech annotations of
a photo collection.

finally, morphemes have also been used as indexing units. recall
that morphemes are the smallest meaning-bearing unit of language.
they can be syllable sized or even word sized. their bene   t is the fact
that they correspond to the modular units that are used to morpho-
logically derive the language. these units are used for morphologically
complex (highly in   ective) and, in particular, agglutinating languages
such as turkish and finnish. an asr vocabulary of a size that would
give good coverage in a language such as english, yields high oov
rates for such languages, since it cannot cover all the morphological
variants of the individual base words. in [271], a set of morpheme-like
units, derived via a statistical process, is used for scr of finnish-
language read news stories. the morpheme-level subword units are
produced directly by the asr system and serve to limit the size of
its lexicon and language model. the morpheme inventory can be cre-
ated by using linguistic decomposition rules, or it can be created with
a statistical approach, as in [271]. in [8], morpheme-based units are
used for both asr and std on a corpus of turkish broadcast news
retrieval.

in [271], the morpheme-based output of a finnish language speech
recognizer is transformed into wcn form using the algorithm. the
vsm with tf    idf weights is used for retrieval. the tf
factor is sub-
stituted with a lattice speci   c score, which is calculated in one of two
di   erent methods. the    rst method sums the posterior id203 of
the morpheme over every occurrence in the confusion network. the
second method ranks the morphemes at each time point by their pos-
terior id203 and calculates the tf factor by summing the inverse

338 exploiting automatic id103 output

of the rank position over every occurrence of the morpheme within
the confusion network. both methods are reported by the authors to
improve retrieval over the use of 1-best transcripts. a priori, the second
method is better motivated than the    rst. the posterior id203 of
a unit can be compared with other units recognized at the same time
point. for these reasons, a ranking of alternatives at a certain time
point in a lattice can be anticipated to be relatively reliable. it is more
di   cult to compare units that the recognizer has hypothesized at dif-
ferent points in the signal. such comparison requires appropriate nor-
malization of the posterior id203, an aspect of the approach not
detailed in [271]. the experimental results reveal that tighter pruning
of the confusion network leads to better performance. pruning, in this
case, has the e   ect of introducing a threshold on the recognized terms
that are admitted for indexing. the tf   idf model used here is a simple
linear tf function. without comparison with a more sophisticated term
weighting model, and also comparison with a simple thresholding of
term hypotheses based on con   dence scores, it is di   cult to conclude
the general contribution of posterior-id203-based weighting to the
improvement of confusion network-based scr.

4.4.3 moving beyond exact matches

thus far, we have seen that subword indexing units have the abil-
ity to compensate for asr error. the modularity of subword units
allows units from a relatively small inventory to be assembled on the
   y during the id103 process to    t to the speech signal. in
this way, subwords have the potential to cover oov words and also
to compensate for unexpected acoustics, such as pronunciation vari-
ants. techniques discussed up until this point extract features from
asr transcripts by using exact matches. in other words, if the fea-
ture does not match the transcript perfectly at a given point, it cannot
be extracted from the transcript at this point. some approaches have
introduced additional    exibility into subword indexing techniques by
loosening this restriction.

there are two basic ways in which an approximate match between
the indexing term and the asr output can be used for indexing

4.4 subwords as indexing features

339

term extraction. the    rst method is to go beyond the 1-best list of
the recognizer and use a lattice that encodes alternative recognition
hypotheses. indexing methods that pursue this approach will be dis-
cussed in subsection 4.6, together with other lattice-based approaches
for detecting spoken terms. the second approach makes use of the 1-
best transcripts of the recognizer. instead of explicit knowledge about
possible recognizer mistakes encoded in a lattice, these methods impose
a simple model of id103 error and use that model to cal-
culate fuzzy matches between indexing terms and id103
transcripts. if a feature is very similar to the 1-best asr transcript at
a given point, then it is extracted at this point. in general, similarity
is determined by the minimum id153 (med) between the tran-
script and the feature at the point in question. the med is the mini-
mum number of insertions, deletions or substitutions that are required
to transform one string into another, and provides a useful re   ection of
similarity. it is calculated using id145 techniques. in
the case of feature extraction for speech indexing, the strings that are
compared are usually phone-level strings, that is, both the transcript
and the indexing feature to be extracted are represented in terms of
their constituent phones.

such    fuzzy matching    techniques for feature extraction should be
deployed with great care in an scr system. their advantage is the
ability to cover more of the speech signal by introducing an improved
robustness to asr error. their disadvantage is that they admit too
many matches with the speech signal, that is, they often identify
similarity where no underlying similarity exists. a di   erence of only
one phone can correspond to a large semantic di   erence, cf.    kill   
vs.       ll.    even if the range of possible di   erences is constrained to
phones that are acoustically similar and thus prone to confusion by
the recognizer, similar phone strings correspond to wide semantic
di   erences:    die    vs.    tie.    whether or not inexact matches should
be used will depend on characteristics of the domain of application
and the use scenario. for example, if di   erent information needs of
users correspond to spoken content with clearly di   erent vocabulary
use, then inexact matches may not be as damaging. further, if the
spoken content to be retrieved takes the form of longer documents,

340 exploiting automatic id103 output

the large number of indexing features extracted from these documents
may compensate for a few unreliable cases.

a technique for fuzzy matching on phoneme transcripts was
introduced by [288, 289], as mentioned above in the context of con-
   dence scores. here, a slot detection approach was applied to identify
putative query word occurrences in phone transcripts. slot detection
compensates for asr errors by admitting exact matches.

in [201], similar phoneme sequences were mined from phoneme tran-
scriptions and used to categorize a set of unseen speech documents. the
inexact matching technique worked well for this rather limited domain.
this result contrasts with scr work such as [195], where expanding
the query with typical errors and close phone strings was shown to
degrade the performance of phone-sequence-based retrieval.

further experiments in [195] pursued a more constrained approach.
phoneme classes were built by grouping the phonemes in the recog-
nizer   s acoustic model inventory into 20 broad classes according to
acoustic similarity. these classes capture phonemes that are poten-
tially easily confusable by the recognizer, introducing an exact match
element designed to compensate for recognizer error. this approach
was shown to be robust and its performance was closely followed by
that of 4-grams built of phoneme classes.

we close our discussion of indexing units that make use of inexact
matches by mentioning that many of the same e   ects can be built into
the scr system at query time. in [155], syllable-level fuzzy matching
techniques are shown to improve over exact match techniques for
german-language spoken document retrieval. this approach di   ers
from other inexact match techniques in that the match is calculated
at two levels:    rst, the phone-strings of syllables are compared to
calculate a syllable score and then syllable strings are compared
to determine the presence of an indexing feature. the approach is
designed to compensate for id103 errors that involve the
compression of longer words into shorter ones     an e   ect that arises
due to the fact that the standard id48-model asr framework does
not e   ectively model the length of words. note that this approach
e   ectively exploits a simple model of asr error expected in the asr
transcripts.

4.5 hybrid approaches for scr 341

another query-time matching technique involves id183.
in [173, 174], a method is proposed that expands the query with a list of
confusable phrases. in e   ect, this approach attempts to make a query-
time    guess    at the way in which the asr system has mis-recognized
terms in the query.

in [257], a probabilistic form of term weighting is used that makes
use of information on phone confusion. the method works well, but the
authors note that for the particular application (a distributed learning
application) the small size and constrained topic could be what makes
the phoned-based approach e   ective. in the next subsection, we turn
to techniques that propose combinations of approaches for improving
scr performance.

4.5 hybrid approaches for scr

as discussed at the end of automatic id103, the higher
goal is to strike a balance between asr and indexing. the ideal system
should retain the advantages (i.e., modularity,    exibility) associated
with isolating asr and search components, while striving to retain
information important for indexing. the tightest integration between
the asr system and indexing is achieved when the identities of the
indexing features are known ahead of time and can be used as units
within the asr system. in most systems, they would be used as units
in the language model, but it is also possible to use them as acoustic
units, if they are known in advance. it would make sense, for example,
to consider using syllables as acoustic units in a syllable-based system.
in practice, it is not possible to have complete knowledge of necessary
indexing features at asr time, in particular due to the oov problem.
a very loose integration of asr and indexing involves treating the
recognition output as a given and applying a post-processing step in
order to extract useful units from this output. a middle road is to use
the asr system to generate an intermediate form of output that is
optimized to be used later for the extraction of features on the    y at
search time.

since the beginning of research and development in scr, the lexical
word has been widely used as an indexing feature and has repeatedly

342 exploiting automatic id103 output

proven its value. the word represents a natural choice, not only because
it is the canonical meaning-bearing unit in language, but also because
it is convenient and text ir also o   ers many models that have been
proven to work well using word indexing features.

the early speech retrieval systems, in particular [131, 181, 231],
used word-level features derived by a word spotter in order to perform
classi   cation and retrieval of spoken documents. a word spotter takes
as input a list of words and the speech signal and outputs points in
time at which words appear. the obvious limitation of this approach is
that the indexing vocabulary must be known before the spoken content
is processed by the asr system. we return to further discussion of
word spotting, as well as vocabulary-independent methods of extracting
word-level indexing features from asr output, in subsection 4.6.

with advances in large vocabulary continuous id103
(lvcsr), word-level transcripts became a valuable source of word-
level indexing features for scr [121, 132, 303]. the advantages of
using word-level transcripts are twofold. first, an lvcsr system uses
a word-level language model spanning a context much larger than that
covered by phoneme-based recognizers. second, word-level transcripts
can be indexed in a relatively straightforward manner by simply apply-
ing conventional ir techniques. the disadvantage of using lvcsr tran-
scripts is the oov problem. a word-level transcript can never contain
a word form that is not explicitly included in the vocabulary of the
asr system.

we have seen that subword units can be used as an indexing unit in
an scr system in order to address the challenge of covering all possible
words. however, they must be deployed with care. the downside of their
robust and    exible matching capability is their ability to introduce into
the system spurious matches between spoken content and query terms,
leading to a drop o    in scr precision. a natural means of integrating
subword indexing units into an scr system is to take a hybrid approach
that makes use of indexing features of di   erent levels. in such a way, the
system is able to exploit the    exibility of subwords (such as syllables),
but also the reliability of larger units (such as words or phrases). there
are three di   erent ways in which indexing units of various levels can
be integrated into an scr system: low-level integration, high-level

4.5 hybrid approaches for scr 343

integration and query-selective integration. in the following discussion,
we treat each in turn.

integration. low-level

integration approaches involve
low-level
extracting indexing features of multiple types and lengths and combin-
ing them into a single index. intuitively, the issue that arises is potential
duplication of the same information within the index. however, above
we saw the advantage of information duplication in the case of phone-
sequence features: overlap of features helped to improve performance.
because an scr system must deal with errors in the transcripts, it is
not possible to know a priori if duplication of features is introducing
redundancy or helpful, new information.

since the early days of scr research, low-level integration has been
used to combine di   erent levels of indexing features. in [120], a com-
bination of features derived from two speaker-dependent asr systems
was used for the task of retrieving read english-language news reports.
one system generated word-level transcripts and the other phone
lattices. query words not present in the word-level transcripts were
scanned for in the phone lattices at search time, using the wordspot-
ting in phoneme lattices technique. the combination led to a sizable
improvement in retrieval performance.

in [132], speaker-independent versions of the same asr systems
were used for the task of retrieving english-language voice messages.
the combination yielded a small
improvement over either feature
source used in isolation. the experimental results were slightly bet-
ter for the system that made use of both word-transcript and phone-
lattice derived indexing terms, independent of whether the query word
was present in the vocabulary of the asr system that produced the
word-level transcripts. this result suggests that it is advisable to risk
information duplication when selecting indexing features, rather than
risk leaving out information.

since this initial work, other approaches have also made use of
low-level feature integration. in [195], an improvement is achieved
on an english-language broadcast news scr task by combining
phone sequence 3-grams and 4-grams. similarly,
in [197], a small
improvement is reported for english-language broadcast news retrieval

344 exploiting automatic id103 output

when phone-sequence features of various lengths were combined. the
combination improves only marginally over the best scoring individual
phone-sequence feature (phone trigrams). the authors comment that
the small improvement may be due to the fact that the same relevant
documents are scoring the same for the di   erent representations. com-
parable results have been obtained in the area of spoken document
classi   cation. in [60], german-language spoken document classi   cation
is performed using a support vector machine and a combination of
di   erent sorts of features, and a small improvement is achieved with
the combination of word and character trigram features.

integration at a high level involves query-
high-level integration.
ing multiple indexes, one for each sort of indexing feature, and combin-
ing the di   erent results lists returned. a common method uses a linear
combination of the retrieval scores of results in each list. high-level
integration became a    rm tradition in the early days of scr. in [132],
document retrieval scores returned from searches using word-level asr
transcripts and from searches using phone lattice spotting wordspotting
in phoneme lattices were linearly combined and the combined scores
used to generate the    nal ranked list of spoken documents. the high-
level integration approach yielded a small improvement over either indi-
vidual approach.

in [304], a mixture of word-level indexing and phone-sequence index-
ing is used for english-language broadcast news retrieval. the use of
phone-sequence features alone did not yield the same performance as
word-level features. however, a linear combination of word-based and
phone-sequence based retrieval outperformed either individual system.
subsequent work on english-language broadcast news scr has also
taken a high-level integration approach for combining features. in [197],
a linear combination of results generated using di   erent length phone
sequence indexing features yields an improvement. the weights with
which the di   erent retrieval scores entered the linear combination need
to be correctly set to favor better performing units. if they are not,
no improvement is achieved. in [172, 174], a substantial improvement
is achieved by a system that uses a linear combination of word-level

4.5 hybrid approaches for scr 345

retrieval scores and retrieval scores from an index using phone 5-grams.
again,
it is important to optimize the weights used in the linear
combination.

in [41], mandarin chinese scr is shown to bene   t from the
addition of character or word information to syllable level results.
retrieval results from word and subword scales are merged via linear
combination to improve cross-language english-mandarin retrieval
in [170]. linear combination of retrieval scores has also been shown to
yield improvement in [167], where an optimal weighted combination
of scores generated by subword units of various sizes yields clear
performance improvement.

in [209], a linear combination of scores from a ranked utterance
retrieval system and a system that uses word-level asr transcripts is
proposed in order to provide a robust approach to the oov problem.
a training set is used to learn e   ective mappings between the scores
and probabilities of relevance to the user query.

linear score combination has been shown to yield good performance
when the results retrieved by the contributing retrieval methods (in this
case, retrieval with respect to di   erent sets of indexing features) are of
high quality [276]. although linear combination of retrieval scores is
widely used in scr, we would like to point out that there are many
other possible methods for score combination that have been devel-
oped in the text-based ir literature (cf. e.g., [19]). due to the large
amount of asr transcript noise faced by most scr systems, other
methods of high-level integration most likely have good potential to
further improve retrieval performance.

query-selective integration. this method analyzes the query and
chooses the index to be used for retrieval on the basis of this analysis.
most notably, this method is used in order to deal with oov query
words. an oov query word is a word occurring in the query, but which
cannot possibly be found in the word-level asr transcripts because it is
not contained in the vocabulary of the asr system. in practical appli-
cations, the vocabulary of the asr system is known to the retrieval
system, or can be surmised by simply compiling a list of the word forms
that occur at least once in the available id103 transcripts.

346 exploiting automatic id103 output

in [172, 174], it is noted that using phoneme sequences rather than
words as indexing features increases recall, but also increases the num-
ber of false alarms. a better balance is achieved by a system that queries
the word-level index if the query word is in the recognizer vocabulary
and the phoneme-sequence index otherwise. the resulting performance
is on par with high-level integration involving a linear combination of
scores, but exhibits two advantages. first, the number of false alarms
remains low and second, it is not necessary to optimize the weights in
the linear combination. although this experiment suggests that query-
selective integration is a promising approach, it does not yet completely
solve the problem. the queries used in [172, 174] are overwhelm-
ingly only one word in length, meaning that the system avoids di   -
cult decisions about which index to use. this experiment bears strong
resemblance to an std task. although the vsm is used as retrieval, a
document is judged to be relevant if it contains the query term.

further, id183 and document expansion are techniques
that can be used to improve the match between query and spoken
content or the suitability of weighting scores. assuming that the use
scenario of the system is compatible with expansion, any attempts to
exploit con   dence scores or word lattices should aim at going above
and beyond the improvement that can be achieved using expansion
techniques. these techniques are discussed further in spoken content
retrieval beyond asr transcripts.

4.6 techniques for detecting spoken terms

the challenge of covering all possible words is a tough one. in the mid-
1990s, a basic strategy emerged in the work of [120, 132] that remains
today a framework for an e   ective way forward in addressing the oov
problem. this strategy involved splitting the process of generating
indexing terms from spoken content into two steps, the    rst occurring
at indexing-time and the second at query-time. for scr, indexing
involves the use of an asr system to generate id103
transcripts, computationally a very intensive process. if the asr sys-
tem can be used to generate an intermediate representation, then that
representation can be used at query-time to identify the positions of

4.6 techniques for detecting spoken terms

347

helpful indexing features contained in the query. the two-step approach
faces several challenges:    rst, determining the appropriate intermediate
representation to be generated by the asr system; second, ensuring
that the intermediate representation can be stored in a way that
makes it e   cient to search at query time; and, third, ensuring that
the process of identifying indexing terms at query-time does not give
rise to spurious terms that negatively impact scr performance. the
two-step paradigm experienced a resurgence of popularity in 2006 with
the introduction of the concept of std by the nist std task [75].
recall from overview of spoken content indexing and retrieval that
there is a close relationship between std and the core scr task,
relevance ranking. the core scr task involves using asr transcripts
to    nd speech media that is topically related to the information need
underlying the initial query. in use scenarios in which the information
need of the user is to    nd mentions of particular words or phrases
within the speech media, the search problem reduces to std and the
ir model used is the most basic model conceivable, namely a boolean
search. in this section, however, our main interest in keyword spotting
and std is their usefulness in extracting indexing features that can
be used to tackle the core scr task. note that many of the techniques
that have been discussed above for scr (e.g., use of lattices and
subword units) also play an integral role in keyword spotting and std.

4.6.1 wordspotting using fixed vocabulary

in the original work in the area, wordspotting is de   ned as    nding
occurrences of a speci   c list of words or phrases within a speech docu-
ment or a speech stream and we adopt this de   nition here. in contrast
to later versions of this task (e.g., keyword spotting and std), in the
wordspotting task it is assumed that the identity of the words to be
located is known to the system already at asr-time and used by the
system as it processes the speech signal. the dawn of the wordspotting
era was arguably 1990, with the publication of [232], which described an
id48-based wordspotting system. these approaches are aptly referred
to as    voice grep    by [241]. such systems were proposed also in order
to handle voice input [301]. as previously mentioned, the application of

348 exploiting automatic id103 output

wordspotting systems to tasks related to searching speech focused on
message classi   cation, e.g., [230]. as commented by [264], they remain
appropriate for monitoring tasks, such as broadcast news monitoring.

4.6.2 keyword spotting in phoneme lattices

wordspotting is limited in its practical application since the words
to be extracted from the speech signal must be known to the system
at asr-time. the keyword spotting paradigm removes that restric-
tion, thereby directly addressing the oov problem. the    rst approach
to vocabulary-independent query-time word location was proposed
by [122]. the approach involves generating a phone-lattice at asr-
time and searching this lattice for instances of query terms at query
time. this approach was applied to scr by [27, 28, 120, 121].

when a linear scan of phone lattices is used, wordspotting is prac-
tical for small collections, but does not scale e   ectively when the quan-
tities of spoken content to be indexed grow larger. the task of deter-
mining matches between this asr output and the query words is then
taken over by other components of the scr system. in the wake of
the initial work on keyword spotting in phoneme lattices, two lines
of investigation were developed that focused on exploiting this basic
principle.

in [72], an approach is proposed that makes use of synchronized
lattices. all alternative hypotheses are synchronized with the    rst best
phoneme hypothesis together with their posterior probabilities. the
synchronized lattice is relatively computationally cheap to sweep and
also outperforms the    rst best phoneme hypothesis.

another opportunity for exploiting lattices, which deserves mention,
is lattice matching. in [283], a method for matching a lattice represent-
ing the query with lattices representing the spoken documents in the
collection is proposed.

4.6.3 spoken term detection

std addresses basically the same task as keyword spotting, namely to
   nd spoken instances of terms in the speech signal that were unknown
to the system at asr-time. research on std was launched by nist in

4.6 techniques for detecting spoken terms

349

2006 with the spoken term detection evaluation plan [199]. there is
not a sharp boundary between keyword spotting and std, but in gen-
eral std approaches are distinguished by their emphasis on speed and
scalability, that is, approximating the sequential lattice scan typically
used by keyword spotting approaches. this distinction is re   ected in the
four requirements for std identi   ed in [243]: high recall, high precision,
time and space e   ciency (i.e., scalability), and    exibility (i.e., adapt-
ability of the system to new conditions). these requirements are based
on an examination of real-world use scenarios. the relative importance
of each varies from scenario to scenario. e   ective approaches to std
will be able to achieve the appropriate balance in these requirements.
in general, std approaches involve an initial decoding stage that
generates an intermediate representation of the search space. this rep-
resentation seeks to extract the information from the speech signal
necessary to be able to identify the occurrence of terms yet unknown
at the time of the initial decoding. more detailed information on std
technology is available in [75]. in this subsection, we brie   y cover some
more recent highlights of the    eld.

an approach called    dynamic match lattice spotting    (dmls) is
presented in [264]. this method uses an inexact, med match to search
the lattice in order to compensate for phoneme recognition errors. in
this way, it aims to compensate for phone strings not included in the
phone lattice, which the authors cite as a major shortcoming of lattice-
based approaches. since the dynamic match is an inexact match, it
can be expected to generate false alarms. false alarms are controlled
by setting a threshold on the match score and also by carrying out
a second, veri   cation step. speed-up of lattice search is achieved by
limiting the length of the search string (i.e., query phone-string) and
storing for every timepoint (i.e., node) in the lattice all phone-strings
that begin at that node.

in [263], a comparison is made between the use of acoustic units
based on graphemes and units based on phonemes for std. the use of
graphemes allows the system to circumvent the step of converting the
queries from a string of characters to a string of phonemes, which can
be a particular weak point in std in the case of oov queries (queries
falling outside the available pronunciation dictionary).

350 exploiting automatic id103 output

4.6.4 spoken utterance retrieval

the task of spoken utterance retrieval (sur) also involves matching
occurrences of query words with their mentions in the speech signal.
here, the collection is segmented into short audio documents referred
to as spoken utterances. in order to be relevant to a query, a document
must contain a mention of the query words.

in [252], a two-step approach is proposed in which promising lattices
of a select group of speech segments (each up to 30 seconds in length)
is    rst identi   ed using a rough match and then scanned using an exact
match.

the relative performance of

lattice and confusion network
approaches applied to sur is evaluated and discussed in [141]. the
authors also use a set-of-words approach, which destroys the timing
and overlap relationships between the links in the lattice, treating it as
a bag-of-words. this method is computationally very cheap and sur-
prisingly e   ective, especially in high wer settings. this result suggests
that it is important to check the performance of simple, computation-
ally light techniques before unleashing sophisticated lattice calculations
to compute e   ective term frequencies.

in [210], sur is approached as a    ranked utterance retrieval    task
in which speech segments are ranked according to a con   dence score
re   ecting whether or not they contain the query. a fuzzy matching
approach is implemented by degrading queries using phone-based con-
fusion, phrase-based confusion statistics and confusion based on factors
consisting of phone classes, with best performance yielded by the latter.

4.6.5 hybrid approaches

the most e   ective approaches to sur and std use multiple
hypotheses of the asr systems (i.e., lattices) and also exploit a hybrid
combination of word-level and subword-level units. a hybrid approach
to sur was proposed by [238], who characterize it as a generalization
of the combination of word indexing and keyword spotting in lattices,
originally used by [132]. within the lattices, the score of a lattice link
(either a word or a phone) is calculated by counting the number of

4.6 techniques for detecting spoken terms

351

times the word occurs on a path and multiplying by the id203
of the path. as the authors observe, this score is a lattice-based con   -
dence measure. three di   erent strategies of combining the word index
and the phone index were investigated: combining the results returned
by both, searching the phone index in case of oov, but otherwise
using the word index, and using the word index if no result is found in
the phone index. the latter strategy returned the best results. use of
word lattices achieved a 5% relative increase in performance over words
alone. use of word and phone lattices achieved an 8%   12% increase.
the experiments demonstrate that use of lattices is more bene   cial in
the case of spontaneous speech (here, teleconferences) than it is in the
case of planned speech (here, broadcast news). in [114], an approach
to sur is presented that makes use of a hybrid confusion network
containing both words and phones.

in [312], a related approach is proposed and applied to both sur
and std. this approach makes use of a lattice indexing technique
aimed at improving the speed of lattice search and thus the scaleabil-
ity of the algorithm. spoken utterances are retrieved using an expected
term frequency (etf), which estimates the e   ective number of occur-
rences of the query term that they contain by multiplying the expected
number of words in the lattice by the id203 of the query occurring
in the lattice. the id203 of a query being observed in a lattice is
estimated by taking a product over the id165 probabilities that have
been estimated on the lattice for each phone in the query. the index
then only needs to store counts of phone strings occurring in the lattice.
etfs for words and phones are integrated using a linear combination.
std is accomplished by carrying out a detailed match on segments
with high etf values in order to localize the position of the query
within the segment.

in addition to phones, syllables have also been combined with words
to create hybrid std systems. in [184], two hybrid approaches for
std that combine syllables and word asr output are proposed. the
   rst approach combines words with fuzzy search in a 1-best syllable
transcript. this approach is outperformed by a second approach that
combines words with an exact search on a syllable lattice.

352 exploiting automatic id103 output

in later work [185], the authors investigate a di   erent variety of
hybrid std system that merges the two syllable search spaces: the
fuzzy search on 1-best syllables and the exact search in the syllable
lattices. although some instances of spoken terms are found by both
approaches, there are also cases that are found by one technique and
not the other. the combination of the two subword-based techniques
was shown to achieve additional gains. the authors note that the lat-
tice matching helps to compensate for asr error and the fuzzy search
on the 1-best transcript contributes to    tting the canonical syllable
decomposition of the query with the syllable pronunciations used by
the speakers.

applying similar reasoning to an sur-type task, [182] observes that
di   erent versions of lattice-indexes and confusion networks make dif-
ferent contributions and proposes a learning-to-rank approach from ir
to optimally integrate the two representations. in general, hybrid tech-
niques for std and sur and those for scr (cf. subsection 4.5) can
be considered to represent a continuous space of approaches that can
contribute to improving scr performance.

in summary, this section has presented approaches that make maxi-
mum use of the output generated by an asr system. approaches using
multiple hypotheses, approaches using subword units and approaches
exploiting combinations have been presented. in the next section, we
step back from the recognizer, taking a broader view and looking at
techniques that can be used to augment asr for the purposes of
improving scr.

5

spoken content retrieval
beyond asr transcripts

the previous sections have focused on the contribution made by auto-
matic id103 (asr) to spoken content retrieval (scr).
here, we discuss approaches that go beyond the basic indexing of asr
transcripts and make use of other techniques and other information
sources in order to improve scr. subsection 5.1 is motivated by the
need to make use of additional dimensions of speech media. then we
turn to treat the    nal two challenges from the list of scr challenges in
subsection 2.4. the challenge of context is addressed by a subsection
presenting techniques for augmenting asr transcripts and,    nally, the
challenge of structuring spoken content is addressed by a subsection
discussing techniques for organizing and representing spoken content.

5.1 motivation for moving beyond asr

the motivation for moving beyond basic asr is to improve the inte-
gration of asr and information retrieval (ir) within the scr system.
improvements are achieved both by exploiting techniques for ir that
have been demonstrated to be e   ective in the text domain and also
techniques speci   c to scr. the driving aim is to take advantage of

353

354

spoken content retrieval beyond asr transcripts

opportunities to target the areas in which scr is more challenging
than ir or otherwise fails to reduce to ir. in particular, we return to
the di   erences between scr and ir and the considerations that should
be made when combining asr and ir,    rst discussed in subsection 3.4.
this subsection covers factors that motivate scr beyond basic asr.

5.1.1 the content of speech media

asr transcripts provide a less than fully complete representation of
the spoken content of speech media. their most obvious shortcom-
ing is noise, in terms of recognition errors, that is, the insertion, dele-
tion or substitution of words spoken in the speech stream. however,
depending on the domain, the speech stream itself can actually be
considered underspeci   ed in terms of the information content it con-
tains. in particular, when scr moves away from domains involving
planned speech, such as broadcast news, speech is frequently informal
or spontaneously produced. in such cases, the meaning conveyed by the
spoken content has a strong dependence on contextual factors, which
are not directly represented in the transcripts, or which, if present,
may be more challenging to extract. such factors include facts about
the speakers    immediate surroundings, recent events or speci   c back-
ground knowledge that is shared between the speakers of the speci   c
topic under discussion.

figures 5.1 and 5.2 give examples of two spoken passages on the
same subject, which illustrate the di   erence. characteristic di   er-
ences between planned speech and informal speech can be observed
by comparing these examples. in planned speech (figure 5.1), the
content words related to the topic are more frequent and repeated
(i.e.,    speculaas    and    shortcrust biscuit   ). in the informal speech
(figure 5.2), words related to the topic are descriptive rather than exact
(i.e.,    spice cookie    is used instead of    speculaas   ;    windmill cookie    is
an inexact reference since not all speculaas are shaped like windmills).
in the informal speech, the context serves to establish and support
the topic. pronouns (they, them) are then used by the speaker to make
the connection to the entities under discussion. other pronominals

5.1 motivation for moving beyond asr 355

speculaas is a shortcrust biscuit containing spices
including cinnamon, nutmeg, cloves, ginger, cardamom
and white pepper. in the netherlands, speculaas is
traditionally associated with the holiday of saint
nicholas, celebrated at the beginning of december.

fig. 5.1 example passage of planned speech.

you can buy these spice cookies here starting at around
this time of year. they are quite different from the dutch
windmill cookies that we have where i   m from. a lot of the
people i know who come to live here don   t uh really like
them. they   re just too, well ... i guess they are spicy in
a strange spicy way. you just have to grow up here eating
them.

fig. 5.2 example passage of spontaneous speech.

establish the connection to the time (   this time of year   ) and the place
(   here   )     the month (december) and the country (netherlands) are
not mentioned directly. further, the people mentioned (   a lot of people
i know   ) can only be identi   ed by knowing more about the person who
is producing the informal speech. an ir system, which relies on word
overlap between query terms and the indexing terms representing items,
will clearly have less di   culty generating a match score for the planned
speech example than the informal speech example. issues of the contrast
between planned and informal speech are explored further in [135].

clearly, the planned versus informal distinction is not limited to the
domain of speech and text ir must also deal with the challenges of infor-
mal text, especially in conversational social media such as blogs and
microblogs. however, scr faces the additional di   culty that increased
levels of spontaneity lead to an increased level of error arising from
the id103 process. for example, articulation may be less
distinct and, as seen in figure 5.2, the asr system faces dis   uencies
and restarts. in short, the production characteristics of informal speech
make it quite challenging to recognize.

356

spoken content retrieval beyond asr transcripts

in subsection 3.4.3, we discussed how ir models are able to naturally
compensate for asr error, to a certain extent. in the following material,
we turn to the discussion of additional techniques that can be deployed
to address both asr error and also the underspeci   cation of informal
content.

5.1.2 the structure of speech media

some speech media are produced as a series of semantically discrete
segments, for instance, as a series of reports in a news program. in
principle, each of these segments can be taken as a document in the
scr system. however, scr systems are rarely faced with the presence
of a single, readily evident manner of decomposing speech media into
units. spoken content produced in informal settings is inherently less
structured than planned content. moreover, in many cases, there is a
mismatch between the topic of spoken content and the units into which
it most readily decomposes. for example, a meeting could be relatively
easy to segment into speaker turns. however, the topic under discussion
may last through multiple speaker turns. in contrast, a podcast may
decompose most readily into episodes corresponding to individual audio
   les, but one podcast can contain multiple topics.

these mismatches create a tension that makes it di   cult to deter-
mine a single set of ideal units for an scr system. the system should
display result units that are intuitive units for the user, but it should
calculate a match with the user query that is maximally topically
homogenous, with respect to retrieval units. neither may correspond
exactly to the most easily identi   ed units within the speech media.
another option is to allow the calculation of units to take place dynam-
ically and dependent on the user query. in short, an scr system stands
to bene   t greatly from structuring techniques that allow it to deal with
the lack of structure, or the lack of optimal structure, that is clearly
demarcated in the speech stream. techniques that move beyond con-
ventional asr transcripts can provide important support in this area.
these techniques will be discussed in subsection 5.3.1. in accessing
information in spoken content, we will return to the issue of speech
media units for scr and in particular consider how speech media
results can be displayed to the user in the interface.

5.1 motivation for moving beyond asr 357

5.1.3 user queries for scr

the basic aim of an ir system is to provide users with items that sat-
isfy their information needs. the introduction stressed that scr is a
      nding content    task, meaning that the system goes beyond locat-
ing mentions of the query terms within the speech stream to returning
results whose content as a whole is relevant to the information need.
a detailed examination of the concept of relevance in ir is beyond the
scope of this survey. it is su   cient to consider relevance to be a match
in topic or other characteristics between the query and the results that
lead to satisfaction of the user information need. the discussion of user
information needs for scr in subsection 1.2.2 mentioned the broad
diversity of goals that motivates users to turn to scr systems. this
diversity of user needs in scr is re   ected in the diversity of user queries
that faces an scr system. in this subsection, we show how user queries
for scr provide motivation for going beyond asr. figure 5.3 gives a
selection of example queries that have been used by retrieval bench-
marks.

these queries specify the topic of the desired speech media results.
topics can be seen to range from very broad to quite speci   c. in addi-
tion to specifying what the desired content should be about, users might
also include in their search requests other facets of the desired relevant
results. for example, they might add an indication of the identity of
the speaker, the language or the name of a particular program to the
queries. the two examples from trecvid show that speech can also be
interesting for users in a setting that is otherwise focused on retrieving
video based on its visual content.

it is good to keep in mind that in scr, as in text ir, the user
query is always an imperfect realization of the user information need.
when formulating a query, the user makes a choice of query terms.
this choice might be less than optimal for a variety of reasons, includ-
ing the user not knowing or not recalling the most speci   c terms, the
user having the    bad luck    of having chosen synonyms or paraphrases,
or the user leaving the query incompletely speci   ed in an attempt to
avoid unnecessary e   ort. although explicit models of user query for-
mulation behavior have potential to lead to better matches between
queries and results, few studies examine the mapping between queries

358

spoken content retrieval beyond asr transcripts

find reports of fatal air crashes. (spoken document retrieval
task, topic 62, trec-7 [82])

what economic developments have occurred in hong kong
since its incorporation in the chinese people   s republic?
(spoken document retrieval task, topic 63, trec-7 [82])

jewish resistance in europe (cross-language speech retrieval,
topic 1148, clef [295])

find the video just of a man wearing a suite, tie, and
glasses, speaking french. (known item search task, example
query 12, trecvid [255] 2010)

find the video where a man talks about unions, i think his
name was miller. (known item search task, example query 19,
trecvid [255] 2010 kis)

masavo x defines voice acting.
development set topic 7, mediaeval 2011 [156])

(rich speech retrieval task,

could you find the portion of the talk where they are
discussing optimal times for talks? (rich speech retrieval
task, development set topic 12, mediaeval 2011 [156])

id98 report on hillary clinton. (rich speech retrieval task,
development set topic 21, mediaeval 2011 [156])

fig. 5.3 example queries from scr benchmarks.

and information needs for scr. an exception is [21], which exam-
ines podcast retrieval. as previously mentioned, this work found that
queries containing a person   s name ambiguously re   ect a user need
either for information about the person or for information spoken by
that person. further, the impact of the imperfect recall of users    mem-
ories on the queries that they formulate is explored. in particular, the

5.2 augmenting asr and spoken content

359

work investigates the di   erence between cases in which the user recalls
the exact quote and cases in which the quote is not recalled exactly
and users use only a few key words and possibly add some indication
of the speaker (e.g., the di   erence between    the internet is a pressure
cooker    and    maris internet pressure cooker   ).

researchers have long been aware of the importance of dimensions
beyond the spoken content, for example speaker- and language-based
queries [202]. the larger vision, formulated early-on in the history of
scr, for example in [275], is that combining di   erent forms of analy-
sis will lead to better scr systems. however, it is often the case that
systems are designed without carrying out user studies to collect infor-
mation on user needs or on how users formulate queries. the diversity
of examples in figure 5.3 serves to illustrate that it is important to
understand what characteristics of spoken content are important for
users and how they formulate queries in order to design the most e   ec-
tive possible scr system. this diversity motivates moving beyond asr
for the design of scr systems, the topic to which the remainder of this
section is devoted.

5.2 augmenting asr and spoken content

the shortcomings of asr transcripts can often be addressed by using
techniques aimed at extending them to make them more complete or
to compensate for error. this subsection presents methods for exploit-
ing metadata that may accompany speech media and then looks at
additional expansion techniques based on text ir approaches.

5.2.1 exploiting metadata

metadata are commonly de   ned as    information about information   
and a wide variety of metadata can be associated with speech media.
speci   c metadata that may accompany spoken content in a partic-
ular collection include title, creator, source, names of speakers, date
of recording or broadcast, language spoken, descriptive keywords or a
content precis or summary and closed captions. with the rise of social
media, social metadata such as tags and user comments have become
increasingly important. the quality and completeness of metadata

360

spoken content retrieval beyond asr transcripts

depends on their source. the variation is enormous, ranging from
very complete metadata records in professional archives to record-
ings of informal discussion that may have no metadata other than the
time stamp re   ecting when they were recorded. whatever metadata
are present, however, should be conscientiously exploited by an scr
system. in this subsection, we examine how metadata can be used to
support retrieval.

5.2.2 exploiting manually generated metadata

the value of metadata in scr depends on a number of factors, includ-
ing its content and quality as well as the scr task to be addressed.
we discuss use of human-generated metadata by covering a series of
example domains in which it has been applied and example systems
that have applied it.

the value of metadata, even very simple metadata, was already
recognized in the early scr work. the early video mail retrieval using
voice (vmr) project [27] worked with video messages, which replaced
the message of conventional email with a video. sender and query    elds
were used to sort retrieved items. such metadata are simple, but very
valuable in narrowing the amount of content that is presented to the
user in the results list. although such narrowing is also convenient for
focusing search in conventional email, in the case of video messages, it
also saves the user a great deal of time in reviewing results, which must
be individually watched.

in some cases, metadata are available because an audio collec-
tion has been annotated by hand for a particular purpose. a well-
studied example of this case is the collection of oral history inter-
views from the shoah foundation institute [33, 204]. these data
were used in the clef cross-language speech retrieval track in
2005, 2006 and 2007 [205, 217, 295]. the manual metadata here were
assigned by domain experts who drew keywords from domain speci   c
ontologies and also wrote short summaries using their knowledge of
the domain. results from the clef workshops showed that retrieval
e   ectiveness using only the asr    elds is poor, while incorporating
the metadata gives much better performance. the usefulness of the

5.2 augmenting asr and spoken content

361

metadata can be related to the challenging nature of the task, which
involved spontaneous speech. many of the query words were not present
in the spoken content. further, recognition was challenging, with rec-
ognizer word error rates of around 20%. these experiments made clear
the utility of combining indexing    elds containing asr transcripts with
indexing    elds containing human generated metadata. however, it is
important to consider how these    elds should best be combined and
to avoid the perils of naive approaches. the formal analysis of    eld
combination in [229] provides further details on relevant issues.

lectures are a domain in which accompanying metadata are often
readily available. in [248], a case was investigated in which very few
metadata were available, only titles, abstracts, and bibliography of the
speakers. metadata still provided improvement over using asr tran-
scripts alone. performance with metadata alone was not satisfactory.
adding speech content improved the retrieval performance by 302%
(relative).

a more detailed source of information associated with lectures is
slides. when slides are available, even highly error-   lled lecture tran-
scriptions can be segmented and assigned to their related slide with
a high degree of reliability [130]. in this way, the textual content of
the slides acts as metadata annotating the lecture stream. the text
derived from slides is particularly valuable since it is likely to contain
concise statements of the key points to be raised in the lecture [165]
and to use carefully selected vocabulary speci   c to the topic under dis-
cussion. by contrast, the asr transcription of the lecture will contain
errors, and words that fall outside of the vocabulary of the recognizer
will be missing entirely [86]. social tagging has also been investigated
in the lecture domain, and has been shown to improve access to lec-
tures [140]. students tagged lectures with handwritten information and
photos, which not only added information, but served to reveal which
parts of the lecture were most watched and potentially most interesting.
podcasts are published on the internet with metadata at both the
episode and the series level. the quality and completeness of the meta-
data varies from podcaster to podcaster, but it generally includes titles
and descriptions. internet search engines generally use metadata alone
to index podcasts. podcasts are an example of a case in which metadata

362

spoken content retrieval beyond asr transcripts

provides information that is di   erent from that which can be expected
to be contained in the spoken content. when the user information need
involves, for example, a speaker identity (cf. figure 5.3), this informa-
tion is more likely to be provided by the metadata than spoken by
the speaker. next we turn to ways in which useful metadata can be
acquired or generated automatically.

5.2.3 exploiting automatically generated metadata

we have seen that metadata are useful for scr not only to compensate
for error in asr transcripts, but also to provide additional information
about spoken content that might correspond to dimensions of the user
information need, but be unlikely to be contained in the transcripts.
here we overview the types of metadata that can be automatically
extracted from speech media in order to aid scr.

speaker identi   cation methods automatically match spoken content
with the name of the person speaking. much research has been devoted
to developing methods to identify speakers independently of the words
spoken [146]. audio analysis can identify such non-verbal features as
silence, music or speech, which are included, for example, in the inven-
tory used in early work carried out by [314]. other useful descriptors for
speech media include speaker gender, channel characteristics (telephone
vs. desktop microphone), speaker language, multiple speakers speaking
at once, applause and coughing. often, detection of these features is
carried out as part of the process of speaker diarization, which will be
discussed in more detail shortly. automatic methods can also detect dis-
   uencies and other non-lexical vocalizations [169]. information about
the location and frequency of dis   uencies has the potential to pro-
vide clues as to the nature of speech, for example the formality of the
style.

the emotional dimension of

further, a   ective information can be derived from spoken content,
which allows
speech media to be
labeled [129]. an overview of technology for extracting emotional con-
tent from the speech signal is provided in [69]. non-speech aspects
of multimedia signals can also be analyzed in order to extract a   ective
information [99]. in general, a   ective features can support scr because

5.2 augmenting asr and spoken content

363

of their correlation with particularly interesting or important segments
of the spoken audio. hot spots in meetings [307] are a speci   c
example of where a   ect can serve to support navigation. in [234],
the sound of baseball hits was combined with detection of excited
speech in order to implement a system for highlights extraction from
baseball games. automatic laughter detection makes an important con-
tribution to indexing spoken content in domains such as television
sitcoms [123].

audio id37 moves beyond what is produced by humans
into the general domain of sound. detectors can be built for identifying
audio events [313]. in [219], experiments are performed using ten cat-
egories: airplane jet, airplane propellor, birds, bus, cat meowing, crowd
applause, dog barking, gun-shot, helicopter, horse walking, sirens, tele-
phone bell, telephone digital, tra   c, and water.

simple features sometimes reveal themselves as surprisingly e   ec-
tive. in particular, [63] highlights the usefulness of    surface features       
characteristics that are easily accessible to an indexing system, such as
the length of an audio recording.

automatic methods are useful for generating metadata, but they
can also aid in simply collecting it. meetings are an important domain
for scr, but one for which manual metadata are less readily avail-
able. however, collections of human-generated documents often exist
within enterprises and contain helpful material speci   c to the meet-
ing   s content. research at ibm made an early contribution to the
exploration of the automated delivery of information associated with
a meeting [25]. the meeting miner system performs live asr on the
audio stream emerging from a meeting. the transcripts are used as a
source of queries, which are extracted automatically and submitted to
archives related to the meeting. the items returned are provided to
participants to enhance their participation in the meeting. information
gathered in this way might potentially be used to annotate the meeting
transcription or to more fully describe the topic under discussion in the
meeting and thus potentially facilitate improved search. the key ques-
tion here is whether materials can be chosen with su   cient selectivity
and reliability to provide useful information to people involved in the
meetings or interested in their content [143].

364

spoken content retrieval beyond asr transcripts

5.2.4 expansion techniques

this subsection examines techniques that can be used for expansions
that make possible a better match between user queries and speech
transcripts. expansion techniques are drawn from text ir, where their
bene   ts derive from their ability to provide lexical enrichment that
compensates for semantic underspeci   cation. in scr, applying expan-
sion techniques has the additional bene   t of compensating for asr
errors. this subsection reviews techniques for the expansion of both
queries and documents.

id183. expansion of queries can be accomplished with
a variety of approaches. in subsection 4.4.3, we discussed the query
expansion method introduced by [173, 174], which uses the lan-
guage model and pronunciation dictionary to determine possible mis-
recognitions of the query word and uses these to expand the query. the
method aims at compensating for id103 error.

other id183 methods have the e   ect of compensating
for both asr error and semantic underspeci   cation. here, we return
to discuss relevance feedback, an ir technique initially introduced in
subsection 2.2. in a standard relevance feedback scenario, users per-
form an initial search after which they provide feedback to the system
indicating which retrieved items they deem relevant to their informa-
tion need. in a pseudo-relevance feedback (prf) scenario, top ranked
items returned by an initial retrieval round are used as feedback, under
the assumption that since they matched the query well they must be
relevant. this feedback information is then used to modify the system
to bias subsequent searches towards the information need.

prf exploits co-occurrence of words within items to expand queries.
two mechanisms serve to make clear why relevance feedback has an
overall tendency to reduce rather than amplify the e   ects of asr error.
first, recall that the match with a user   s query is dependent on the
presence of terms in the asr transcript of an item. for spoken content
items transcribed with low word error rates (wers), there will be little
impact in matching; items with higher wers will be more signi   cantly
a   ected. we can then expect that items with lower wers will appear
at higher ranks in the results list, which was indeed observed by [237].

5.2 augmenting asr and spoken content

365

because prf chooses top-ranked items, it prefers well-recognized items
to less well-recognized items. in short, prf can be expected to have the
tendency to disfavor items with worse wers and the greatest chances
of introducing word errors into the query.

second, recall that the    xed vocabulary of an asr system means
that there are no spelling errors in the asr transcript and no intro-
duction of new words. for this reason, id103 transcripts
do not contain very rare words. rare words can be dangerous for rel-
evance feedback, because they are highly speci   c and have large nega-
tive impact should they be inappropriately selected to expand a query.
in [154], it is demonstrated that prf can yield a greater percentage
improvement in scr tasks than in text ir tasks.

in work by [136, 137], a range of id183 techniques making
use of language resources (i.e., id1381), a collateral corpus and blind
relevance feedback are explored. a recent approach to id183
using a parallel corpus is presented by [189]. this approach uses topics
discovered by way of id84 in order to enrich user
queries.

document expansion. the process of id183 extends
queries using terms that have a strong statistical association with
already-identi   ed relevant items. these terms represent words that the
user might have included in the original search request, but, for a vari-
ety of reasons, did not. by analogy we can consider the possibility
of document expansion. we could use material relevant to a particular
document as a source of terms to extend the document, with the goal of
improving its representation of the underlying topic. in the case of spo-
ken content, we are particularly interested in compensating for words
missing from the asr transcripts due to recognition errors, including
oov errors. this consideration motivated [251] to introduce document
expansion for scr. a selected document is used as a search request
to a collection of text documents, and prf methods are applied to
select expansion terms for addition to the document transcript. since
there is no way of knowing whether the word has actually been spoken

1 http://id138.princeton.edu/

366

spoken content retrieval beyond asr transcripts

or not, the technique strives to add words that were either actually
spoken or that the user could have potentially spoken within the con-
text of the document. results on the trec-7 sdr tasks by at&t
showed promise for this technique [250]. however, it has not been widely
explored since this early work. in [192], phone confusion probabilities
were used to expand documents, but this technique does not target the
bene   ts of semantic enrichment. in particular, the relative bene   ts of
document expansion for collections containing informal versus planned
speech have yet to be investigated thoroughly.

using collateral information. collateral information is supple-
mental information derived from sources beyond the immediate col-
lection of speech media. collateral information can be used at many
di   erent stages in an scr system. for example, in subsection 3.2.1, we
mentioned its usefulness for adaptation of language models. here, we
turn to its usefulness for improving ir and for organizing and enriching
speech media for the purpose of presentation to the user.

in comparison to text retrieval collections, speech media collections
are typically relatively small. for retrieval, this means that parameters
in the ir model could be poorly estimated, particularly with respect
to the speci   city of terms in individual items. the errors in asr tran-
scripts are likely to introduce further degradation of these estimates.
this observation was made quite early in the development of scr
methods. in an attempt to address these problems, supplemental text
corpora     much larger document collections, free of asr errors    
were successfully applied as a source of pseudo-relevant documents for
prf [137].

it should be noted that this technique is only e   ective if the collat-
eral text corpora used are properly representative of the domain of the
speech media collection. the sdr track at the trec-7 and trec-8
workshops used collateral text collections with notable success [127].
the trec sdr materials were taken from north american radio and
television news during a period in 1998. the text data sets that were
used to augment retrieval from this collection consisted of text news
stories from the same period. for domains that change rapidly over
time, it is important that the data is not just in the same topical

5.2 augmenting asr and spoken content

367

space, but that it is from the same time period since important items
of the vocabulary and their usage will often be signi   cantly di   erent,
even from those in the previous or following year [134].

another important source of collateral

information is closed
captions, which are generally more accurate than contemporary asr
systems and can be used to support scr [103, 206]. in such situations
the only reason to perform asr would be to obtain an exact align-
ment between the spoken content and the transcript. forced alignment
techniques are treated in more detail in the following discussion.

5.2.5 forced alignment

forced alignment is the process of using the asr system to temporally
match speech media with related material, usually with a human-
generated transcript. the alignment is    forced    in the sense that
the asr system is constrained so that it recognizes only the words
contained in the transcript and only in the sequence in which they
occur in the transcript. instead of recognizing the words themselves,
the asr system is recognizing the positions of words, whose identity
has been supplied in advance. the result is an enrichment of the
original transcript in which each word is associated with a time code
that indicates where that word was spoken in the speech stream.
forced alignment is the radio oranje
an example of the use of
system [109], in which human-generated transcripts of speeches of the
queen of the netherlands are synchronized with historical recordings.
after alignment, the transcripts can be indexed and used to provide
search functionality for users. users can carry out searches and the
system returns results that correspond to jump-in points in the
speech stream. we discuss the usefulness of forced alignment for user
interfaces further in accessing information in spoken content. an
overview of uses for alignment techniques is available in [62].

in [171], forced alignment is performed between stenographic tran-
scriptions of parliamentary speeches and recordings made in the
parliament. the match between these two resources is close, but inex-
act. in order to carry out forced alignment, the transcriptions are used
to generate lattices that allow for the deletion and skipping of words.

368

spoken content retrieval beyond asr transcripts

these lattices are then matched to the speech signal in order to create a
correspondence between the spoken word and the transcribed content.
forced alignment can be considered a method for expansion, since
it associates speech media with additional information from an aligned
transcript. however, it can also be considered a structuring method.
any structure that exists within the transcript, for example, paragraph
markings, de   nes a structural spoken content unit when it is aligned
with the speech media. in the next subsection, we turn to additional
techniques for structuring spoken content.

5.3 structuring and representing spoken content

segmentation techniques and techniques for generating alternate
semantic representations create information about spoken content that
is useful for ir. here we introduce techniques and comment on their
interaction with ir algorithms. in accessing information in spoken
content, we further discuss how these techniques can be used in the
user interface.

5.3.1 segmentation

segmentation of spoken content can take place either based on direct
analysis of the audio content or by using the asr transcripts. the
importance of forming audio and topical segments has long been rec-
ognized in management of speech content. early work on this subject
was conducted on topic and speaker identi   cation using id48-based
id103 in studies such as [85]. segmentation using the audio
content prior to recognition can help to improve the quality of the asr
transcripts. in general, asr systems do not process a continuous string
of speech, but rather disassemble it into smaller pieces, generating a
hypothesis string for each. segments created for the purposes of asr
may be of    xed length or may be divided at pauses (i.e., places at
which the speech signal drops o    in energy). segments may roughly
correspond to utterances, but whether they will also be useful for scr
will depend on the application. some recognizers hypothesize punctu-
ation and output units that can be equated with sentences [18, 169].
such units are more clearly directly semantic in nature and helpful

5.3 structuring and representing spoken content

369

for scr. in general, the quality of the segmentation will be strongly
dependent on the segmentation algorithm used and on the nature of
the audio signal being segmented.

multiple levels of segmentation may be relevant for an scr system.
for example, in an interview, both topical segments and individual
speaker turns may be relevant. the ir model may need to combine the
scores. score combination is not trivial: a larger item may score well
on its own because overall it contains a large number of terms, but it
may still not speci   cally address the information need of the particular
query. in contrast, individual segments may score well without being
representative of the document as a whole. one example of an approach
from text ir that is potentially helpful in scr because it combines dif-
ferent levels of scores is described in [30]. here, the whole document
score is combined with the highest scoring segment from within the
document. the appropriateness of this approach will depend on the
application. some techniques carry out topic labeling and segmenta-
tion simultaneously, and we will return to discuss such approaches in
subsection 5.3.4.

5.3.2 diarization

much work on structuring spoken content has taken place within the
context of the research e   ort devoted to diarization systems. diariza-
tion is the task of automatically identifying sections of spoken audio
and correctly labeling them with their characteristics, for example,
speech, non-speech, male-speech, female-speech, music, noise. although
speaker identi   cation played a role in early segmentation approaches,
e.g., [300], determination of the identity of the speaker, called speaker
identi   cation, or con   rmation of a presumed speaker identity, called
speaker veri   cation, does not fall into the scope of the diarization task.
work on diarization was promoted by the rich transcription (rt)
evaluation project [74, 317]2 of the us national institute of standards
and technology, which ran from 2002   2009. rich transcripts are asr
transcripts that include information above and beyond conventional
asr transcripts. the enriching information makes the transcripts more

2 http://www.itl.nist.gov/iad/mig/tests/rt/

370

spoken content retrieval beyond asr transcripts

readable for humans and more useful to machines. the rt evaluation
initially used english-language broadcast news, telephone speech and
meeting room speech. later, it ran tasks involving mandarin and ara-
bic. tasks included metadata extraction (mde) tasks such as detection
of words used by speakers as    llers or to correct themselves (i.e.,    ller
and edit word detection), speaker attributed speech-to-text (transcribe
the spoken words associated with a speaker) and speech-to-text (stt)
tasks such as speaker diarization.

diarization techniques usually make use of a technique based on
the bayesian information criterion (bic). a sliding window is moved
along the speech signal, which is represented as a series of    speech
vectors    encoding information about signal properties such as fre-
quency and energy. for each window position, the bic is used to
decide whether a single model or two di   erent underlying models pro-
vide a better explanation (i.e.,       t   ) for the observed speech signal. if
a window is better explained by two underlying models, then a seg-
ment boundary is hypothesized to bisect that window. once segment
boundaries have been located in this manner, id91 is carried out
in order to determine which segments are similar to each other, that
is, were spoken by the same speakers. an audio classi   er, trained on
labeled training data, is then used to assign a label to each group of
segments. early work on diarization includes [44, 95]. subsequently,
much research has been dedicated to speaker diarization, with an
overview available in [266] and recent work including [6, 118, 309, 316].
the diarization task can be extended to include automatic methods for
discovering the role that speakers play within conversations, such as
investigated by [274].

5.3.3 compact representations

techniques that capture the essence of spoken content and represent
it in succinct form can be used both for indexing purposes and for the
purposes of visualizing speech media, which is relevant to the discussion
of scr interfaces in section 6. here we cover several techniques that go
beyond asr in that they represent spoken content in a manner more
condensed than a word-for-word transcription.

5.3 structuring and representing spoken content

371

automatic term extraction. term extraction can be viewed as a
specialized summarization technique. automatic term extraction can
be used for the purpose of providing a select set of indexing terms
for scr. frequently, however, automatically extracted terms are used
to represent speech media for the users in the interface. in [65], term
extraction was investigated for the broadcast news domain. the tech-
nique was shown to produce useful keyphrases at relatively high word
error rates. in [101], terms are extracted from highly noisy transcripts
and ranked in terms of descriptiveness. the usefulness of extracted
terms displayed to the user in the form of a term cloud is investigated
in [268]. a term cloud is a collection of terms that is visually weighted,
in other words, the size and bolding of the font in which a term is dis-
played re   ects its importance. importance is usually calculated on the
basis of term counts within an item. when used as a representation of
spoken content, a term cloud is created using term frequencies of words
occurring in the id103 transcripts. because a word needs
to occur multiple times in the transcripts in order to be included in the
cloud, term clouds have the potential to compensate for asr errors.
the largest, most important terms in the cloud are highly likely to
have actually occurred in the spoken content and not to be the result
of asr errors.

the cross-language speech retrieval track (cl-sr) [205, 217,
295] carried out scr experiments that compared the usefulness of
automatically-assigned keywords versus manually-assigned keywords.
the results clearly indicate that much work must be done before auto-
matically generated metadata can rival human generated metadata in
usefulness.

summarization. summarization of spoken content takes two basic
forms: speech-to-text summarization and speech-to-speech summa-
rization [79]. if video is also taken into account, then both static
storyboard-type summaries as well as video clip summaries can be
considered. the two basic approaches to speech summarization, as iden-
ti   ed by [79], are sentence extraction     in which entire sentences are
selected from the spoken content and then concatenated     and sen-
tence compaction     in which sentences are shortened or the transcript

372

spoken content retrieval beyond asr transcripts

is modi   ed. as with scr, the main di   erences between speech and text
summarization arise due to asr errors and also due to the unstruc-
tured nature of spoken content. however, speech also has prosodic
cues, which can help to create summaries. for example, in [42], empha-
sis is used to help automatically summarize spoken discourse. recent
work on summarization includes research in the area of lectures and
presentations [80], voicemail summarization [149], and broadcast news
summarization [45].

another interesting type of summarization task is title generation.
for example, in [126], a system is presented that uses speech recogni-
tion transcripts to automatically generate titles for a collection of news
stories. automatic title generation, as well as automatic summaries, is
provided by the chinese news prototype described in [166]. the auto-
matic generation of lecture slides from asr transcripts is explored
in [130].

5.3.4 extracting topic information

in its e   ort to match user queries to collection items, an scr system
needs to deal successfully with the problem of vocabulary mismatch.
vocabulary-mismatch arises because, in human language, it is possible
to discuss one topic using a wide variety of lexical items. vocabulary-
mismatch poses a challenge for both text ir and scr, although in the
scr case it can be more extreme due to asr error. an approach to
dealing with mismatch is to choose abstract representations of topics
and then attempt to assign speci   c spoken content items to one of these
abstract representations. topic inventories are either pre-determined
and take the form of a categorization scheme or an ontology, or else
they can be determined dynamically by analyzing the content. in the
remainder of this section, we discuss techniques that belong to both
approaches.

spoken content classi   cation. category schemes group similar
items, both for the purposes of computing similarity within the scr
system and for display to the user. the widespread use of category
schemes to organize spoken content attests to their ability to capture

5.3 structuring and representing spoken content

373

useful semantic regularities among spoken media items. for example,
users looking for podcasts on itunes can make use of topic categories
in their searches. category labels include, arts, science and medicine,
and technology.3 each of these larger labels can be broken down into
sub-labels.

the task of spoken audio classi   cation involves assigning items to
categories. generally, a classi   er is trained on a set of labeled training
examples and then used to assign these labels to new, yet unseen, items.
the collection is usually considered to be static. the    rst work done
in the area of automatic classi   cation of spoken audio was likely [231],
which describes a system for classifying spontaneous speech messages
into one of six classes related to topic. text classi   cation techniques can
be applied to speech transcripts in order to automatically tag spoken
audio with subject tags. one of the major goals of automatic classi   -
cation research is to reproduce the classi   cation that would be gener-
ated by a human, given a speci   c classi   cation scheme. for example,
in [213], classes are drawn from the media topic taxonomy of the
international press telecommunications council,4 in [159] classes are
drawn from the thesaurus used by archivists at the netherlands insti-
tute for sound and vision, a large audio   video archive, and in [161]
classes are drawn from the set of tags assigned by users to video on
blip.tv, a video sharing platform. note that some forms of classi   ca-
tion that are potentially useful for scr perform their categorization
of speech media based on characteristics not directly related to topic,
such as genre in video [267] or dialogue acts in spoken conversation
(i.e., statement, question, apology) [259].

latent topic analysis. the vocabulary mismatch problem can also
be addressed with latent topical analysis, a process that maps word
forms onto dimensions of meaning. these dimensions are referred to
as    latent    because they are expressed in terms of word co-occurences,
but are not otherwise explicitly present in the content. in contrast to
topic category schemes, the dimensions are not known in advance and
may not always be naturally interpretable to humans, even though

3 http://www.apple.com/itunes/podcasts/specs.html#categories
4 http:www.iptc.org

374

spoken content retrieval beyond asr transcripts

they do succeed in capturing useful underlying semantic regularities.
the technique of id45 (lsi) was introduced for
ir in [64]. lsi involves applying singular value decomposition (svd)
to a matrix containing information about which terms are included in
which documents. the svd technique identi   es latent dimensions in
their order of importance and top dimensions are then used as indexing
terms. words from the documents and queries are mapped onto these
new indexing terms, meaning that retrieval takes places within the
latent semantic space. work in the area of scr that has exploited lsi
and related techniques includes [40, 115, 152].

topic segmentation. we close this subsection with a discussion of
techniques that consider structure and topic not as two separate steps,
but rather as part of an integrated process. speech media can be parti-
tioned into topic-homogenous segments by using methods adopted from
text segmentation. a popular method for automatically determining
topic boundaries is the texttiling algorithm [106], which hypothesizes
topic boundaries at points at which a major shift in the vocabulary
use is observed. alternative segmentation algorithms are described
in [48, 66] and [175]. the work in [17, 116] concentrates speci   cally
on segmentation of meetings and dialogues. texttiling operates on
words alone, but other characteristics of the speech signal can also be
exploited for topic segmentation. for example, in [260], lexical items
and id144 are combined in order to predict topical segments.

topic detection and tracking (tdt). topic detection and track-
ing (tdt) is a suite of tasks aimed at both segmenting a speech stream
and identifying the topics that it contains. much productive research
e   ort was devoted to these tasks during the tdt program [287],5
which was sponsored by the us department of defense during the
years 1998   2004. tdt conducted tasks for topic segmentation, track-
ing and detection as well as tasks for detecting    rst occurrence of new
topics and also for linking items that are topically related. tdt focuses
on the discovery of new material and emphasizes events over thematic

5 http://www.itl.nist.gov/iad/mig/tests/tdt/

5.3 structuring and representing spoken content

375

subject categories [4]. the tdt research program comprises    ve sub-
tasks: story segmentation, detection of boundaries between stories in
a news show, first story detection, detection of new, unknown sto-
ries, cluster detection, grouping incoming stories into topically related
groups, tracking, monitoring the incoming stream to    nd more stories
resembling a set of sample stories, and link detection, for deciding
whether a given pair of stories is related or not. detailed descriptions
of tasks and performance can be found in [5].

tdt techniques were developed for deployment in a system that
monitors the speech media stream and generates an alert when inter-
esting items are encountered. ultimately, all techniques covered in this
section that move beyond basic asr transcripts are only useful in so
far as they can support scr and users of scr systems.

this section has covered methods for moving beyond asr for
the augmentation, structuring and representation of speech media. it
concludes our treatment of the component technologies for scr and
techniques for combining them. in the next section, we turn to the issue
of the interfaces that support user/system interaction, which play a
vital role in satisfying users    needs.

6

accessing information in spoken content

the ultimate usefulness of a spoken content retrieval (scr) system to
users is determined by the user interface. technically, an scr system
may return results representing high quality matches with the query.
however, users must be able to e   ciently evaluate these results and
identify individual items of interest in order for an scr system to
ful   ll its function of satisfying user information needs. further, the
interface should make full use of feedback from users in order to re   ne
queries. as mentioned in the introduction, scr systems have conven-
tionally paid little attention to user requirements. the interface has
been explicitly identi   ed as an important scr challenge in [11, 203],
and representation of spoken content is noted as an important open
issue for scr in [3]. this section overviews the key issues of interface
design and discusses classic examples of how these issues have been
addressed in research prototypes and real-world systems. we focus on
aspects that are speci   c to scr, with the intent of providing a help-
ful complement to the existing literature on user interfaces for search,
such as [107].

376

6.1 query entry

377

6.1 query entry

scr systems often invite users to enter queries by o   ering a query
box, such as is in widespread use with web search engines. the query
interface of podcastle (http://podcastle.jp) [92, 93], a browser-based
scr system for podcasts, is shown in figure 6.1 to illustrate such a
query box. although the simplicity of the query box echoes the google
web search engine (http://www.google.com), podcastle o   ers the user
much more information concerning the search system and what it can
be used to    nd. for example, the scope of the content indexed (over
100,000 episodes) is explicitly mentioned and links to recommended
podcast episodes are provided.

the google election gadget [2], which o   ered search functionality
for political speeches during the 2008 us elections, also displayed hints
for users. it provided information about the scope and the functional-
ity of the system by prompting users with the question    what did the

fig. 6.1 query interface of podcastle.

378 accessing information in spoken content

candidates say?    in addition to the query box, there was a choice of
constraining the search to    mccain,       obama    and    debates.    these
choices implicitly supply information about what is present in the sys-
tem. if users limit the    eld of search to a particular category of content,
the accuracy and speed of the system stands to improve. in general, the
trend can be observed towards designing query interfaces that inform
and guide the user. because users are not yet widely aware of the exis-
tence of spoken-content-based retrieval technology, as suggested by the
user study in [21], this information may support them in formulating
more e   ective queries.

speechfind [100, 144] is another system whose query interface o   ers
users information about what can be found in the system. as shown
in figure 6.2, speechfind displays buttons at the top that show the
speci   c sources of content available in the system and also displays
sample content below.

fig. 6.2 query interface of speech   nd.

6.2 display of results for selection

379

the observation that longer, richer queries lead to better scr per-
formance has led to the suggestion that interfaces consider how users
can be encouraged to formulate longer queries [3]. the additional boxes
for query entry, as depicted in figure 6.2, both invite the user to enter
information of a speci   c type and help the system to disambiguate
between query terms that the user expects to hear spoken in the audio
content and other characteristics of the speech media, such as the iden-
tity of the speaker. this form of query entry resolves ambiguity in
queries containing person names, as discussed by [21] and mentioned
in subsection 5.1. ultimately, users will probably    nd entering a struc-
tured query too cumbersome and systems that automatically di   eren-
tiate the relationship between query terms and di   erent components of
the user information need will prove more e   ective.

as a    nal comment on queries, we mention that in addition to
text queries, scr systems can also enable users to specify non-textual
queries to initiate the search process. for example, the informedia sys-
tem o   ers functionality, described in [278], that allows a user to initiate
a query by specifying a location on a map. the system returns news sto-
ries containing references to places in this region. other functionality,
such as described in [49], makes it possible to query by visual features
such as visual concept or image. finally, we mention again that voice
search, which uses spoken queries, is a rapidly growing area [285].

6.2 display of results for selection

results returned by the scr system are generally displayed as a results
list. as a guiding principle, selection interfaces of scr systems should
be designed to let users make maximum use of their innate human
ability to quickly ascertain the interest and relevance of particular
objects [202, 203]. the importance of informative result display is
re   ected in, for example, [265], which presents the results of a user
study suggesting that multimedia retrieval may be a   ected if the user
has a low perception of the relevance of the retrieved results. the results
display should be optimized in order to allow quick and easy assessment
of relevance by users.

380 accessing information in spoken content

fig. 6.3 excerpt of results list generated by the dutch broadcast news retrieval system of
the university of twente in response to the query    amsterdam.   

we use an excerpt from a results list generated by the broadcast
news retrieval system of the university of twente [211] in figure 6.3
to illustrate some key issues of results display. the results list has been
returned by the system in response to the query    amsterdam.    the
results are displayed as a ranked list of jump-in points each associ-
ated with a speech media fragment that the system has matched with
the query. each item is represented by a surrogate comprising a short
excerpt of the asr transcript, a keyframe, the name and date of the
program and a timecode locating the result within the program. the
function of the surrogate is to give the user the information necessary
to evaluate the relevance of the result to the original information need
and to decide whether to review the result in more depth. surrogates
are also used in text retrieval, but are particularly important for scr
systems. reviewing a spoken media result requires listening to an audio
   le or watching a video    le. this process is considerably more time con-
suming that skimming a page of text. the bene   t of snippets is related
to the quality of the underlying speech transcripts     in [108] it is

6.2 display of results for selection

381

observed that the accuracy of surrogates determines their usefulness. if
subword indexing is used, subword units need to be reconstituted into
words before they are appropriate for use in a snippet.

the surrogate is usually    biased    towards the user query, mean-
ing that its form is especially chosen to highlight the match between
the query and the result. note that in figure 6.3, the query word has
been highlighted in each snippet. the presence of the query word is
strong evidence for the user that the result is relevant to the informa-
tion request. interface design should take into consideration the user   s
expectation level of seeing the query word in the snippet and hearing
the query word very quickly after the playback of the result is initi-
ated. for video applications, presenting a keyframe may provide the
user with an additional hint as to the content of the result. a further
dimension to the selection of the appropriate form of surrogates is the
background knowledge of the user, which has been observed to have an
impact on the types of surrogates that are preferred [158, 268].

additionally, it is important to mention how results selection dis-
play may e   ectively limit the application of advanced ir techniques in
practice. in spoken content retrieval beyond asr transcripts, ir tech-
niques that are capable of overcoming words missing in the transcripts
due to id103 errors were discussed, in particular, query
expansion. however, such techniques may return result items that are
relevant to the user   s original information need, but where none of the
original query words are actually uttered in the spoken content. unless
there is a mechanism to convince the user of the relevance of a result
without showing evidence of the query word being directly associated
with the content, users will pass over this result and the system may
fail to meet its goal of satisfying the user information need.

one of the challenges of displaying a ranked list of results is to
e   ectively communicate to the user the relationship between the results
and the structure of the speech media in the underlying collection.
recall, from section 5, that multiple levels of units may be used by
the scr system and that the retrieval unit is not necessarily the only
important or useful unit of structure within the collection. for example,
in figure 6.3 there is a tension between the retrieval unit (a fragment)
and a larger, natural unit in the collection (a news item). two results

382 accessing information in spoken content

are returned from the same news program on monday, 12 september
2011. depending on the application, two results from the same program
might confuse a user, who may consider them actually to constitute a
single, duplicated result. even if they contain di   erent spoken content,
it is di   cult to indicate the di   erence clearly in the results list because,
as illustrated by this example, results display often depends on program
level metadata, in this case the date, which is the same for each.

a typical approach is to choose the larger unit with which meta-
data is associated as the retrieval unit that is ranked and displayed
in the results list. this approach is taken by podscope (http://www.
podscope.com), a spoken-content-based podcast search engine. a
results list returned by podscope in response to the query    search
engine    is displayed in figure 6.4.

each result is an individual podcast episode displayed together with
its metadata. note that information about the relevance of individual

fig. 6.4 results of the podscope search engine in response to the query    search engine.   

6.2 display of results for selection

383

fragments within the podcast episode is contained in the surrogate. on
the left is a scrolling list of jump-in points, displayed as time codes,
which allow the user to initiate playback of a particular fragment-
level result directly from the episode-level results list. displaying both
episodes and fragments together in the results list gives greater    exi-
bility for result review. however, this bene   t is o   set by the relatively
large amount of space required by the surrogate and the fact that
time codes provide little information to the user about which fragment
would be most interesting to select. another approach to results display
is represented by the search application developed by the university
of twente for the collection at http://www.buchenwald.nl contain-
ing interviews with dutch survivors of the buchenwald concentration
camp [211].

a results list returned in response to the query    bezetting    (eng.
   occupation   ) is displayed in figure 6.5. each result is an individual

fig. 6.5 results of the search application for the dutch language oral history interview
collection at http://www.buchenwald.nl in response to the query    bezetting.   

384 accessing information in spoken content

interview. the user is not presented with the relevant fragments
directly, but rather merely supplied with information on how many
there are (e.g., three fragments is indicated with    3 fragmenten   ). the
user enters an interview in order to explore the fragments that it con-
tains. here again, as already mentioned in subsection 5.3.1, segmenta-
tion interacts with ir models. in order to rank interviews, it is neces-
sary to combine fragment-level relevance to an overall interview-level
score. the exact balance to be used when making this computation is
important. for example, in some cases a unit containing a single highly
relevant fragment should out-rank a unit containing multiple less rele-
vant fragments. in other cases, the opposite could be expected to hold.
this combination itself should depend on how users use the system.

6.3 review and playback of results

scr systems include a player that allows the user to play back indi-
vidual results. playback is an important functionality for both content
review and content consumption.

playback should have the goal of saving the user time and reducing
memory load, since humans generally have poor facilities for extracting
and remembering detailed information from audio content [112]. as a
design guideline, players should provide as much information as is pos-
sible, without clutter, about the content of speech media and give users
   exible control over navigation. naturally, an important part of access-
ing spoken content will remain actually listening to audio material. the
process can be made more e   cient by enabling fast playback. in fact,
experiments show that speech can still be intelligible if the speed of
delivery is doubled [9]. the speechskimmer system used di   erent lev-
els of compression for fast playback [9, 10]. however, the cognitive load
of listening to speech at this speed is considerably higher than natural
speech, leading to the listener rapidly losing focus or becoming over-
loaded. in the end, the gain is also only half of realtime, which means
that reviewing speech media results remains a time consuming process.
other approaches for compressing speech involve not only altering the
speech rate but also removing unimportant words and segments (called
excision) [269, 270].

6.3 review and playback of results

385

another option available to scr systems is presenting users with
asr transcripts to read. scanmail [297] is an example of a system
that presents the user with the asr transcript directly, in this case
a voicemail message. a user study suggested that this feature was
appreciated. however, a relationship does exist between the error levels
of asr transcripts and their usefulness in the system, with higher
error rates being less useful [194, 258]. in [194], a user study on the
usefulness and usability of asr transcripts for a web archive was con-
ducted. transcripts with wer > 45% were found to be unsatisfac-
tory while transcripts with wer < 25% were found to be useful and
usable.

it is important to keep in mind, that an scr system must not allow
users to develop an unfounded trust in the asr transcripts. in [35],
professional users were found to have signi   cant con   dence in the scr
system, the transcripts and their own ability to work with them. this
resulted in the users failing to seek relevant content not explicitly
re   ected by the transcripts, reducing the recall of their results. the
same e   ect was reported in user tests of the scanmail system [297]. in
the domain of voicemail, recall is more critical and misplaced trust in
the asr transcripts caused users in the study to miss crucial informa-
tion that was not recognized by the asr system.

if asr transcripts have word- or sentence-level time codes, these
can be o   ered to users to read in the interface, linked to the speech
stream so that users can click in the transcript to jump directly to
listening to the stream, when they    nd a portion that interests them.
e   ectively, the text of the transcript becomes the playback interface.
an example of an audio browser, dating back to the early 1980s, with
an interface that linked text with speech is the intelligent ear [242].
this system presented a representation that depicted the amplitude of
the waveform. during playback the current play position is indicated
by a sound cursor that moves forward synchronously with playback and
highlights that portion of the wave currently being played. selected key-
words spoken in the audio are written in under the waveform. keywords
that are recognized with higher con   dence are displayed more brightly.
modern interfaces tend to present variations on this basic theme. a
key feature is that the interfaces are linked to the media recording and

386 accessing information in spoken content

that the user can initiate playback at any point by clicking on the
corresponding point in the interface.

typically scr players combine functionality that allows users ran-
dom access to the speech stream with a tape-recorder metaphor,
unchanged since early mentions such as [29, 76]. in these interfaces,
time runs from left to right [29], with events positioned along the time-
line proportionately to where they occur in the broadcast. thought
should be devoted to making the time scale consistent in order to facil-
itate comparison [202]. many interfaces incorporate a slider bar that
the user can manipulate to control the point of playback, which doubles
as a timeline. the timeline contains markers indicating the positions of
particularly relevant material. an early example of this functionality is
illustrated in the video mail retrieval system browsing interface [27],
shown in figure 6.6. the interface shows a graphical timebar with

fig. 6.6 early video mail restrieval system interface.

6.3 review and playback of results

387

individual hits on query words in the audio    le highlighted. similar
to [242], search term con   dence is indicated by the brightness of the
results when displayed. the user can click to start playback at any
point on the timeline. this example is only 20 seconds in length     for
longer    les, clusters of search term hits can direct the user to regions
more likely to be relevant to the query.

if the speech media is a video, a storyboard of clickable keyframes
can be used to depict the temporal progression graphically for the user.
an example of this principle in use is the cmu news-on-demand
system [105].

figure 6.7 depicts a dutch podcast search engine called kunst-
stofzuiger [46] developed at the university of amsterdam, chosen to
illustrate typical strategies for scr players. the player page displays

fig. 6.7 player for the kunststofzuiger search machine.

388 accessing information in spoken content

when the user clicks on a podcast episode that was presented in a
results list in response to a query, in this case    nederland    (eng.
   netherlands   ). the player page has a query-independent represen-
tation of the episode, in the form of the podcast title, broadcast date
and description, and also a term cloud that has been extracted from
the transcript of the podcast. it also has a query-biased representa-
tion of the episode in the form of the player, which contains markers
pointing to the moments within the podcast at which the query word
occurs. these markers depict the overall frequency of the query word.
clicking one of these markers moves the user to the point in the speech
stream at which the query word is spoken. note that playback should
begin a few words before the spoken word in order to allow the user
to process the speech. the time lag before the spoken word should be
approximately constant so that a few interactions with the system will
inform the user how long to listen at a particular jump-in point before
concluding that point was a false alarm and contained no mention of
the keywords.

note that the kunststofzuiger search engine, whose interface is
depicted in figure 6.7, is an scr system that takes a       nding content   -
oriented approach to retrieving podcasts. however, once the content
has been located, the occurrence of certain words within the podcast
is made visible to the user. in other words, a      nding mentions    point
of view is used to support the user in navigating within the episode.

two variants on these strategies deserve mention. first, the term
cloud can be spread out along the player to give the user a general
idea of the topical development over the course of the speech media.
such an approach was proposed in [78]. second, instead of pointers
to the exact place at which a word is mentioned, a heat map can be
displayed that uses shading or color to re   ect the relative likelihood of
a position along the timeline being relevant to a query. this approach
is adopted in the vmr broadcast news browser [26], whose browser
bar is shown in figure 6.8. in order to create this representation, the
asr transcript is divided into equal-length segments each of which
is scored against the query. the brightness of the shade indicates the
strength of the match of each segment, allowing identi   cation of likely
relevant regions of the broadcast. this particular example illustrates

6.3 review and playback of results

389

fig. 6.8 heat map from the vmr broadcast news browser depicting the likelihood that
regions of spoken content are relevant to the user query.

fig. 6.9 player for the radio oranje system.

the typical structure of a news broadcast where the story of interest is
mentioned in the headlines and covered in detail in the main broadcast
and then appears later when the broadcast headlines are repeated. a
similar heat map approach is currently used in the commercial player
called the limelight video platform1 (previously pluggd).

in addition to depicting the position of semantic content within the
speech media items, players can also represent segmentation structure.
an interface that displays the whole recording together with a posi-
tional window and a simultaneous an enlargement of the positional
window is described in [145]. in this case, signi   cantly more informa-
tion must be packed into the player timeline and a magnifying glass
metaphor becomes useful.

in figure 6.9, the player bar of the previously mentioned radio
oranje [108, 109] application is depicted as an example of the magnify-
ing glass metaphor. the player displays the entire speech in a timeline,
as well as a magni   ed view showing a window of 45 seconds around

1 http://www.delvenetworks.com

390 accessing information in spoken content

the current position of the cursor. above the play bar, the transcript of
the currently-playing segment is displayed, with the query word in bold
and a moving underline tracking the progression of the playback. the
magni   ed view makes it possible to also depict segmentation informa-
tion for the entire program in a relatively compact space without losing
detail.

segmentation structure is a key characteristic of speech media used
for depiction in players. segmentation patterns act as an identifying
   ngerprint for speech media. a global pattern may serve to implicitly
convey to the user information about the nature of the media, for exam-
ple, if it is a conversational interview or a political speech. further, dis-
playing segments within the playback interface provides the user with
the context of the results being examined, which is an important aid to
interpretation. segmentation provides an alternative for rewinding and
fast-forwarding: a user can jump back to the beginning of a segment
boundary. such jumps can be considered to be    intelligent    in so far as
the underlying segmentation provides a good representation of useful
semantic structure of the speech media.

the most appropriate use of segmentation structure will depend
on the segmentation information available, its quality and also the
types of user needs and tasks the scr system is designed to support.
a large number of segments can be simultaneously displayed on the
player interface by adopting the playlist metaphor and listing segments
vertically [157]. depending on the use case, space can be conserved
by dropping length information and representing each segment as an
equal-height line. horizontal layout with tracks is a choice preferred for
situations in which multiple overlapping segmentations exist. an early
example is the parc audio browser [76, 145], which displays separate
tracks for announcer, speaker, audience, silence, and applause.

in the ideal case, manual segmentation information should be
included in the metadata of the spoken content. however, this is often
not the case and automatic methods such as texttiling, presented in
subsection 5.3.4, must be applied to generate segmentation boundary
points. such methods inevitably make errors, dropping real boundary
markers in some places and inserting false boundary markers in others.

6.3 review and playback of results

391

the utility of the browsing interface may potentially be impacted by
these errors, particularly if they are numerous or occur at signi   cant
points in the semantic    ow of the content.

colors in the playback interface can be used to represent speaker
characteristics such as gender and speaker age (i.e., child or adult)
or even speaker identity or speech/non-speech di   erences [202]. the
patterns of segment alternation are a potential source of valuable infor-
mation that can aid the user in the selection process [157, 202, 203].
however, care must be taken, since alternating patterns may be di   cult
for users to grasp [253]. in general, the more immediately obvious it is
to the user why the system    chose    the particular object as relevant,
the more comfortable the user will feel using the interface. system
designers need to pay careful attention to how the player links to the
media    le.

the radio oranje application in figure 6.9, as mentioned in subsec-
tion 5.2.4, was implemented by using asr to create a forced alignment
of human-generated transcripts with spoken content. at this juncture,
we present further details on several other types of alignment that can
be used to improve the ability of an interface to visually represent the
spoken content of a speech media item or otherwise support the user   s
process of reviewing or examining results. alignment is not just limited
to transcripts, but rather for any sort of speech media, if it has a paral-
lel resource containing text, the two can potentially be aligned with the
help of asr. as previously mentioned in [130], slides of presentations
are aligned with the speech media. the slides provide structure for the
speech stream and act as surrogates for displaying spoken content in
the interface. in [52], a system is described that transcribes broadcast
news in real time, analyzes it for named entities and topic, formu-
lates a set of queries, and uses those queries to extract information
from other information sources (e.g., newspapers, www). excerpts
of information are inserted into the news stream in order to provide
viewers with background information on the topics treated. a similar
system for dutch-language broadcast news is presented in [190]. these
types of interfaces invite exploration and support the user in browsing
activity, to which we now turn in more detail.

392 accessing information in spoken content

6.3.1 browsing

the search process is only one way in which users seek information.
this point was made by [203], which highlighted the important role of
alternatives to the ranked list, such as spatial visualization of document
collections. an example of an approach that captures the entire docu-
ment collection in one structure is provided by the chinese broadcast
news prototype described in [166], which o   ers, in addition to a bottom
up retrieval functionality, the option of top-down browsing. a list of 20
news categories is provided on the portal page. these categories pro-
vide an entry point for a 2d topic tree. clicking on a category reveals a
grid representing the latent topical structure of that category. clicking
on a grid cell reveals a    ner breakdown of that category. if the user has
retrieved an item via a query submitted to the system, a click on this
item will reveal its position within the 2d tree structure.

exploration is supported by a browsing interface that visualizes the
structure of items and connections within the collection. browsers that
go above and beyond audio and video material, for example, by inte-
grating slides and notes, have been designated artifact browsers [299].
the design of suitable interfaces to support interaction is again crucial
to the success of such systems. note that there is not a hard bound-
ary separating playback interfaces, discussed in the previous subsection,
and browser interfaces. the di   erence lies in the emphasis that browser
interfaces put on presenting a complete picture and on supporting dis-
covering.

much research e   ort on browsing has been devoted to the domain
of meetings. interfaces can provide access to spoken audio via time-
speci   c links to a meeting   s agenda, to images made during the meeting,
for example of the whiteboard, or to automatically identify elements
such as topic, functional category (presentation, discussion, break) or
   hot spots    [59, 299]. we use a speci   c example from this domain to
illustrate browser interfaces: the jferret meeting browser developed by
the ami/amida project,2 depicted in figure 6.10.

2 http://www.amiproject.org/showcase/integrated-systems/meeting-archive-browsing

6.3 review and playback of results

393

fig. 6.10 ami meeting archive browser, jferret.

the browser supports the user in    nding elements of interest within
a recorded meeting. a range of data types is displayed for this purpose,
including speaker diarization, linked slides and transcripts. a curve
that plots speaker dominance within the meeting is also displayed. this
browser is intended as one part of a large system that would allow users
to explore a corpus of meetings, for example to search for the most
relevant pieces across meetings to allow the user to answer a speci   c
question. more information on ami/amida and the jferret meeting
browser can be found in [70, 119, 292, 293].

here, we have covered basic issues in the design of user interfaces
for scr, including query entry, results display and results playback.
it can be expected that new players will make great strides over what
has been shown here. especially for mobile devices in which screen real
estate is limited and in which interaction possibilities range beyond

394 accessing information in spoken content

keyboard and mouse input, innovation can be expected. however, the
underlying topics we have treated here, that is, graphically depicting
the relevance of items and also of enabling semantic playback, can be
expected to remain among the major challenges.

7

conclusion and outlook

the goal of this survey has been to provide an overview of research in
the area of spoken content retrieval (scr). we have taken the view
that scr involves not only    nding mentions of query words within
the speech signal, but also relating these mentions to the user informa-
tion need via a meaning-based notion of relevance. scr can be na    vely
characterized as a simple combination of automatic speech recogni-
tion (asr), which generates text transcripts from spoken audio, and
information retrieval (ir), which identi   es text documents relevant to
user information needs. in this survey, we have placed special emphasis
on discussing techniques and technologies that make it possible to go
beyond a na    ve combination and enable a tighter integration of asr
and ir. we argue that careful consideration of how asr and ir are
integrated within speci   c user scenarios will result in more e   ective
scr systems, better able to meet user needs. the survey has been
organized around    ve key challenges of scr that are important to
address in order to achieve an optimal integration of asr and ir for
a given scr use scenario.

after having presented a compact overview of the    eld of ir
and introducing these    ve challenges in overview of spoken content

395

396 conclusion and outlook

indexing and retrieval (cf. subsection 2.4    challenges for scr   ), the
survey continued, in automatic id103, with an overview
of asr technology. this overview set the background for a high-level
discussion of considerations that are advisable to take into account
when integrating asr and ir (cf. subsection 3.4,    considerations for
the combination of asr and ir   ).

the remainder of the survey presented material that delves deeper
into these considerations, addressing the    ve key challenges of scr.
we summarize the relationship of the individual topics we have pre-
sented, and issues we have examined, to these challenges in the following
summary:

    the challenge of handling uncertainty: in exploiting auto-
matic id103 output, we present techniques for
using asr output within an scr system, covering n-best
lists, lattices and confusion networks, con   dence scores and
also the use of subwords in scr. all of these techniques
go beyond the 1-best word-level transcript produced by the
recognizer. these techniques make it possible to represent
uncertainty that arises during the id103 pro-
cess. they bene   t from careful attention to considerations
of id172 and pruning. properly applied, they make
it possible to deal more e   ectively with uncertainty during
retrieval. they provide opportunities for a tighter integration
between asr and ir components, which can be optimized
for particular use scenarios.
    the challenge of covering all possible words: large vocabu-
lary continuous id103 (lvcsr) systems make
use of huge lexica containing many word entries. however,
in practice it is not possible to provide the lvcsr system
in advance with information concerning every word it could
possibly encounter in the incoming speech signal. for this
reason, the out of vocabulary (oov) problem remains a
major bottleneck for asr and also for scr. oov issues
can be addressed by better representations of uncertainty,
but we have also discussed the problem as a challenge in its

397

own right. techniques that have been developed to deal with
oov include subword units, lattices and fuzzy matching.
we reviewed systems that make hybrid use of combinations
of these techniques and also of    searching speech    techniques
such as spoken term detection (std), which can be used to
extract terms from the speech signal without necessitating
full lvcsr.
    the challenge of context: humans produce spoken language
in real-world contexts containing much more information
(e.g., concerning recent events or physical surroundings) than
what is encoded in the speech signal. lack of contextual infor-
mation can mean that an scr system makes a less accurate
match between user needs and speech media results. to con-
front this challenge, use can be made of multiple information
sources that go beyond the output of the asr system. in
spoken content retrieval beyond asr transcripts, we dis-
cussed how asr output can be supplemented in order to
improve scr. in particular, we addressed the use of expan-
sion techniques and of the exploitation of both manually and
automatically generated metadata.
    the challenge of structuring spoken content: many use sce-
narios for scr systems involve spoken content that either
lacks information about segment boundaries or is inherently
unstructured in nature. in particular, lack of formal structure
characterizes speech media that is produced spontaneously
and recorded outside the studio or in conversational settings.
techniques that can structure spoken audio have the poten-
tial to support the recognizer in two ways: by determining
the regions over which ir models should calculate relevance
to user information needs and by determining the form of the
results that should be presented to users. in spoken content
retrieval beyond asr transcripts, we presented techniques
for segmentation, diarization and compact representation of
speech media that can be used to address this challenge.
    the challenge of visualization and playback: user satisfaction
with an scr system is determined not only by the quality

398 conclusion and outlook

of the retrieval results, but also by the way in which these
results are presented. an scr system should provide repre-
sentations of spoken results that allow human users to quickly
and easily ascertain which results best suit their information
needs and select results to examine in more detail. in access-
ing information in spoken content, we discussed techniques
for result visualization and playback that can be used to
address issues of human interaction with the scr system.

the survey has made clear that the issues faced when developing
an scr system that optimizes the integration of asr and ir are quite
substantial. scr use scenarios can di   er widely from each other (e.g.,
declarative vs. conversational content, planned vs. spontaneous speech,
short vs. discursive queries and structured collection vs. unstructured
speech streams). for di   erent scenarios, the combination of approaches
that will yield the most e   ective scr system can be expected to vary.
fortunately, as re   ected in this survey, the techniques and technologies
available to address the challenges of scr are numerous and quite
sophisticated. the past two decades of scr research has yielded a
wealth of useful approaches, which can be drawn upon to implement
scr systems or to stimulate the development of new scr technologies.
in the introduction, four recent developments in speech media were
cited: volume, variety, function, and user attitude. the importance of
speech media can be expected to continue to grow, following these,
or related, lines. the convergence of these developments has led to
the opening of a new era of scr. moving forward, the driver of scr
research and development can be expected to be the user, whose infor-
mation needs will determine and de   ne new use scenarios for scr.
the goal of scr research should be to build systems that users    nd
genuinely useful. motivated by their satisfaction with scr technol-
ogy, users in turn will grow more accepting of technologies that make
possible spoken-content-based search in speech media.

references

[1] t. akiba, h. nishizaki, k. aikawa, t. kawahara, and t. matsui,    overview
of the ir for spoken documents task in ntcir-9 workshop,    in proceedings
of the nii test collection for ir systems workshop, pp. 223   235, 2011.

[2] c. alberti, m. bacchiani, a. bezman, c. chelba, a. drofa, h. liao, p. moreno,
t. power, a. sahuguet, m. shugrina, and o. siohan,    an audio indexing
system for election video material,    in proceedings of the ieee international
conference on acoustics, speech, and signal processing, pp. 4873   4876, 2009.
[3] j. allan,    perspectives on information retrieval and speech,    in information
retrieval techniques for speech applications, (a. r. coden, e. w. brown,
and s. srinivasan, eds.), pp. 323   326, springer berlin/heidelberg, 2002.

[4] j. allan,    topic detection and tracking: event-based information organiza-
tion,    in the kluwer international series on information retrieval, vol. 12,
springer, 2002.

[5] j. allan,    robust techniques for organizing and retrieving spoken docu-
ments,    eurasip journal on advances in signal processing, vol. 2003, no. 1,
pp. 103   114, 2003.

[6] x. anguera, c. wooters, b. peskin, and m. aguilo,    robust speaker segmen-
tation for meetings: the icsi-sri spring 2005 diarization system,    in pro-
ceedings of the nist machine learning for multimodal interaction, meeting
recognition workshop, pp. 26   38, 2005.

[7] j. archibald and w. o   grady, contemporary linguistics. bedford/st.

martin   s, 2001.

[8] e. arisoy, d. can, s. parlak, h. sak, and m. saraclar,    turkish broadcast
news transcription and retrieval,    ieee transactions on audio, speech, and
language processing, vol. 17, no. 5, pp. 874   883, 2009.

399

400 references

[9] b. arons,    speechskimmer: interactively skimming recorded speech,    in pro-
ceedings of the acm user interface software and technology conference,
atlanta, 1993.

[10] b. arons,    speechskimmer: a system for interactively skimming recorded
speech,    transactions on computer human interaction, vol. 4, no. 1, pp. 3   38,
1997.

[11] b. arons and e. mynatt,    the future of speech and audio in the interface: a

chi    94 workshop,    sigchi bulletin, vol. 26, no. 4, pp. 44   48, 1994.

[12] x. aubert,    an overview of decoding techniques for large vocabulary con-
tinuous id103,    computer speech & language, vol. 16, no. 1,
pp. 89   114, 2002.

[13] c. auzanne, j. s. garofolo, j. g. fiscus, and w. m. fisher,    automatic
language model adaptation for spoken document retrieval,    in proceedings
of the riao conference on content-based multimedia information access,
pp. 132   141, 2000.

[14] r. a. baeza-yates and b. ribeiro-neto, modern information retrieval: the
concepts and technology behind search. addison-wesley longman publish-
ing co., inc., 2010.

[15] b.-r. bai, l.-f. chien, and l.-s. lee,    very-large-vocabulary mandarin voice
message    le retrieval using speech queries,    in proceedings of the international
conference on spoken language processing, pp. 1950   1953, 1996.

[16] j. baker,    the dragon system     an overview,    ieee transactions on

acoustics, speech and signal processing, vol. 23, no. 1, pp. 24   29, 1975.

[17] s. banerjee and a. rudnicky,    a texttiling based approach to topic boundary

detection in meetings,    in proceedings of interspeech, 2006.

[18] f. batista, d. caseiro, n. mamede, and i. trancoso,    recovering capitaliza-
tion and punctuation marks for automatic id103: case study
for portuguese broadcast news,    speech communication, vol. 50, no. 10,
pp. 847   862, 2008.

[19] n. j. belkin, p. kantor, e. a. fox, and j. a. shaw,    combining the evidence
of multiple query representations for information retrieval,    information pro-
cessing & management, vol. 31, no. 3, pp. 431   448, 1995.

[20] m. benzeguiba, r. d. mori, o. deroo, s. dupont, t. erbes, d. jouvet, l. fis-
sore, p. laface, a. mertins, c. ris, r. rose, v. tyagi, and c. wellekens,
   automatic id103 and intrinsic speech variation,    in proceed-
ings of the ieee international conference on acoustics, speech, and signal
processing, pp. v/1021   v/1024, 2006.

[21] j. besser, m. larson, and k. hofmann,    podcast search: user goals and

retrieval technologies,    online information review, vol. 34, p. 3, 2010.

[22] h. bourlard, h. hermansky, and n. morgan,    towards increasing speech
recognition error rates,    speech communication, vol. 18, pp. 205   231, may
1996.

[23] h. bourlard and s. renals,    recognition and understanding of meetings
overview of the european ami and amida projects,    idiap-rr 27 techni-
cal report, 2008.

references

401

[24] s. brin and l. page,    the anatomy of a large-scale hypertextual web search
engine,    computer networks and isdn systems, vol. 30, no. 1   7, pp. 107   117,
1998.

[25] e. w. brown, s. srinivasan, a. coden, d. ponceleon, j. w. cooper, and
a. amir,    toward speech as a knowledge resource,    ibm systems journal,
vol. 40, no. 4, pp. 985   1001, 2001.

[26] m. g. brown, j. t. foote, g. j. f. jones, k. s. jones, and s. j. young,
   automatic content-based retrieval of broadcast news,    in proceedings of the
annual acm international conference on multimedia, pp. 35   43, 1995.

[27] m. g. brown, j. t. foote, g. j. f. jones, k. s. jones, and s. j. young,    open-
vocabulary speech indexing for voice and video mail retrieval,    in proceedings
of the acm international conference on multimedia, pp. 307   316, 1996.

[28] m. g. brown, j. t. foote, g. j. f. jones, k. sp  arck jones, and s. j. young,
   video mail retrieval using voice: an overview of the cambridge/olivetti
retrieval system,    in proceedings of the acm multimedia workshop on mul-
timedia database management systems, pp. 47   55, 1994.

[29] m. g. brown, j. t. foote, g. j. f. jones, k. sp  arck jones, and s. j. young,
   automatic content-based retrieval of broadcast news,    in proceedings of the
third acm international conference on multimedia, pp. 35   43, 1995.

[30] c. buckley, g. salton, j. allan, and a. singha,    automatic id183
using smart: trec 3,    in proceedings of the third text retrieval confer-
ence, pp. 69   80, 1995.

[31] j. butzberger, h. murveit, e. shriberg, and p. price,    spontaneous speech
e   ects in large vocabulary id103 applications,    in proceedings of
the workshop on speech and natural language, pp. 339   343, 1992.

[32] s. b  uuttcher, c. l. a. clarke, and g. v. cormack, information retrieval:

implementing and evaluating search engines. mit press, 2010.

[33] w. byrne, d. doermann, m. franz, s. gustman, j. hajic, d. oard,
m. picheny, j. psutka, b. ramabhadran, d. soergel, t. ward, and w.-j. zhu,
   automatic recognition of spontaneous speech for access to multilingual oral
history archives,    ieee transactions on speech and audio processing, spe-
cial issue on spontaneous speech processing, vol. 12, no. 4, pp. 420   435,
2004.

[34] j. carletta, s. ashby, s. bourban, m. flynn, m. guillemot, t. hain, j. kadlec,
k. vasilis, w. kraaij, m. kronenthal, g. lathoud, m. lincoln, a. lisowska,
i. mccowan, w. post, d. reidsma, and p. wellner,    the ami meeting
corpus: a pre-announcement,    in machine learning for multimodal interac-
tion, chapter 3, pp. 28   39, springer, 2006.

[35] j. carmichael, m. larson, j. marlow, e. newman, p. clough, o. oomen, and
s. sav,    multimodal indexing of digital audio-visual documents: a case study
for cultural heritage data,    in proceedings of the international workshop on
content-based multimedia indexing, pp. 93   100, london, u.k., 2008.

[36] j. k. chambers, p.trudgill, and n. schilling-estes, eds., the handbook of
language variation and change, blackwell handbooks in linguistics. wiley-
blackwell, 2004.

402 references

[37] c. chelba and a. acero,    position speci   c posterior lattices for indexing
speech,    in proceedings of the annual meeting on association for compu-
tational linguistics, pp. 443   450, morristown, nj, usa, 2005.

[38] c. chelba, t. j. hazen, and m. saraclar,    retrieval and browsing of spoken
content,    ieee signal processing magazine, vol. 25, no. 3, pp. 39   49, 2008.
[39] c. chelba, j. silva, and a. acero,    soft indexing of speech content for
search in spoken documents,    computer speech and language, vol. 21, no. 3,
pp. 458   478, 2007.

[40] b. chen,    exploring the use of latent topical information for statistical chi-
nese spoken document retrieval,    pattern recognition letters, vol. 27, no. 1,
pp. 9   18, 2006.

[41] b. chen, h.-m. wang, and l.-s. lee,    discriminating capabilities of syllable-
based features and approaches of utilizing them for voice retrieval of speech
information in mandarin chinese,    ieee transactions on speech and audio
processing, vol. 10, no. 5, pp. 303   314, 2002.

[42] f. r. chen and m. withgott,    the use of emphasis to automatically
summarize a spoken discourse,    in proceedings of the ieee international con-
ference on acoustics, speech, and signal processing, pp. i/229   i/232, 1992.

[43] s. f. chen and j. goodman,    an empirical study of smoothing techniques
for id38,    computer speech and language, vol. 13, no. 4,
pp. 359   393, 1999.

[44] s. s. chen and p. s. gopalakrishnan,    speaker, environment and channel
change detection and id91 via the bayesian information criterion,    in
proceedings of the darpa broadcast news transcription and understanding
workshop, 1998.

[45] y.-t. chen, b. chen, and h.-m. wang,    a probabilistic generative framework
for extractive broadcast news speech summarization,    ieee transactions on
audio, speech, and language processing, vol. 17, no. 1, pp. 95   106, 2009.

[46] t. cheong, r. kok, j. schuurman, and b. stukart,    improving the front-end
of kunststofzuiger,    final report project information retrieval, university
of amsterdam, 2008.

[47] t. k. chia, k. c. sim, h. li, and h. t. ng,    statistical lattice-based spoken
document retrieval,    acm transactions on information systems, vol. 28,
no. 1, pp. 1   30, 2010.

[48] f. y. y. choi,    advances in domain independent linear text segmentation,   
in proceedings of the north american chapter of the association for compu-
tational linguistics conference, pp. 26   33, 2000.

[49] m. g. christel and r. yan,    merging storyboard strategies and automatic
retrieval for improving interactive video search,    in proceedings of the acm
international conference on image and video retrieval, pp. 486   493, 2007.

[50] k. w. church,    speech and language processing: can we use the past to
predict the future?,    in text, speech and dialogue, vol. 3206 of lecture notes
in computer science, (p. sojka, i. kopecek, and k. pala, eds.), pp. 3   13,
springer berlin/heidelberg, 2004.

[51] j. clark, c. yallop, and j. fletcher, an introduction to id102 and phonol-

ogy (blackwell textbooks in linguistics). wiley-blackwell, 2007.

references

403

[52] a. r. coden and e. w. brown,    speech transcript analysis for automatic
search,    in proceedings of the annual hawaii international conference on
system sciences, 2001, 2001.

[53] a. r. coden, e. w. brown, and s. srinivasan,    acm sigir 2001 workshop
   information retrieval techniques for speech applications   ,    sigir forum,
vol. 36, no. 1, pp. 10   13, 2002.

[54] r. cole, l. hirschman, l. atlas, m. beckman, a. biermann, m. bush,
m. clements, l. cohen, o. garcia, b. hanson, h. hermansky, s. levin-
son, k. mckeown, n. morgan, d. g. novick, m. ostendorf, s. oviatt,
p. price, h. silverman, j. spiitz, a. waibel, c. weinstein, s. zahorian, and
v. zue,    the challenge of spoken language systems: research directions for the
nineties,    ieee transactions on speech and audio processing, vol. 3, no. 1,
pp. 1   21, 1995.

[55] p. r. comas, j. turmo, and l. marquez,    sibyl, a factoid id53
system for spoken documents,    acm transactions on information systems,
vol. 30, no. 3, 2012.

[56] f. crestani and h. du,    written versus spoken queries: a qualitative and
quantitative comparative analysis,    journal of the american society for infor-
mation science and technology, vol. 57, no. 7, pp. 881   890, 2006.

[57] b. croft, d. metzler, and t. strohman, search engines: information retrieval

in practice. addison wesley, 1st edition, february 2009.

[58] t. h. crystal, a. schmidt-nielsen, and e. marsh,    speech in noisy environ-
ments (spine) adds new dimension to id103 r&d,    in pro-
ceedings of the international conference on human language technology
research, pp. 212   216, 2002.

[59] r. cutler, y. rui, a. gupta, j. j. cadiz, i. tashev, l.-w. he, a. colburn,
z. zhang, z. liu, and s. silverberg,    distributed meetings: a meeting capture
and broadcasting system,    in proceedings of the acm international confer-
ence on multimedia, pp. 503   512, 2002.

[60] p. dai, u. iurgel, and g. rigoll,    a novel feature combination approach for
spoken document classi   cation with support vector machines,    in proceedings
of the acm special interest group on information retrieval (sigir) multi-
media information retrieval workshop, 2003.

[61] f. m. g. de jong, d. w. oard, w. f. l. heeren, and r. j. f. ordel-
man,    access to recorded interviews: a research agenda,    acm journal on
computing and cultural heritage, vol. 1, no. 1, pp. 3:1   3:27, 2008.

[62] f. m. g. de jong, r. j. f. ordelman, and m. a. h. huijbregts,    automated
speech and audio analysis for semantic access to multimedia,    in semantic
multimedia, vol. 4306 of lecture notes in computer science, chapter 18,
(y. avrithis, y. kompatsiaris, s. staab, and n. o   connor, eds.), pp. 226   240,
springer berlin/heidelberg: berlin, heidelberg, 2006.

[63] f. m. g. de jong, t. westerveld, and a. p. de vries,    multimedia search
without visual analysis: the value of linguistic and contextual information,   
ieee transactions on circuits and systems for video technology, vol. 17,
no. 3, pp. 365   371, 2007.

404 references

[64] s. deerwester,    improving information retrieval with latent

semantic
indexing,    in proceedings of the 51st asis annual meeting, vol. 25, (c. l.
borgman and e. y. h. pai, eds.), 1988.

[65] a. d  esilets, b. de bruijn, and j. martin,    extracting keyphrases from spoken
audio documents,    in information retrieval techniques for speech applica-
tions, pp. 36   50, london, uk, springer, 2002.

[66] g. dias, e. alves, and j. g. p. lopes,    topic segmentation algorithms for
text summarization and passage retrieval: an exhaustive evaluation,    in pro-
ceedings of the national conference on arti   cial intelligence     volume 2,
pp. 1334   1339, 2007.

[67] r. m. w. dixon, the rise and fall of languages. cambridge university

press, 1998.

[68] e. eide, h. gish, p. jeanrenaud, and a. mielke,    understanding and improv-
ing id103 performance through the use of diagnostic tools,    in
proceedings of the ieee international conference on acoustics, speech, and
signal processing, pp. i/221   i/224, 1995.

[69] m. el ayadi, m. s. kamel, and f. karray,    survey on speech emotion recog-
nition: features, classi   cation schemes, and databases,    pattern recognition,
vol. 44, no. 3, pp. 572   587, 2011.

[70] m. fap  so, p. smr  z, p. schwarz, i. sz  oke, j. schwarz,

, m.   cernock  y,
m. kara     at, and l. burget,    information retrieval from spoken documents,   
in proceedings of the international conference on intelligent text processing
and computational linguistics, pp. 410   416, 2006.

[71] m. federico,    a system for the retrieval of italian broadcast news,    speech

communication, vol. 32, no. 1   2, pp. 37   47, 2000.

[72] a. ferrieux and s. peillon,    phoneme-level indexing for fast and vocabulary-
independent voice/voice retrieval,    in proceedings of the esca workshop:
accessing information in spoken audio, 1999.

[73] j. fiscus,    a post-processing system to yield reduced word error rates: recog-
niser output voting error reduction (rover),    in proceedings of the ieee work-
shop on automatic id103 and understanding, pp. 347   352, 1997.
[74] j. g. fiscus, j. ajot, and j. s. garofolo,    the rich transcription 2007 meet-
ing recognition evaluation,    in multimodal technologies for perception of
humans, (r. stiefelhagen, r. bowers, and j. g. fiscus, eds.), pp. 373   389,
berlin/heidelberg: springer-verlag, 2008.

[75] j. g. fiscus, j. ajot, j. s. garofolo, and g. doddington,    results of the 2006
spoken term detection evaluation,    in proceedings of the acm special interest
group on information retrieval (sigir), searching spontaneous conversa-
tional speech workshop, pp. 45   50, amsterdam, netherlands, 2007.

[76] j. t. foote,    an overview of audio information retrieval,    multimedia systems,

vol. 7, no. 1, pp. 2   10, 1999.

[77] j. t. foote, g. j. f. jones, k. sp  arck jones, and s. j. young,    talker-
independent keyword spotting for information retrieval,    in proceedings of
eurospeech, pp. 2145   2148, 1995.

[78] m. fuller, m. tsagkias, e. newman, j. besser, m. larson, g. j. f. jones, and
m. de rijke,    using term clouds to represent segment-level semantic content of

references

405

podcasts,    in proceedings of the acm special interest group on information
retrieval (sigir), searching spontaneous conversational speech workshop,
2008.

[79] s. furui and t. kawahara,    transcription and distillation of spontaneous
speech,    in springer handbook of speech processing, chapter 32, (j. ben-
esty, m. m. sondhi, and y. a. huang, eds.), pp. 627   652, berlin/heidelberg:
springer berlin/heidelberg, 2008.

[80] s. furui, t. kikuchi, y. shinnaka, and c. hori,    speech-to-text and speech-to-
speech summarization of spontaneous speech,    ieee transactions on speech
and audio processing, vol. 12, no. 4, pp. 401   408, 2004.

[81] m. gales and s. j. young, the application of id48 in speech

recognition. now publishers inc., february 2008.

[82] j. s. garofolo, c. g. p. auzanne, and e. m. voorhees,    the trec spoken
document retrieval track: a success story,    in proceedings of the riao con-
ference on content-based multimedia information access, (j.-j. mariani and
d. harman, eds.), pp. 1   20, 2000.

[83] j. s. garofolo, e. m. voorhees, c. g. p. auzanne, and v. m. stanford,
   spoken document retrieval: 1998 evaluation and investigation of new met-
rics,    in proceedings of the esca workshop: accessing information in spoken
audio, pp. 1   7, 1999.

[84] j.-l. gauvain, l. lamel, and g. adda,    transcribing broadcast news for audio
and video indexing,    communications of the acm, vol. 13, no. 2, pp. 64   70,
2000.

[85] l. gillick, j. baker, j. bridle, m. hunt, y. ito, s. lowe, j. orlo   , b. peskin,
r. roth, and f. scattone,    application of large vocabulary continuous speech
recognition to topic and speaker identi   cation using telephone speech,    in
proceedings of the ieee international conference on acoustics speech, and
signal processing, pp. ii/471   ii474, 1993.

[86] j. glass, t. j. hazen, s. cyphers, i. malioutov, d. huynh, and r. barzila,
   recent progress in the mit spoken lecture processing project,    in proceedings
of interspeech, pp. 2556   2556, 2007.

[87] u. glavitsch and p. sch  auble,    a system for retrieving speech documents,    in
proceedings of the international acm special interest group on information
retrieval (sigir) conference on research and development in information
retrieval, pp. 168   176, 1992.

[88] u. glavitsch, p. sch  auble, and m. wechsler,    metadata for integrating speech
documents in a text retrieval system,    sigmod record, vol. 23, no. 4,
pp. 57   63, december 1994.

[89] a. goker, j. davies, and m. graham, information retrieval: searching in the

21st century. john wiley & sons, 2007.

[90] b. gold and n. morgan, speech and audio signal processing: processing and

perception of speech and music. john wiley & sons, inc., 1999.

[91] j. goldman, s. renals, s. g. bird, f. m. g. de jong, m. federico, c. fleis-
chhauer, m. kornbluh, l. lamel, d. w. oard, c. stewart, and r. wright,
   accessing the spoken word,    international journal on digital libraries,
vol. 5, no. 4, pp. 287   298, 2005.

406 references

[92] m. goto and j. ogata,    podcastle: recent advances of a spoken document
retrieval service improved by anonymous user contributions,    in proceedings
of interspeech, pp. 3073   3076, 2011.

[93] m. goto, j. ogata, and k. eto,    podcastle: a web 2.0 approach to speech

recognition research,    in proceedings of interspeech, pp. 2397   2400, 2007.

[94] s. gustman, d. soergel, d. oard, w. byrne, m. picheny, b. ramabhadran,
and d. greenberg,    supporting access to large digital oral history archives,   
in proceedings of the acm/ieee-cs joint conference on digital libraries,
pp. 18   27, 2002.

[95] t. hain, s. e. johnson, a. tuerk, p. c. woodland, and s. j. young,    segment
generation and id91 in the htk broadcast news transcription system,   
in proceedings of the broadcast news transcription and understanding work-
shop, pp. 133   137, 1998.

[96] t. hain, p. c. woodland, g. evermann, m. j. f. gales, x. liu, g. l. moore,
d. povey, and l. wang,    automatic transcription of conversational telephone
speech,    ieee transactions on speech and audio processing, vol. 13, no. 6,
pp. 1173   1185, 2005.

[97] d. hakkani-t  ur, f. bechet, g. riccardi, and g. t  ur,    beyond asr 1-best:
using word confusion networks in spoken language understanding,    computer
speech and language, vol. 20, no. 4, pp. 495   514, 2006.

[98] d. hakkani-t  ur and g. riccardi,    a general algorithm for word graph matrix
decomposition,    in proceedings of the ieee international conference on
acoustics, speech, and signal processing, pp. i/596   i/599, 2003.

[99] a. hanjalic and l.-q. xu,    a   ective video content representation and mod-

eling,    ieee transactions on multimedia, vol. 7, no. 1, pp. 143   154, 2005.

[100] j. h. l. hansen, r. huang, b. zhou, m. seadle, j. r. deller, a. r. gurijala,
m. kurimo, and p. angkititrakul,    speechfind: advances in spoken document
retrieval for a national gallery of the spoken word,    ieee transactions on
speech and audio processing, vol. 13, no. 5, pp. 712   730, 2005.

[101] a. haubold,    selection and ranking of text from highly imperfect transcripts
for retrieval of video content,    in proceedings of the international acm special
interest group on information retrieval (sigir) conference on research and
development in information retrieval, pp. 791   792, 2007.

[102] a. g. hauptmann,    id103 in the informedia digital video
library: uses and limitations,    in proceedings of the international confer-
ence on tools with arti   cial intelligence, p. 288, 1995.

[103] a. g. hauptmann and m. g. christel,    successful approaches in the trec
video retrieval evaluations,    in proceedings of the annual acm international
conference on multimedia, pp. 668   675, 2004.

[104] a. g. hauptmann and h. wactlar,    indexing and search of multimodal infor-
mation,    in proceedings of the ieee international conference on acoustics,
speech, and signal processing, pp. i/195   i/198, 1997.

[105] a. g. hauptmann and m. j. witbrock,    informedia: news-on-demand
multimedia information acquisition and retrieval,    in intelligent multimedia
information retrieval, (m. t. maybury, ed.), pp. 215   239, the mit press,
1997.

references

407

[106] m. a. hearst,    multi-paragraph segmentation of expository text,    in pro-
ceedings of the annual meeting on association for computational linguistics,
pp. 9   16, 1994.

[107] m. a. hearst, search user interfaces. cambridge university press, 2009.
[108] w. f. l. heeren and f. m. g. de jong,    disclosing spoken culture: user
interfaces for access to spoken word archives,    in proceedings of the british
hci group annual conference on human computer interaction, pp. 23   32,
2008.

[109] w. f. l. heeren, l. van der wer   , r. j. f. ordelman, a. van hessen, and
f. m. g. de jong,    radio oranje: searching the queen   s speech(es),    in
proceedings of the international acm special interest group on information
retrieval (sigir) conference on research and development in information
retrieval, p. 903, 2007.

[110] i. l. hetherington and v. w. zue,    new words: implications for continuous

id103,    in proceedings of eurospeech, pp. 2121   2124, 1993.

[111] d. hiemstra,    using language models for information retrieval,    phd thesis,

university of twente, 2001.

[112] j. hirschberg and s. whittaker,    studying search and archiving in a real audio
database,    in working notes of the aaai spring symposium on intelligent
integration and use of text, image, video and audio corpora, pp. 70   76,
1997.

[113] j. hirschberg, s. whittaker, d. hindle, f. pereira, and a. singhal,    finding
information in audio: a new paradigm for audio browsing/retrieval,    in pro-
ceedings of the esca workshop: accessing information in spoken audio,
pp. 117   122, 1999.

[114] t. hori, i. l. hetherington, t. j. hazen, and j. r. glass,    open-vocabulary
spoken utterance retrieval using confusion networks,    in proceedings of the
ieee international conference on acoustics, speech, and signal processing,
pp. iv/73   iv/76, 2007.

[115] y.-c. hsieh, y.-t. huang, c.-c. wang, and l.-s. lee,    improved spoken
document retrieval with dynamic key term lexicon and probabilistic latent
semantic analysis (plsa),    in proceedings of the ieee international con-
ference on acoustics, speech, and signal processing, pp. i/961   i/964, 2006.

[116] p.-y. hsueh and j. d. moore,    automatic topic segmentation and labeling
in multiparty dialogue,    in ieee spoken language technology workshop,
pp. 98   101, 2006.

[117] x. huang, a. acero, and h.-w. hon, spoken language processing: a guide

to theory, algorithm and system development. prentice hall, 2001.

[118] m. a. h. huijbregts, d. a. leeuwen, and f. m. g. jong,    the majority
wins: a method for combining speaker diarization systems,    in proceedings of
interspeech, pp. 924   927, 2009.

[119] a. jaimes, h. bourlard, s. renals, and j. carletta,    recording, summarizing,
and accessing meeting videos: an overview of the ami project,    in procee-
ings of the ieee international conference of image analysis and processing
workshops, pp. 59   64, 2007.

408 references

[120] d. a. james,    a system for unrestricted topic retrieval from radio news broad-
casts,    in proceedings of the ieee international conference on acoustics,
speech, and signal processing, pp. i/279   i/282, 1996.

[121] d. a. james,    the application of classical information retrieval techniques to

spoken documents,    phd thesis, university of cambridge, june 1995.

[122] d. a. james and s. j. young,    a fast lattice-based approach to vocabulary
independent wordspotting,    in proceedings of the ieee international confer-
ence on acoustics, speech, and signal processing, pp. i/377   i/380, 1994.

[123] a. janin, l. gottlieb, and g. friedland,    joke-o-mat hd: browsing sitcoms
with human derived transcripts,    in proceedings of the acm international
conference on multimedia, pp. 1591   1594, 2010.

[124] f. jelinek, statistical methods for id103 (language, speech, and

communication). the mit press, 1998.

[125] h. jiang,    con   dence measures for id103: a survey,    speech

communication, vol. 45, no. 4, pp. 455   470, 2005.

[126] r. jin and a. g. hauptmann,    automatic title generation for spoken broad-
cast news,    in proceedings of the international conference on human lan-
guage technology research, pp. 1   3, 2001.

[127] s. e. johnson, p. jourlin, k. s. jones, and p. woodland,    spoken document
retrieval for trec-9 at cambridge university,    in proceedings of the text
retrieval conference, (e. voorhees and d. harman, eds.), pp. 117   126, 2000.
[128] g. j. f. jones,    exploring the incorporation of acoustic information into term
weights for spoken document retrieval,    in proceedings of the bcs informa-
tion retrieval specialist group colloquium on information retrieval research,
pp. 118   131, 2000.

[129] g. j. f. jones and c. h. chan,    multimedia information extraction,    chapter
a   ect-based indexing for multimedia data. ieee computer society press,
2012.

[130] g. j. f. jones and r. edens,    automated alignment and annotation of
audio-visual presentations,    in research and advanced technology for digi-
tal libraries, vol. 2458 of lecture notes in computer science, (m. agosti and
c. thanos, eds.), pp. 187   196, springer berlin/heidelberg, 2002.

[131] g. j. f. jones, j. t. foote, k. sp  arck jones, and s. j. young,    video mail
retrieval: the e   ect of word spotting accuracy on precision,    in proceedings
of the ieee international conference on acoustics, speech, and signal pro-
cessing, pp. i/309   i/312, 1995.

[132] g. j. f. jones, j. t. foote, k. sp  arck jones, and s. j. young,    retrieving
spoken documents by combining multiple index sources,    in proceedings of the
international acm special interest group on information retrieval (sigir)
conference on research and development in information retrieval, pp. 30   38,
1996.

[133] g. j. f. jones and d. a. james,    a critical review of state-of-the-art technolo-
gies for cross-language speech retrieval,    in cross-language text and speech
retrieval papers from the 1997 aaai spring symposium, technical report
ss-97-05, menlo park, california, 1997.

references

409

[134] g. j. f. jones and a. m. lam-adesina,    exeter at clef 2003: cross-
language spoken document retrieval experiments,    in comparative evaluation
of multilingual information access systems, vol. 3237 of lecture notes in
computer science, (c. peters, j. gonzalo, m. braschler, and m. kluck, eds.),
pp. 553   558, springer berlin/heidelberg, 2004.

[135] g. j. f. jones, k. zhang, e. newman, and a. m. lam-adesina,    examining
the contributions of automatic speech transcriptions and metadata sources for
searching spontaneous conversational speech,    in proceedings of the acm spe-
cial interest group on information retrieval (sigir) searching spontaneous
conversational speech workshop, 2007.

[136] p. jourlin, s. e. johnson, k. sp  arck jones, and p. c. woodland,    improving
retrieval on imperfect speech transcriptions (poster abstract),    in proceedings
of the international acm special interest group on information retrieval
(sigir) conference on research and development in information retrieval,
pp. 283   284, 1999.

[137] p. jourlin, s. e. johnson, k. sp  ark jones, and p. c. woodland,    spoken
id194s for probabilistic retrieval,    speech communication,
vol. 32, pp. 21   36, 2000.

[138] b. h. juang and l. r. rabiner,    automatic id103     a brief his-
tory of the technology,    in elsevier encyclopedia of language and linguistics,
second edition, elsevier, 2005.

[139] d. jurafsky and j. h. martin, speech and language processing: an introduc-
tion to natural language processing, computational linguistics and speech
recognition. prentice hall, 2008.

[140] v. kalnikait  e and s. whittaker,    social summarization: does social feedback
improve access to speech data?,    in proceedings of the acm conference on
computer supported cooperative work, pp. 9   12, 2008.

[141] s. kazemian, f. rudzicz, g. penn, and c. munteanu,    a critical assessment
of spoken utterance retrieval through approximate lattice representations,    in
proceeding of the acm international conference on multimedia information
retrieval, pp. 83   88, 2008.

[142] t. kemp and t. schaaf,    estimating con   dence using word lattices,    in pro-

ceedings of eurospeech, pp. 827   830, 1997.

[143] j. kilgour, j. carletta, and s. renals,    the ambient spotlight: queryless
desktop search from meeting speech,    in proceedings of the acm multimedia
searching spontaneous conversational speech workshop, pp. 49   52, 2010.

[144] w. kim and j. hansen, speech   nd: advances in rich content based spoken

document retrieval. pp. 173   187. information science reference, 2009.

[145] d. g. kimber, l. d. wilcox, f. r. chen, and t. p. moran,    speaker segmen-
tation for browsing recorded audio,    in conference companion on human
factors in computing systems, pp. 212   213, 1995.

[146] t. kinnunen and h. li,    an overview of text-independent speaker recogni-
tion: from features to supervectors,    speech communication, vol. 52, no. 1,
pp. 12   40, 2010.

[147] j. k  ohler,    multilingual phone models for vocabulary-independent speech
recognition tasks,    speech communication, vol. 35, no. 1   2, pp. 21   30, 2001.

410 references

[148] k. koumpis and s. renals,    content-based access to spoken audio,    ieee

signal processing magazine, vol. 22, no. 5, pp. 61   69, 2005.

[149] k. koumpis and s. renals,    id54 of voicemail messages
using lexical and prosodic features,    acm transactions on speech and lan-
guage processing, vol. 2, no. 1, pp. 1   24, 2005.

[150] f. kubala, s. colbath, d. liu, and j. makhoul,    rough   n   ready: a meeting

recorder and browser,    acm computing surveyes, vol. 1, no. 2, 1999.

[151] j. kupiec, d. kimber, and v. balasubramanian,    speech-based retrieval using
semantic co-occurrence    ltering,    in proceedings of the international confer-
ence on human language technology research, pp. 350   354, 1994.

[152] m. kurimo,    thematic indexing of spoken documents by using self-organizing

maps,    speech communication, vol. 38, no. 1   2, pp. 29   45, 2002.

[153] m. kurimo and v. turunen,    an evaluation of a spoken document retrieval
baseline system in    nnish,    in proceedings of interspeech, pp. 1585   1588, 2004.
[154] a. m. lam-adesina and g. j. f. jones,    using string comparison in context
for improved relevance feedback in di   erent text media,    in proceedings of the
string processing on information retrieval conference, pp. 229   241, 2006.

[155] m. larson and s. eickeler,    using syllable-based indexing features and lan-
guage models to improve german spoken document retrieval,    in proceedings
of interspeech, pp. 1217   1220, 2003.

[156] m. larson, m. eskevich, r. j. f. ordelman, c. ko   er, s. schmiedeke, and
g. j. f. jones,    overview of mediaeval 2011 rich speech retrieval task and
genre tagging task,    in working notes proceedings of the mediaeval work-
shop, ceur-ws.org, 2011.

[157] m. larson and j. k  ohler,    structured audio player: supporting radio archive
work   ows with automatically generated structure metadata,    in proceedings
of the riao conference on large-scale semantic access to content (text,
image, video and sound), 2007.

[158] m. larson, e. newman, and g. j. f. jones,    overview of videoclef
2008: automatic generation of topic-based feeds for dual language audio-
visual content,    in proceedings of the cross-language evaluation forum
conference on evaluating systems for multilingual and multimodal informa-
tion access, (c. peters, t. deselaers, n. ferro, j. gonzalo, a. pe  nas, g. j. f.
jones, m. kurimo, t. mandl, and v. petras, eds.), pp. 906   917, springer
berlin/heidelberg, 2009.

[159] m. larson, e. newman, and g. j. f. jones,    overview of videoclef 2009:
new perspectives on speech-based multimedia content enrichment,    in multi-
lingual information access evaluation ii. multimedia experiments, vol. 6242
of lecture notes in computer science, (c. peters, b. caputo, j. gonzalo,
g. j. f. jones, j. kalpathy-cramer, h. m  uller, and t. tsikrika, eds.),
pp. 354   368, springer berlin/heidelberg, 2010.

[160] m. larson, m. soleymani, m. eskevich, p. serdyukov, r. ordelman, and
g. j. f. jones,    the community and the crowd: developing large-scale data
collections for multimedia benchmarking,    ieee multimedia, ieee computer
society digital library. ieee computer society, 15 may 2012.

[161] m. larson, m. soleymani, p. serdyukov, s. rudinac, c. wartena, v. a. mur-
dock, g. friedland, r. j. f. ordelman, and g. j. f. jones,    automatic tagging

references

411

and geotagging in video collections and communities,    in proceedings of the
1st acm international conference on multimedia retrieval, pp. 1   51, 2011.
[162] m. larson, m. tsagkias, j. he, and m. de rijke,    investigating the global
semantic impact of id103 error on spoken content collections,   
in advances in information retrieval. proceedings of the european con-
ference on ir research, vol. 5478 of lecture notes in computer science,
(m. boughanem, c. berrut, j. mothe, and c. soule-dupuy, eds.), pp. 755   760,
springer berlin/heidelberg, 2009.

[163] j. laver, principles of id102 (cambridge textbooks in linguistics).

cambridge university press, 1994.

[164] v. lavrenko and w. b. croft,    relevance based language models,    in proceed-
ings of the international acm special interest group on information retrieval
(sigir) conference on research and development in information retrieval,
pp. 120   127, 2001.

[165] d. lee and g. g. lee,    a korean spoken document retrieval system for lecture
search,    in proceedings of the acm special interest group on information
retrieval (sigir) searching spontaneous conversational speech workshop,
2008.

[166] l.-s. lee and b. chen,    spoken document understanding and organization,   

ieee signal processing magazine, vol. 22, no. 5, pp. 42   60, 2005.

[167] s.-w. lee, k. tanaka, and y. itoh,    combining multiple subword represen-
tations for open-vocabulary spoken document retrieval,    in proceedings of the
ieee international conference on acoustics, speech, and signal processing,
vol. 1, pp. 505   508, 2005.

[168] b. liu and d. w. oard,    one-sided measures for evaluating ranked retrieval
e   ectiveness with spontaneous conversational speech,    in proceedings of the
international acm special interest group on information retrieval (sigir)
conference on research and development in information retrieval, pp. 673   
674, 2006.

[169] y. liu, e. shriberg, a. stolcke, d. hillard, m. ostendorf, and m. harper,
   enriching id103 with automatic detection of sentence bound-
aries and dis   uencies,    ieee transactions on audio, speech, and language
processing, vol. 14, no. 5, pp. 1526   1540, 2006.

[170] w.-k. lo, h. meng, and p. c. ching,    cross-language spoken document
retrieval using id48-based retrieval model with multi-scale fusion,    acm
transactions on asian language information processing, vol. 2, no. 1,
pp. 1   26, 2003.

[171] j. l  o   er, k. biatov, c. eckes, and j. k  ohler,    ifinder: an mpeg-7-based
retrieval system for distributed multimedia content,    in proceedings of the
acm international conference on multimedia, pp. 431   435, 2002.

[172] b. logan, p. moreno, and o. deshmukh,    word and sub-word indexing
approaches for reducing the e   ects of oov queries on spoken audio,    in
proceedings of the international conference on human language technology
research, pp. 31   35, 2002.

[173] b. logan and j. m. v. thong,    confusion-based id183 for oov
words in spoken document retrieval,    in proceedings of interspeech, pp. 1997   
2000, 2002.

412 references

[174] b. logan, j. m. van thong, and p. j. moreno,    approaches to reduce the
e   ects of oov queries on indexed spoken audio,    ieee transactions on
multimedia, vol. 7, no. 5, pp. 899   906, 2005.

[175] i. malioutov and r. barzilay,    minimum cut model for spoken lecture seg-
mentation,    in proceedings of the international conference on computational
linguistics and the annual meeting of the association for computational
linguistics, pp. 25   32, 2006.

[176] j. mamou, d. carmel, and r. hoory,    spoken document retrieval from call-
center conversations,    in proceedings of the international acm special interest
group on information retrieval (sigir) conference on research and devel-
opment in information retrieval, pp. 51   58, 2006.

[177] l. mangu, e. brill, and a. stolcke,    finding consensus among words: lattice-
based word error minimisation,    computer speech and language, vol. 14,
no. 4, pp. 373   400, 2000.

[178] c. d. manning, p. raghavan, and h. sch  utze, introduction to information

retrieval. cambridge university press, 2008.

[179] j. mauclair, y. est`eve, s. petitrenaud, and p. del  eglise,    automatic detection
of well recognized words in automatic speech transcription,    in proceedings of
the international conference on language resources and evaluation, 2006.

[180] m. t. maybury, ed., intelligent multimedia information retrieval. the mit

press, 1997.

[181] j. mcdonough, k. ng, p. jeanrenaud, h. gish, and j. r. rohlicek,
   approaches to topic identi   cation on the switchboard corpus,    in proceed-
ings of the ieee international conference on acoustics, speech, and signal
processing, pp. i/385   i/388, 1994.

[182] c.-h. meng, h.-y. lee, and l.-s. lee,    improved lattice-based spoken docu-
ment retrieval by directly learning from the evaluation measures,    in proceed-
ings of the ieee international conference on acoustics, speech, and signal
processing, pp. 4893   4896, 2009.

[183] h. meng, b. chen, s. khudanpur, g. levow, w. lo, d. w. oard, p. schone,
k. tang, h. wang, and j. wang,    mandarin-english information (mei):
investigating translingual speech retrieval,    computer speech and language,
vol. 18, no. 2, pp. 163   179, 2004.

[184] t. mertens and d. schneider,    e   cient subword lattice retrieval for ger-
man spoken term detection,    in proceedings of the ieee international
conference on acoustics, speech, and signal processing, pp. 4885   4888,
2009.

[185] t. mertens, d. schneider, and j. k  ohler,    merging search spaces for spoken

term detection,    in proceedings of interspeech, pp. 2127   2130, 2009.

[186] d. metzler and w. b. croft,    a markov random    eld model for term depen-
dencies,    in proceedings of the international acm special interest group on
information retrieval (sigir) conference on research and development in
information retrieval, pp. 472   479, 2005.

[187] g. mishne and m. de rijke,    boosting web retrieval through query
operations,    in advances in information retrieval, pp. 502   516, springer,
2005.

references

413

[188] j. mizuno, j. ogata, and m. goto,    a similar content retrieval method for
podcast episodes,    in ieee spoken language technology workshop, pp. 297   
300, 2009.

[189] l. l. molgaard, k. w. jorgensen, and l. k. hansen,    castsearch     context
based spoken document retrieval,    in proceedings of the ieee international
conference on acoustics, speech, and signal processing, pp. iv/93   iv/96,
2007.

[190] j. morang, r. j. f. ordelman, f. m. g. de jong, and a. j. van hessen,
   infolink: analysis of dutch broadcast news and cross-media browsing,    in
ieee international conference on multimedia and expo, pp. 1582   1585, 2005.
[191] n. moreau, s. jin, and t. sikora,    comparison of di   erent phone-based spoken
document retrieval methods with text and spoken queries,    in proceedings of
interspeech, pp. 641   644, 2005.

[192] n. moreau, h.-g. kim, and t. sikora,    phonetic confusion based docu-
ment expansion for spoken document retrieval,    in proceedings of interspeech,
pp. 1593   1596, 2004.

[193] p. j. moreno, j. m. van thong, b. logan, and g. j. f. jones,    from multime-
dia retrieval to knowledge management,    computer, vol. 35, no. 4, pp. 58   66,
2002.

[194] c. munteanu, r. baecker, g. penn, e. toms, and d. james,    the e   ect of
id103 accuracy rates on the usefulness and usability of webcast
archives,    in proceedings of the special interest group on computer-human
interaction (sigchi) conference on human factors in computing systems,
pp. 493   502, 2006.

[195] c. ng, r. wilkinson, and j. zobel,    experiments in spoken document retrieval
using phoneme id165s,    speech communication, vol. 32, no. 1   2, pp. 61   77,
2000.

[196] k. ng and v. w. zue,    subword unit representations for spoken document

retrieval,    in proceedings of eurospeech, pp. 1607   1610, 1997.

[197] k. ng and v. w. zue,    subword-based approaches for spoken document

retrieval,    speech communication, vol. 32, no. 3, pp. 157   186, 2000.

[198] t. niesler,    language-dependent state id91 for multilingual acoustic

modelling,    speech communication, vol. 49, no. 6, pp. 453   463, 2007.

[199] nist, the spoken term detection (std) 2006 evaluation plan, 2006.
[200] j. nouza, j.   z  d  ansk  y, p.   cerva, and j. koloren  c,    a system for informa-
tion retrieval from large records of czech spoken data,    in text, speech
and dialogue, vol. 4188 of lecture notes in computer science, (p. sojka,
i. kope  cek, and k. pala, eds.), pp. 485   492, springer berlin/heidelberg, 2006.
[201] p. nowell and r. k. moore,    the application of id145
techniques to non-word based topic spotting,    in proceedings of eurospeech,
pp. 1355   1358, 1995.

[202] d. w. oard,    speech-based information retrieval

for digital

libraries,   

technical report cs-tr-3778, university of maryland, 1997.

[203] d. w. oard,    user interface design for speech-based retrieval,    bulletin of
the american society for information science and technology, vol. 26, no. 5,
pp. 20   22, 2000.

414 references

[204] d. w. oard, d. soergel, d. doermann, x. huang, c. g. murray, j. wang,
b. ramabhadran, m. franz, s. gustman, j. may   eld, l. kharevych, and
s. strassel,    building an information retrieval test collection for spontaneous
conversational speech,    in proceedings of the international acm special inter-
est group on information retrieval (sigir) conference on research and
development in information retrieval, pp. 41   48, 2004.

[205] d. w. oard, j. wang, g. j. f. jones, r. white, p. pecina, d. soergel,
x. huang, and i. shafran,    overview of the clef-2006 cross-language speech
retrieval track,    in evaluation of multilingual and multi-modal informa-
tion retrieval, vol. 4730 of lecture notes in computer science, (c. peters,
p. clough, f. gey, j. karlgren, b. magnini, d. oard, m. de rijke, and
m. stempfhuber, eds.), pp. 744   758, springer berlin/heidelberg, 2007.

[206] n. a. o   connor, h. lee, a. f. smeaton, g. j. f. jones, e. cooke, h. le
borgne, and c. gurrin,    fischlar-trecvid-2004: combined text- and image-
based searching of video archives,    in proceedings of the ieee international
symposium on circuits and systems, 2006.

[207] j. ogata, m. goto, and k. eto,    automatic transcription for a web 2.0 service

to search podcasts,    in proceedings of interspeech, pp. 2617   2620, 2007.

[208] j. s. olsson,    vocabulary independent discriminative term frequency

estimation,    in proceedings of interspeech, pp. 2187   2190, 2008.

[209] j. s. olsson and d. w. oard,    combining lvcsr and vocabulary-
independent ranked utterance retrieval for robust speech search,    in proceed-
ings of the international acm special interest group on information retrieval
(sigir) conference on research and development in information retrieval,
pp. 91   98, 2009.

[210] j. s. olsson and d. w. oard,    phrase-based query degradation modeling for
vocabulary-independent ranked utterance retrieval,    in proceedings of human
language technologies conferemce of the north american chapter of the
association for computational linguistics, pp. 182   190, 2009.

[211] r. j. f. ordelman, w. f. l. heeren, m. a. h. huijbregts, f. m. g. de jong,
and d. hiemstra,    towards a   ordable disclosure of spoken heritage archives,   
journal of digital information, special issue on information access to cul-
tural heritage, vol. 10, no. 6, 2009.

[212] r. j. f. ordelman, a. j. van hessen, and f. m. g. de jong,    speech recogni-
tion issues for dutch spoken document retrieval,    in proceedings of the inter-
national conference on text, speech and dialogue, pp. 258   265, 2001.

[213] g. paa  , e. leopold, m. larson, j. kindermann, and s. eickeler,    id166 clas-
si   cation using sequences of phonemes and syllables,    in principles of data
mining and knowledge discovery, vol. 2431 of lecture notes in computer sci-
ence, (t. elomaa, h. mannila, and h. toivonen, eds.), pp. 373   384, springer
berlin/heidelberg, 2002.

[214] d. s. pallett, j. s. garofolo, and j. g. fiscus,    measurements in support
of research accomplishments,    communications of the acm, vol. 43, no. 2,
pp. 75   79, 2000.

[215] y.-c. pan and l.-s. lee,    performance analysis for lattice-based speech index-
ing approaches using words and subword units,    ieee transactions on speech
and audio processing, vol. 18, no. 6, pp. 1562   1574, 2010.

references

415

[216] s. p  araic, m. wechsler, and p. sch  auble,    cross-language speech retrieval:
establishing a baseline performance,    in proceedings of the international
acm special interest group on information retrieval (sigir) conference
on research and development in information retrieval, pp. 99   108, 1997.

[217] p. pecina, p. ho   mannova, g. j. f. jones, y. zhang, and d. w. oard,
   overview of the clef 2007 cross-language speech retrieval track,    in
advances in multilingual and multimodal information retrieval, vol. 5152
of lecture notes in computer science, (c. peters, v. jijkoun, t. mandl,
h. m  uller, d. w. oard, a. pe  nas, v. petras, and d. santos, eds.), pp. 674   686,
springer berlin/heidelberg, 2008.

[218] j. m. ponte and w. b. croft,    a id38 approach to information
retrieval,    in proceedings of the international acm special interest group on
information retrieval (sigir) conference on research and development in
information retrieval, pp. 275   281, 1998.

[219] j. portelo, m. bugalho, i. trancoso, j. neto, a. abad, and a. serralheiro,
   non-speech audio id37,    in proceedings of the ieee international
conference on acoustics, speech, and signal processing, pp. 1973   1976, 2009.
[220] a. potamianos and s. narayanan,    robust recognition of children   s speech,   
ieee transactions on speech and audio processing, vol. 11, no. 6,
pp. 603   616, 2003.

[221] l. rabiner and b.-h. juang, fundamentals of id103. prentice

hall, 1993.

[222] l. r. rabiner,    a tutorial on id48 and selected applications
in id103,    proceedings of the ieee, vol. 77, no. 2, pp. 257   286,
1989.

[223] d. r. reddy,    id103 by machine: a review,    proceedings of the

ieee, vol. 64, no. 4, pp. 501   531, 1976.

[224] g. rigoll,    the alert system: advanced broadcast id103
technology for selective dissemination of multimedia information,    in pro-
ceedings of the ieee workshop on automatic id103 and under-
standing, pp. 301   306, 2001.

[225] s. e. robertson,    on term selection for id183,    journal of docu-

mentation, vol. 46, no. 4, pp. 359   364, 1990.

[226] s. e. robertson and k. sp  ark jones,    relevance weighting of search terms,   
journal of the american society of information science, vol. 27, no. 3,
pp. 129   146, 1976.

[227] s. e. robertson and s. walker,    some simple e   ective approximations to
for probabilistic weighted retrieval,    in proceedings
the 2-poisson model
of the international acm special interest group on information retrieval
(sigir) conference on research and development in information retrieval,
pp. 232   241, 1994.

[228] s. e. robertson, s. walker, s. jones, m. m. hancock-beaulieu, and
m. gatford,    okapi at trec-3,    in proceedings of the text retrieval con-
ference, pp. 109   126, 1996.

[229] s. e. robertson, h. zaragoza, and m. j. taylor,    simple bm25 extension to
multiple weighted    elds,    in proceedings of the international conference on
information and knowledge management, pp. 42   49, 2004.

416 references

[230] r. c. rose,    techniques for information retrieval from speech messages,   

lincoln laboratory journal, vol. 4, no. 1, pp. 45   60, 1991.

[231] r. c. rose, e. i. chang, and r. p. lippmann,    techniques for information
retrieval from voice messages,    in proceedings of the ieee international con-
ference on acoustics, speech, and signal processing, pp. i/317   i/320, 1991.

[232] r. c. rose and d. b. paul,    a hidden markov model based keyword
recognition system,    in proceedings of the ieee international conference on
acoustics, speech, and signal processing, pp. i/129   i/132, 1990.

[233] s. rosset, o. galibert, g. adda, and e. bilinski,    the limsi qast systems:
comparison between human and automatic rules generation for question-
answering on speech transcriptions,    in proceedings of the ieee workshop
on automatic id103 and understanding, pp. 647   652, 2007.

[234] y. rui, a. gupta, and a. acero,    automatically extracting highlights for tv
baseball programs,    in proceedings of the acm international conference on
multimedia, pp. 105   115, 2000.

[235] g. salton and c. buckley,    term-weighting approaches in automatic
text retrieval,    information processing and management, vol. 24, no. 5,
pp. 513   523, 1988.

[236] m. sanderson and f. crestani,    mixing and merging for spoken docu-
ment retrieval,    in proceedings of the european conference on research and
advanced technology for digital libraries, pp. 397   407, 1998.

[237] m. sanderson and x.-m. shou,    search of spoken documents retrieves well
recognized transcripts,    in advances in information retrieval. proceedings
of the european conference on ir research, (g. amati, c. carpineto, and
g. romano, eds.), pp. 505   516, springer berlin/heidelberg, 2007.

[238] m. saraclar and r. w. sproat,    lattice-based search for spoken utterance
retrieval,    in proceedings of the human language technology conference of
the north american chapter of the association for computational linguistics,
pp. 129   136, 2004.

[239] t. schaaf and t. kemp,    con   dence measures for spontaneous speech recog-
nition,    in proceedings of the ieee international conference on acoustics,
speech, and signal processing, pp. ii/875   ii/878, 1997.

[240] p. sch  auble and u. glavitsch,    assessing the retrieval e   ectiveness of a speech
retrieval system by simulating recognition errors,    in proceedings of the work-
shop on human language technology, pp. 347   349, 1994.

[241] p. sch  auble and m. wechsler,    first experiences with a system for content
based retrieval of information from speech recordings,    in proceedings of the
ijcai workshop on intelligent multimedia information retrieval, pp. 59   69,
1995.

[242] c. schmandt,    the intelligent ear: a graphical interface to digital audio,   
in proceedings of the internationl conference on cybernetics and society,
pp. 393   397, 1981.

[243] d. schneider,    holistic vocabulary independent spoken term detection,    phd

thesis, university of bonn, 2011.

[244] b. schuller, g. rigoll, and m. lang,    speech emotion recognition combining
acoustic features and linguistic information in a hybrid support vector

references

417

machine-belief network architecture,    in proceedings of the ieee international
conference on acoustics, speech, and signal processing, pp. i/577   i/580,
2004.

[245] a. siegler, m. a. amd berger, m. witbrock, and a. hauptmann,    experiments
in spoken document retrieval at cmu,    in proceedings of the text retrieval
conference, pp. 319   326, 1998.

[246] m. siegler and m. witbrock,    improving the suitability of imperfect transcrip-
tions for information retrieval from spoken documents,    in proceedings of the
ieee international conference on acoustics, speech, and signal processing,
pp. i/505   i/508, 1999.

[247] m. a. siegler,    integration of continuous id103 and information
retrieval for mutually optimal performance,    phd thesis, carnegie mellon
university, 1999.

[248] j. silva, c. chelba, and a. acero,    integration of metadata in spoken doc-
ument search using position speci   c posterior latices,    in proceedings of the
ieee spoken language technology workshop, pp. 46   49, 2006.

[249] a. singhal, c. buckley, and m. mitra,    pivoted document length normal-
ization,    in proceedings of the international acm special interest group on
information retrieval (sigir) conference on research and development in
information retrieval, pp. 21   29, 1996.

[250] a. singhal, j. choi, d. hindle, d. d. lewis, and f. pereira,    at&t at trec-

7,    in proceedings of the text retrieval conference, pp. 239   252, 1999.

[251] a. singhal and f. pereira,    document expansion for speech retrieval,    in
proceedings of the international acm special interest group on information
retrieval (sigir) conference on research and development in information
retrieval, pp. 34   41, 1999.

[252] o. siohan and m. bacchiani,    fast vocabulary-independent audio search using

path-based graph indexing,    in proceedings of interspeech, pp. 53   56, 2005.

[253] l. slaughter, d. w. oard, v. l. warnick, j. l. harding, and g. j. wilkerson,
   a graphical interface for speech-based retrieval,    in proceedings of the acm
conference on digital libraries, pp. 305   306, 1998.

[254] a. f. smeaton, m. morony, g. quinn, and r. scaife,    taisc  eala    : infor-
mation retrieval from an archive of spoken radio news,    in research and
advanced technology for digital libraries, vol. 1513 of lecture notes in com-
puter science, (c. nikolaou and c. stephanidis, eds.), pp. 429   442, springer
berlin/heidelberg, 1998.

[255] a. f. smeaton, p. over, and w. kraaij,    evaluation campaigns and
trecvid,    in proceedings of the acm international workshop on multi-
media information retrieval, pp. 321   330, 2006.

[256] k. sp  arck jones, g. j. f. jones, j. t. foote, and s. j. young,    experiments in
spoken document retrieval,    information processing and management, vol. 32,
no. 4, pp. 399   417, 1996.

[257] s. srinivasan and d. petkovic,    phonetic confusion matrix based spoken doc-
ument retrieval,    in proceedings of the international acm special interest
group on information retrieval (sigir) conference on research and devel-
opment in information retrieval, pp. 81   87, 2000.

418 references

[258] l. a. stark, s. whittaker, and j. hirschberg,    asr satis   cing: the e   ects of
asr accuracy on speech retrieval,    in proceedings of interspeech, pp. 1069   
1072, 2000.

[259] a. stolcke, k. ries, n. coccaro, e. shriberg, r. bates, d. jurafsky, p. taylor,
r. martin, c. v. ess-dykema, and m. meteer,    dialogue act modeling for
automatic tagging and recognition of conversational speech,    computational
linguistics, vol. 26, no. 3, pp. 339   373, 2000.

[260] a. stolcke, e. shriberg, d. hakkani-t  ur, g. t  ur, z. rivlin, and k. s  onmez,
   combining words and speech id144 for automatic topic segmentation,   
in proceedings of darpa broadcast news transcription and understanding
workshop, pp. 61   64, 1999.

[261] h. strik and c. cucchiarini,    modeling pronunciation variation for asr: a
survey of the literature,    speech communication, vol. 29, no. 2   4, pp. 225   246,
1999.

[262] j. tejedor, m. fapso, i. szoke, j. cernocky, and f. grezl,    comparison
of methods for language-dependent and language-independent query-by-
example spoken term detection,    acm transactions on information systems,
vol. 30, no. 3, 2012.

[263] j. tejedor, d. wang, j. frankel, s. king, and j. colas,    a comparison
of grapheme and phoneme-based units for spanish spoken term detection,   
speech communication, vol. 50, no. 11   12, pp. 980   991, 2008.

[264] k. thambiratnam and s. sridharan,    rapid yet accurate speech indexing
using dynamic match lattice spotting,    ieee transactions on audio, speech,
and language processing, vol. 15, no. 1, pp. 346   357, 2007.

[265] t. tombros and f. crestani,    a study of users    perception of relevance of
spoken documents,    technical report tr-99-013, international computer
science institute, 1999.

[266] s. e. tranter and d. a. reynolds,    an overview of automatic speaker diariza-
tion systems,    ieee transactions on audio, speech, and language processing,
vol. 14, no. 5, pp. 1557   1565, 2006.

[267] b. t. truong, s. venkatesh, and c. dorai,    automatic genre identi   cation
for content-based video categorization,    in proceedings of the international
conference on pattern recognition, vol. 4, pp. 230   233, 2000.

[268] m. tsagkias, m. larson, and m. de rijke,    term clouds as surrogates for user
generated speech,    in proceedings of the international acm special interest
group on information retrieval (sigir) conference on research and devel-
opment in information retrieval, pp. 773   774, 2008.

[269] s. tucker, n. kyprianou, and s. whittaker,    time-compressing speech: asr
transcripts are an e   ective way to support gist extraction,    in machine learn-
ing for multimodal interaction, vol. 5237 of lecture notes in computer sci-
ence chapter 21, (a. popescu-belis and r. stiefelhagen, eds.), pp. 226   235,
springer berlin/heidelberg, 2008.

[270] s. tucker and s. whittaker,    temporal compression of speech: an evaluation,   
ieee transactions on audio, speech, and language processing, vol. 16, no. 4,
2008.

references

419

[271] v. t. turunen and m. kurimo,    indexing confusion networks for morph-based
spoken document retrieval,    in proceedings of the international acm special
interest group on information retrieval (sigir) conference on research and
development in information retrieval, pp. 631   638, 2007.

[272] a. van den bosch and w. daelemans,    data-oriented methods for grapheme-
to-phoneme conversion,    in proceedings of the conference of the european
chapter of the association for computational linguistics, pp. 45   53, 1993.

[273] c. j. van rijsbergen, information retrieval. butterworths, 1979.
[274] a. vinciarelli,    speakers role recognition in multiparty audio recordings using
social network analysis and duration distribution modeling,    ieee transac-
tions on multimedia, vol. 9, no. 6, pp. 1215   1226, 2007.

[275] m. viswanathan, h. s. m. beigi, s. dharanipragada, and a. tritschler,
   retrieval from spoken documents using content and speaker information,    in
proceedings of the international conference on document analysis and recog-
nition, pp. 567   572, 1999.

[276] c. c. vogt and g. w. cottrell,    fusion via a linear combination of scores,   

information retrieval, vol. 1, no. 3, pp. 151   173, 1999.

[277] e. m. voorhees and d. k. harman, trec: experiment and evaluation in
information retrieval. digital libraries and electronic publishing, the mit
press, 2005.

[278] h. d. wactlar, a. g. hauptmann, m. g. christel, r. a. houghton, and a. m.
olligschlaeger,    complementary video and audio analysis for broadcast news
archives,    communications of the acm, vol. 43, no. 2, pp. 42   47, 2000.

[279] a. waibel and k.-f. lee, eds., readings in id103. morgan

kaufmann, 1990.

[280] d. wang,    out-of-vocabulary spoken term detection,    phd thesis, university

of edinburgh, 2009.

[281] d. wang, s. king, j. frankel, r. vipperla, n. evans, and r. troncy,    direct
posterior con   dence estimation for out-of-vocabulary spoken term detection,   
acm transactions on information system, vol. 30, no. 3, 2012.

[282] h.-m. wang,    experiments in syllable-based retrieval of broadcast news
speech in mandarin chinese,    speech commununication, vol. 32, no. 1   2,
pp. 49   60, 2000.

[283] h.-m. wang,    mandarin spoken document retrieval based on syllable lat-
tice matching,    pattern recognition letters, vol. 21, no. 6   7, pp. 615   624,
2000.

[284] y.-y. wang, a. acero, and c. chelba,    is word error rate a good indica-
tor for spoken language understanding accuracy,    in proceedings of the ieee
workshop on automatic id103 and understanding, pp. 577   582,
2003.

[285] y.-y. wang, d. yu, y.-c. ju, and a. acero,    an introduction to voice search,   

ieee signal processing magazine, vol. 25, no. 3, pp. 28   38, 2008.

[286] v. warnke, s. harbeck, e. noth, and h. niemann,    topic spotting
using subword units,    in 9. aachener kolloqium    signaltheorie    bild- und
sprachsignale, pp. 287   291, 1997.

420 references

[287] c. l. wayne,    multilingual topic detection and tracking: successful research
enabled by corpora and evaluation,    in proceedings of the international con-
ference on language resources and evaluation, 2000.

[288] m. wechsler, e. munteanu, and p. sch  auble,    new techniques for open-
vocabulary spoken document retrieval,    in proceedings of the international
acm special interest group on information retrieval (sigir) conference
on research and development in information retrieval, pp. 20   27, 1998.

[289] m. wechsler, e. munteanu, and p. sch  auble,    new approaches to spoken
document retrieval,    information retrieval, vol. 3, no. 3, pp. 173   188, 2000.
[290] m. weintraub,    lvcsr log-likelihood ratio scoring for keyword spotting,    in
proceedings of the ieee international conference on acoustics, speech, and
signal processing, pp. i/297   i/300, 1995.

[291] m. weintraub, k. taussig, k. hunicke-smith, and a. snodgrass,    e   ect of
speaking style on lvcsr performance,    in proceedings of the international
conference on spoken language processing, pp. 16   19, 1996.

[292] p. wellner, m. flynn, and m. guillemot,    browsing recorded meetings with
ferret,    in machine learning for multimodal interaction, vol. 3361 of lecture
notes in computer science, (s. bengio and h. bourlard, eds.), pp. 12   21,
springer berlin/heidelberg, 2005.

[293] p. wellner, m. flynn, a. tucker, and a. whittaker,    a meeting browser eval-
uation test,    in computer-human interaction extended abstracts on human
factors in computing systems, 2005.

[294] f. wessel, r. schluter, k. macherey, and h. ney,    con   dence measures
for large vocabulary continuous id103,    ieee transactions on
speech and audio processing, vol. 9, no. 3, pp. 288   298, 2001.

[295] r. w. white, d. w. oard, g. j. f. jones, d. soergel, and x. huang,
   overview of the clef-2005 cross-language speech retrieval track,    in access-
ing multilingual information repositories, vol. 4022 of lecture notes in
computer science, (c. peters, f. gey, j. gonzalo, h. m  uller, g. j. f.
jones, m. kluck, b. magnini, and m. de rijke, eds.), pp. 744   759, springer
berlin/heidelberg, 2006.

[296] e. w. d. whittaker, j. m. van thong, and p. j. moreno,    vocabulary
independent id103 using particles,    in proceedings of the ieee
workshop on automatic id103 and understanding, pp. 315   318,
2001.

[297] s. whittaker, j. hirschberg, b. amento, l. stark, m. bacchiani, l. isenhour,
p. stead, g. zamchick, and a. rosenberg,    scanmail: a voicemail interface
that makes speech browsable readable and searchable,    in proceedings of the
special interest group on computer-human interaction (sigchi) conference
on human factors in computing systems, pp. 275   282, 2002.

[298] s. whittaker, j. hirschberg, j. choi, d. hindle, f. pereira, and a. singhal,
   scan: designing and evaluating user interfaces to support retrieval from
speech archives,    in proceedings of the international acm special interest
group on information retrieval (sigir) conference on research and devel-
opment in information retrieval, pp. 26   33, 1999.

references

421

[299] s. whittaker, s. tucker, k. swampillai, and r. laban,    design and evaluation
of systems to support interaction capture and retrieval,    personal ubiquitous
computing, vol. 12, no. 3, pp. 197   221, 2008.

[300] l. wilcox, f. chen, and v. balasubramanian,    segmentation of speech using
speaker identi   cation,    in proceedings of the ieee international conference
on acoustics, speech, and signal processing, pp. i/161   i/164, 1994.

[301] l. d. wilcox and m. a. bush.,    id48-based wordspotting for voice editing

and indexing,    in proceedings of eurospeech, pp. 25   28, 1991.

[302] d. willett, a. worm, c. neukirchen, and g. rigoll,    con   dence measures for
id48-based id103,    in proceedings of the international confer-
ence on spoken language processing, pp. 3241   3244, 1998.

[303] m. j. witbrock and a. g. hauptmann,    id103 and information
retrieval: experiments in retrieving spoken documents,    in proceedings of the
darpa id103 workshop, 1997.

[304] m. j. witbrock and a. g. hauptmann,    using words and phonetic strings
for e   cient information retrieval from imperfectly transcribed spoken doc-
uments,    in proceedings of the acm international conference on digital
libraries, pp. 30   35, 1997.

[305] i. h. witten, a. mo   at, and t. c. bell, managing gigabytes: compressing

and indexing documents and images. morgan kaufmann, 1999.

[306] p. c. woodland, s. e. johnson, p. jourlin, and k. sp  arck jones,    e   ects of
out of vocabulary words in spoken document retrieval,    in proceedings of the
international acm special interest group on information retrieval (sigir)
conference on research and development in information retrieval, pp. 372   
374, 2000.

[307] b. wrede and e. shriberg,    spotting    hot spots    in meetings: human
judgments and prosodic cues,    in proceeindgs of eurospeech, pp. 2805   2808,
2003.

[308] c.-h. wu, c.-l. huang, w.-c. lee, and y.-s. lai,    speech-annotated photo
retrieval using syllable-transformed patterns,    ieee signal processing letters,
vol. 16, no. 1, pp. 6   9, 2009.

[309] h. yan, o. vinyals, g. friedland, c. muller, n. mirghafori, and c. wooters,
   a fast-match approach for robust faster than real-time speaker diarization,   
in proceedings of the ieee workshop on automatic id103 and
understanding, pp. 693   698, 2007.

[310] h. yang, l. chaisorn, y. zhao, s. y. neo, and t. s. chua,    videoqa: question
answering on news video,    in proceedings of the acm international confer-
ence on multimedia, pp. 632   641, 2003.

[311] s. r. young,    detecting misrecognitions and out-of-vocabulary words,    in
proceedings of the ieee international conference on acoustics, speech, and
signal processing, pp. ii/21   ii/24, 1994.

[312] p. yu, k. chen, c. ma, and f. seide,    vocabulary-independent indexing of
spontaneous speech,    ieee transactions on speech and audio processing,
vol. 13, no. 5, pp. 635   643, 2005.

[313] t. zhang and c. c. kuo, content-based audio classi   cation and retrieval

for audiovisual data parsing. kluwer academic publishers, 2001.

422 references

[314] t. zhang and c.-c. j. kuo,    heuristic approach for generic audio data
segmentation and annotation,    in proceedings of the acm international con-
ference on multimedia (part 1), pp. 67   76, 1999.

for

[315] z.-y. zhou, p. yu, c. chelba, and f. seide,    towards spoken-document
retrieval
large-scale web-search
architectures,    in proceedings of the human language technology conference
of the north american chapter of the association of computational linguis-
tics, pp. 415   422, 2006.

the internet: lattice indexing for

[316] x. zhu, c. barras, l. lamel, and j.-l. gauvain,    speaker diarization: from
broadcast news to lectures,    in machine learning for multimodal interaction,
vol. 4299 of lecture notes in computer science, chapter 35, (s. renals,
s. bengio, and j. g. fiscus, eds.), pp. 396   406, springer berlin/heidelberg,
2006.

[317] g. zweig, j. makhoul, and a. stolke,    introduction to the special section
on rich transcription,    ieee transactions on audio, speech, and language
processing, vol. 14, no. 5, pp. 1490   1491, 2006.

