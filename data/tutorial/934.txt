predicting structures in nlp  
constrained conditional models  

and  

integer id135 

dan goldwasser, vivek srikumar, dan roth 
department of computer science 
university of illinois at urbana-champaign 

june 2012  
naacl  

page 1 

nice to meet you 

0: 2 

learning and id136 in nlp 

    natural language decisions are structured  

    global decisions in which several local decisions play a role  but there 

are mutual dependencies on their outcome. 

    it is essential to make coherent decisions in a way that takes 
the interdependencies into account. joint, global id136. 

    today: 

    how to support making global, coherent decisions 
    how to learn models that are used, eventually, to make global decisions 

 

    a framework that allows one to exploit interdependencies among 

decision variables both in id136 (decision making) and in learning. 

    id136: a formulation for id136 with expressive declarative 

knowledge. 

    learning: ability to learn simple models; amplify it power by exploiting 

interdependencies.  

page 3 

constraints driven learning and decision making 

    the focus of this tutorial is on  

    augmenting statistical learning models with declarative knowledge. 
    the knowledge will be expressed as constraints on the possible 

predictions our models can make. 

    why constraints? 

    the goal: building a good nlp systems easily 
    we have prior knowledge at our hand 
    within our framework we will see that we can use this knowledge to : 

    improve decision making  
    guide learning 
    simplify the models we need to learn 
    replace labeled data 

0: 4 

comprehension 

(england,  june,  1989)  -  christopher  robin  is  alive  and  well.    he  lives  in 
england.  he is the same person that you read about in the book, winnie the 
pooh. as a boy, chris lived in a pretty home called cotchfield farm.  when 
chris was three years old, his father wrote a poem about him.  the poem was 
printed in a magazine for others to read.  mr. robin then wrote a book.  he 
made up a fairy tale land where chris lived.  his friends were animals.  there 
was a bear called winnie the pooh.  there was also an owl and a young pig, 
called a piglet.  all the animals were stuffed toys that chris owned.  mr. robin 
made them come to life with his words.  the places in the story were all near 
cotchfield farm. winnie the pooh was written in 1925.  children still love to 
read about christopher robin and his animal friends.  most people don't know 
he is a real person who is grown now.  he has written two books of his own.  
they tell what it is like to be famous. 

1. christopher robin was born in england.      2.  winnie the pooh is a title of a book.   
3. christopher robin   s dad was a magician.     4. christopher robin must be at least 65 now   

this is an id136 problem 

0: 5 

learning and id136  

    global decisions in which several local decisions play a role  

but there are mutual dependencies on their outcome. 
    in current nlp we often think about simpler structured problems: 

parsing, information extraction, srl, etc.  

    as we move up the problem hierarchy (id123, qa,   .) not 

all component models can be learned simultaneously 

    we need to think about (learned) models for different sub-problems 
    knowledge relating sub-problems  (constraints) may appear only at 

evaluation time 

    goal: incorporate models    information, along with prior 
knowledge (constraints) in making coherent decisions  
    decisions that respect the local models as well as domain & context 

specific knowledge/constraints. 

0: 6 
page 6 

goal of the tutorial 

 
    by the end of the tutorial you should be able to: 

    model structure prediction problems  

   

injects declarative (domain, background) knowledge into your problem 
formulation   

    think about problem representation and the decomposition of the 

problem into natural components. 
   

independently of algorithmic solutions 

 

    represent domain and other relevant knowledge as linear constraints 

 

    think about possible way to support id136 

 

    think about possible ways to learn your models 

    reason about several paradigms, their advantages and disadvantages. 

 

0: 7 

this tutorial: constrained conditional models  (ccms) 

 
    part 1: introduction to constrained conditional models  (30min) 

    examples:  

    ne + relations  
    information extraction     correcting models with ccms 

    first summary: what are ccms 
    problem setting 

    features and constraints; some hints about training issues 

0: 8 

this tutorial: constrained conditional models 

 
    part 2: modeling nlp via ccms (45 minutes) 

    introduction to ilp  
    posing nlp problems as ilp problems 

    1. sequence tagging          (id48/crf + global constraints) 
    2. srl                                    (independent classifiers + global constraints)  
    3. sentence compression (language model + global constraints) 

    less detailed examples  

    1. co-reference  
    2. a bunch more ... 

    part 3: id136 algorithms  (15 minutes) 

    exact algorithms 
    relaxation methods 
    approximate algorithms 

break 

0: 9 

this tutorial: constrained conditional models (part ii) 

    part 4: training paradigms for ccms (20 min) 

    independently of constraints (l+i); jointly with constraints (ibt) 
    decomposed to simpler models 

    part 5: constraints driven training (60 min) 

    learning constraints    penalties 

    independently of learning the model  
    jointly, along with learning the model  

    dealing with lack of supervision 

    constraints driven semi-supervised learning (codl) 
    indirect supervision  

    learning constrained latent representations 

0: 10 

this tutorial: constrained conditional models (part ii) 

    part 6: conclusion (& discussion)  (10 min) 

    summary 
    building ccms;  features and constraints. mixed models vs. joint models;  
    where is knowledge coming from 

the end 

0: 11 

part 1: introduction 

1: 1 

this tutorial: constrained conditional models 

 
    part 1: introduction to constrained conditional models  (30min) 

    examples:  

    ne + relations  
    information extraction     correcting models with ccms 

    first summary: what are ccms  
    problem setting 

    features and constraints; some hints about training issues 

1: 2 

three ideas underlying constrained conditional models 

    idea 1:  
     separate modeling and problem formulation from algorithms 

modeling 

    similar to the philosophy of probabilistic modeling 

 

    idea 2:  
     keep model simple, make expressive decisions (via constraints) 
    unlike probabilistic modeling, where models become more expressive  

id136 

 

    idea 3:  
     expressive structured decisions can be supported by simply  
     learned models  

learning 

    global id136 can be used to amplify the simple models (and even 

minimal supervision). 

1: 3 

pipeline 

raw data 

      most problems are not single classification problems 

id52 

phrases 

semantic entities  

relations 

parsing 

wsd 

id14 

    conceptually, pipelining is a crude approximation  

    interactions occur across levels and down stream decisions often interact 

with previous decisions. 

    leads to propagation of errors 
    occasionally, later stages are easier but cannot correct earlier errors. 

    but, there are good reasons to use pipelines  
    putting everything in one basket may not be right  
    how about choosing some stages and think about them jointly? 

1: 4 

id136 with general constraint structure [roth&yih   04,07] 
recognizing entities and relations  

improvement over no 

id136: 2-5% 

0.10 
0.10 
0.60 
0.60 
0.30 
0.30 

0.05 
0.05 
0.85 
0.85 
0.10 
0.10 

other 
other 

other 
other 

other 
other 
other 

loc 
loc 

loc 
loc 

per 
per 

per 
per 

per 
per 
per 

 
y = argmax    y score(y=v) [[y=v]] =  
 
   = argmax score(e1 = per)   [[e1 = per]] + score(e1 = loc)   [[e1 = loc]] +       
                score(r1 = s-of)   [[r1 = s-of]] +   ..  
- how to guide the global   
 
      id136?  
subject to constraints 
r23 
r12 
-     why not learn jointly? 
 

dole    s wife, elizabeth , is a native of n.c. 
 e1                   e2                              e3   

key questions:  

loc 
loc 
loc 

0.05 
0.05 
0.05 
0.50 
0.50 
0.50 
0.45 
0.45 
0.45 

irrelevant 
irrelevant 
irrelevant 

spouse_of 
spouse_of 
spouse_of 

born_in 
born_in 
born_in 

0.05 
0.05 
0.05 
0.45 
0.45 
0.45 
0.50 
0.50 
0.50 

irrelevant 
irrelevant 

spouse_of 
spouse_of 

born_in 
born_in 

0.10 
0.10 
0.05 
0.05 
0.85 
0.85 

note:  
non sequential 
model 

models could be learned separately; constraints may come up only at decision time.  

1: 5 

task of interests: structured output 

    for each instance, assign values to a set of variables 
    output variables depend on each other 
    common nlp tasks  

    parsing; id29; summarization; id68; co-

reference resolution, id123     
    common information extraction tasks: 

    entities, relations,    

 

    many pure machine learning approaches exist 

    id48 (id48s) ; crfs 
    structured id88s and id166s    

    however,     

 

1: 6 

information extraction via id48 

lars ole andersen . program analysis and specialization for the 

c programming language.  phd thesis. diku , 

 
university of copenhagen, may 1994 . 

[author]   
 
[title]    
[editor]  
 
[booktitle]  
 
[tech-report]   
[institution]  
 
[date]   

 

prediction result of a trained id48  
    lars ole andersen . program analysis and 
 
 
 
 
 
 
 

specialization for the  
c  
programming language 
.  phd thesis . 
diku , university of copenhagen , may 
1994 . 

unsatisfactory results ! 

 
 
 
  
 
 

many    natural constraints    are violated  

1: 7 

strategies for improving the results 

    (pure) machine learning approaches 

    higher order id48/crf? 
    increasing the window size? 
    adding a lot of new features  

    requires a lot of labeled examples 
 

increasing the model complexity 

increase difficulty of learning 

    what if we only have a few labeled examples? 
 
 

can we keep the learned model simple and 
still make expressive decisions?  

 

    other options?  

    constrain the output to make sense 
    push the  (simple) model in a direction that makes sense 

1: 8 

information extraction without prior knowledge 

lars ole andersen . program analysis and specialization for the 

c programming language.  phd thesis. diku , 

 
university of copenhagen, may 1994 . 

[author]   
 
[title]    
[editor]  
 
[booktitle]  
 
[tech-report]   
[institution]  
 
[date]   

 

prediction result of a trained id48 
    lars ole andersen . program analysis and 
 
 
 
 
 
 
 

specialization for the  
c  
programming language 
.  phd thesis . 
diku , university of copenhagen , may 
1994 . 

 
 
 
  
 
 

violates lots of natural constraints! 

1: 9 

 
examples of constraints 
 

    each field must be a consecutive list of words and can appear 

at most once in a citation.  

 
    state transitions must occur on punctuation marks. 
 
    the citation can only start with author or editor.  
 
    the words pp., pages correspond to page. 
    four digits starting with 20xx and 19xx are date. 
    quotations can appear only in title 
          . 

easy to express pieces of    knowledge    
non propositional; may use quantifiers  

1: 10 

information extraction with constraints 

   

adding constraints, we get correct results! 
    without changing the model 

 
 
    [author]      lars ole andersen .  
      [title]  
        
 
 
 
  [tech-report]  
  [institution]    
  [date]    
 

  may, 1994 . 

    

 
 

 

 

program analysis and specialization for the  
c programming language . 
phd thesis . 
diku , university of copenhagen ,  

constrained conditional models allow: 
    learning a simple model  
    making decisions with a more complex model 
    accomplished by directly incorporating constraints to bias/re-

ranks decisions made by the simpler model 

1: 11 

problem setting 

    random variables y: 

 
 
 
 
 
 

y1 

c(y1,y4) 

y2 

c(y2,y3,y6,y7,y8) 

y4 

y5 

y6 

y7 

y3 

y8 

observations 

    conditional distributions p (learned by models/classifiers)  
    constraints c    any boolean function  
        defined over partial assignments  (possibly:  + weights w ) 
 
    goal:  find the    best    assignment 

    the assignment that achieves the highest global performance. 

    this is an integer programming problem 

y*=argmaxy p   y                         subject to constraints c 

(+ w   c) 

1: 12 

constrained conditional models 

weight vector for 
   local    models 

features, classifiers; log-
linear models  (id48, 
crf) or a combination 

penalty for violating 
the constraint. 

(soft) constraints 
component 

how far y is from  
a    legal    assignment 

how to solve? 
this is an integer linear program 
solving using ilp packages gives an  
exact solution.  
cutting planes, id209 & 
other search techniques are possible  

how to train? 
training is learning the objective 
function 
decouple? decompose?  
how to exploit the structure to        
minimize supervision? 

1: 13 

what is a constrained conditional model? 

modeling nlp problem 
    variables, features and constraints 

objective function 
    constrained conditional model 

constrained optimization language 
    how to represent id136? 

integer linear program 

id136 
    how to solve it? 

learning 
    how to learn the objective 

function? 

several id136 algorithms: exact 
ilp, search, relaxation; dynamic prog. 

learning    and   . several learning 
strategies: l+i, ibt, others. 

1: 14 

examples: ccm formulations 

ccms can be viewed as a general interface to easily combine 
declarative domain knowledge with data driven statistical models 

formulate nlp problems as ilp problems         (id136 may be done otherwise) 

  1. sequence tagging            (id48/crf + global constraints) 
  2. sentence compression   (language model + global constraints) 
  3. srl                                      (independent classifiers + global constraints)  

sequential prediction 
sentence 
 
compression/summarization: 
id48/crf based: 
 
                     argmax       ij xij 
language model based: 
                     argmax       ijk xijk 

linguistics constraints 
linguistics constraints 
 
 
cannot have both a states and b states 
 
in an output sequence.  
if a modifier chosen, include its head 
if verb is chosen, include its arguments  

1: 15 

context: there are many formalisms  

    our goal is to assign values to multiple interdependent discrete variables  
    these problems can be formulated and solved with multiple approaches 

    markov random fields (mrfs) provide a general framework for it. but: 

    the decision problem for mrfs can be written as an ilp too 

    [roth & yih 04,07, taskar 04] 

    key difference: in mrf approaches the model is learned globally.  

    not easy to systematically incorporate problem understanding and knowledge 
    our approach, on the other hand,  is designed to address also cases in which 

some of the component models are learned in other contexts and at other 
times, or incorporated as background knowledge.  

    that is, some components of the global decision need not, or cannot, be 

trained in the context of the decision problem. 

    markov logic networks (mlns) attempt to compile knowledge into an mrf, 

thus provide one example of a global training approach. 

    caveat: everything can be done with everything, but there are key 

conceptual differences that impact what is easy to do 
 

1: 16 

context: constrained conditional models 

conditional markov random field 

y1 

y4 

y5 

y2 
y6 

y7 

y3 
y8 

constraints network 

y1 

y4 

y5 

y2 
y6 

y7 

y3 
y8 

       y* = argmaxy     wi   (x; y) 
  
    linear objective functions  
    typically   (x,y) will be local 
functions, or   (x,y) =   (x)   

                   i   i dc(x,y) 

 

variables  

    expressive constraints over output 

    soft, weighted constraints  
    specified declaratively as fol formulae 

    clearly, there is a joint id203 distribution that represents 

this mixed model.  
    we would like to:  

key difference from mlns which provide a concise 
definition of a model, but the whole joint one. 

    learn a simple model or several simple models 
    make decisions with respect to a complex model                  

1: 18 

features versus constraints in ccms 

    in principle, constraints and features can encode the same properties 

    fi : x    y ! {0,1} or r;             ci : x    y ! {0,1};  
     in practice, they are very different 
 
      features  

    local , short distance properties     to allow tractable id136  
    propositional (grounded):       
    e.g. true if:              the    followed by a noun occurs in the sentence     

      constraints 

    global properties 
    quantified, first order logic expressions  
    e.g.true if:          all yis in the sequence y are assigned different values.     
 

indeed, used differently 

1: 19 

role of constraints: encoding prior knowledge 

    consider encoding the knowledge that:  

    entities of type a and b cannot occur simultaneously in a sentence  

    the    feature    way        

    many new (possible) features: propsitionalizing;  
    only a    suggestion    to the learning algorithm; need to learn weights 
    wastes parameters to learn indirectly knowledge we have. 
    results in higher order models; may require tailored models  

    the  constraints  way 

a form of supervision 

    tell the model what it should attend to 
    keep the model simple;  add expressive constraints directly 
    a small  set of constraints 
    allows for decision time incorporation of constraints  

details depend on whether (1) learned model use   (x,y) or    (x) 
                                                   (2) hard or soft constraints  

1: 20 

constrained conditional models   before a summary 

 

    constrained conditional models     ilp formulations     have been 

shown useful in the context of many nlp problems 
    [roth&yih, 04,07: entities and relations; punyakanok et. al: srl     ] 

    summarization; co-reference; information & id36; event 

identifications; id68; id123; knowledge 
acquisition; sentiments; temporal reasoning, id33,    

    some theoretical work on training paradigms [punyakanok et. al., 05 

more; constraints driven learning, pr, constrained em   ]  

    we will provide some insights into theoretical issues and cover 

 
 

some of the applications. 
 

    summary of work & a bibliography: http://l2r.cs.uiuc.edu/tutorials.html 
  

 
 

1: 21 

constrained conditional models     1st summary  

 
    introduced ccms as a formalisms that allows us to 

    learn simpler models than we would otherwise 
    make decisions with expressive models, augmented by declarative 

constraints 

    focused on modeling     posing nlp problems as ccms 

    1. sequence tagging          (id48/crf + global constraints) 
    2. srl                                    (independent classifiers + global constraints)  
    3. sentence compression (language model + global constraints) 

    next: more modeling & id136 

    from declarative constraints to ccms; solving ilp, exactly & approximately 

    second half     learning 

    supervised setting, and supervision-lean settings 

1: 22 

part 2: modeling 

2:1 

this tutorial: constrained conditional models 

 
    part 2: modeling nlp via ccms (45 minutes) 

    introduction to ilp  
    posing nlp problems as ilp problems 

    1. sequence tagging          (id48/crf + global constraints) 
    2. srl                                    (independent classifiers + global constraints)  
    3. sentence compression (language model + global constraints) 

    less detailed examples  

    1. co-reference  
    2. a bunch more ... 

what is a constrained conditional model? 

modeling nlp problem 
    variables, features and constraints 

objective function 
    constrained conditional model 

constrained optimization language 
    how to represent id136? 

integer linear program 

id136 
    how to solve it? 

learning 
    how to learn the objective 

function? 

several id136 algorithms: exact 
ilp, search, relaxation; dynamic prog. 

learning    and   . several learning 
strategies: l+i, ibt, others. 

1: 3 

modeling nlp via ccms 

    id136 is a discrete optimization problem 

    goal: to assign values to a collection of variables of interest 

 

    we choose to model id136 step using the language of 

integer id135 
 

    ccms provides: 

    a way to focus on problem definition rather than how to solve it 
    simple (to write down) but expressive formulation 
    a way to use of declarative knowledge  

2:4 

integer id135: review 

    telfa co. produces tables and chairs; wants to maximize profit 

    each table makes $8 profit, each chair makes $5 profit. 
    a table requires 1 hour of labor and 9 sq. feet of wood 
    a chair requires 1 hour of labor and 5 sq. feet of wood 
    we have only 6 hours of work and 45sq. feet of wood 

y1: number of tables manufactured 
y2: number of chairs manufactured 

    variables 
    objective function 

 

    constraints 

    labor 
    wood 
    variable 

we cannot build fractional tables or chairs! 

2:5 

geometry of integer linear programs 

cost (profit) vector 

2:6 

modeling your problem 

penalty for violating 
the constraint. 

(soft) constraints 
component 

weight vector for 
   local    models 

a collection of classifiers; 
id148  (id48, 
crf) or a combination 

how far y is from  
a    legal    assignment 

    how do we write our models in this form? 

    what goes in an objective function? 
    how to design constraints? 

 

we will consider the 
case when y   s are 
restricted to 0 or 1. 

2:7 

modeling your problem: decision variables 

functions from 
(x,y) to 0 or 1 

    f(x,y) is a collection of features from x and y 

    eg: does the ith word have the pos tag nn? 
    several such features, not all active in a given (x,y) instance 

 

    define indicator variables 

     is fi is active in an input? 

 

     f(x,y) can be rewritten using indicator variables as 

 

2:8 

modeling your problem: constraints 

penalty for violating 
the constraint. 

score for this 
variable 

 
 
 
 

id136 variables can be 0 or 1 

how far y is from  
 
a    legal    assignment 
    suppose we want to disallow all    illegal    assignments. 

    make all the   i infinity 
    hard constraints; can be written as linear inequalities in terms of the 

id136 variables 

 

1f is the vector of all id136 variables 

2:9 

ccm examples 

    many works in nlp make use of constrained conditional 

models, implicitly or explicitly. 

    next we describe three examples in detail. 
 
    example 1: id14 

    the use of id136 with constraints to improve id29 

    example 2: sequence tagging 

    adding long range constraints to a simple model 

    example 3: sentence compression 

    simple language model with constraints outperforms complex models 

2:10 

example 1: id14 

who did what to whom, when, where, why,    

demo:http://l2r.cs.uiuc.edu/~cogcomp 

top ranked system in conll   05 

shared task  

key difference is the id136 

2:11 

a simple sentence 

i left my pearls to my daughter in my will . 
[i]a0 left [my pearls]a1 [to my daughter]a2 [in my will]am-loc . 
 
    a0 
    a1 
    a2 
    am-loc  location 

leaver 
things left 
benefactor 

 

i left my pearls to my daughter in my will . 

2:12 

algorithmic approach 

candidate arguments 

i left my nice pearls to her 

    identify argument candidates 

    pruning  [xue&palmer, emnlp   04] 
    argument identifier  
    binary classification 

    classify argument candidates 

    argument classifier  

    multi-class classification 

    id136 

    use the estimated id203 distribution 

given by the argument classifier 

    use structural and linguistic constraints 
    infer the optimal global output 

 

i left my nice pearls to her 
i left my nice pearls to her 
[ [    [       [      [ 
 ]    ]  ]            ]     ] 

i left my nice pearls to her 

2:13 

id14 (srl) 

i left my pearls to my daughter in my will . 

0.5 
0.15 
0.15 
0.1 
0.1 

0.15 
0.6 
0.05 
0.05 
0.05 

0.05 
0.05 
0.7 
0.05 
0.15 

0.05 
0.1 
0.2 
0.6 
0.05 

0.3 
0.2 
0.2 
0.1 
0.2 

2:14 

id14 (srl) 

i left my pearls to my daughter in my will . 

0.5 
0.15 
0.15 
0.1 
0.1 

0.15 
0.6 
0.05 
0.05 
0.05 

0.05 
0.05 
0.7 
0.05 
0.15 

0.05 
0.1 
0.2 
0.6 
0.05 

0.3 
0.2 
0.2 
0.1 
0.2 

2:15 

id14 (srl) 

i left my pearls to my daughter in my will . 

0.5 
0.15 
0.15 
0.1 
0.1 

0.15 
0.6 
0.05 
0.05 
0.05 

one id136 
problem for each 
verb predicate.  

0.05 
0.05 
0.7 
0.05 
0.15 

0.05 
0.1 
0.2 
0.6 
0.05 

0.3 
0.2 
0.2 
0.1 
0.2 

2:16 

constraints 

    no duplicate argument classes 

 
 

any boolean rule can be encoded as 
a set of linear inequalities. 

    reference-ax 

if there is an reference-ax phrase, there is an ax 

 
 

 

    continuation-ax 

if there is an continuation-x phrase, there is an ax before it 

 
 
many other possible constraints: 

universally quantified 

rules 

learning based java: allows a developer 
to encode constraints in first order 
logic; these are compiled into linear 
inequalities automatically.  

    unique labels 
    no overlapping or embedding 
    relations between number of arguments; order constraints 
    if verb is of type a, no argument of type  b 

2:17 

srl: posing the problem 

2:18 

ccm examples 

    many works in nlp make use of constrained conditional 

models, implicitly or explicitly. 

    next we describe three examples in detail.  
 
    example 1: id14 

    the use of id136 with constraints to improve id29 

    example 2: sequence tagging 

    adding long range constraints to a simple model 

    example 3: sentence compression 

    simple language model with constraints outperforms complex models 

2:19 

example 2: sequence tagging 

id48 : 

here, y   s are labels; x   s are observations. 

the ilp   s objective function must 
include all entries of the 
id155 table. 

every edge is a boolean variable 
that selects a transition cpt entry. 

example: 

they are related: if we choose  
y0 = d  then we must choose an edge 
y0 = d     y1 = ? . 

the 
d 

n 

a 

v 

man 
d 

n 

a 

v 

saw 
d 

n 

a 

v 

the 
d 

n 

a 

v 

dog 
d 

n 

a 

v 

every assignment to the y   s is a path. 

2:20 

example 2: sequence tagging 

id48: 

example: 

as an ilp: 

id136 variables 

the 
d 

n 

a 

v 

man 
d 

n 

a 

v 

saw 
d 

n 

a 

v 

the 
d 

n 

a 

v 

dog 
d 

n 

a 

v 

learned parameters 

2:21 

example 2: sequence tagging 

id48: 

example: 

as an ilp: 

the 
d 

n 

a 

v 

man 
d 

n 

a 

v 

saw 
d 

n 

a 

v 

the 
d 

n 

a 

v 

dog 
d 

n 

a 

v 

unique label for each word 

2:22 

example 2: sequence tagging 

id48 : 

example: 

as an ilp: 

the 
d 

n 

a 

v 

man 
d 

n 

a 

v 

saw 
d 

n 

a 

v 

the 
d 

n 

a 

v 

dog 
d 

n 

a 

v 

unique label for each word 

edges that are chosen 
must form a path 

2:23 

example 2: sequence tagging 

id48 : 

example: 

as an ilp: 

the 
d 

n 

a 

v 

man 
d 

n 

a 

v 

saw 
d 

n 

a 

v 

the 
d 

n 

a 

v 

dog 
d 

n 

a 

v 

unique label for each word 

edges that are chosen 
must form a path 

there must be a verb! 

2:24 

constraints 

    we have seen three different constraints in this example 

1. unique label for each word 
2.
3.

chosen edges must form a path 
there must be a verb 

    all three can be expressed as  linear inequalities 

 

    in terms of modeling, there is a difference 

a conventional 

model 

    the first two define the output structure (in this case, a sequence) 
    the third one adds knowledge to the problem 

 
 

in ccms, knowledge is an integral 

part of the modeling 

2:25 

ccm examples 

    many works in nlp make use of constrained conditional 

models, implicitly or explicitly. 

    next we describe three examples in detail.  
 
    example 1: id14 

    the use of id136 with constraints to improve id29 

    example 2: sequence tagging 

    adding long range constraints to a simple model 

    example 3: sentence compression 

    simple language model with constraints outperforms complex models 

2:26 

example 3: sentence compression (clarke & lapata) 

2:27 

example 

example: 
 
 
 
 
 
 

0 
big 
big 

2 
eat 

3 
small 

4 
fish 

1 
fish 
fish 

5 
in 
in 

=

          
=
0
8
1
  
=

=
=
1
    
156

5
=

=

015

568

6

7 
small 

8 
pond 
pond 

6 
a 
a 

=

1

2:28 

language model-based compression 

2:29 

example: summarization 

this formulation requires some additional constraints 
big fish eat small fish in a small pond 

no selection of decision variables can make these trigrams appear 
consecutively in output. 
 
we skip these constraints here.  

2:30 

trigram model in action 

2:31 

modifier constraints 

2:32 

example 

2:33 

example 

2:34 

sentential constraints 

2:35 

example 

2:36 

example 

2:37 

more constraints 

2:38 

sentence compression: posing the problem 

learned parameters 

id136 variables 

n  2xi=0

n  1xj=i+1

nxk=j+1

maximize

subject to

  k;i;j   i;j;k

if the id136 variable is on, the three 

if the three corresponding auxiliary variables are on, 
corresponding auxiliary variables must also be on. 

the id136 variable must be on. 

8i; j; k; 0    i < j < k    n;

3  i;j;k      i +   j +   k
2 +   i;j;k      i +   j +   k

(k    i    2)  i;j;k +

  s    k    i    2

j  1xs=i+1

  s +

k  1xs=j+1

if the id136 variable is on, no intermediate 

auxiliary variables may be on. 

the tutorial web page has good notes on how to 
convert boolean constraints to linear inequalities. . 

2:39 

other examples: coreference resolution 

     k. chang et. al 2011. id136 protocols for coreference 

resolution 
    also denis and baldridge, 2009 

 
 
 
 
 
 
 
input: a set of pairwise mention scores over a document  
 
  output: globally consistent cliques representing entities 

2:40 

best-link id136 

    for each mention u, best-link considers the best mention on 

its left to connect to 

    then, it creates a link between them if the score is above some 

threshold (typically 0) 

 
 
 
 

 

 

1.5 

-1.5 

m* 

1.2 

3.1 
3.1 

u 

0.2 

    best-link id136 is simple and effective (bengtson and roth, 

2008) 

 
 

2:41 

all-link id136 

    it scores a id91 of mentions by including all possible 

1.5 

pairwise links in the score: 
 
 
 
 
 

score: 1.5 + 3.1 - 0.5 + 1.5 = 5.6  

-0.5 

3.1 

1.5 

     mccallum and wellner, 2003; finley and joachims, 2005 

 

 
 
 
 

2:42 

integer id135 (ilp) formulation for id136 

    best-link 

 
 
 
 
 
 

    all-link 

 
 
 
 
 
 

pairwise  mention score 

binary variable 

enforce the transitivity 
closure of the id91 

2:43 

opinion recognition  

    y. choi, e. breck, and c. cardie. joint extraction of entities and 

relations for opinion recognition emnlp-2006 

 
 
 
    id29 variation: 

    agent=entity 
    relation=opinion 

    constraints: 

    an agent can have at most two opinions. 
    an opinion should be linked to only one agent. 
    the usual non-overlap constraints. 

2:44 

extending id14 

changing 

a1:  the thing  

a0: the causer  

am-tmp: temporal 

of the transformation 

   the field goal of brien changed the game in the fourth quarter 
 
 
 
 
 
 
 
 
 

v. srikumar and d. roth. a joint model for extended semantic role 
labeling. emnlp 2011. 

consistency enforced 

preposition relations 

verb arguments 

agent of 
action 

temporal 

2:45 

temporal ordering 

    n. chambers and d. jurafsky. jointly combining implicit 
constraints improves temporal ordering. emnlp-2008. 

2:46 

temporal ordering 

    n. chambers and d. jurafsky. jointly combining implicit 
constraints improves temporal ordering. emnlp-2008. 

three types of edges: 

1) annotation relations before/after 
2) transitive closure constraints 
3) time id172 constraints 

2:47 

event timeline construction 

    q. do,  l. wei and d. roth. joint id136 for event timeline 

construction. emnlp 2012 

[   ] the iraqi insurgents attacked a police station in tal afar on tuesday 
killing 6 policemen and injuring 8 other people. this action brings the 
casualties  to  over  3000  since  the invasion  of  the  coalition armies  on 
3/20/2003.  police  wants  to  arrest  the  insurgents  in  a  campaign  next 
week. [   ] 
publishing date: wed., may 24th, 2006 
casualties 

attacked 

injuring 

arrest 

e4 

invasion 
e5 

killing 
e2/e3 

e1 

-    

i4 
since [   ] 
3/20/2003 

i3 

i2 

i1 

3/20/2003 

tuesday 

wed., may 
24th, 2006 

next week 

e6 

i5 

+    

language generation. 

    r. barzilay and m. lapata. aggregation via set partitioning for 

id86.hlt-naacl-2006. 
 
 
 
 

 
    constraints: 

    transitivity: if (ei,ej)were aggregated, and (ei,ejk) were too, then (ei,ek) 

get aggregated. 

    max number of facts aggregated, max sentence length.  

 

2:49 

mt & alignment  

    ulrich germann, mike jahr, kevin knight, daniel marcu, and 

kenji yamada. fast decoding and optimal decoding for 
machine translation. acl 2001. 

    john denero and dan klein. the complexity of phrase 

alignment problems. acl-hlt-2008. 

2:50 

learning based java: translating to ilp 

    constraint syntax based on first order logic 

    declarative; interspersed within pure java 
    grounded in the program   s java objects 

    automatic run-time translation to linear inequalities 

    creates auxiliary variables 
    resulting ilp size is linear in size of propositionalization 

3:51 

summary of examples 

    we have shown several different nlp solutions that use 

ccms. 
 

    in all cases, knowledge about the problem can be stated as 
constraints in a high level language, and then transformed 
into linear inequalities.  
 

    learning based java (lbj) [rizzolo&roth    07,    10]  describes an 
automatic way to compile high level description of constraint 
into linear inequalities.  

http://cogcomp.cs.illinois.edu/page/software 

 

2:52 

part 3: id136 

3:1 

this tutorial: constrained conditional models 

 
    part 3: id136 algorithms  (15 minutes) 

    exact algorithms 
    relaxation methods 
    approximate algorithms 

what is a constrained conditional model? 

modeling nlp problem 
    variables, features and constraints 

objective function 
    constrained conditional model 

constrained optimization language 
    how to represent id136? 

integer linear program 

id136 
    how to solve it? 

learning 
    how to learn the objective 

function? 

several id136 algorithms: exact 
ilp, search, relaxation; dynamic prog. 

learning    and   . several learning 
strategies: l+i, ibt, others. 

1: 3 

id136 strategies 

solving an ilp directly 

1.
2. problem specific approaches 

    id145 
3. relaxing constraints 
    cutting place strategy 
    id209 and lagrangian relaxation 
    lp relaxation 

4. approximation methods 

    id125 

5. amortized id136 

    data set optimization rather than instance based optimization 

3:4 

1. id136 by solving an ilp directly 

    several powerful off-the-shelf solvers available 

    gurobi 
    xpress-mp 
    glpk 
    lpsolve  

    r 
    mosek 
    cplex 

- most solvers have good  
     apis 
-
     gurobi, xpress-mp,   glpk 

lbj provides hooks for  

    id136 problems in nlp 

    sometimes actually easy for ilp solvers 

    id14  

    5217 instances, with an average of 146 variables and 51 constraints each 

takes ~13 seconds to solve 

    entities-relations [roth and yih, 2004] 

    problem is known to be not totally unimodular 
    ilp solver still efficient! 

 

3:5 

2. problem specific approaches 

    how to get  a score for the pair? 
    the ccm approach: 

    introduce an internal structure (characters) 
    constrain character mappings to    make sense   . 

3:6 

id68 discovery with ccm 

    natural constraints 

score = sum of the mappings     
weight 
s. t. mapping satisfies constraints 

score = sum of the 
mappings    weight 

 
 
 
 
 
    pronunciation constraints 
 
    one-to-one  
    non-crossing 
 
       
 
 

a weight is assigned to each edge. 
 
include it or not? a binary decision. 

    the problem now: id136 

    how to find the best mapping that satisfies the constraints? 

 

3:7 

finding the best character mappings 

    an integer linear 

maximize the mapping score 

pronunciation constraint 

programming problem 
 
 
 
 
 
 
 

non-crossing constraint 

one-to-one constraint 

    what is the best id136 

algorithm? 

 

 

3:8 

finding the best character mappings 

a id145 algorithm 

 
 
 
 
 
 
 
 

maximize the mapping score 

restricted mapping constraints 

weighted id153! 

one-to-one constraint 

non-crossing constraint 

    exact and fast!  

take home message: 
although ilp can solve most 
problems, the fastest id136 
algorithm depends on the 
constraints and can be simpler  

3:9 

3. relaxing constraints 

    given an ilp, find a set of constraints that make the problem 

   hard    to solve 
    eg. in sequence labeling, without long-range constraints, the id136 

is tractable. the long-range constraints make the problem difficult. 

    solve the easier problem without these constraints  

    maybe incrementally introduce the    difficult    constraints into the 

problem 
    examples 

    cutting plane approach [riedel and clarke, emnlp 2006] 
    id209/ lagrangian relaxation [rush and collins. 2010, 2011, 

chang and collins 2011] 

    lp relaxation [roth and yih, icml 2005] 
    dropping the integrality constraints 
    exact solution if the constraints are totally unimodular 

3:10 

extending id14 

changing 

a1:  the thing  

a0: the causer  

am-tmp: temporal 

of the transformation 

agent of 
action 

   the field goal of brien changed the game in the fourth quarter 
 
 
 
 
 
 
 
 
 

uses cutting plane approach: introduce agreement constraints only if 
they are violated 

consistency enforced 

preposition relations 

v. srikumar and d. roth. a joint model for extended semantic role 
labeling. emnlp 2011. 

verb arguments 

temporal 

2:11 

id209: combining different parsers 

[rush et al, 2010] 

    combining dependency parser and constituent parser  
    the parsers should agree on their dependencies 

 
 

the two trees do 
not agree on the 
head of the word 
some. 

id136 iteratively removes disagreements to reach  
consensus 

3:12 

4. approximate id136 

    when ilp solver isn   t fast enough, and one can resort to 

approximate solutions. 
 

    id125 

    we will see an example next 

 

3:13 

example: search based id136 for srl 

    the objective function 

 
         
          
 
 
 

max

       
ij x
c
ij

ji
,

maximize total score subject to 
linguistic constraints 

classification confidence 

indicator variable 
assigns the j-th class for the i-th token 

    constraints 

    unique labels 
    no overlapping or embedding 
    if verb is of type a, no argument of type  b 
        

    intuition: check constraints    violations on partial assignments 

 

3:14 

id136 using id125 

shape: argument 
 
color: label 
 
beam size = 2, 
constraint: 
only one red 

rank them 
according to 
classification 
confidence!  

    for each step, discard partial assignments that violate 

constraints! 

rank them according to 
classification confidence!  

3:15 

heuristic id136 

    problems of heuristic id136 

    problem 1: possibly, sub-optimal solution 
    problem 2: may not find a feasible solution 

    drop some constraints, solve it  again 

 

    using search on srl gives comparable results to using ilp, 

but is much faster.   

3:16 

other id136 options 

    amortized id136 

    [srikumar et. al, emnlp 2012] 
    a way of speeding up any id136 algorithm 
    key idea:  

    consider id136 over an entire data set  
    identify examples for which previous solutions can be re-used 
    speedup obtained by not running id136 

    other search algorithms 

    a-star, hill climbing    
    id150 id136 [finkel et. al, acl 2005] 

    id39: enforce long distance constraints 
    can be considered as : learning + id136 
    one type of constraints only 
 

 
 

3:17 

id136 methods     summary 

    why ilp?  a powerful way to formalize the problems  

    however, not the only algorithmic solution 
 

    heuristic id136 algorithms are useful sometimes! 

    id125 
    other approaches: annealing     

 

    sometimes, a specific id136 algorithm can be designed 

    according to your constraints 

 

3:18 

constrained conditional models     1st part 

 
    introduced ccms as a formalisms that allows us to 

    learn simpler models than we would otherwise 
    make decisions with expressive models, augmented by declarative 

constraints 

    focused on modeling     posing nlp problems as ilp problems 

    1. sequence tagging          (id48/crf + global constraints) 
    2. srl                                    (independent classifiers + global constraints)  
    3. sentence compression (language model + global constraints) 

    described id136 

    solving id136 problems, exactly & approximately 

    second half     learning 

    supervised setting, and supervision-lean settings 

part 4: learning paradigms 

4: 1 

this tutorial: constrained conditional models (part ii) 

 

    part 4: training paradigms (20 min) 

    learning models 

    independently of constraints (l+i); jointly with constraints (ibt) 
    decomposed to simpler models 

4: 2 

training constrained conditional models  

decompose model 

   

learning model 
    independently of the constraints (l+i) 
    jointly, in the presence of the constraints (ibt) 
    decomposed to simpler models 

decompose model from constraints 

 
 
 

4: 3 

where are we?  

    modeling & algorithms for incorporating constraints  

    showed that ccms allow for formalizing many problems  
    showed several ways to incorporate global constraints in the decision.  

 
   

training: coupling vs. decoupling training and id136.  
    incorporating global constraints is important but 
    should it be done only at evaluation time or also at training time? 
    how to decompose the objective function and train in parts? 
    issues related to: 

    modularity, efficiency and performance, availability of training data 
   

problem specific considerations 

4: 4 

training constrained conditional models  

   

learning model 
    independently of the constraints (l+i) 
    jointly, in the presence of the constraints (ibt) 

decompose model from constraints 

    note that id170 algorithms (s-id166, s-id88, crfs) 

can be used both  

    when we decide to learn jointly (ibt) 
    when the left part is structured but we still want to add additional 

constraints at id136 time  

    it is possible to train a model (left side), but make decisions with 

additional information (right side)  that was not incorporated during 
learning model.  (not available, not needed, or just too expensive)  

4: 5 

comparing training methods 

    option 1: learning + id136 (with constraints) 

    ignore (some) constraints during training 
 

    option 2:  id136 (with constraints) based training  

    consider constraints during training 

 

    in both cases: global decision making with constraints 

 

    question: isn   t option 2 always better?  

 

    not so simple     

    next,  the    local model  story    

4: 6 

intuition: solving multi-class with binary classifiers 

    multiclass classifier 

    function    f : rd     {1,2,3,...,k} 

 

    not always easy 
    constrained classification:  
     [har-peled et. el 2002; crammer et. al 2002] 
    but the way we typically address it is via 1-vs- all:  

real 
problem: 

    decompose into binary problems 
    it works quite well even though 1-vs-all is not expressive enough 

 

4: 7 

training methods 

learning + id136  (l+i) 
learn models independently 

id136 based training (ibt) 
learn one model, all y   s together! 

 
intuition: learning with 
constraints may make 
learning more difficult  
 

x3 

x4 

x1 

x2 

x6 

x5 

x 
x7 

f1(x) 

f2(x) 

f3(x) 

y2 

y3 

y1 

y4 

y5 

y 

f4(x) 

f5(x) 

4: 8 

training with constraints 
example: id88-based global learning: structured id88 

true global labeling 

y 

-1  1 

-1 

-1 

1 

apply constraints (id136):  
local predictions:  

y      
y      

-1  1 
-1  1 

1 
-1 

1 
1 

1 
1 

f1(x) 

f2(x) 

f3(x) 

x3 

x4 

x 

x1 

x2 

x6 

x5 

x7 

y 

f4(x) 

f5(x) 

which one is better?  
when and why? 

4: 9 

l+i & ibt: general view     structured id88 

    graphics for the case:  f(x,y) = f(x) 

for each iteration 
  for each (x, ygold ) in the training data 
 
 
 
 
 
  endfor 
 

ypred= 
if  ypred != ygold 
 
endif 

 
 
 

 

 

   =    + f(x, ygold ) - f(x, ypred)  

the difference between  

l+i and ibt 

4: 10 

claims [punyakanok et. al , ijcai 2005] 

    theory applies to the case of local models  

    f(x,y) = f(x ); applies broadly, e.g., srl 

 

    when the local modes are    easy    to learn, l+i outperforms ibt. 

    in many applications, the components are identifiable and easy to learn (e.g., 

argument, open-close, per). 

    only when the local problems become difficult to solve in isolation, ibt 

outperforms l+i, but needs a larger number of training examples. 
 
 
 
 

l+i: cheaper computationally; modular 
ibt  is better in the limit, and when there is strong 
interaction among y   s   

    other training paradigms are possible 

    pipeline-like sequential models: [roth, small, titov: ai&stat   09] 

     identify a preferred ordering among components 
     learn k-th model jointly with previously learned models 

    constrained driven learning [chang et. al   07,12; later] 

4: 11 

l+i vs ibt  

l+i vs. ibt: the more identifiable 
indiviid78 are, the better 
overall performance is with l+i 

    local         

    global 

         opt + ( ( d log m + log 1/   ) / m )1/2 
       0 + ( ( cd log m + c2d +  log 1/   ) / m )1/2 

indication for 
hardness of 

problem 

bounds 

simulated data 

 

y
c
a
r
u
c
c
a

  opt=0.1 
  opt=0 

  opt=0.2 

 

y
c
a
r
u
c
c
a

# of examples 

# of examples 

4: 12 

relative merits: srl 

in some cases problems are hard due 
to lack of training data.  

semi-supervised learning  

l+i is better. 
 
when  the problem is 
artificially made 
harder, the tradeoff is 
clearer.  

hard 

difficulty of the learning problem 

(# features) 

easy 

4: 13 

training constrained conditional models (ii) 

decompose model 

   

   

   

   

decompose model from constraints 

learning model 
    independently of the constraints (l+i) 
    jointly, in the presence of the constraints (ibt) 
    decomposed to simpler models 
local models (trained independently)  vs.  structured models 
    in many cases,  structured models might be better due to expressivity 
but, what if we use constraints? 
local models + constraints     vs.            structured models + constraints 
    hard to tell: constraints are expressive  
    for tractability reasons, structured models have less expressivity than what   s 

possible with constraints; l+i could be better then, and easier to learn. 

4: 14 

recall: example 1: sequence tagging (id48/crf) 

id48 / crf: 

example: 

y   = argmax

y2y

p (y0)p (x0jy0)

n  1yi=1

p (yijyi  1)p (xijyi)

the 
d 

n 

a 

v 

man 
d 

n 

a 

v 

saw 
d 

n 

a 

v 

the 
d 

n 

a 

v 

dog 
d 

n 

a 

v 

  0;y1fy0=yg +

  i;y;y0 1fyi=y ^ yi  1=y0g

  0;y = log(p (y)) + log(p (x0jy))
  i;y;y0 = log(p (yjy0)) + log(p (xijy))

as an ilp: 

maximize xy2y

subject to

1fy0=yg = 1

n  1xi=1xy2y xy02y
xy2y
8y; 1fy0=yg = xy02y
1fyi  1=y0 ^ yi=yg = xy002y

8y; i > 1 xy02y
n  1xi=1xy2y

1fy0=\v"g +

1fyi  1=y ^ yi=\v"g    1

discrete predictions 

1fy0=y ^ y1=y0g

1fyi=y ^ yi+1=y00g

feature consistency 

there must be a verb! 

4: 15 

example: crfs are ccms   

but, you can do better 

    consider a common model for sequential id136: id48/crf                                                                          

    id136 in this model is done via                 
     the  viterbi algorithm.  

y1  y2  y3  y4  y5 
y 
x x1  x2  x3  x4  x5 

 
 

 
 
s 

a 
b 
c 

a 
b 
c 

a 
b 
c 
id136. 
    it is a shortest path problem, which is a lp, with a canonical matrix that is 

    viterbi is a special case of the id135 based 

a 
b 
c 

a 
b 
c 

totally unimodular. therefore, you get integrality constraints for free.  

t 

    one can now incorporate non-sequential/expressive/declarative 

constraints by modifying this canonical matrix 

    no value can appear twice; a specific value must appear at least once; a   b 

    and, run the id136 as an ilp id136. 

 

learn a rather simple model; make decisions with a more expressive model 

4: 16 

experiment: id14 revisited 

s 

a 
b 
c 

a 
b 
c 

a 
b 
c 

a 
b 
c 

a 
b 
c 

t 

    sequential models 

    conditional random field  
    global id88 

    training: sentence based 
    testing: find best global 

assignment (shortest path)  
    + with constraints 

    local models 

    id28 
    avg. id88 

    training: token based 
    testing: find best assignment 

locally   
    + with constraints (global) 

4: 17 

which model is better? id14 

    experiments on srl: [roth and yih, icml 2005] 

    story: inject expressive constraints into conditional random field 

 

 

sequential models 
l+i 

ibt 

model 
baseline 

crf-ml 
66.46 

crf-d 

crf-ibt 

69.14 

 

local 
l+i 

avg. p 
58.15 

+ constraints 
training time 

71.94 
48 

73.91 
38 

 local models are now better than sequential models!  

sequential models are better than local models !  

69.82 
145 

74.49 
0.8 

(with constraints)  
(no constraints)  

4: 18 

summary: training methods     supervised case 

    many choices for training a ccm 

    learning + id136  (training w/o  constraints; add constraints later) 
    id136 based learning (training with constraints) 

    based on this, what kind of models should you use?  

    decomposing models can be better that structured models   

 

    advantages of l+i 

    require  fewer training examples 
    more efficient; most of the time, better performance 
    modularity; easier to incorporate already learned models. 

    bottom line: l+i is better when y-level interactions are not 

very strong  

    next: soft constraints; supervision-lean models 

4: 19 

part 5: constraints driven 

learning 

5: 1 

training constrained conditional models  

 
 
 
 
learning constraints    weights 
    independently of learning the model  
    jointly, along with learning the model  
dealing with lack of supervision 
    constraints driven semi-supervised learning (codl) 
    learning constrained latent representations  
    indirect supervision  

   

   

5: 2 

soft constraints 

 

 

  pk

    hard versus soft constraints 
  i = 1
    hard constraints:  fixed penalty 
    soft constraints:   need to set the penalty 

i=1   kd(y; 1ci(x))

 

    why soft constraints? 

    some constraint violations are more serious than others 
    an example can violate multiple constraints, multiple times! 
    sometime we cannot make a prediction that violates no constraint 
    degree of violation is only meaningful when constraints are soft! 

5: 3 

information extraction without prior knowledge 

lars ole andersen . program analysis and specialization for the 

c programming language.  phd thesis. diku , 

 
university of copenhagen, may 1994 . 

[author]   
 
[title]    
[editor]  
 
[booktitle]  
 
[tech-report]   
[institution]  
 
[date]   

 

prediction result of a trained id48 
    lars ole andersen . program analysis and 
 
 
 
 
 
 
 

specialization for the  
c  
programming language 
.  phd thesis . 
diku , university of copenhagen , may 
1994 . 

 
 
 
  
 
 

violates lots of natural constraints! 

1: 4 

 
examples of constraints 
 

    each field must be a consecutive list of words and can appear 

at most once in a citation.  

 
    state transitions must occur on punctuation marks. 
 
    the citation can only start with author or editor.  
 
    the words pp., pages correspond to page. 
    four digits starting with 20xx and 19xx are date. 
    quotations can appear only in title 
          . 

easy to express pieces of    knowledge    
non propositional; may use quantifiers  

1: 5 

degree of violations 

one possibility: count how many times the assignment y violates a  constraint  

d(y; 1c(x)) =pt

j=1   c (yj)

1 - if assigning yi to xi violates the constraint c  
with respect to assignment (x1,..,xi-1;y1,   ,yi-1) 
 
0  - otherwise 

  c(yj) =

state transition must occur on  
punctuations. 

 
   
 

i y
,
i

       

y
i

x
i

1
   

1
   

 is a punctuation

lars 
lars 
auth 
editor 
editor 
auth 
  c(y1)=0    c(y2)=1    c(y3)=1    c(y4)=0 
  c(y1)=0    c(y2)=0    c(y3)=1    c(y4)=0 

andersen 
andersen 
editor 
editor 

ole 
ole 
book 
auth 

. 
. 

     c(yi) =1 
     c(yj) =2 

5: 6 

reason for using degree of violation 

    an assignment might violate a constraint multiple times 
    allows us to chose a solution with fewer constraint violations 

lars 
auth 
editor 
  c(y1)=0    c(y2)=0    c(y3)=1    c(y4)=0 

andersen 
editor 

ole 
auth 

. 

lars 
auth 
editor 
  c(y1)=0    c(y2)=1    c(y3)=1    c(y4)=0 

andersen 
editor 

ole 
book 

. 

       the first one is 
better because of 
d(y,1c(x))!  

5: 7 

learning the constraints    weights 

 
 

      f (x; y)   pk

i=1   kd(y; 1ci(x))

    strategy 1: independently of learning the model  
    handle the learning parameters     and the penalty     separately 
    learn a feature model and a constraint model 
    similar to l+i, but also learn the penalty weights 
    keep the model simple 
 
    strategy 2: jointly, along with learning the model  
    handle the learning parameters      and the penalty     together 
    treat soft constraints as high order features 
    similar to ibt, but also learn the penalty weights 
 

5: 8 

strategy 1: independently of learning the model  

    model: (first order) hidden markov model p   (x,y) 
    constraints: long distance constraints 

    the i-th the constraint: 
    the id203 that the i-th constraint ci is violated p(ci = 1) 

    assumption: product of experts 

 

    the learning problem 

    given labeled data, estimate    and p(ci = 1) 
    for one labeled example, 

 
score(x; y) = id48 id203    constraint violation score
 

    training: maximize the score of all labeled examples! 

  

 

 

5: 9 

strategy 1: independently of learning the model (cont.) 

 
 

score(x; y) = id48 id203    constraint violation score

    the new scoring function is a ccm! 

    setting  
    new score: 

  i =    log p (ci=1)

p (ci=0)

 
    maximize this new scoring function on labeled data 

log score(x; y) =       f (x; y)   pk

i=1   id(y; 1ci(x)) + c

    learn a id48 separately 
    estimate p(ci=1)  separately by counting how many times the 

constraint is violated by the training data! 

    the product of experts assumption justifies optimizing the 

model and the constraints    weights separately 

 

 

5: 10 

strategy 2: jointly, along with learning the model  

    the problem is now a standard structured learning problem 

    structured id88, structured id166 
    need to supply the id136 algorithm: 
    for example, structured id166  

maxy wt   (x; y)

 
 

minw kwk2

2 + cpl

i=1 ls(xi; yi; w),

    the function  ls(x,s,w)  measures the distance between gold label and 

the id136 result for this example! 

    simple solution for joint  parameter learning 
    add constraints directly into the id136 problem 
     w=[     ],   (x,y) contains both features and constraint violations    
    it   s also possible to learn the joint weight vector with crf: 

    sum problem in id136 during training [cannot use ilp] 

5: 11 

summary: learning constraints    weights 

    the need for soft constraints 

    constraints can be violated by gold data; some are more important 
    want to have degree of violation 
    experimental evidence: domain specific     soft constraints help for the 

citation & advertisement domain  

    learning constraints    weights 

    independent approach: fix the model 

    learn constraints weights by counting violations  

    joint approach 

    treat constraints as long distance/abstract features 
    structured learning problem: can use sum or max (easier)  

    experimental evidence: joint learning of constraints improves only 

with enough training data. 

    see details in: [chang, ratinov, roth, machine learning journal 2012] 

 

5: 13 

training constrained conditional models  

   

   

learning constraints    weights 
    independently of learning the model  
    jointly, along with learning the model  
dealing with lack of supervision 
    constraints driven semi-supervised learning (codl) 
    learning constrained latent representations  
    indirect supervision  

5: 14 

dealing with lack of supervision 

    goal of this tutorial: learning structured models 
    learning structured models requires annotating structures. 

    very expensive process 

 

    constraints can easy the burden in two ways: 

 

 
 

    constraints can serve as a supervision source 

    will be discussed in the context of semi-supervised learning 
    constrained em 

    the presence of constraints can help amplify simple forms of 

supervision 

    use binary supervision to learn structure 
    indirect supervision  

5: 15 

role of constraints: encoding prior knowledge 

    consider encoding the knowledge that:  

    entities of type a and b cannot occur simultaneously in a sentence  

    the    feature    way        
    requires larger models 
    needs more training data 

    the  constraints  way 

a form of supervision 

    tells the model what it should attend to 
    keep the model simple;  add expressive constraints directly 
    a small  set of constraints 
    allows for decision time incorporation of constraints  

we can use constraints to replace training data 

5: 16 

guiding (semi-supervised) learning with constraints 

    in traditional semi-supervised learning the model can drift 

away from the correct one.  

    constraints can be used to generate better training data 

   

   

at training to improve labeling of un-labeled data (and thus 
improve the model) 
at decision time, to bias the objective function towards favoring 
id124.  

seed examples 

model 

constraints 

better predictions 

better model-based labeled data 

decision time  

constraints 

un-labeled data 

5: 17 

constraints driven learning (codl)  

 

 

[chang, ratinov, roth, acl   07;icml   08,mlj   12] 

supervised learning algorithm parameterized by 
 (w,  ). learning can be justified as an optimization 
 procedure for an objective function 

id136 with constraints:  
augment the training set  

 

t=    

 
(w0,  0)=learn(l)  
for n iterations do 
 
     for each x in unlabeled dataset 
 
 
 
    (w,  ) =    (w0,  0) + (1-   ) learn(t) 

 
 
 

 
 
 

 

 h    argmaxy wt   (x,y) -       k dc(x,y) 
 t=t     {(x, h)} 

 

learn from new training data 
weigh supervised &  
unsupervised models. 

excellent experimental results showing the advantages of using constraints, 
especially with small amounts on labeled data [chang et. al, others] 

5: 18 

value of constraints in semi-supervised learning 

objective function:  

learning w/o constraints: 300 examples. 

learning w 10 constraints 

constraints are used to 
bootstrap a semi-
supervised learner  
poor model + constraints 
used to annotate 
unlabeled data, which in 
turn is used to keep 
training the model.  

# of available labeled examples 

5: 19 

train and test with constraints 

 

key : 
 
no need to modify the id48 
algorithm.  
 
- constraints are used to  
      train the model  
- contribute both to a 
better model and to 
better final predictions. 

5: 20 

codl as constrained hard em 

    hard em is a popular variant of em 
    while em estimates a distribution over all y variables in the e-

step, 

        hard em predicts the best output in the e-step 

y*= argmaxy p(y|x,w) 

    alternatively, hard em predicts a peaked distribution 

q(y) =   y=y* 

 

    constrained-driven learning (codl)     can be viewed as a 

constrained version of hard em:  
 

constraining the 

feasible set 

 

 

     y*= argmaxy:uy   b pw(y|x) 

5: 21 

constrained em: two versions 

    while constrained-driven learning  [codl; chang et al, 07] is a 

constrained version of hard em: 
 

constraining the 

feasible set 

                        y*= argmaxy:uy   b pw(y|x) 
        it is possible to derive a constrained version of em: 
    to do that, constraints are relaxed into expectation 

constraints on the posterior id203 q:  

eq[uy]    b 

    the e-step now becomes: 
              q    =  

 

    this is the posterior id173  model [pr; ganchev et al, 10] 

 
 

5: 22 

which (constrained) em to use? 

    there is a lot of literature on em vs hard em 

    experimentally, the bottom line is that with a good enough (???) 

initialization points , hard em is probably better (and more efficient). 

    e.g., em vs hard em (spitkovsky et al, 10) 

    similar issues exist in the constrained case: codl vs. pr 
    new     unified em (uem)   

    [samdani et. al., talk this wednesday, ml-ii session] 
    uem is a family of em algorithms,  

    parameterized by a single parameter       that  

infinitely many new em algorithms in between.   

    provides a continuum of algorithms     from em to hard em, and 

    implementation wise, not more complicated than em 

 

5: 23 

unsupervised id52: different em instantiations 

    measure percentage accuracy relative to em 

 
 
 

 

 

l

m
e
o
t
 
e
v
i
t
a
e
r
 
e
c
n
a
m
r
o
f
r
e
p

em 

gamma 

initialization with 
40-80 examples 
initialization with 
20 examples 
initialization with 
10 examples 
initialization with 
5 examples 
uniform 
initialization 

hard em 
5: 24 

summary: constraints as supervision 
    introducing domain knowledge-based constraints can help 

guiding semi-supervised learning 
    e.g.    the sentence must have at least one verb   ,    a field y appears once 

in a citation     

    constrained driven learning (codl) : constrained hard em  
    pr: constrained  soft em 
    uem : beyond    hard    and    soft    

    related literature:  

 

    unified em (samdani et al 2012: talk on wednesday) 
    constraint-driven learning (chang et al, 07), 
    posterior id173 (ganchev et al, 10), 
    generalized expectation criterion (mann & mccallum, 08), 
    learning from measurements (liang et al, 09) 

 

5: 25 

training constrained conditional models  

   

   

learning constraints    penalties 
    independently of learning the model  
    jointly, along with learning the model  
dealing with lack of supervision 
    constraints driven semi-supervised learning (codl) 
    learning constrained latent representations  
    indirect supervision  

5: 26 

different types of structured learning tasks 

    type 1: structured output prediction 

    dependencies between different output decisions 
    we can add constraints on the output variables 
    examples: information extraction, parsing, id52,    . 

 

    type 2: binary output tasks with latent structures 

    output: binary, but requires an intermediate representation 

(structure) 

    the intermediate representation is hidden 
    examples: paraphrase identification, te,     

 

5: 27 

id123 

x3 

x4 

    entailment requires an intermediate representation  
    alignment based features  
    given the intermediate features     learn a decision 
    entail/ does not entail  

former military specialist carpenter took the helm at fictitiouscom inc. 
after five years as press official at the united states embassy in the 
united kingdom. 
                            
 
 
 
 
x1 
 
 
 
 
 
                                         
 
 
 
 

 
jim carpenter worked for the us government. 

but only positive entailments are expected to have 
a meaningful intermediate representation 

x4 

x2 

x2 

x7 

x6 

x5 

x3 

x1 

 
 

 
 

5: 28 

paraphrase identification 

given an input x 2 x 
learn a model f : x !  {-1, 1} 

    consider the following sentences:  

 

    s1:           druce will face murder charges, conte said. 
 
    s2:           conte said druce will be charged with murder . 

 
 

we need latent variables that explain:  

why this is a positive example. 

    are s1 and s2 a paraphrase of each other? 
    there is a need for an intermediate representation to justify 

this decision 
 

given an input x 2 x 
learn a model f : x  ! h !  {-1, 1} 

5: 29 

structured output learning 

structure output problem: 
dependencies between 
different outputs 

use constraints 
to capture the 
dependencies 

y2 

y3 

y1 

x3 

x4 

x1 

x2 

x6 

x5 

x7 

x 

y4 

y5 

y 

5: 30 

standard binary classification problem 

single output problem: 
only one output 

constraints!? 

y1 

y 

x3 

x4 

x1 

x2 

x6 

x5 

x7 

x 

5: 31 

binary classification problem with latent representation 

 

binary output problem 
with latent variables 

f2 

f1 

f3 

f4 

f5 

x 

x3 

x4 

x1 

x2 

x6 

x5 

x7 

y1 

y 

use constraints 
to capture the 
dependencies on 
latent 
representation 

5: 32 

algorithms: two conceptual approaches  

 
 
 

 

feedback 

structure prediction 

binary prediction 

            

      ,            

b 

binary 
 label 

predicted  structure 

feature  representation 

input 

    two stage approach (typically used for te and paraphrase identification) 

    learn hidden variables; fix it 

    need supervision for the hidden layer (or heuristics) 

    for each example, extract features over x and (the fixed) h. 
    learn a binary classier 

    proposed approach: joint learning  

    drive the learning of h from the binary labels 
    find the best h(x)  [use constraints here to search only for    legitimate    h   s] 
    an intermediate structure representation is good to the extent is supports 

better final prediction.  

    algorithm? 

5: 33 

learning with constrained latent representation (lclr): intuition 

    if x is positive 

    there must exist a good explanation (intermediate representation) 
     9 h, wt   (x,h)    0 
    or, maxh wt   (x,h)    0 

this is an id136 step that will 
gain from the ccm formulation  

ccm on the latent structure 

    if  x is negative  

    no explanation is good enough to support the answer  
     8 h, wt   (x,h)    0 
    or, maxh wt   (x,h)    0 

new feature vector for the final decision. 
chosen h selects a representation. 

 

    altogether, this can be combined into an objective function: 
                minw 1/2||w||2   +  c   i l(1-zimaxh 2 c wt    {s} hs   s (xi)) 
id136: best h subject to constraints c 

 

5: 34 

optimization 

    non convex, due to the maximization term inside the global 

minimization problem 

    in each iteration: 

    find the best feature representation h* for all positive examples (off-

the shelf ilp solver) 

    having fixed the representation for the positive examples, update w 

solving the id76 problem: 
 
 

    not the standard id166/lr: need id136 

    asymmetry: only positive examples require a good 

intermediate representation that justifies the positive label.  
    consequently, the objective function decreases monotonically  

5: 35 

iterative objective function learning 
generate features 

id136 

 
 
best h subj. to c 
 
 
 
 
 
 
 
 

update weight 
vector 

initial objective 

function  

training 
w/r to binary 
decision label 

a ccm goes here: restrict 
possible hidden structures 

considered.  

prediction 
with inferred h 

feedback relative 
to binary problem 

    formalized as structured id166 + constrained hidden structure 
    lcrl: learning constrained latent representation 

5: 36 

learning with constrained latent representation (lclr): framework 

    lclr provides a general id136 formulation that allows the 

use of expressive constraints to determine the hidden level 
    flexibly adapted for many tasks that require latent representations.  
 
 
 

h: problem specific  
declarative constraints   x 

lclr model 

h 

    id141: model input as graphs, v(g1,2), e(g1,2) 

    four (types of) hidden variables:  

    hv1,v2     possible vertex mappings; he1,e2     possible edge mappings  

    constraints: 

    each vertex in g1 can be mapped to a single vertex in g2 or to null 
    each edge in g1 can be mapped to a single edge in g2 or to null 
    edge mapping active iff the corresponding node mappings are active 

 
 

5: 37 

y 

experimental results 

id68: 

recognizing id123: 

paraphrase identification:* 

5: 38 

summary 

    many important nlp problems require latent structures 

 

    lclr: 

    an algorithm that applies ccm to a latent structure  
    can be used for many different nlp tasks 
    easy to inject linguistic constraints on latent structures 
    a general learning framework that is good for many id168s 

 

    take home message: 

    it is possible to apply constraints on many important problems with 

latent variables! 

5: 39 

training constrained conditional models  

   

   

learning constraints    penalties 
    independently of learning the model  
    jointly, along with learning the model  
dealing with lack of supervision 
    constraints driven semi-supervised learning (codl) 
    learning constrained latent representations  
    indirect supervision  

5: 40 

indirect supervision for id170 

    our goal is to exploit constraints to aid learning structures. 
    intuition:   

    if the y variables we are after are tightly coupled (via constraints)  
       perhaps supervising some of them could be propagated to others 

and amplify the weak supervision 

 

    before, the structure was in the intermediate level 

    we cared about the structured representation only to the extent it 

helped the final binary decision 

    the binary decision variable was given as supervision 

    what if we care about the structure? 

    information & id36; id52, id29  

    invent a companion binary decision problem! 

5: 41 

information extraction 

lars ole andersen . program analysis and specialization for the 

c programming language.  phd thesis. diku , 

 
university of copenhagen, may 1994 . 

[author]   
[title]    
 
[editor]  
 
 
[booktitle]  
[tech-report]   
[institution]  
 
[date]   

 

prediction result of a trained id48 
    lars ole andersen . program analysis and 
 
 
 
 
 
 
 

specialization for the  
c  
programming language 
.  phd thesis . 
diku , university of copenhagen , may 
1994 . 

 
 
 
  
 
 

5: 42 

id170 

x 

h 

y 

    the problem we have now is a    real    structure learning 

problem.  
    we will reduce the previous problem to this one by introducing a 
   fictitious    companion binary variable that is easy to supervise.  

 

    invent a companion binary decision problem! 
    parse citations: lars ole andersen . program analysis and specialization 

for the c programming language.  phd thesis. diku , university of 
copenhagen, may 1994 . 
    companion: given a citation; does it have a legitimate citation parse? 

    id52 

    companion: given a word sequence, does it have a legitimate pos 

tagging sequence? 

    binary supervision is almost free 

5: 43 

companion task binary label as indirect supervision 

    the two tasks are related just like the binary and structured 

tasks discussed earlier 
 
 
 
 

    all positive examples must have a good structure 
    negative examples cannot have a good structure 
x 
    we are in the same setting as before 

h 

y 

    binary labeled examples are easier to obtain 
    we can take advantage of this to help learning a structured model  
    algorithm: combine binary learning and structured learning 

 

5: 44 

learning structure with indirect supervision 

    in this case we care about the predicted structure 
    use both structural learning and binary learning 

predicted 

correct 

the feasible structures 
of an example 

negative examples cannot 
have a good structure 

negative examples restrict 
the space of hyperplanes  
supporting the decisions for x 

5: 45 

joint learning framework 

    joint learning  : if available, make use of both supervision types 

i 
   

companion task 

target task 
a 
l 
t 
yes/no 
       
   
   
   
id168     same as described earlier. 

i   l   l  i  n o  i  s 
    
   
   

y 
   

   

   

 key: the same parameter w for both components 

    

1min
2

w

t
cww
1

+

   

si
   

cwyxl
s

+

)

(

;

,

i

i

   

bi
   

2

wzxl
)
b

(

;

,

i

i

loss on target task 

loss on companion task 

5: 46 

experimental result 

    very little direct (structured) supervision.  

 
 
85
 
80
 
75
 
70

65

60

55

phonetic
alignment

pos

ie

 a

direct
(structured
labeled) only

5: 47 

experimental result 

    very little direct (structured) supervision.  
    (almost free) large amount binary indirect supervision 

 
85
 
80
 
75
 
70
 
65

60

55

direct
(structured
labeled) only

direct + indirect
(both structured
and binary)

5: 48 

phonetic
alignment

pos

ie

more on dealing with minimal of supervision 

    constraint driven learning 

    use constraints to guide semi-supervised learning! 

 
 

    use binary labeled data to help structured output prediction 
    training structure predictors by inventing (easy to supervise) binary 

labels  
 

    driving supervision signal from world   s response  

    efficient id29 = ccm + world   s response 

5: 50 

connecting language to the world 
connecting language to the world 

can we rely on this interaction to 
provide supervision (and,  
eventually, recover meaning) ? 

can i get a coffee with sugar 
and no milk 

great! 

arggg 

semantic parser 

make(coffee,sugar=yes,milk=no) 

    how to recover meaning from text? 
    annotate with meaning representation; use (standard)    example based    ml 

    teacher needs deep understanding of the learning agent  
    annotation burden; not scalable. 

    instructable computing 

    natural communication between teacher/agent  

 

5: 51 

real world feedback 

supervision = expected response 
x 

   what is the largest state that borders ny?" 

 
 

traditional approach: 
our approach: use 
learn from logical forms 
only the responses  
and gold alignments 

nl 
query 
logical 
query  
query 
response: 
query  
response: 

 

r 
y 

expensive! 

 
 
                     
binary 
                              check if predicted response == expected response 
supervision 

pennsylvania 

pennsylvania 

r 

  

system 

largest( state( next_to( const(ny)))) 

interactive computer 

id29 is a  id170 problem:  
identify mappings from text to a meaning representation 
 
we will use a ccm formulation with a lot of    legitimacy    constraints 

expected : pennsylvania 
predicted : pennsylvania 
positive response 

expected : pennsylvania 
predicted : nyc 

negative response 

train a structured predictor with this binary supervision ! 

5: 52 

response based learning  

difficulty:  
- need to generate training examples 
- negative examples give no information 

basic algorithm: 
- try to generate good structures 
- update parameters based on current 

examples 

- coarse use of incorrect structures 

repeat 
   for all input sentences do 
     find best logical representation y 
      given current w 
     query feedback function 
   end for 
   learn new w using feedback 
until convergence 

train: try to get more positive examples (representations with positive feedback) 
direct (binary) protocol: a binary classifier on positive/negative ex   s 
                   (problem: many good structures are being demoted) 
structured protocol: use only correct structures.  
                    (problem: ignores negative feedback) 

5: 53 

empirical evaluation [conll   10,acl   11, ijcai   11] 

    key question: can we learn from this type of supervision? 

 

algorithm 

# training 
structures 

no learning: initial objective fn 
binary signal: binary protocol                                          
binary signal: structured protocol  
improved protocol: 
wm*2007   (fully supervised     uses 
gold structures)  

0 
0 
0 
0 
310 

test set  
accuracy 
22.2% 
69.2 %  
73.2 % 
79.6% 
75 % 

*[wm]   y.-w. wong and r. mooney. 2007. learning synchronous grammars for semantic 
parsing with id198. acl. 

5: 54 

summary 

    constrained conditional models:  computational framework 
for global id136 and a vehicle for incorporating knowledge 
 

    direct supervision for structured nlp tasks is expensive 

    indirect supervision is cheap and easy to obtain 

 

    we suggested learning protocols for indirect supervision  

    make use of simple, easy to get, binary supervision  
    showed how to use it to learn structure and latent structures 
    ccm id136 is key in propagating the simple supervision 

 

    learning structures from real world feedback 

    obtain binary supervision from    real world    interaction 
    indirect supervision replaces direct supervision 

5: 55 

this tutorial: constrained conditional models (part ii) 

    part 6: conclusion (& discussion)  (10 min) 

    building ccms;  features and constraints. mixed models vs. joint models;  
    where is knowledge coming from 

the end 

6: 1 

conclusion 

    constrained conditional models combine 

    learning conditional models with using declarative expressive constraints 
    within a constrained optimization framework 

 

    our goal was to describe: 

    a clean way of incorporating constraints to bias and improve decisions of 

learned models 

    a clean way to use (declarative) prior knowledge to guide semi-supervised 

learning 

    ways to make use of (declarative) prior knowledge when choosing 

intermediate (latent) representations.   
 

    provide examples for the diverse usage ccms have already found 

in nlp 
    significant success on several nlp and ie tasks (often, with ilp)  

6: 2 

what is a constrained conditional model? 

modeling nlp problem 
    variables, features and constraints 

objective function 
    constrained conditional model 

constrained optimization language 
    how to represent id136? 

integer linear program 

id136 
    how to solve it? 

learning 
    how to learn the objective 

function? 

several id136 algorithms: exact 
ilp, search, relaxation; dynamic prog. 

learning    and   . several learning 
strategies: l+i, ibt, others. 

1: 3 

technical conclusions 

    presented and discussed modeling issues  

    how to improve existing models using declarative information  
    incorporating expressive global constraints into simpler learned models  

    discussed id136 issues 

    often, the formulation is via an integer id135 formulation, 

but algorithmic solutions can employ a variety of algorithms. 

    training issues     training protocols matters 

    training with/without constraints; soft/hard constraints;  
    performance, modularity and ability to use previously learned models.  
    supervision-lean models 

    we did not attend to the question of    how to find constraints    

    emphasis on: background knowledge is important, exists, use it. 
    but, it   s clearly possible to learn constraints. 

6: 4 

summary: constrained conditional models 

conditional markov random field 

y1 

y4 

y5 

y2 
y6 

y7 

y3 
y8 

constraints network 

y1 

y4 

y5 

y2 
y6 

y7 

y3 
y8 

       y* = argmaxy     wi   (x; y) 
  
    linear objective functions  
    typically   (x,y) will be local 
functions, or   (x,y) =   (x)   

                   i   i dc(x,y) 

 

variables  

    expressive constraints over output 

    soft, weighted constraints  
    specified declaratively as fol formulae 

    clearly, there is a joint id203 distribution that represents 

this mixed model.  
    we would like to:  

    learn a simple model or several simple models 
    make decisions with respect to a complex model                  

6: 5 

designing ccms 

thanks! 

y1 

y4 

y5 

y2 
y6 

y7 

y3 
y8 

y1 

y4 

y5 

y2 
y6 

y7 

y3 
y8 

       y* = argmaxy     wi   (x; y) 
  
    linear objective functions  
    typically   (x,y) will be local functions, 

or   (x,y) =   (x)   

                   i   i dc(x,y) 

 

variables  

    expressive constraints over output 

    soft, weighted constraints  
    specified declaratively as fol formulae 

    decide what variables are of interest     learn model (s)  
lbj (learning based java): http://l2r.cs.uiuc.edu/~cogcomp 
    think about constraints among the variables of interest 
programming along with building learned models, high level specification of 
    design your objective function  

a modeling language for constrained conditional models. supports 

constraints and id136 with constraints 

6: 6 

questions? 
    thank you! 

6: 7 

global id136 using integer id135

wen-tau yih

august 15, 2004

1 introduction

this report is a supplemental document of some of our papers [5, 3, 4].
it gives a simple but
complete step-by-step case study, which demonstrates how we apply integer id135 to
solve a global id136 problem in natural language processing. this framework    rst transforms
an optimization problem into an integer linear program. the program can then be solved using
existing numerical packages.

the goal here is to provide readers an easy-to-follow example to model their own problems in
this framework. there are two main parts in this report. sec. 2 describe a problem of labeling
entities and relations simultaneously as our id136 task. it then discusses the constraints among
the labels and shows how the objective function and constraints are transformed to an integer linear
program. although transforming the constraints to their linear forms is not di   cult in this entity
and relation example, sometimes it can be tricky, especially when more variables are involved.
therefore, we discuss how to handle di   erent types of constraints in sec. 3.

2 labeling entities & relations

given a sentence, the task is to assign labels to the entities in this sentence, and identify the relation
of each pair of these entities. each entity is a phrase and we assume the boundaries of these entities
are given.

figure 1 gives an example of the sentence    dole   s wife, elizabeth, is a native of n.c.    in this
sentence, there are three entities, dole, elizabeth, and n.c. we use e1, e2, and e3 to represent
their entity labels. in this example, possible entity labels include other, person, and location.
in addition, we would like to know the relation between each pair of the entities. for a pair of
two entities ei and ej, the relation is represented by rij. in this example, there will be 6 relation
variables     r12, r21, r13, r31, r23, r32. since most entities have no special relation, the value of
most relation variables should be irrelevant. besides this special label, the relations of interest in
this example are spouse of and born in.

assume that magically some local classi   ers have already provided some con   dence scores on

possible labels, as shown in table 1.

if we want to choose the labels that maximize the sum of those con   dence scores, it   s the same
as choosing the label that has the highest score for each variable. the global labeling then becomes:

1

figure 1: a sentence that has 3 entities

variable

e1
e2
e3

other person location
0.05
0.10
0.05

0.85
0.60
0.50

0.10
0.30
0.45

variable

irrelevant

spouse of born in

r12
r21
r13
r31
r23
r32

0.05
0.75
0.85
0.80
0.10
0.65

0.45
0.10
0.05
0.05
0.05
0.20

0.50
0.15
0.10
0.15
0.85
0.15

table 1: the con   dence scores on the labels of each variable.

variable

e1
e2
e3
r12
r21
r13
r31
r23
r32

label
person
person
person
born in
irrelevant
irrelevant
irrelevant
born in
irrelevant

sum

score
0.85
0.60
0.50
0.50
0.75
0.85
0.80
0.85
0.65
6.35

at this point, the problem seems to have been solved by the magic local classi   ers. however,
after a second look at this labeling, we can easily    nd the inconsistency between entity and relation
labels. for example, r12 cannot be born in if both entities e1 and e2 are persons. indeed, there
exists some natural constraints between the labeling of entity and relation variables that the local
classi   ers may not know or respect. in our example, we know the global labeling also strati   es the
following two constraints.

2

dole 's wife, elizabeth , is a native of n.c.(cid:13)e(cid:13)1(cid:13)e(cid:13)2(cid:13)e(cid:13)3(cid:13)r(cid:13)13(cid:13)r(cid:13)31(cid:13)r(cid:13)12(cid:13)r(cid:13)23(cid:13)r(cid:13)21(cid:13)r(cid:13)32(cid:13)    if rij = spouse of, then ei = person and ej = person
    if rij = born in, then ei = person and ej = location
in summary, the problem we want to solve here really is to    nd the best legitimate global

labeling, which is subject to the constraints and maximizes the sum of the con   dence scores.

note that although exhaustive search seems plausible in this toy problem, it soon becomes
intractable when the number of variables or the number of possible labels grows. in the rest of this
section, we are going to show that how we transfer this problem to an integer linear program, and
let the numerical packages help us to    nd the answer.

2.1 indicator variables

in order to apply (integer) id135, both the objective function and constraints have to
be linear. since the con   dence score could be any real number, the original function is not linear.
in addition, the logical constraints we have are not linear as well.

to overcome this di   culty, the    rst step of the transformation is to introduce several indicator
(binary) variables, which represent the assignment of the original variables. for each entity or
relation variable a and each legitimate label k, we introduce a binary variable xa,k. when the
original variable a is assigned label k, xa,k is set to 1. otherwise, xa,k is 0. in our toy example, we
then have 27 such indicator variables:

xe1,other,
xe2,other,
xe3,other,

xe1,person
xe2,person,
xe3,person,

xe1,location,
xe2,location,
xe3,location,
xr12,irrelevant, xr12,spouse of , xr12,born in,
xr21,irrelevant, xr21,spouse of , xr21,born in,
xr13,irrelevant, xr13,spouse of , xr13,born in,
xr31,irrelevant, xr31,spouse of , xr31,born in,
xr23,irrelevant, xr23,spouse of , xr23,born in,
xr32,irrelevant, xr32,spouse of , xr32,born in.

to simplify the notation, let le = {other, person, location} and lr = {irrelevant, spouse of, born in}

represent the sets of entity and relation labels, respectively. assume n = 3 means the number of
entities we have in the sentence. the indicator variables we introduce are:

xei,le, where 1     i     n and le     le
xrij ,lr , where 1     i, j     n, i 6= j, and lr     lr

2.2 objective function
suppose cei,le represents the con   dence score of ei being le, where 1     i     n and le     le, and
crij ,lr represents the con   dence score of rij being lr, where 1     i, j     n, i 6= j and lr     lr. the
objective function f (i.e., the sum of con   dence scores) can be represented by

f = x

1   i   n,le   le

x

cei,lexei,le +

1   i,j   n,i6=j,lr   lr

crij ,lr xrij ,lr

if we plug in the numbers in table 1, the function f is:

f = 0.05   xe1,other + 0.85   xe1,person +       + 0.65   xr32,irrelevant + 0.20   xr32,spouse of + 0.15   xr32,born in

3

inevitably, this transformation also brings new constraints, which come from the fact that one
entity/relation variable can only have one label, and must have one label. for example, only exact
one of the labels other, person, location can be assigned to e1. as a result, only one of the indicator
variables xe1,other, xe1,person, xe1,location can and must be 1. this restriction can be easily written

as the following linear equations.p
p

le   le
lr   lr

xei,le = 1    1     i     n
xrij ,lr = 1    1     i, j     n, i 6= j

2.3 logical constraints

the other reason of introducing indicator variables is to handle the real constraints we have     the
logical constraints between entity and relation labels. let me remind you what they are in our
example:

    if rij = spouse of, then ei = person and ej = person, where 1     i, j     n and i 6= j
    if rij = born in, then ei = person and ej = location, where 1     i, j     n and i 6= j

if we treat the indicator variables as boolean variables, where 1 means true and 0 means false,

the constraints can be rephrased as:

xrij ,spouse of     xei,person     xej ,person 1     i, j     n, and i 6= j
xrij ,born in     xei,person     xej ,location 1     i, j     n, and i 6= j

in fact, these two boolean constraints can be modeled by the following two linear inequalities.

2    xrij ,spouse of     xei,person + xej ,person 1     i, j     n, and i 6= j
2    xrij ,born in     xei,person + xej ,location 1     i, j     n, and i 6= j

let   s do a simple check to see if they are correct. when xrij ,spouse of is 0 (false), xei,person
and xej ,person can be either 0 or 1, and the inequality still holds. however, when xrij ,spouse of is 1
(true), both xei,person and xej ,person have to be 1 (true).

transforming the logical constraints into linear forms is the key of this framework. it is not
hard, but may be tricky sometimes (which makes it an interesting brain exercise). we will talk
more about transforming other types of logical constraints in sec. 3 later.

2.4 solving the integer linear program using xpress-mp

figure 2 shows the complete integer linear program. now, all we need to do now is to apply
some numeric packages, such as xpress-mp [7], cplex [1], or the lp solver in r [6], to solve
it. transferring the solution back to the global labeling we want is straightforward     just    nd
those indicator variables that have the value 1. in this section, i will demonstrate how to apply
xpress-mp to do the job.

the syntax in xpress-mp is fairly easy and straightforward. here i simply list the source code

with some comments, which are the lines beginning with the    !    symbol.

4

max p

1   i   n,le   le

cei,lexei,le +p

1   i,j   n,i6=j,lr   lr

crij ,lr xrij ,lr

subject to:

xei,le     {0, 1}
p
xrij ,lr     {0, 1}
p
xei,le = 1
le   le
xrij ,lr = 1
lr   lr

   1     i     n
   1     i, j     n, i 6= j
   1     i     n
   1     i, j     n, i 6= j

2    xrij ,spouse of     xei,person + xej ,person 1     i, j     n, and i 6= j
1     i, j     n, and i 6= j
2    xrij ,born in     xei,person + xej ,location

(1)
(2)
(3)
(4)
(5)
(6)

figure 2: the complete integer linear program

model "entity relation id136"

uses "mmxprs"

parameters

datafile = "er.dat"
num_entities = 3;

end-parameters

declarations

entities = 1..num_entities
ent_classes = {"other", "person", "location"}
rel_classes = {"irrelevant", "spouseof", "bornin"}

scoreent: array(entities, ent_classes) of real
scorerel: array(entities, entities, rel_classes) of real

end-declarations

! datafile stores the confidence scores from the local classifiers.
initializations from datafile

scoreent

scorerel

end-initializations

! these are the indicator variables. declarations

ent : array(entities, ent_classes) of mpvar
rel : array(entities, entities, rel_classes) of mpvar

end-declarations

! the objective function: sum of confidence scores
obj := sum(u in entities, e in ent_classes) scoreent(u,e)*ent(u,e)

+ sum(u,v in entities, r in rel_classes | u <> v) scorerel(u,v,r)*rel(u,v,r)

5

! constraints (1) and (2): the indicator variables take only binary values
forall(u in entities, e in ent_classes)

ent(u,e) is_binary

forall(e1,e2 in entities, r in rel_classes | e1 <> e2)

rel(e1,e2,r) is_binary

! constraints (3) and (4): sum = 1
forall(u in entities) sum(e in ent_classes)

ent(u,e) = 1

forall(u,v in entities | u <> v) sum(r in rel_classes)

rel(u,v,r) = 1

! constraints (5) and (6): logical constraints on entity and relation labels
forall(e1,e2 in entities | e1 <> e2)

2*rel(e1,e2,"spouseof") <= ent(e1,"person") + ent(e2,"person")

forall(e1,e2 in entities | e1 <> e2)

2*rel(e1,e2,"bornin") <= ent(e1,"person") + ent(e2,"location")

! solve the problem
maximize(obj)

! output the indicator variables that are 1
forall(u in entities, e in ent_classes | getsol(ent(u,e)) >= 1)

writeln(u, " ", e)

forall(e1,e2 in entities, r in rel_classes | e1 <> e2 and getsol(rel(e1,e2,r)) >= 1)

writeln(e1, " ", e2, " ", r)

end-model

3 transforming logical constraints into linear forms

this section summarizes and revises some rules of transforming logical constraints to linear (in)equalities
described in [2]. to simplify the illustration, symbols a, b, c and x1, x2,       , xn are used to represent
indicator variables, which are treated as boolean variables and binary variables at the same. as
usual, the values 0, 1 represents the truth values false and true, respectively.

3.1 choice among several possibilities

in our entity and relation example, we have already processed the constraint    exactly k variables
among x1, x2,       , xn are true   , where k = 1. the general form of this linear equation is:

x1 + x2 +        + xn = k

another constraint,    at most k variables among x1, x2,       , xn can be true   , can be represented

in a similar inequality.

x1 + x2 +        + xn     k

6

uninterestingly,    k or more variables among x1, x2,       , xn must be true    will be

x1 + x2 +        + xn     k

3.2 implications

implications are usually the logical constraints we encounter. while handling two or three variables
may be trivial, extending it to more variables may be tricky. here we illustrate how to develop the
ideas from the simplest case to complicated constraints.

two variables suppose there are only two indicator variables a, b in the implication. the con-
straint, a     b, can be represented as a     b. this can be easily veri   ed by the following truth
table.

a     b
a = 0
a = 1

b = 0
true
false

b = 1
true
true

what if we need to deal with something like a       b? the value of the compliment of b is exactly
the relation    if and only if    is straightforward too. a     b is identical to a     b and b     a. the

1     b. therefore, the corresponding linear constraint is a     1     b, or a + b     1.
corresponding linear constraints are a     b and b     a, which is in fact a = b.

three variables now, let   s try to generalize the implication a little bit to cover three variables.
since a     b     c can be separated as a     b and a     c, the straightforward transformation is to put
two linear inequalities a     b and a     c. alternatively, the transformation in our entity and relation
example    2a     b + c    also su   ce, which is easy to check using a truth table.
another implication, a     b     c, can be modeled by a     b + c. this is because when a = 1, at

least one of b and c has to be 1 to make the inequality correct.
what about the inverse of the above two implications? they can be derived using the compli-
ment and demorgan   s theorem. b     c     a is equivalent to   a     b     c, which is   a       b       c. use the
above rule and the the compliment, it can be modeled by (1    a)     (1    b) + (1    c), or a     b + c    1.
b     c     a is equivalent to b     a and c     a, so it can be modeled by two inequalities b     a and
c     a. alternatively, this can be transformed to   a     b     c, which is   a       b       c. therefore, it can be
modeled by 2(1     a)     (1     b) + (1     c), or b+c

2     a.

more variables a logical constraint that has more variables can be complicated. therefore, we
only discuss some common cases here. suppose we want to model    if a, then k or more variables
among x1, x2,       , xn are true.    we can extend the transformation of a     b   c, and use the following
linear inequality.

a     x1 + x2 +        + xn

k

this transforation is certainly valid for k = 1. it is also easy to verify for other cases. if a = 0,
then the right-hand-side is always larger or equal to 0, and the inequality is satis   ed. however,
when a = 1, it forces at least k x   s are true, which is exactly what we want.
the next case we would like to try is the inverse, which is    if k or more variables among
x1, x2,       , xn are true, then a is true.    this might be somewhat tricker than others. our    rst guess
might be:

(x1 + x2 +        + xn)     (k     1)     a

7

original constraint
exactly k of x1, x2,       , xn
at most k of x1, x2,       , xn
at least k of x1, x2,       , xn
a     b
a =   b
a       b
  a     b
a     b
a     b     c
a     b     c
b     c     a
b     c     a
if a then at least k of x1, x2,       , xn
if at least k of x1, x2,       , xn then a
a = x1    x2        xn

linear form

x1 + x2 +        + xn = k
x1 + x2 +        + xn     k
x1 + x2 +        + xn     k

a     b
a = 1     b
a + b     1
a + b     1
a = b

2

a     b and a     c

or, a     b+c
a     b + c
a     b + c     1
a     b+c
a     x1+x2+      +xn

2

a     x1+x2+      +xn

n

k

a     x1+x2+      +wn   (k   1)
and a     x1 + x2 +        + xn     (n     1)

n   (k   1)

table 2: rules of mapping constraints to linear (in)equalities

although this may seem correct at the    rst glance, we observe that the left-hand-side (lhs) will
be larger than 1 when more than k of the x variables are 1. because a can be either 0 or 1, this
constraint will be infeasible.
in fact, what we really need is to squash the lhs to less than 1.
currently, the largest possible value of the left-hand-side is n     (k     1). therefore, dividing the
lhs by n     (k     1) should su   ce.

(x1 + x2 +        + xn)     (k     1)

n     (k     1)

    a

let   s examine two special cases of this transformation to see if they are correct. remember b   c     a
2     a is exactly
is indeed one of these cases, given that n = 2 and k = 1. the linear inequality b+c
the same as what we derived previously. the other special case is    x1     x2              xn     a   , which is
equivalent to say k = n here. obviously, a     x1 + x2 +       + wn     (n    1) is correct. one interesting
observation is that the conjunction of a set of boolean variables is the same as the product of the
corresponding binary variables. therefore, the nonlinear constraint a = x1    x2        xn is the same
as a = x1     x2                xn. its linear transformation is therefore a     x1 + x2 +        + xn     (n     1) and
a     x1+x2+      +xn

.

n

table 2 summarizes all the transformations we have discussed in this section.

4 conclusions

thanks to the theoretical developments of integer id135 in the last two decades, and
the tremendous improvement on hardware and software technology, numerical packages these days
are able to solve many integer id135 problems within very short time, even though
ilp is in general np-hard.

8

in this report, we have provided an entity and relation problem as example, and discussed several
cases for transforming boolean constraints. we hope these illustrations are helpful to remodeling
your id136 problem, and allow you to take advantage of the numerical lp solvers as well.

references

[1] cplex. ilog, inc. cplex. http://www.ilog.com/products/cplex/, 2003.

[2] c. gu  eret, c. prins, and m. sevaux. applications of optimization with xpress-mp. dash

optimization, 2002. translated and revised by susanne heipcke.

[3] v. punyakanok, d. roth, w. yih, and d. zimak. id14 via integer linear

programming id136. in proceedings of coling 2004, 2004.

[4] v. punyakanok, d. roth, w. yih, d. zimak, and y. tu. id14 via generalized

id136 over classi   ers. in proceedings of conll 2004, 2004.

[5] d. roth and w. yih. a id135 formulation for global id136 in natural language

tasks. in proceedings of conll-2004, pages 1   8, 2004.

[6] the r project for statistical computing. http://www.r-project.org/, 2004.

[7] xpress-mp. dash optimization. xpress-mp. http://www.dashoptimization.com/products.html,

2003.

9

readings on constrained conditional models

[1] e. althaus, n. karamanis, and a. koller. computing locally coherent

discourses. in acl, pages 399   406, barcelona, spain, july 2004.

[2] r. barzilay and m. lapata. aggregation via set partitioning for natural
language generation. in proceedings of the human language technology
conference of the naacl, main conference, pages 359   366, new york
city, usa, june 2006. association for computational linguistics.

[3] p. bramsen, p. deshpande, y. k. lee, and r. barzilay.

inducing tem-
poral graphs. in emnlp, pages 189   198, sydney, australia, july 2006.
association for computational linguistics.

[4] n. chambers and d. jurafsky. jointly combining implicit constraints im-
proves temporal ordering. in proc. of the conference on empirical methods
in natural language processing (emnlp), 2008.

[5] k. chang, r. samdani, a. rozovskaya, n. rizzolo, m. sammons, and
d. roth. id136 protocols for coreference resolution. in conll shared
task, pages 40   44, portland, oregon, usa, 2011. association for compu-
tational linguistics.

[6] m. chang, d. goldwasser, d. roth, and v. srikumar. discriminative learn-

ing over constrained latent representations. in naacl, 6 2010.

[7] m. chang, l. ratinov, n. rizzolo, and d. roth. learning and id136
with constraints. in proceedings of the national conference on arti   cial
intelligence (aaai), july 2008.

[8] m. chang, l. ratinov, and d. roth. guiding semi-supervision with
constraint-driven learning.
in proc. of the annual meeting of the acl,
pages 280   287, prague, czech republic, jun 2007. association for compu-
tational linguistics.

[9] m. chang, l. ratinov, and d. roth. constraints as prior knowledge. in
icml workshop on prior knowledge for text and language processing,
pages 32   39, july 2008.

[10] m. chang, l. ratinov, and d. roth. structured learning with constrained

conditional models. machine learning journal, 2012.

[11] m. chang, v. srikumar, d. goldwasser, and d. roth. structured output

learning with indirect supervision. in icml, 2010.

[12] y. chang and m. collins. exact decoding of phrase-based translation
models through lagrangian relaxation. in proceedings of the 2011 confer-
ence on empirical methods in natural language processing, pages 26   37,
edinburgh, scotland, uk., july 2011. association for computational lin-
guistics.

1

[13] w. che, z. li, y. hu, y. li, b. qin, t. liu, and s. li. a cascaded syntac-
tic and semantic id33 system. in conll, pages 238   242,
manchester, england, august 2008. coling 2008 organizing committee.

[14] y. choi, e. breck, and c. cardie. joint extraction of entities and relations
in proc. of the 2006 conference on empirical

for opinion recognition.
methods in natural language processing (emnlp), 2006.

[15] j. clarke, d. goldwasser, m. chang, and d. roth. driving id29

from the world   s response. in conll, 7 2010.

[16] j. clarke and m. lapata. constraint-based sentence compression: an
integer programming approach. in proc. of the coling/acl 2006 main
conference poster sessions (acl), 2006.

[17] j. clarke and m. lapata. modelling compression with discourse constraints.
in proc. of the conference on empirical methods in natural language
processing and on computational natural language learning (emnlp-
conll), 2007.

[18] j. clarke and m. lapata. global id136 for sentence compression: an
integer id135 approach. journal of arti   cial intelligence
research (jair), 31:399   429, 2008.

[19] hal daum  e iii. cross-task knowledge-constrained self training. in proceed-
ings of the 2008 conference on empirical methods in natural language
processing, pages 680   688, honolulu, hawaii, october 2008. association
for computational linguistics.

[20] j. denero and d. klein. the complexity of phrase alignment problems. in
proceedings of acl-08: hlt, short papers, pages 25   28, columbus, ohio,
june 2008. association for computational linguistics.

[21] p. denis and j. baldridge. joint determination of anaphoricity and corefer-
ence resolution using integer programming. in proc. of the annual meeting
of the north american chapter of the association for computational lin-
guistics - human language technology conference (naacl-hlt), 2007.

[22] p. denis and p. muller. predicting globally-coherent temporal structures
from texts via endpoint id136 and graph decomposition. in proc. of the
international joint conference on arti   cial intelligence (ijcai), 2011.

[23] p. deshpande, r. barzilay, and d. karger. randomized decoding for
selection-and-ordering problems. in human language technologies 2007:
the conference of the north american chapter of the association for com-
putational linguistics; proceedings of the main conference, pages 444   451,
rochester, new york, april 2007. association for computational linguis-
tics.

2

[24] q. do, y. chan, and d. roth. minimally supervised event causality iden-

ti   cation. in emnlp, edinburgh, scotland, 7 2011.

[25] q. do, w. lu, and d. roth. joint id136 for event timeline construction.

in emnlp, 2012.

[26] k. filippova and m. strube. dependency tree based sentence compression.

in iid86, 2008.

[27] k. filippova and m. strube. sentence fusion via dependency graph com-
in emnlp, pages 177   185, honolulu, hawaii, october 2008.

pression.
association for computational linguistics.

[28] j. r. finkel and c. d. manning. the importance of syntactic parsing and
id136 in semantic rolelabeling. in proc. of the annual meeting of the
association for computational linguistics - human language technology
conference, short papers (acl-hlt), 2008.

[29] k. ganchev, j. gra  ca, j. gillenwater, and b. taskar. posterior regulariza-
tion for structured latent variable models. journal of machine learning
research, 2010.

[30] u. germann, m. jahr, k. knight, d. marcu, and k. yamada. fast decod-
ing and optimal decoding for machine translation. in proceedings of 39th
annual meeting of the association for computational linguistics, pages
228   235, toulouse, france, july 2001. association for computational lin-
guistics.

[31] j. v. graca, k. ganchev, and b. taskar. expectation maximization and

posterior constraints. in nips, volume 20, 2007.

[32] m. klenner. grammatical role labeling with integer id135.

in eacl, 2006.

[33] m. klenner. enforcing consistency on coreference sets. in ranlp, 2007.

[34] m. klenner. shallow dependency labeling. in acl, pages 201   204, prague,

czech republic, june 2007. association for computational linguistics.

[35] p. koomen, v. punyakanok, d. roth, and w. yih. generalized id136
with multiple id14 systems (shared task paper). in ido
dagan and dan gildea, editors, proc. of the annual conference on com-
putational natural language learning (conll), pages 181   184, 2005.

[36] r. mcdonald. a study of global id136 algorithms in multi-document

summarization. in ecir, 2007.

[37] v. punyakanok, d. roth, w. yih, and d. zimak. id14 via
integer id135 id136. in proc. the international confer-
ence on computational linguistics (coling), pages 1346   1352, geneva,
switzerland, august 2004.

3

[38] v. punyakanok, d. roth, w. yih, and d. zimak. learning and id136
over constrained output. in proc. of the international joint conference on
arti   cial intelligence (ijcai), 2005.

[39] v. punyakanok, d. roth, w. yih, d. zimak, and y. tu. semantic role
labeling via generalized id136 over classi   ers (shared task paper). in
hwee tou ng and ellen rilo   , editors, proc. of the annual conference on
computational natural language learning (conll), pages 130   133, 2004.

[40] s. riedel and j. clarke. incremental integer id135 for non-
projective id33. in emnlp, pages 129   137, sydney, aus-
tralia, july 2006. association for computational linguistics.

[41] n. rizzolo and d. roth. modeling discriminative global id136. in proc.
of the first international conference on semantic computing (icsc),
pages 597   604, irvine, california, september 2007. ieee.

[42] n. rizzolo and d. roth. learning based java for rapid development of nlp

systems. in lrec, valletta, malta, 5 2010.

[43] d. roth. learning based programming. 2005.

[44] d. roth and w. yih. integer id135 id136 for conditional
random    elds. in proc. of the international conference on machine learn-
ing (icml), pages 737   744, 2005.

[45] d. roth and w. yih. global id136 for entity and relation identi   cation
in lise getoor and ben taskar,

via a id135 formulation.
editors, introduction to statistical relational learning. mit press, 2007.

[46] a.m. rush, d. sontag, m. collins, and t. jaakkola. on dual decomposi-
tion and id135 relaxations for natural language processing. in
proceedings of the 2010 conference on empirical methods in natural lan-
guage processing, pages 1   11. association for computational linguistics,
2010.

[47] k. sagae, y. miyao, and j. tsujii. hpsg parsing with shallow dependency
constraints. in acl, pages 624   631, prague, czech republic, june 2007.
association for computational linguistics.

[48] r. samdhani, m. chang, and d. roth. uni   ed expectation maximization.

in naacl, 6 2012.

[49] v. srikumar, g. kundu, and d. roth. on amortizing id136 cost for

id170. in emnlp, 2012.

[50] v. srikumar and d. roth. a joint model for extended semantic role label-

ing. in emnlp, edinburgh, scotland, 2011.

4

[51] t.h. tsai, c.w. wu, y.c. lin, and w.l. hsu. exploiting full parsing
information to label semantic roles using an ensemble of me and id166
via integer id135.
in conll, pages 233   236, ann arbor,
michigan, june 2005. association for computational linguistics.

5

