the elements of id54

daniel jacob gillick

electrical engineering and computer sciences
university of california at berkeley

technical report no. ucb/eecs-2011-47
http://www.eecs.berkeley.edu/pubs/techrpts/2011/eecs-2011-47.html

may 12, 2011

copyright    2011, by the author(s).

all rights reserved.

 
permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. to copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission.

the elements of id54

by

daniel jacob gillick

a dissertation submitted in partial satisfaction

of the requirements for the degree of

in

computer science

in the

graduate division

of the

university of california, berkeley

committee in charge:

professor nelson morgan, chair

professor daniel klein

professor thomas gri   ths

spring 2011

the elements of id54

copyright    2011

by

daniel jacob gillick

abstract

the elements of id54

by

daniel jacob gillick

doctor of philosophy in computer science

university of california, berkeley
professor nelson morgan, chair

this thesis is about id54, with experimental results on multi-
document news topics: how to choose a series of sentences that best represents a col-
lection of articles about one topic. i describe prior work and my own improvements
on each component of a summarization system, including preprocessing, sentence
valuation, sentence selection and compression, sentence ordering, and evaluation of
summaries. the centerpiece of this work is an objective function for summariza-
tion that i call "maximum coverage". the intuition is that a good summary covers
as many possible important facts or concepts in the original documents.
it turns
out that this objective, while computationally intractable in general, can be solved
e   ciently for medium-sized problems and has reasonably good fast approximate so-
lutions. most importantly, the use of an objective function marks a departure from
previous algorithmic approaches to summarization.

1

acknowledgements

getting a ph.d. is hard. not really hard in the day-to-day sense, but more because
i spent a lot of time working on a few small problems, and at the end of six years,
i have only made a few small contributions to the world. and here it is, my    nal
attempt to communicate those contributions, typed out over unusual    avors of tea
inside oakland   s cafes while the sun drifts past. snow piled up on the east coast,
egypt shed its dictator, libya revolted, a giant earthquake shook japan. few people
will be aware of my work in this    eld, and fewer still will read this thesis. so to those
who are reading, and to those who helped support me while i computed, wrote, and
just sat thinking, thank you!

i

to: my parents. not because they are particularly in need of summaries.

ii

contents

1 introduction

1.1 who needs summaries? . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 on the prospect of automation . . . . . . . . . . . . . . . . . . . . .
1.3 a note on computers and language . . . . . . . . . . . . . . . . . . .
1.4 what to expect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 news data

2.1 anatomy of a summarization problem . . . . . . . . . . . . . . . . . .
2.2 evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
2.3 summaries versus documents

3 preprocessing: preparing text

3.1 cleaning up formatted text
. . . . . . . . . . . . . . . . . . . . . . .
3.2 sentence compression as preprocessing . . . . . . . . . . . . . . . . .
3.3 sentence segmentation . . . . . . . . . . . . . . . . . . . . . . . . . .

4 selecting sentences

4.1 baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 maximum marginal relevance
. . . . . . . . . . . . . . . . . . . . . .
4.3 using document statistics
. . . . . . . . . . . . . . . . . . . . . . . .
4.4 from procedure to objective . . . . . . . . . . . . . . . . . . . . . . .
4.5 more ideas in extraction . . . . . . . . . . . . . . . . . . . . . . . . .

5 the maximum coverage objective

5.1 approximate and exact solving . . . . . . . . . . . . . . . . . . . . .
5.2
integer linear programs . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 theoretical guarantees . . . . . . . . . . . . . . . . . . . . . . . . . .
5.4
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.5 adding compression to the ilp . . . . . . . . . . . . . . . . . . . . .

ilp run-time

1
1
2
5
6

7
7
8
12

16
16
17
18

22
22
23
24
26
29

37
37
39
40
41
44

iii

47
47
48

51
51
53
54
59

61
61
62
63

65

6 ordering sentences

6.1 ordering is hard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
6.2

independent sentences

7 evaluating summaries

7.1 id165 recall
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.2 maximum recall . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.3 id104
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.4 towards extrinsic evaluation . . . . . . . . . . . . . . . . . . . . . . .

8 in summary

8.1 review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8.2 data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8.3 methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

bibliography

iv

chapter 1

introduction

1.1 who needs summaries?
in the year 77 of the common era, pliny the elder published his magnum opus, nat-
uralis historia, an encyclopedic account in 36 chapters of everything he knew. pliny,
a roman lawyer, army o   cer, local governor, author, naturalist, and philosopher,
had a voracious appetite for knowledge, which he hoped to impart to his friend, the
emperor titus as he set about the task of ruling the unwieldy roman empire. the
work was groundbreaking, a model for the modern encyclopedia in terms of breadth,
citation, and indexing. it covered mathematics, the sciences, agriculture, medicine,
and the arts. and, just in case titus found himself short on reading time, pliny wrote
summaries of each section: the    rst known abstracts.

since pliny   s time, the amount of published material has grown rather dramat-
ically. the wisdom implied by his obsessive scholarship ("there is no book so bad
that some good could not be got out of it") just sounds foolish in the era of amazon
and wikipedia. as a result, the value of summaries has likewise increased, even for
ordinary citizens.

nowadays, we are so surrounded by summaries of information that we often take
them for granted. imagine a newspaper without headlines! books and movies are
described in blurbs and reviews, scholarly articles begin with abstracts, and search
results are summarized with a few snippets from each page. summaries are of great
potential value to journalists, lawyers, students, ceos, and casual browsers of the
internet1.

these days, nearly all summaries we are likely to encounter are written by people.
they are typically abstracts, distillations of the original written in new language, as
opposed to extracts, patchworks of text selected from the source (though empirical

1take the recent internet meme: tl;dr, which stands for too long; didn   t read.

1

chapter 1. introduction

figure 1.1: a 12th century version of pliny the elder   s naturalis historia.

studies have shown there is more copying than you might expect when the new
york times archivists summarize articles, for example.). still, writing summaries is
something of an art: experts have trouble explaining exactly what steps they follow
in absorbing and reformulating information. thus, while the prospect of automatic
summarization is appealing, it is not surprising that there remains a considerable gap
between human and machine-written summaries.

1.2 on the prospect of automation
research in id54 began in the 1950s. hans peter luhn, a com-
puter scientist at ibm, perhaps best known for developing the algorithm used for
validating credit card numbers, published a paper in the 1958 edition of the ibm
journal called the automatic creation of literature abstracts [luhn, 1958]. re-
markably, luhn took a mathematical approach to the problem which was largely
abandoned and then rediscovered in the last 15 years as statistical techniques have
rejuvenated arti   cial intelligence research:

statistical information derived from word frequency and distribution is

2

chapter 1. introduction

figure 1.2: luhn began thinking about statistical properties of documents (left); ed-
mundson   s system diagram contains many components of a modern summarizer (right).

used by the machine to compute a relative measure of signi   cance,    rst
for individual words and then for sentences. sentences scoring highest in
signi   cance are extracted and printed out to become the "auto-abstract."

luhn had to transcribe each document he wished to summarize onto a punch
card, which made things painfully slow, but his approach to summarization is truly
foundational. surprisingly little has changed in today   s state-of-the-art systems. and,
while some have taken a few small steps towards language generation, extractive
summarization is still the focus of most research. h. p. edmundson   s description in
1964 [edmundson, 1964] is still apt today:

present systems of automatic abstracting are capable of producing noth-
ing more than extracts of documents, i.e. a selection of certain sentences
of a document. this is not to say, however, that future automatic ab-
stracting systems cannot be conceived in which the computer generates
its own sentences by means of a suitable generative grammar program.
theoretically there is no linguistic or mechanical reason why such a sys-
tem could not be designed and operated [...] such a system, however, is
apt to be costly both in time and money.

what has changed is the context in which we understand the problem. most
publications are available in what luhn would have called machine-readable form,

3

chapter 1. introduction

and are readily accessible online. computing power and accessibility are no longer
primary limitations. practical issues associated with summarization include:

preprocessing: many segments of raw text are not suitable for extraction. tables,
quotes, bylines, and all kinds of formatting markup are unwelcome in a sum-
mary, and should be removed before analysis begins. this problem is di   cult,
and often overlooked, but poor preprocessing can eclipse improvements in other
aspects of summarization.

sentence segmentation: sentences are a natural unit for extraction, but identi-
fying sentence boundaries is not trivial because periods are also used to denote
abbreviations. segmentation mistakes are costly because a fragmented sentence
can ruin a summary.

valuation: to choose which sentences to include in a summary, we need some way
of measuring the value of each sentence, or more generally, selection unit. luhn
proposed inferring the value of a sentence from the number of times the words
it contains appear in the original input. other features of sentences, like their
position or the presence of various key phrases like in conclusion, have also been
studied.

selection: luhn   s system simply selected the sentences with the largest inferred
values. but this neglects redundancy in the relevant sentences. thus, selection
is a constrained optimization problem: maximize relevant content subject to a
summary length constraint.

ordering: the ordering of a set of sentences can dramatically a   ect the meaning
and readability of the summary. while a proper treatment of sentence ordering
ought to be more holistic, that is, considered jointly with selection, little work
in this area has had much impact.

evaluation: measuring and understanding the di   erences between di   erent sum-
marization systems is crucial to advancing the    eld. since manual evaluation
by experts is slow and costly, a variety of automatic and semi-automatic al-
ternatives are used. the somewhat standard, though problematic id8
metric measures word overlap between machine-generated summaries and a set
of human-written abstracts.

signi   cant research has been devoted to most of these areas, and the next chap-
ters will survey some key results. but often, results are hard to compare. because
most researchers build their own systems from the ground up, without standardized

4

chapter 1. introduction

components, it is hard to tell if an improvement is due to a better method for select-
ing sentences or simply better preprocessing, for example. one motivation for this
work is to provide some simple but state-of-the-art components to facilitate further
development and comparison.

there are many varieties of summarization. ongoing research investigates sum-
marization of email, legal proceedings, customer reviews, search results, meetings, and
videos. many of the standard techniques, however, have their origins in document
summarization, where the goal is to convey the most important information from a
set of documents within a length constraint using natural language. the focus of this
thesis is narrow: summarization of multiple news documents, but the ideas are more
broadly applicable.

1.3 a note on computers and language
initially, the prospect of summarizing automatically sounds either impossible or like
magic. considering more carefully how extraction actually works, usually by counting
simple features of the words in the documents, can be something of a disappointment.
far from magic, this application of arti   cial intelligence seems unrelated to our intu-
itive sense of what intelligence is. the computer does not comprehend any meaning;
it doesn   t even have a framework for representing semantics. all told, we are a long
way from creating intelligence of the sort that goes on in the human brain.

so rather than approach problems like summarization by trying to emulate people,
we take a far less ambitious tack. instead, we make simplifying assumptions about
what a summary ought to be, and then use the computer for what it does best:
calculating quickly. that is, rather than employ a series of human-like steps (pick a
"topic sentence",    nd a series of "supporting sentences", etc.), we use the computer
to search for the summary that maximizes the value of a made-up formula (   nd the
set of sentences that together contain the most frequent words in the documents).

this transition, from procedural to statistical thinking, has been amazingly suc-
cessful in other related    elds. machine translation and id103 improved
tremendously when researchers stopped trying to write software to imitate people
and started    tting statistical models from reams of data. summarization research
has lagged a bit behind in this respect, perhaps because it is hard to think of simple
but reasonable models for how a few thousand sentences should be reduced to three
or four. still, the primary philosophical contribution of this work is to help promote
the transition to statistical approaches for summarization.

5

chapter 1. introduction

1.4 what to expect
as i write, i am keeping in mind a particular reader. my reader is like me, but
just beginning to choose a research topic. like most areas, there are a great many
summarization papers and ideas    oating around, but little top-down organization. i
am trying to write the thesis that would have been most useful for me to read three
years ago, focusing in particular on the work i   ve done since then. i will try to keep
things simple and clear, and avoid making assumptions about what my reader knows
and does not know.

the next chapter describes the data used in my experiments and the variety
of evaluation methods, both manual and automatic; it also provides some general
statistics that highlight di   erences between summaries and full documents. chapter
3 discusses preprocessing issues, including the sentence boundary detection problem.
chapter 4 steps through a series of advances in sentence selection methods, high-
lighting the advantages of an objective function for summarization over procedural
approaches. chapter 5 gives a deeper analysis of the maximum coverage objective
proposed in chapter 4, and suggests an expansion for solving selection and sentence
compression jointly. chapter 6 discusses the often ignored problem of sentence order-
ing, suggesting a simple alternative to the di   cult task of    nding the the best way
to order a small set of sentences. chapter 7 addresses evaluation. it describes a sim-
pler alternative to id8 with more desirable properties, and discusses the results
of an evaluation by non-expert annotators. chapter 8 suggests future directions for
summarization.

6

chapter 2

news data

for the sake of consistency and simplicity, all experiments and discussion that follow
involve a subset of a dataset called the english gigaword corpus (3rd edition) [gra   
et al., 2007]. the corpus consists of millions of newswire articles published by six
di   erent sources between 1994 and 2006.

2.1 anatomy of a summarization problem
the summarization problems presented here are the o   cial sets distributed as part of
a competition organized annually since 2001 by the national institute of standards
and technology (nist). each problem was created by an expert who assembled a
group of related documents and wrote a "topic" and "narrative description" to help
focus the summary. the 2006 and 2007 sets asked for 250 word summaries1 from
groups of 25 documents. in the hopes of nudging research away from pure sentence
extraction, the 2008 and 2009 sets asked for 100 word summaries of 10 documents2.
each year   s problem set includes around 50 such problems.

to facilitate evaluation, a total of four summaries are written by experts for each
problem. notably, these are abstracts rather than extracts. the same experts run
a comprehensive evaluation of all the summaries submitted by participating teams.
figure 2.1 shows two sample summaries generated automatically and one written by
an expert, along with evaluation results from nist   s 2008 text analysis conference.

1this is a hard cap on the number of space-separated words. other summarization problems
employ di   erent sorts of limits, but here we assume that the summary will be truncated after the
word limit is reached.

2it didn   t work. most submitted systems remain extractive.

7

chapter 2. news data

topic:
narrative: describe developments in the production and launch of the airbus a380.

airbus a380

reference

the european airbus a380
   ew its maiden test    ight
from france 10 years after
design development started.
the a380
super-jumbo
passenger jet surpasses the
boeing 747 and breaks their
monopoly. airlines world-
wide have placed orders but
airports may need modi   -
cation to accommodate the
weight and width of the
a380. u.s. airlines have not
placed an order. airbus has
fallen behind in production
and a backlog of orders has
developed. airbus must sell
at least 250 planes to break
even    nancially. the a380
is overweight and modi   -
cations to meet the weight
requirements impacted the
budget. additional
test
   ights are planned.

summary a

summary b

european airplane maker
airbus "is likely to discuss
before the end of the year"
a possible increase in pro-
duction capacity of its new
super-jumbo a380 aircraft,
airbus    production chief
gustav humbert said in a
magazine interview released
tuesday. the superjumbo
airbus a380, the world   s
largest commercial airliner,
took o    wednesday into
cloudy skies over southwest-
ern france for its second
test    ight. this second test
   ight is part of a program
of tests that will
intensify
from here on out, and
we   re not going to make
statements on each phase of
the program," airbus said.

aviation

airbus unveiled the world   s
biggest passenger jet at a
spectacular sound-and-light
ceremony in toulouse. and
even though no us airline
has ordered the european
jet,
authorities
are getting los angeles
san francisco and miami
airports ready for the a-380
passenger    ights in 2006.
airbus has 139    rm a380
orders from 13 airlines and
freight
companies, worth
$39
any
discounts on the plane   s
$280 million
price.
airbus sees global demand
for 1,250 a380-size behe-
moths to shuttle passengers
between the world   s largest
airports, which serve as
connecting hubs for    ights
to less busy destinations.

billion

before

list

system oq lq pyramid id8-2
reference
system a
system b

0.550
0.000
0.276

0.065
0.026
0.048

4.5
3.0
4.0

5
1
4

figure 2.1: a problem from the 2008 evaluation. one reference summary and two system
summaries are shown along with evaluation results. cr is the content responsiveness
as judged by an expert; lq is the average of    ve expert-judged linguistic quality scores.
both are on a 1 (very poor) to 5 (very good) scale. pyramid measures overlap between
the important facts in the summary and the set of references. id8 measures overlap
in id165s; id8-2 uses n = 2.

2.2 evaluation
compared with other natural language processing research areas, summarization is
a particularly vague and subjective task. what makes a good summary may vary

8

chapter 2. news data

greatly depending on the circumstances and the intended audience. so while evalua-
tion is essential to the progression of the    eld, it remains a thorny issue.

as it turns out, evaluating individual summaries is considerably more di   cult
than evaluating summarization systems. this is because averaging together scores
for summaries produced by a single system can eventually reduce variability enough
to allow for meaningful comparison of systems. luckily, we are usually interested
in comparing systems rather than summaries, to see whether a system-level change
improves average results.

2.2.1 manual evaluation
traditional evaluation of summaries involves human judgments in categories like con-
tent and grammaticality. the annotator is instructed to read the original documents
and then score summaries without respect to any particular task or goal. this sort of
evaluation is intrinsic: it attempts to measure quality directly. extrinsic methods, by
contrast, measure performance on a particular task, like timed id53.

the annual competitions adjusted their criteria somewhat from year to year, but
the general setup remained fairly consistent. scores range from 1 (very poor) to 5
(very good), but changed to a 10-point scale in 2009. here are the instructions given
to the annotators in 2006 and 2007:

content responsiveness: responsiveness should be measured primarily in terms of the
amount of information in the summary that actually helps to satisfy the information
need expressed in the topic statement. the linguistic quality of the summary should
play only an indirect role in your judgment, insofar as poor linguistic quality interferes
with the expression of information and reduces the amount of information that is
conveyed.

grammaticality: the summary should have no datelines, system-internal formatting,
capitalization errors or obviously ungrammatical sentences (e.g., fragments, missing
components) that make the text di   cult to read.

non-redundancy: there should be no unnecessary repetition in the summary. unneces-
sary repetition might take the form of whole sentences that are repeated, or repeated
facts, or the repeated use of a noun or noun phrase (e.g., "bill clinton") when a
pronoun ("he") would su   ce.

referential clarity: it should be easy to identify who or what the pronouns and noun
phrases in the summary are referring to. if a person or other entity is mentioned, it
should be clear what their role in the story is. so, a reference would be unclear if an
entity is referenced but its identity or relation to the story remains unclear.

9

chapter 2. news data

focus: the summary should have a focus; sentences should only contain information that

is related to the rest of the summary.

structure and coherence: the summary should be well-structured and well-organized.
the summary should not just be a heap of related information, but should build from
sentence to sentence to a coherent body of information about a topic.

i will often show linguistic quality scores, averages across the    ve grammatical
categories, and overall quality scores, a new category used in the 2008 and 2009
evaluations in place of content responsiveness. one set of instructions suggested that
overall quality should be related to the relative dollar value of a summary.

2.2.2 automatic evaluation
manual evaluation is slow and costly3. to facilitate comparison between systems,
more immediate feedback is important. id8, or recall-oriented understudy
for gisting evaluation [lin, 2004], is the awkwardly named adaptation of machine
translation   s id7 scoring [papineni et al., 2002] to summarization. id8-n is
roughly a measure of the id165 overlap between a summary and the set of reference
summaries, where the value of each id165 is the number of references in which
it appears.
it is recall-oriented because the summary length constraint implicitly
penalizes the inclusion of irrelevant words. the id8 toolkit includes a variety of
options (ignore stopwords4? perform word id30 before comparing?) and can use
word combinations more complex then id165s: id8-su4, for example, counts
unigrams and pairs of words separated by up to four intervening words.

id8 is not good at predicting the manually evaluated quality of a particular
summary, but when id8 scores are averaged over many problems, the correlation
is usually good. figure 2.2 shows the relationship between id8 and manually
evaluated quality in the 2008 evaluation. to achieve 90% correlation for this data,
id8-2 requires around 30 problems, and with nearly 100 problems, the correlation
improves to almost 95%. the    gure also suggests that with fewer than 15 topics,
id8-1 is actually more reliable than higher order id165s.
in general, while
higher order id165s are more meaningful, more predictive, they are rarer, so their
absence in a particular summary might not be signi   cant.

id8 certainly permits rapid system development, but it is an easy target for
criticism. given the references, it is a simple matter to create completely nonsensical

3it is also highly subjective; one additional advantage of id8 is that scores are readily
comparable across evaluations, whereas di   erent annotators are likely to have di   erent personal
score ranges.

4stopword lists include common function (as opposed to content) words like articles, prepositions,

and pronouns)

10

chapter 2. news data

but high-id8-scoring summaries. and, as with any simple automatic metric,
the more it is used to optimize machine learning methods, the less useful it becomes
in evaluation. while it is still used during development, good research papers cite
results of manual comparisons to support their    ndings.

figure 2.2: correlation between id8-n and human-evaluated summary quality when
scores are averaged over di   erent numbers of topics. (2008 data).

2.2.3 semi-automatic evaluation
the primary shortcoming of id8 is that it treats word id165s as if they were
facts. nenkova   s pyramid method for evaluation [nenkova and passonneau, 2004] is
an attempt to bridge the gap between fully manual evaluation and id8. rather
than rely on id165s as a unit of information, semantic content units are manually
identi   ed in the reference summaries (phrases, sentence fragments, etc.) so that a
system summary can be scored based on the relevant facts it contains. pyramid
shows considerably stronger correlation than id8 with manual responsiveness,
and remarkably, a simple id75 over average pyramid scores and average

11

01020304050607080901000.40.50.60.70.80.91number of topicspearson correlation  id8   1id8   2id8   3id8   4chapter 2. news data

linguistic quality scores can almost exactly predict overall responsiveness (correlation
is 99%). note that this regression is at the system level: very little work has been
done on evaluating individual summaries.

the pyramid method is not particularly automatic: human annotators are needed
to identify facts in the references and match these facts in new summaries. still, it
is an attempt to begin mechanizing the identi   cation of important content, separate
from linguistic quality, in summaries. and, pyramid scores reasonably represent
the gap between human summaries and automatically generated summaries: while
human id8 scores just about match the best systems   , human pyramid scores
are about twice as large as the best systems   .

2.3 summaries versus documents
before diving into a discussion of the di   erent components involved in a summariza-
tion system, it is helpful to outline some of the di   erences between the human-written
summaries and the original documents in our data.

to give some intuition about which words translate from documents into sum-
maries, table 2.1 shows the estimated id203 that a word appears in a summary
given that it appeared somewhere in the source documents, for some sample words.
note that for this dataset, the word token-level compression rate, from documents
to summaries is 2.8%; the word type-level compression rate is 19.4%: the words in
10 related documents, of course, are much more redundant than the text in a single
summary.

topical words like russia or fbi carry enough meaning to merit use in a sum-
mary, but relative dates, question words, and conversational pronouns (i, you), usually
don   t. that articles, conjunctions, and common prepositions appear in both docu-
ments and summaries is of little consequence as they are required for proper grammar
in any text.

table 2.2 highlights words that would likely be useful to a classi   er in distinguish-
ing summaries from documents using the piecewise (per word) symmetric kullback-
leibler distance (kld) between the distribution over words in the documents pd(  )
and words in the summaries ps(  ):

kldw = pd(w) log

pd(w)
ps(w)

+ ps(w) log

ps(w)
pd(w)

(2.1)

over 6% of document words are the, but only 1.4% of summary words, an indication
that human abstractors use di   erent syntactic forms. this claim is reinforced by the
distributional di   erences in the words of, to, a, and that. first- and second-person
personal pronouns we, our, and i almost never appear in summaries where facts

12

chapter 2. news data

word
the
and
to
of
in
a
for
on
was
with

  p (w     s|w     d) word
russia
china
prison
fbi
arrest
indian
france
drug
nuclear
judge

0.9766
0.9375
0.9271
0.8464
0.8411
0.8151
0.7057
0.5443
0.4427
0.4141

  p (w     s|w     d) word
think
ago
i
thursday
wednesday
you
u.s.
come
monday
what

0.5167
0.4118
0.3889
0.3750
0.3667
0.3542
0.3500
0.3421
0.3409
0.3365

  p (w     s|w     d)

0.0144
0.0132
0.0130
0.0074
0.0071
0.0047
0.0042
0.0038
0.0035
0.0033

table 2.1: the estimated id203 that a word appears in a 100 word summary given
that it appeared in at least one of the 10 source documents: words with the highest
id203 (left column); content words with the highest id203 (middle column);
words with the lowest id203 (right column). only words appearing in at least 10
di   erent problems are shown (2008 data).

dominate opinions. months and years (absolute dates) are common in summaries
which often give chronologies, while relative dates (tuesday) lose meaning without
context. also, note that said appears nearly eight times more frequently in docu-
ments because summaries typically disregard attribution phrases so common in news
documents ("..., sources said").

table 2.3 gives distributional statistics for part-of-speech tags. overall, nouns are
used more in documents (with the exception of plural proper nouns) while verbs are
used more in summaries. these di   erences are quite striking: nns appear with a
third their ordinary rate in summaries; even proper nouns, which we might expect to
be particularly relevant are less than half as frequent in summaries. the various verb
varieties are roughly twice as prevalent in summaries, which, these results suggest,
are about actions. summaries also contain more comparative adjectives and adverbs,
and fewer prepositions and determiners.

what are we to make of these di   erences? how are words and phrases kept
or removed in summary creation? one way to understand summarization from a
structural perspective is through the machine translation concept of alignment5. the
idea, as it applies to translation, is that a pair of parallel sentences in two di   erent
languages share a hidden alignment, a mapping of words or phrases or syntactic sub-
trees that shows how meaning is transferred. armed with an alignment model trained
from a corpus of parallel sentences, we can translate a new sentence by decoding
it:    nd the sequence of words in the other language that maximizes the alignment

5see kevin knight   s excellent tutorial [knight, 1999].

13

chapter 2. news data

word
the
we
said
of
to
a
our
tuesday
that
i

kld
0.0719
0.0227
0.0192
0.0131
0.0107
0.0097
0.0071
0.0066
0.0061
0.0060

  pd(w)
0.0623
0.0022
0.0109
0.0279
0.0278
0.0247
0.0007
0.0007
0.0131
0.0023

  ps(w) word
2005
0.014069
u.s.
< 10   7
2004
0.001426
0.012193
january
continue
0.013356
include
0.011743
investigation
< 10   7
japan
< 10   7
2006
0.005778
0.000150
april

kld
0.0112
0.0053
0.0018
0.0017
0.0016
0.0015
0.0015
0.0013
0.0013
0.0012

  pd(w)
0.000082
0.000799
0.000332
0.000171
0.000459
0.000991
0.001023
0.000087
0.000129
0.000197

  ps(w)
0.0031
0.0040
0.0015
0.0018
0.0017
0.0026
0.0026
0.0007
0.0008
0.0009

table 2.2: the piecewise kullback-leibler distance (kld) is shown between the empir-
ical distributions over words in the documents pd(  ) and words in the summaries ps(  ):
highest kld words appearing more frequently in the documents (left); highest kld words
appearing more frequently in the summaries (right). (2008 data).

id203 given by the model.

daume and marcu try applying the alignment idea to abstracts and source docu-
ments [daum   iii and marcu, 2005]. human annotators create a set of roughly 2000
phrase-aligned pairs with reasonably good inter-annotator agreement, that they use
to    t a model (that ends up being fairly complex) giving better precision and recall
than more heuristic baselines6. while this is an intriguing avenue of research, leading
away from extraction, i will not pursue it here. however, it would be nice to build
a purely abstractive summarization system that combined the alignment or transla-
tion model hypotheses with a language model (as in machine translation) to produce
summaries. they would be somewhat nonsensical because both the alignment model
and standard id165 language models are too weak for actual language generation.
but at least the summaries would be amusing.

6see also jing and mckeown   s work on mapping summary text to source text [jing and mckeown,
1999]: in particular, a study of single news document summaries shows that nearly 80% of abstract
sentences are derived from cutting and pasting phrases from the source.

14

chapter 2. news data

tag
nn
in
nnp
dt
"
jj
nns

,

prp

$
jjr
()

nnps
rbr
jjs
pos
ex
prp$
rp
md

:

examples

government year state time court

of in for on that

bush abramo    house senate

the a an this that

other last new    rst many

people years o   cials days members

he it they we i

more less better larger smaller

states democrats republicans americans

more earlier less longer better
most least largest best latest

"

,

$

()

   s

there

his their its her our
up out down o    on

will would could can should

: ; ... -    
most best

are have do say    re

one two million 000 three

that which whatever what

when where how why wherever

rbs
wdt
vbg being according including saying going
vbp
cd
wrb
to
cc
vbn
vbz
vb
wp
rb
.

been made known used expected

who what whom whoever whole

be have make take get

not also n   t now only

is has    s says does

and but or nor

. ? !

to

pdt
vbd

all such half

said was had were did

  pd(t)
0.1323
0.1102
0.1020
0.0900
0.0220
0.0593
0.0573
0.0532
0.0261
0.0011
0.0027
0.0026
0.0027
0.0012
0.0019
0.0092
0.0012
0.0113
0.0031
0.0096
0.0019
0.0005
0.0048
0.0181
0.0132
0.0212
0.0032
0.0242
0.0254
0.0252
0.0187
0.0286
0.0043
0.0295
0.0373
0.0004
0.0442

  ps(t)
0.0486
0.0486
0.0481
0.0485
0.0154
0.0486
0.0483
0.0453
0.0258
0.0063
0.0111
0.0096
0.0091
0.0038
0.0059
0.0267
0.0033
0.0282
0.0077
0.0238
0.0048
0.0011
0.0113
0.0409
0.0275
0.0424
0.0065
0.0450
0.0468
0.0454
0.0295
0.0444
0.0065
0.0411
0.0486
0.0005
0.0445

table 2.3: comparison of the empirical distribution of part-of-speech tags in documents
  pd(t) and summaries   ps(t). the most common example words are shown for each tag
type. 2008 data (results are nearly identical with 2009 data).

15

chapter 3

preprocessing: preparing text

text is widely available, but there are few consistent standards, so di   erent sources
look quite di   erent. still, the preparation of raw text for summarization, or other
natural language processing tasks, is often neglected by researchers as mundane. this
chapter discusses formatting, removal of unwanted phrases (a kind of compression),
and sentence segmentation, in each case showing why the problem deserves more
serious consideration.

3.1 cleaning up formatted text
each publication has its own standards, which evolve constantly. the new york
times, xinhua, and associated press documents in our dataset, for example, have
tables, bulleted lists, bylines, location headers, image captions, and various kinds of
contact information all lurking in the text, often indicated by di   erent markers. table
3.1 gives examples of headers. removing such content-less segments is important for
two reasons: it improves the readability of extracted sentences and it clears space for
more real content.

as a result, each summarization system includes code for handling formatted text,
usually a set of carefully designed, highly specialized id157. anecdotal
reports suggest that some teams devote hundreds of hours to this process. while
automated text cleaning would be valuable, there is no research, to my knowledge, in
this area, at least in the context of summarization. one way to address cleaning a bit
more systematically is to segment text into sentences and then prune those rejected
by a parser or without a su   cient number of dictionary words. but such methods
are too coarse: if there are oddly formatted bylines attached to the    rst sentence of
each document, these sentences, though otherwise useful, would be discarded.

while i have not implemented any systematic solution (i use a limited set of

16

chapter 3. preprocessing: preparing text

original

cleaned

vatican-pope-us-women,newseries washing-
ton pope john paul ii is credited for some
advances by women in the catholic church.
new orleans     a thoroughfare in new orleans
that ends where the water begins has become a
launching point for rescue workers.

pope john paul ii is credited for some
advances by women in the catholic
church.
a thoroughfare in new orleans that
ends where the water begins has become
a launching point for rescue workers.

table 3.1: examples of undesirable, source-speci   c formatting that is removed from the
   rst sentences of news documents using id157 (2008 data).

id157), one way to approach the problem is to assume two underlying
states: good and bad. usable text is generated by the good state and unusable
text is generated by the bad state. under the hidden markov model framework, we
would learn emission id203 distributions p (text|good) and p (text|bad) based
on features of the text like letter id165s, and transition distributions over sequences
of underlying states. labeled data for supervised learning can come from the output
of a rule based system. the derived labels for new data can be inferred from a viterbi
decoding, and sections marked as bad can be excised. most likely, some additional
constraints on keeping sentences intact would be necessary, and parameters might
need adjusting for new domains. still, this would be a step towards better automation
of a painful part of summarization.

3.2 sentence compression as preprocessing
beyond formatting issues, there are segments of clean text that have no bearing on
the summary. removing unnecessary parts of sentences while maintaining meaning
and grammar is called sentence compression. the idea was pioneered by knight and
marcu [2000], who trained a noisy-channel style system1 to remove subtrees from
the parse tree of an original sentence. subsequent work improved the readability of
the compressed sentences [mcdonald, 2006], but no results have shown signi   cant
improvements in overall summary quality, despite a number of attempts [lin, 2003;
nenkova, 2008; martins and smith, 2009; gillick and favre, 2009].

why should this be? one problem is that compression invariably introduces some
semantically or grammatically bad sentences, which work against gains in informa-
tion content. perhaps the real problem is a bit more subtle. most systems create

1as in id103 and machine translation, a combination of an acoustic or translation

model and a language model is used.

17

chapter 3. preprocessing: preparing text

compressed sentence alternatives and allow the id136 algorithm to choose among
them. but since the words and phrases we want to remove (days of the week, attri-
bution clauses, etc.) are common, they are likely to receive unsuitably high weight
during valuation. either we should learn to compress and extract sentences jointly,
or compression should precede extraction.

to demonstrate the potential of preprocessing compression, i wrote a few regular
expressions to remove some days of the week and attribution clauses. the regular
expression for days of the week is shown here (note: this is not simple!), which is
designed to have high recall2   virtually no syntactic or semantic issues arise as a
result of the deletions:

s = re . sub (r '(^| |, )( on | last | this | next | late )

( monday | tuesday | wednesday | thursday | friday | saturday | sunday )
( morning | afternoon | evening | night )?
($| | ,|\.) ', r '\5 ', s)

table 3.2 gives some sense for how this sort of compression can improve perfor-
mance. the di   erences in id8 score are signi   cant and more improvement seems
possible if more rules are added, or if the rules are replaced by a statistical system.

compression id8-2 lq words per sent.

2008

2009

none
rules
none
rules

0.108
0.114
0.110
0.114

4.95
n.a.
n.a.
5.40

24.8
24.0
26.3
25.4

table 3.2: a summary of the performance gap created by more extensive text cleaning.
the linguistic quality (lq) scores are not directly comparable since not all of these
systems were submitted for evaluation. the 2008 lq score is multiplied by two here since
the scale was changed from 1-5 to 1-10 in 2009. the di   erences in id8-2 and the
average number of words per summary sentence are signi   cant at 95% con   dence.

3.3 sentence segmentation
the sentence segmentation problem   the disambiguation of periods that arises be-
cause periods signal abbreviations as well as sentence boundaries   is mostly dis-
regarded because a few rules treat most examples e   ectively. however, even the

2in the 2008 data, 8.7% of input sentences contain at least one of these days of the week; after

application of this rule, 5.7% have at least one.

18

chapter 3. preprocessing: preparing text

strongest rule-based system [aberdeen et al., 1995], highly specialized and labor in-
tensive to create, has an error rate of 1% on a standard corpus of wall street journal
text [marcus et al., 1993]. the unsupervised punkt system [kiss and strunk, 2006] is
widely used (1.65% error rate on the wall street journal corpus; 3% error rate with
the included model3). similar error rates (close to 1%) were reported with supervised
systems [palmer and hearst, 1997; reynar and ratnaparkhi, 1997].

figure 3.1: the overlapping label space of the wall street journal test data: sentence
boundaries (76%), abbreviations (26%), intersection (2%). the distribution of errors
given by the classi   er is shown as well (not to scale with all data).

all these systems either employ a hard-coded list of abbreviations or seek to
compile one automatically. this appears to be a mistake. as figure 3.1 shows, most
of the di   cult cases involve abbreviations that can also end sentences (he was born
in the u.s.). based on this observation, i built a new system for classifying sentence
boundaries that achieves an error rate of 0.25% on the wall street journal corpus
[gillick, 2009].

the success of this system is partly due to a rich feature set. each example takes
the general form "l. r", where l is the context on the left side of the period in
question, and r is the context on the right (only one word token of context on each
side is used). we are interested in the id203 of the binary sentence boundary
class s, conditional on its context: p (s|"l. r"). features used in supervised learning

3punkt ships with the natural language processing toolkit [loper and bird, 2002].

19

  sentence boundaries (s)abbreviations (a)(a+s)(a)(a+s)all dataerrorschapter 3. preprocessing: preparing text

are extracted from "l. r", described in table 3.3. each individual feature is binary,
so the actual feature vector for an example is very large but very sparse.

left context

length of left context

# feature description
1
2 right context
3
4 capitalization of right context
5
6
7
8
{1,2}
{1,2,3,4}
{1,2,3,4,5,6}
{1,2,3,4,5,6,7,8}

error
1.88%
9.36%
9.12%
12.56%
log count of left context without following period in the training set 12.14%
18.79%
log count of right context appearing lowercased in the training set
10.01%
left context and right context
left context and capitalization of right context
7.54%
0.77%
0.36%
0.32%
0.25%

table 3.3: the eight feature classes. all individual features are binary. id166 classi   cation
results shown (linear kernel).

feature classes 1 and 2 together give surprisingly good performance. adding
feature classes 3 and 4 better than cuts the remaining errors in half. the length of
the left token serves as a proxy for the abbreviation class (mean abbreviation length
is 2.6, compared to 6.1 for non-abbreviation sentence enders). the capitalization of
the right token is a proxy for a sentence starter. every new sentence that starts with
a word (as opposed to a number or punctuation) is capitalized, but 70% of words
following abbreviations are also, so this feature is mostly valuable in combination.

while the training set includes nearly a million words, most of these are ignored
because features are extracted only near possible sentence boundaries. consider the
fragment "... the u.s. apparently some ...", which the system with only the    rst four
features fails to split after "u.s." the word "apparently" starts only 8 sentences in
the training data, but since it usually appears lowercased (89 times in training), its
capitalization here is meaningful. feature class 6 encodes this idea, indicating the
log count4 of lowercased appearances of the right context: the higher the count, the
better the chances that this uppercase appearance starts a new sentence. similarly,
the more times the left context appears without a period in training, the better the
chances that this is not an abbreviation.

another way to incorporate all of the training data is to build a model of p (s|"l r"),
as is often used in sentence segmentation for id103. without a period in

4the log count is rounded to the nearest integer value.

20

chapter 3. preprocessing: preparing text

the conditional, many more negative examples are included in the training set. the
resulting id166 model is very good at placing periods given input text without them
(0.31% error rate), but when limiting the input to examples with ambiguous periods,
the error rate is not competitive with our original model (1.45%).

corpus examples
wsj
brown
poe

26977
53688
11249

in s id166 err nb err
0.35%
74%
0.45%
91%
95%
0.44%

0.25%
0.36%
0.52%

table 3.4: id166 and naive bayes classi   cation error rates on di   erent corpora using a
model trained from a disjoint wall street journal dataset. the fraction of examples that
are sentence boundaries is denoted by in s.

classi   cation results using a support vector machine5 and naive bayes6 are sum-

marized in table 3.4 for three di   erent datasets.

while the wall street journal corpus (wsj) is limited to news, the brown corpus
includes 500 documents, distributed across 15 genres, intended as roughly represen-
tative of all published english; the complete works of edgar allen poe includes an
introduction, prose, and poetry. table 3.5 shows how performance on these corpora
scale with the number of training examples. reasonable performance is possible with
limited labeled data, and it appears that additional data would continue to yield
improvements.

corpus
wsj
brown
poe

5

50

500

5000

40000
7.26% 3.57% 1.36% 0.52% 0.25%
5.65% 4.46% 1.65% 0.74% 0.36%
4.01% 2.68% 2.22% 0.98% 0.52%

table 3.5: id166 error rates on the test corpora, using models built from di   erent numbers
of wall street journal training sentences.

while i have no experiments quantifying the improvements in summary quality
that result from this improved sentence segmentation, i observed that only one sum-
mary out of the 88 submitted for evaluation in 2009 contained a sentence boundary
error. the 2008 submission included 96 summaries, 20 of which had at least one
sentence boundary error (i used the punkt system).

5a simple linear kernel is used.
6with add-k smoothing, k = 0.25.

21

chapter 4

selecting sentences

okay, so we have a set of input documents. they   ve been selected by a person to
match the query we want to address (in principle this may be the job of information
retrieval), they   ve been cleaned up and split into sentences. this chapter is concerned
with selecting sentences appropriate for a summary.

4.1 baselines
we   ll begin by establishing a few baselines. the standard baseline for our data (b1)
is to select the    rst l words from the most recent input document, where l is the
length limit. a much stronger baseline (b2) recognizes that in news documents,    rst
sentences tend to summarize well, and simply selects    rst sentences starting with the
most recent document, up to the length limit. table 4.1 shows some id8 scores
for b1 and b2. the automatic metrics of choice will be id8-2 and id8-su4,
since their correlation tends to be lower than any other pair of id8 metrics.

id8-2 id8-su4

data set b1
0.060
0.059
0.060
0.063

2006
2007
2008
2009

b2
0.070
0.094
0.084
0.092

b1
0.108
0.106
0.091
0.099

b2
0.122
0.143
0.119
0.127

table 4.1: baseline id8 results for four datasets. b1 selects sentences in order from
the most recent document up the length limit; b2 selects    rst sentences only up to the
length limit. note that the 2006 and 2007 are 250-word summaries of 25 documents while
2008 and 2009 are 100-word summaries of 10 documents.

22

chapter 4. selecting sentences

these scores are not particularly meaningful by themselves, but serve as a refer-
ence point for further comparison. for now, note how large the gap is between b1
and b2: over 20% improvement in most cases1.

4.2 maximum marginal relevance
imagine you are asked to generate a summary by selecting sentences. you read the
topic and the narrative description and scan through the documents. perhaps you
choose a sentence that responds well to the query and put it in your summary. how
to select a second sentence? you   d like it to respond to the query as well, but don   t
want it to contain the same information as the    rst sentence you chose. after all, you
are working under a tight length constraint.

maximum marginal relevance [carbonell and goldstein, 1998; goldstein et al.,
2000] is an algorithm that formalizes this intuition: maximize relevant information
and minimize redundancy. here   s their equation for choosing the next sentence:

mmr = argmax

si   d\s (cid:18)  (sim1(si, q))     (1       ) max

sj   s

(sim2(si, sj))(cid:19)

(4.1)

initially developed for information retrieval, the formulation applies to summa-
rization with r, the set of all input sentences, s, the set of sentences already selected
for the summary, q, a query, and sim1 and sim2, functions that return the similar-
ity between two sentences. at each step, the mmr sentence is the highest scoring
remaining sentence: maximally similar to the query and penalized by its similarity to
its nearest neighbor in the selected set. with    = 1, the algorithm chooses all relevant
sentences, without regard to redundancy; with    = 0, relevance is of no importance
and selected sentences are as dissimilar as possible   maximizing coverage. to cre-
ate a summary with mmr, sentences are selected greedily until the length limit is
reached. ideal values of    appear to vary widely depending on the source documents.
to fully specify mmr, we need to choose similarity functions. the standard
similarity function, also borrowed from information retrieval, is the cosine distance
between the word vectors for each sentence where the value for each word is the
term frequency inverse document frequency (tfidf). in my experiments, how-
ever, equivalent performance was obtained using a simple word overlap measure: the
number of shared non-stopwords normalized by the length of the longer sentence.

table 4.2 shows scores for summaries generated with mmr with the optimal      
1statistical signi   cance at 95% con   dence for these data requires a gap of roughly 0.005 in
id8 (the standard toolkit uses id64, sampling sets of 3 out of 4 references, to estimate
score variances).

23

chapter 4. selecting sentences

value (shown for mmr1). the cosine distance function (with tfidf) is used for
both similarity functions. mmr1 considers all the input sentences, while mmr2 is
limited to the set of    rst sentences.

id8-2

id8-su4
data set       mmr1 mmr2 mmr1 mmr2
0.130
0.155
0.128
0.132

0.085
0.100
0.076
0.084

0.138
0.150
0.113
0.119

0.079
0.106
0.100
0.097

2006
2007
2008
2009

0.8
0.8
1.0
0.9

table 4.2: maximum marginal relevance scores: mmr1 selects from all input sentences;
mmr2 selects from the set of    rst sentences.

while mmr gives reasonable improvements over the baselines, the    rst-sentence
version remains stronger overall. this suggests that mmr, or perhaps our choice of
similarity function, is not powerful enough to make productive use of the full set of
input sentences.

4.3 using document statistics
it turns out that mmr is broken in a number of ways. first of all, the query in
these problems is too limited to allow for meaningful measurement of similarity. in
particular, many valuable sentences do not overlap substantially with the the query
(in word usage). one option is to follow the course set by information retrieval and
try expanding the query [goldstein et al., 1999]. still, relying on the query as the
seed for importance is usually too risky. better results can be obtained by ignoring
the query altogether.

this may be a product of these datasets: because the input documents are hand-
picked to match the query, a more reliable signal can be extracted from the document
set than from the query alone. still, it seems likely that the underlying structure in
any query-derived document set would convey a stronger signal than the query alone;
a systematic study of this claim would be useful.

in general, the shift away from information retrieval approaches has been pro-
ductive for summarization. this trend was demonstrated nicely by nenkova and van-
derwende when they introduced sumbasic in 2005, a simple system based on word
frequency in the document set that outperformed most everything more complex at
the time [nenkova and vanderwende, 2005; nenkova et al., 2006].

figure 4.1 shows how frequency in the input documents e   ects the id203
that a word makes it into at least one of the references. the sumbasic algorithm

24

chapter 4. selecting sentences

tries to build a summary that includes lots of high-frequency words. it runs as follows:

1. estimate the unigram distribution from the input documents: p(wi) = count(wi)

,

where n is the total number of words in the documents.

n

2. for each sentence sj assign a weight equal to the average id203 of the

words it contains: weight(sj) =(cid:80)wi   sj

p(wi)
|wi   sj|

3. pick the highest scoring sentence that also contains the best scoring word and

add it to the summary (so long as it    ts within the length limit).

4. for each word in the sentence selected in step 3, update its id203:

pnew(wi) = pold(wi)     pold(wi)

5. if the desired summary length has not been reached, go to step 2.

bigrams work better than unigrams (if both words are stopwords, the bigram is
ignored), document frequency works better than raw frequency, and making sure the
most valuable bigram is in the selected sentence actually degrades performance. table
4.3 shows modi   ed sumbasic (sumbasic+) results on our data. as above, we show
two versions: sumbasic+1 uses the full set of input sentences while sumbasic+2 uses
only    rst sentences. these results are signi   cantly better than the mmr results, and
at last, we are able to match or exceed the    rst-sentences-only version using the full
set of sentences.

data set sumbasic+1 sumbasic+2 sumbasic+1 sumbasic+2

id8-2

id8-su4

2006
2007
2008
2009

0.092
0.119
0.097
0.103

0.083
0.113
0.104
0.101

0.140
0.165
0.130
0.134

0.132
0.158
0.133
0.133

table 4.3: the modi   ed version of sumbasic

sumbasic improves on mmr by estimating relevance directly from the input
documents, rather than mediated through the query. but it also improves on mmr
in its treatment of redundancy. rather than model redundancy explicitly, by trying to
select sentences that overlap minimally with those already selected, there is an implicit
redundancy constraint: words lose value once they   ve been added to the summary.
without the id203 updates (skipping step 4), average id8 scores decrease
by 6-18%.

25

chapter 4. selecting sentences

figure 4.1: the number of documents in which a word appears (stopwords excluded)
has a strong (nearly linear) relationship with the estimated id203 that word appears
in at least one of the reference summaries. the blue area shows the fraction of words
with each count: only around 10 words per document set appear in 20 or more of the 25
documents. 2006 and 2007 data.

remarkably, sumbasic+ produces summaries that are nearly state-of-the-art (see
   gure 4.2). adding a heuristic to upweight (by a factor of 2) id165s appearing in the
   rst sentence of a document yields a simple system (sumbasic++) that outperforms
all other (typically far more complex) systems submitted to the annual evaluations,
at least as measured by id8.

4.4 from procedure to objective
at each iteration, sumbasic chooses the most valuable remaining sentence   a greedy
search of some kind. but what are we searching for? the goal is never de   ned, so
sumbasic is just a procedure. it works well but it doesn   t allow us to identify a model
of what makes a good summary.

26

10.04870.569920.11380.168730.18400.083440.23780.049650.30150.032060.33840.022870.39310.016580.43570.012390.49730.0094100.50610.0070110.59530.0054120.61650.0046130.65680.0037140.67240.0029150.70740.0022160.73670.0019170.80540.0015180.86640.0012190.89600.0010200.90640.0009210.92060.0006220.95610.0006230.97500.0004240.96080.0005250.97870.000900.250.50.75112345678910111213141516171819202122232425id203 of appearing in the referencesdocument countchapter 4. selecting sentences

instead, let   s frame the process as an objective function on summaries. first, we
observe that sumbasic   s down-weighting may as well be zeroing (this switch indeed
has a minimal e   ect on the resulting scores). then, the value s of an entire summary
is simply the sum of the id165 values it contains, except that credit is only given
to each id165 once, an implicit constraint on redundancy replacing mmr   s clunky
pairwise similarity estimate. this idea speci   es a model for summarization. here is
some notation:

s =(cid:88)i

wici

to remain as general as possible, let   s replace "id165s" with "concepts": con-
cepts are indexed by i, with ci an indicator for the presence of concept i in the
summary; wi is the weight, or value, of concept i.

now, assuming we are given a set of weights, we can search for the summary that
maximizes the objective function, subject of course to a constraint on word length l.
since we are limiting the summary to extracted sentences, let   s describe the length
constraint in terms of selected sentences. as with the concepts, sentences are indexed
by j, with sj an indicator for the presence of sentence j in the summary; lj is the
length (in words) of sentence j.

maximize: (cid:88)i
subject to: (cid:88)j

wici

ljsj     l

one way to understand this objective is as a weighted maximum coverage problem.
the input space contains weighted concepts, blocked into overlapping units of selec-
tion (sentences). we want to    nd the set of sentences that covers as much of the
space as possible subject to a constraint on the sum of the sentence lengths. this
maximum coverage formulation was introduced for summarization some time ago [fi-
latova and hatzivassiloglou, 2004], but the focus was on entity extraction rather than
useful objective functions, and they used a greedy approximation.

searching for the best solution to this problem will most likely require an algorithm
exponential in the number of input sentences: a reduction from the well-known set-
cover problem [hochbaum, 1996] is straightforward. or, consider the np-complete
knapsack problem: maximize the value of items in a knapsack, where each item has
a value and a weight, and the knapsack has a weight limit. in the case where our
sentences have no overlapping concepts, it is precisely the knapsack problem (each

27

chapter 4. selecting sentences

id8-2 scores for the systems discussed in this section. the best
figure 4.2:
nist scores are shown for comparison (excluding systems submitted by me). recall
that sumbasic+ refers to my enhanced version of the original sumbasic algorithm, and
sumbasic++ adds a heuristic to increase the weight of id165s appearing in    rst sen-
tences.

sentence is an item and the summary is the knapsack). thus the more general case,
where concepts are shared among sentences, reduces to the knapsack problem in this
limited case. as a result, a polynomial time solver for our problem could surely solve
the knapsack problem in polynomial time, so our problem is also np-complete.

the next chapter will address approximate and exact id136 in this framework.
the results shown in figure 4.2 include an exact solution to the maximum coverage
objective, obtained using an integer linear program (ilp) solver. the maximum
coverage system is otherwise identical in its setup to sumbasic++ (the value wi
of bigram i is it   s document count, with upweights for bigrams appearing in    rst
sentences), except that concepts appearing in fewer than three documents are ignored
for e   ciency.

since i submitted the maximum coverage system, with only a few small enhance-
ments [gillick et al., 2010], to the 2009 evaluation, these summaries were evaluated
by experts. figure 4.3 compares its performance to all the other submitted systems.
such strong results are good evidence that our model is reasonable: even though the
objective is extremely simple, the scores are substantially better than other, far more
complex systems.

28

baselinemmrsumbasic+sumbasic++ilpnist bestreferences20062007200820090.0600.0850.0920.0940.1020.0950.1130.0590.1000.1190.1240.1320.1240.1410.0600.0760.0970.1090.1180.1030.0630.0840.1030.1130.1230.10800.020.040.060.080.10.120.142006200720082009id8-2data setbaselinemmrsumbasic+sumbasic++ilpnist bestchapter 4. selecting sentences

figure 4.3: 2009 results for a variety of metrics: the maximum coverage system (with
a few small enhancements) is the yellow bar; the blue bars show scores for all the other
systems submitted to the evaluation. note that the standard baseline (   rst 100 words of
the most recent document) is included, and has the highest linguistic quality score.

4.5 more ideas in extraction
so far, i   ve created a very linear narrative for progress in summarization research. a
number of other ideas in sentence selection co-evolved and merit some discussion.

4.5.1 learning sentence values
what makes for a good summary sentence? lots of frequently occurring words, posi-
tion in the document, similarity to a query? if we had training data   input sentences
labeled as summary or non-summary   we could run standard machine learning algo-
rithms to learn weights for these features.

since the data includes reference abstracts rather than extracts, we need to choose
sentence labels before learning. while some studies employ a classi   cation framework
with binary sentence labels, regression seems a more natural choice, so we   ll set the
label of each sentence to be the average number of reference bigrams it contains.
we treat the four references given for each problem as samples from the space of all
possible references, so if a bigram appears in three of them, the maximum likelihood

29

036overall quality03.57linguistic quality00.20.4pyramid00.0650.13id8-2tuesday, november 9, 2010chapter 4. selecting sentences

figure 4.4: the value of a sentence increases sharply as its length (in words) increases,
but then begins to fall o    if the sentence gets too long (over 40 words). the blue area
shows the fraction of sentences in each length bin. 2006 and 2007 data.

estimate of the id203 it appears in a random reference is 3
4.

i experimented with a variety of regression models, including id166 regression as
used by schilder and kondadadi [2008], id75, and poisson regression.
poisson regression assumes the response variable has a poisson distribution and thus
is especially well-suited to model counts. while poisson regression appears to    t the
training data best, id75 gave comparable or slightly better id8 scores
when a summary is built using the top-scoring sentences2.

the following set of simple features were used as predictors in the regression, and
an analysis of predictive power is shown in table 4.4. figures 4.4, 4.5, 4.6 give a more
   ne-grained analysis for some features.

frequency average per-word document frequency
topic average per-word topic description overlap
2id75 has the undesirable property of producing a few negative expected counts,

which we force to zeros.

30

lengthper-word overlap with referencesfraction of data0-40.01020.01815-90.04600.083610-140.07080.147715-190.07560.179420-240.08190.181825-290.08610.147330-340.08750.105735-390.08850.064140-440.08620.034745-490.07640.020250+0.07420.0174100.020.040.060.080.10-45-910-1415-1920-2425-2930-3435-3940-4445-4950+per-word overlap with referencessentence lengthchapter 4. selecting sentences

figure 4.5: sentence value and position in the document are strongly related. value
declines roughly monotonically (some noise here), but the    rst sentence is in a class of
its own. the blue area shows the fraction of sentences of each length, which declines
because not all documents are of equal length. 2006 and 2007 data.

first sentence indicator for the    rst sentence in a document
sentence position log of the sentence position in the document
sentence length log of the sentence length in words
document length log of the document length in sentences

in the table, the constant term estimates the mean, 0.75 reference words per sentence,
and average document frequency is the strongest individual feature. document length,
which i   ve never seen used as a feature before, is interesting: longer documents have
fewer reference words per sentence. using all features together gives a root mean
squared error (rmse) of 0.85, a 36% improvement over the constant term baseline.
modeling pairwise products between features improves the rmse to 0.79, a total
improvement of 47%.

while the predicted sentence values given by regression are reasonable, using the
predictions to create a summary is not straightforward. most documented work along

31

00.17920.042510.11590.042520.11370.042230.10730.041840.11190.041050.10730.039560.11040.037470.10320.035680.10440.033690.09960.0319100.09450.0302110.09680.0287120.08870.0273130.08800.0259140.08370.0247150.08140.0234160.07780.0223170.07800.0215180.07480.0204190.07840.0196200.07300.0187210.07270.0178220.06820.0172230.06460.0162240.06400.0156250.06260.0149260.06360.0141270.06170.0135280.06090.0129290.05950.012230+0.05380.215200.050.10.150.2024681012141618202224262830+per-word overlap with referencessentence orderchapter 4. selecting sentences

figure 4.6: the more a sentence overlaps (fraction of content words) with the query,
the more valuable it tends to be. the blue area shows the fraction of sentences in each
bin: most sentences have no overlap but the few sentences with very high overlap have
considerably higher value than even    rst sentences. 2006 and 2007 data.

these lines involves using mmr-like criteria to maximize relevance and minimize re-
dundancy. treating the learned sentence values as priors to help weight bigram
counts, the inputs to the maximum coverage system, can give some small improve-
ments.

but this sort of training is fundamentally    awed because the learning happens at
an intermediate stage of summarization: it is not obvious how to create high id8
summaries from high id8 sentences. still, nearly all supervised learning for sum-
marization has been along these lines, with various takes on regression, classi   cation,
and algorithms for minimizing redundancy during sentence selection [kupiec et al.,
1995; conroy and o   leary, 2001; shen et al., 2007; li et al., 2007].

[2007] learn word probabilities with id28 (as
opposed to sentence values). while this leads to a more natural criteria for cre-
ating summaries with maximum total word value (similar to my ilp formulation),
the learning still su   ers from the same mismatch problem, ignoring the underlying
relationships between sentences and words.

note that yih et al.

32

00.04350.5376.1-.40.08250.0257.5-.90.08890.2213.1-.140.12250.1219.15-.190.15050.0521.2-.240.17950.0242.25-.290.22190.0109.3-.340.24190.0045.35+0.29460.001800.050.10.150.20.250.30.350.1-.4.5-.9.1-.14.15-.19.2-.24.25-.29.3-.34.35+per-word overlap with referencessentence-level word overlap with querychapter 4. selecting sentences

feature
constant
frequency
topic
first sentence
sentence position
sentence length
document length
all

estimate cv rmse % gain

0.75
0.49
0.18
0.15
-0.09
0.28
-0.04

1.16
0.96
1.07
1.10
1.09
1.09
1.13
0.85

20.6%
8.5%
5.2%
6.3%
6.3%
2.7%
36.4%

table 4.4: analysis of a id75 predicting the expected number of reference
bigrams in a sentence. each feature is normalized to have mean 0 and variance 1. cv
rmse is the cross-validation root mean squared error of the predictions, with 90% train
10% test splits. 2008 data.

learning feature weights that directly maximize id8 or some other automatic
metric given, for example, the maximum coverage objective, is a task for structured
prediction: the learning algorithm tries to select a set of weights that actually recover
the ideal training summaries. some of my more recent work [berg-kirkpatrick et al.,
2011] addresses this problem.

4.5.2 graph algorithms
so far, we   ve treated the set of input documents as a bag of words (bigrams, really).
but perhaps we   d bene   t from respecting some of the structure that connects words
and sentences, for example. we can construct a graph with a vertex for each word and
each sentence with edges connecting a sentence to the words it contains. a variety
of similar graph structures are employed in the literature [erkan and radev, 2004;
mihalcea and tarau, 2004]. then we can use an iterative id95-style algorithm
[page et al., 1998] to estimate the value or relevance of each word and each sentence
as follows (see figure 4.7):

initialization: maintain a distribution over word values and a distribution over

sentence values, initialized uniformly.

iteration step: the new value of a word is the sum of the sentence values it   s
connected to; the new value of a sentence is likewise the sum of word values it   s
connected to. the distributions are normalized.

this algorithm is typically allowed to run until convergence, though sometimes
with a damping factor that prefers uniformity. intuitively, sentences with the highest

33

chapter 4. selecting sentences

figure 4.7: a graphical depiction of a simple graph algorithm over three sentences and
four words. at iteration 0, the values are initialized to be uniform, and by iteration 2, the
values have nearly converged.

values are most central to the collection. in fact, there is a theoretical interpretation
for the resulting values: if we take a su   ciently lengthy walk through the graph,
moving at random along edges, the normalized values represent the id203 of
stopping at some word or sentence vertex.

of course, while the random walk model may be a reasonable proxy for internet
browsing, it seems less relevant for reading a bunch of documents: reading (or writ-
ing) is mostly linear and doesn   t involve jumping between similar sentences. as a
result, the decision to use the word values after one iteration (these are just simple
counts as in the maximum coverage formulation), is no harder to justify than allowing
the algorithm to run to convergence.

34

w1=1/4w2=1/4w3=1/4w4=1/4s1=1/3s2=1/3s3=1/3iteration 0w1=2/6w2=1/6w3=1/6w4=2/6s1=3/10s2=5/10s3=2/10iteration 1w1=8/23w2=3/23w3=5/23w4=7/23s1=11/38s2=20/38s3=7/38iteration 2chapter 4. selecting sentences

thus, the advantages conferred by a graph representation of a document set may
not be realized through the id95 algorithm. and in fact, i have not been able to
produce any performance gains using any sort of simple iterative procedure like the
algorithm described here.

4.5.3 topic models
all this talk of word values is a little    shy. the value of a word comes more from the
context in which it   s used than from its inherent meaning or the number of times it
appears. and while sentence value makes more sense, sentences come in all di   erent
ideally, summarization
shapes and sizes, with varying amounts of embellishment.
should concern itself with some more appropriate semantic units:
facts and topics
seem preferable to words and sentences.

one step in this direction is id96, where a latent set of topics are inferred
from word occurrences in a set of documents. the simplest form is latent semantic
indexing (lsi): a term-sentence matrix is constructed with a row for each sentence
and a column for each content word in the document set; the values in the cells could
be binary, counts, or tfidf values, for example. running principal components
analysis (pca) on this matrix yields a series of eigenvectors, linear combinations
of the words, that capture the most remaining variance. each resulting component
can be considered a kind of topic, and each sentence, projected onto the    rst few
components, has an interpretation as a weighted mixture of the important topics.
gong and liu [2001] describe summary generation by selecting the sentences that
best represent the    rst few topics.

a simple generative model for topics yields a probabilistic interpretation of lsi
[hofmann, 1999] called probabilistic latent semantic analysis (plsa). the plsa
generative story explains how a set of documents is made:

1. pick a document d from d with id203 p (d)
2. pick a latent topic z from z with id203 p (z|d)
3. generate a word w of w with id203 p (w|z)

the parameters of the model can be estimated via the expectation maximization
(em) algorithm, which reaches a local maximum of the likelihood function:

l =(cid:88)d   d(cid:88)w   w

n(d, w) log(cid:88)z   z

p (w|z)p(z|d)p (d)

where n(d, w) is the number of times word w appears in document d.

35

chapter 4. selecting sentences

by replacing documents with sentences, the topic interpretation matches lsi at
the sentence level. each sentence is represented by a vector in the "topic-space" of
the document set. the id203 distribution p (z|s) tells us to what extent each
topic is covered by a sentence:

p (z|s) = (p (z1|s), p (z2|s), . . . , p (zk|s))

a comparative study suggests plsa may be preferable to lsi for the purposes of sum-
marization [hennig and labor, 2009]; at least it has a more intuitive interpretation
as a model.

augmenting the standard plsa model with a dirichlet prior on topics gives an
improved topic model called id44 (lda) [blei et al., 2003;
steyvers and gri   ths, 2007]. the main change to the generative story is that the
   rst word is picked from the    rst topic and subsequent words are picked from the
set of existing topics with probabilities proportional to the sizes of the topics. since
this model generates an entire dataset at once, whereas plsa generates each word
independently, alternate id136 procedures are needed. variational methods or
id150 are standard3.

recent implementations of topic models for summarization have shown good re-
sults, [haghighi and vanderwende, 2009; celikyilmaz and hakkani-tur, 2010] though
i have no head-to-head comparison with the maximum coverage model (id8 re-
sults suggest comparable performance). note that haghighi and vanderwende use
topic models for estimating word values and then proceed to select sentences greed-
ily with a kl-divergence criteria4; celikyilmaz and hakkani-tur use topic models to
estimate sentence values and are then stuck with the familiar problem of choosing
maximally relevant sentences with minimal redundancy.

3see kevin knight   s

tutorial

on bayesian id136

for

a

readable

introduction:

http://www.isi.edu/natural-language/people/bayes-with-tears.pdf

4choose the summary that minimizes the kl-divergence in word distribution between summary

and source documents.

36

chapter 5

the maximum coverage objective

the last chapter outlined a variety of approaches to valuing sentences and choosing
summaries, all reasonably well motivated. still, as best i can tell, applying the
maximum coverage objective over word bigrams gives results at least as good. it is
simple and appealing: a summary attempts to cover as many important concepts as
possible. rather than attempt to approximate the redundancy between two sentences
with a function, redundancy is treated implicitly: the summary only receives credit
once for each concept.

this chapter describes methods for solving the maximum coverage objective, ap-

proximately and exactly.

5.1 approximate and exact solving

recall that maximum coverage seeks to maximize (cid:80)i wici, where ci is an indicator

for the presence of concept (bigram) c in the summary. the value wi for bigram i
is estimated simply as the document count of ci, with an extra count for each    rst
sentences it appears in. of course, these values could be estimated in some more
principled manner, but for present purposes, i   ll focus just on solving the objective
regardless of the source of the values.

the simplest way to generate a summary from the objective is to choose one
sentence at a time that maximizes the objective value so far. there are really two
ways to do this in our case. the value added by a sentence could be (1) the actual sum
of bigram values in the sentence that do not appear in the partial summary, or (2)
the value of (1) normalized by the by the length of the sentence. in practice, method
(2) usually gives higher total objective values since method (1) tends to over-prefer
long sentences. our greedy algorithm generates a summary with both (1) and (2)
and chooses the one with the larger objective value.

37

chapter 5. the maximum coverage objective

exact solutions can be obtained, as suggested earlier, with a formulation of the
objective as an integer linear program [nemhauser and wolsey, 1988]. while ilp
solutions are exponential in the size of the input, solvers have been highly optimized
to give fast solutions for many problems. here is the full ilp speci   cation of our
objective:

wici

sjlj     l

maximize: (cid:88)i
subject to: (cid:88)j
(cid:88)j

sjoccij     ci,    i, j

sjoccij     ci,    i
ci, cj     {0, 1},    i, j

(5.1)

(5.2)

(5.3)
(5.4)

(5.5)

(5.1) and (5.2) we have seen before; (5.3) and (5.4) are structural constraints ensuring
that sentences and their concepts are consistent. occ is a matrix with occij an indi-
cator for the presence of concept i in sentence j. thus, (5.3) requires that selecting a
sentence entails selecting all the concepts it contains, and (5.4) requires that selecting
a concept is only possible if it is present in at least one selected sentence.

data set greedy exact (ilp) gain
5.4%
6.0%

0.111
0.115

0.117
0.121

2008
2009

table 5.1: comparing average id8-2 scores of greedy and exact solutions to the
maximum coverage objective.

table 5.1 compares id8 scores of greedy and exact solutions. while standard
ilp solvers give solutions quickly for any of the 10- or 25-document summarization
problems addressed here, exact solutions may not always be feasible. when the input
space is prohibitively large, approximate solvers can help bridge the gap between
greedy and exact solutions. experiments with a local search algorithm that maintains
a beam of hypotheses, swapping random sentences and updating the beam on each
iteration, were able to give id8 scores on par with the ilp. this is somewhat
similar in spirit to the stack-based search employed by yih et al. [yih et al., 2007].

38

chapter 5. the maximum coverage objective

5.2 integer linear programs
i have shown that the mysterious ilp solves this the maximum coverage objective
quite handily (the section on ilp run-time, below, quanti   es its miraculous perfor-
mance). to get a sense for how hard the problem is that i   m asking the ilp to solve,
consider a typical problem from the 2007 dataset: choose the set of about 10 sen-
tences (250 words) from the roughly 500 sentences in the document set that gives the

best objective score. this gives a lower bound of about (cid:0)500

summaries. this is a lower bound because the length limit is over words, so there are
many combinations of 8, 9, 10, 11, 12 (etc.) sentences that have under 250 words.
still, this is a lot of summaries to consider. a brute-force enumeration would take
hundreds if not thousands of years on a single fast computer.

10(cid:1) = 2.5    1020 possible

certainly, the problem has some internal structure that can make it more tractable.
some sentences may have no relevant concepts; some may be full subsets of others
in the space of concepts. such observations could allow us to create a more e   cient
algorithm for the speci   c problem of optimizing this particular objective function
on sentences containing concepts. but remarkably, the ilp has no such knowledge.
it can be applied to a wide variety of optimization problems, often with impressive
results.

a linear program is simply one that can be expressed in this canonical form, with
c the weight vector applied to the non-negative vector to be determined x, subject to
linear equality or inequality constraints expressed in matrix form (the elements of b
are also non-negative):

maximize: ct x
subject to: ax <= b

linear programs were developed in the late 1930s and early 1940s and initially
used during world war ii to optimize army expenditures. as a result, it was kept
secret until 1947, and was immediately adopted by industry to aid    nancial decisions.
george danzig   s original approach to solving linear programs is still common
today. the simplex algorithm    nds a feasible point at a vertex of the convex shape
de   ned by the constraints and walks along edges to other vertices with non-decreasing
objective function values until a maximum value is reached. while this method
worked well in practice (and on random problems), it was not until the late 1970s
that a di   erent class of algorithms showed that solving linear programs is polynomial
even in the worst case. the complexity is roughly o(n3.5).

39

chapter 5. the maximum coverage objective

figure 5.1: a graphical representation of the general ilp problem.

if, however, the unknown variables x are required to be integers, things get a bit
trickier: complexity is np hard in the worst case. but since the 1950s, techniques for
solving these integer-only versions of linear programs, ilps, have led to increasingly
fast solutions for real problems. the oldest such algorithm is ralph gomroy   s cutting
plane method, which works by    rst solving the non-integer relaxation of a problem,
and then iteratively adding constraints induced by the separation or cut between this
optimum and the feasible set. at each iteration, the non-integer solution has more
and more integral values. modern systems often use a hybrid of cutting plane and the
branch and bound method, a way of developing alternative sets of cutting planes to
avoid potential numerical instability. this is essentially the default method employed
by the ilp solver (glpk) i used for optimizing the maximum coverage objective.

many natural language processing tasks lend themselves naturally to ilp formu-
lations. casting a problem as an ilp allows us to largely ignore the speci   c hard
optimization required for exact solutions. as a result, ilps have been used to    nd
globally optimal solutions for producing phrase tables for machine translation,    nding
sentence compressions, inferring document structure, and many others.

5.3 theoretical guarantees
while the ilp solutions are appealing, our greedy approximation to the maximum
coverage objective has an interesting theoretical guarantee: the greedy solution must
be within a constant factor (1     1/e) of the optimal solution (e is the natural log-

40

chapter 5. the maximum coverage objective

arithm). while we have shown that the gap between greedy and exact solutions is
in fact substantial in our case (see table 5.1), the nature of this particular theoreti-
cally property, submodularity, has received considerable attention, and deserves some
elaboration.
let f be a function on sets that maps subsets s     v to real numbers. f (  ) is
called monotone if f (s)     f (t ) whenever s     t , and submodular if for any s, t     v :

f (s     t ) + f (s     t )     f (s) + f (t )

an equivalent de   nition of submodularity is the property of diminishing returns. that
is, f (  ) is submodular if for any r     s     v and s     v \ s,

f (s     {s})     f (s)     f (r     {s})     f (r)

this says that the value added by s in the larger set s is no larger than the value
added by s in the smaller subset r.

submodularity is a discrete analog of convexity [lov  sz, 1983]; both properties
tend to facilitate optimization. while maximization of submodular functions is np-
complete, nemhauser et al. showed famously that monotone submodular functions
with a cardinality constraint can be solved with a greedy algorithm within a constant
factor (1   1/e), around 63%, of the optimal solution [nemhauser et al., 1978]. krause
and guestrin [2005] give an improved bound 1
2(1   1/e) for budgeted maximum cover-
age problems, like ours. note that including the two variants in our greedy algorithm
is necessary for receiving the bene   ts of submodularity.

the takeaway message from this discussion is that should we endeavor to summa-
rize giant documents or sets of documents where an exact solution is infeasible, our
greedy approximation for optimizing the maximum coverage objective is guaranteed
to give a solution within about 30% of the optimal solution. lin and bilmes [2010]
have a more extensive analysis of submodular functions for summarization.

5.4 ilp run-time
while ilps have been used extensively for natural language processing problems,
designing an objective to allow fast solutions can be tricky. to assess performance
of our ilp, let   s compare the solving time to an adaptation of maximum marginal

41

chapter 5. the maximum coverage objective

relevance to an ilp. in particular, mcdonald [2007] formulates mmr as follows:

maximize: (cid:88)j
subject to: (cid:88)j

  jj(cid:48)red(j, j(cid:48))

sjrel(j)    (cid:88)j(cid:48)

   j

sjlj     l

  jj(cid:48)     sj     0,    j, j(cid:48)
  jj(cid:48)     sj(cid:48)     0,    j, j(cid:48)
sj + sj(cid:48)       jj(cid:48)     0,    j, j(cid:48)
sj,   jj(cid:48)     {0, 1},    j, j(cid:48)

(5.6)

(5.7)

(5.8)
(5.9)
(5.10)
(5.11)

given some de   nitions of relevance (rel) and pairwise redundancy (rel), this ver-
sion of mmr gives a globally optimal selection of sentences subject to a length con-
straint. because redundancy is de   ned over sentence pairs, mcdonald coerces a kind
of quadratic knapsack problem [gallo et al., 1980] into an a linear program by intro-
ducing indicators   jj(cid:48) for selecting a pair of sentences j and j(cid:48). as in the original
paper, relevance and redundancy are computed as follows:

relj = cosine(j, d) + 1/pos(j, d)

redjj(cid:48) = cosine(j, j(cid:48))

here, cosine(j, d) is the cosine distance between the tfidf vectors for sentence j
and document d, and pos(j, d) is the integer-valued order of sentence j in document
d.

with n input sentences and m concepts (bigrams), both formulations generate
a quadratic number of constraints. however, the mmr formulation has o(n2) vari-
ables while the maximum coverage formulation has o(n + m).
in practice, scala-
bility is largely determined by the sparsity of the redundancy matrix red and the
sentence-concept matrix occ. e   cient solutions thus depend heavily on the redun-
dancy measure and the concept unit. pruning to reduce complexity involves removing
low-relevance sentences or ignoring low-redundancy values in the mmr formulation,
and corresponds to removing low-weight concepts in the maximum coverage formu-
lation. note that pruning concepts may be more desirable: pruned sentences are
irretrievable, but pruned concepts may well appear in the selected sentences through
co-occurrence with unpruned concepts.

figure 5.2 compares ilp run-times for the two formulations, using a set of 25
problems from the 2007 dataset, each of which have at least 500 input sentences.
while the ilp solver1    nds optimal solutions e   ciently for the maximum coverage

1i use glpk with all the default settings (gnu.org/software/glpk)

42

chapter 5. the maximum coverage objective

figure 5.2: a comparison of ilp run-times (on an amd 1.8ghz machine) of mcdonald   s
mmr formulation and the maximum coverage formulation on an increasing number of
input sentences.

formulation, the mmr solution times grow rapidly with the size of the input. the
plot includes timing results for both 100 and 250 word summaries, showing that
fast solutions are given even for much harder problems: a rough estimate for the

number of possible summaries has (cid:0)500
10(cid:1) = 2.5    1020 for 250 word summaries.
(cid:0)500

the maximum coverage objective also gives far better id8 scores than mc-
donald   s mmr formulation, at least with these choices of similarity and redundancy
functions.

4(cid:1) = 2.6    109 for 100-word summaries and

43

formulations,usingasetof25topicsfromduc2007,eachofwhichhaveatleast500inputsen-tences.theseareverysimilartothetac2008topics,butmoreinputdocumentsareprovidedforeachtopic,whichallowedustoextendtheanalysistolargerproblems.whiletheilpsolver   ndsopti-malsolutionsef   cientlyforourconcept-basedfor-mulation,run-timeformcdonald   sapproachgrowsveryrapidly.theplotincludestimingresultsfor250-wordsummariesaswell,showingthatourap-proachisfastevenformuchmorecomplexprob-lems:aroughestimateforthenumberofpossiblesummarieshas   5004   =2.6  109for100-wordsum-mariesand   50010   =2.5  1020for250wordssum-maries.whileexactsolutionsaretheoreticallyappealing,theyareonlyusefulinpracticeiffastapproxima-tionsareinferior.agreedyapproximationofourobjectivefunctiongives10%lowerid8scoresthantheexactsolution,agapthatseparatesthehigh-estscoringsystemsfromthemiddleofthepackinthetacevaluation.thegreedysolution(linearinthenumberofsentences,assumingaconstantsum-marylength)marksanupperboundonspeedandalowerboundonperformance;theilpsolutionmarksanupperboundonperformancebutissubjecttotheperilsofexponentialscaling.whilewehavenotexperimentedwithmuchlargerdocuments,ap-proximatemethodswilllikelybevaluableinbridg-ingtheperformancegapforcomplexproblems.pre-liminaryexperimentswithlocalsearchmethodsarepromisinginthisregard.6extensionsherewedescribehowourilpformulationcanbeextendedwithadditionalconstraintstoincor-poratesentencecompression.inparticular,weareinterestedincreatingcompressedalternativesfortheoriginalsentencebymanipulatingitsparsetree(knightandmarcu,2000).thisideahasbeenappliedwithsomesuccesstosummarization(turnerandcharniak,2005;hovyetal.,2005;nenkova,2008)withthegoalofremovingirrelevantorredun-dantdetails,thusfreeingspaceformorerelevantin-formation.onewaytoachievethisendistogen-eratecompressedcandidatesforeachsentence,cre-atinganexpandedpoolofinputsentences,andem-50100150200250300350400450500012345678number of sentencesaverage time per problem (seconds)  100 word summaries250 word summaries100 word summaries (mcdonald)figure2:acomparisonofilprun-times(onanamd1.8ghzdesktopmachine)ofmcdonald   ssentence-basedformulationandourconcept-basedformulationwithanincreasingnumberofinputsentences.ploysomeredundancyremovalonthe   nalselec-tion(madnanietal.,2007).weadaptthisapproachto   ttheilpformulationssothattheoptimizationproceduredecideswhichcompressedalternativestopick.formally,eachcompressioncandidatebelongstoagroupgkcorre-spondingtoitsoriginalsentence.wecanthencraftaconstrainttoensurethatatmostonesentencecanbeselectedfromgroupgk,whichalsoincludestheoriginal:   i   gksi   1,   gkassumingthatallthecompressedcandidatesarethemselveswell-formed,meaningfulsentences,wewouldexpectthisapproachtogeneratehigherqual-itysummaries.ingeneral,however,compressionalgorithmscangenerateanexponentialnumberofcandidates.withinmcdonald   sframework,thiscanincreasethenumberofvariablesandconstraintstremendously.thus,weseekacompactrepresenta-tionforcompressioninourconceptframework.speci   cally,weassumethatcompressionin-volvessomecombinationofthreebasicoperationsonsentences:extraction,removal,andsubstitution.inextraction,asub-sentence(perhapsthecontentofaquotation)maybeusedindependently,andtherestofthesentenceisdropped.inremoval,asubstringchapter 5. the maximum coverage objective

5.5 adding compression to the ilp
moving from extractive to abstractive summarization, while desirable in the long
run, is a daunting challenge.
incorporating sentence compression into extractive
summarization is something of a middle ground; while the modi   cations we can make
to the source sentences are limited, we have an exponentially expanded set of textual
units to choose from. this gives us improved    exibility for removing redundancy, for
example. here i   ll describe how the ilp formulation for maximum coverage selection
can be extended with additional constraints to include sentence compression.

one way to combine selection and compression is to generate compressed candi-
dates for each sentence, creating an expanded pool of input sentences, and employ
some redundancy removal on the    nal selection [madnani et al., 2007]. this approach
can be adapted so that the ilp decides which alternatives to pick. if each compression
candidate belongs to a group gk corresponding to its source sentence, then we can
include a constraint in the ilp to ensure that at most one sentence can be selected
from each group (which also includes the source):

(cid:88)i   gk

si     1,    gk

assuming that all the compressed candidates are themselves well-formed, meaning-
ful sentences, this approach should generate higher quality summaries. in general,
however, compression algorithms can generate an exponential number of candidates.
most compression strategies are based on the parse-tree structure of a sentence,
where some subset of nodes are removed that preserve the grammaticality of the sen-
tence [knight and marcu, 2000; mcdonald, 2006; turner and charniak, 2005]. while
we   ll focus on node removals, we   ll also allow extraction (a sub-sentence, perhaps the
content of a quotation, may be used independently and the rest of the sentence is
dropped) and substitution (one substring is replaced by another: us replaces united
states, for example).

arbitrary combinations of these operations are too general to be represented e   -
ciently in an ilp. in particular, we need to compute the length of a sentence and the
concepts it covers for all compression candidates. thus, we insist that the operations
only a   ect non-overlapping spans of text.

in our tree representation of a sentence, nodes correspond to compression oper-
ations and leaves map to words. each node includes the word length it contributes
to the sentence recursively, as the sum of the lengths of its children. similarly, the
concepts covered by a node are the union of the concepts covered by its children.
when a node is active in the ilp, the words below it in the tree are included in

44

chapter 5. the maximum coverage objective

the summary; active words contribute to the length constraint and objective score.
figure 5.3 gives an example of a tree representation, showing derivations of some
compression candidates.

figure 5.3: a compression tree for an example sentence. e-nodes (diamonds) can be
extracted and used as independent sentences; r-nodes (circles) can be removed; s-nodes
(squares) contain substitution alternatives. the table shows the word bigram concepts
covered by each node and the length it contributes to the summary. examples of resulting
compression candidates are given on the right side, with the list of nodes activated in their
derivations.

this framework can be used to implement a wide range of compression techniques.
as a baseline proof-of-concept, we derive the compression tree from the sentence   s
parse tree given by the berkeley parser [petrov and klein, 2007], and use a set of
rules to label parse tree nodes with compression operations. for example, declarative
clauses containing a subject and a verb are labeled with the extract (e) operation;
adverbial clauses and non-mandatory prepositional clauses are labeled with the re-
move (r) operation; acronyms can be replaced by their full form by using substitution
(s) operations, and a primitive form of co-reference resolution is used to allow the
substitution of noun phrases with their referents.

while this compression framework gave a small improvement in id8 over the
extractive system, pyramid and linguistic quality scores declined signi   cantly. an
analysis of the resulting summaries showed that the rules used for identifying candi-

45

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:6)(cid:8)(cid:10)(cid:10)(cid:10)(cid:10)(cid:12)(cid:13)(cid:11)(cid:4)(cid:4)(cid:7)(cid:4)(cid:14)(cid:10)(cid:5)(cid:2)(cid:10)(cid:15)(cid:2)(cid:13)(cid:16)(cid:10)(cid:5)(cid:15)(cid:8)(cid:10)(cid:8)(cid:3)(cid:6)(cid:2)(cid:11)(cid:9)(cid:10)(cid:12)(cid:11)(cid:6)(cid:5)(cid:10)(cid:2)(cid:17)(cid:10)(cid:5)(cid:15)(cid:8)(cid:7)(cid:6)(cid:10)(cid:10)(cid:10)(cid:10)(cid:6)(cid:8)(cid:9)(cid:8)(cid:6)(cid:18)(cid:8)(cid:9)(cid:5)(cid:15)(cid:8)(cid:10)(cid:19)(cid:11)(cid:14)(cid:11)(cid:20)(cid:7)(cid:4)(cid:8)(cid:10)(cid:21)(cid:3)(cid:2)(cid:5)(cid:8)(cid:16)(cid:10)(cid:10)(cid:10)(cid:10)(cid:1)(cid:15)(cid:7)(cid:8)(cid:17)(cid:10)(cid:22)(cid:7)(cid:13)(cid:19)(cid:10)(cid:23)(cid:7)(cid:9)(cid:8)(cid:4)(cid:24)(cid:8)(cid:6)(cid:14)(cid:11)(cid:9)(cid:10)(cid:9)(cid:11)(cid:25)(cid:7)(cid:4)(cid:14)(cid:11)(cid:13)(cid:6)(cid:8)(cid:11)(cid:16)(cid:25)(cid:26)(cid:27)(cid:28)(cid:29)(cid:10)(cid:30)(cid:10)(cid:27)(cid:3)(cid:6)(cid:2)(cid:12)(cid:8)(cid:11)(cid:4)(cid:10)(cid:28)(cid:8)(cid:4)(cid:5)(cid:6)(cid:11)(cid:13)(cid:10)(cid:29)(cid:11)(cid:4)(cid:31)(cid:32)(cid:17)(cid:2)(cid:6)(cid:8)(cid:7)(cid:14)(cid:4)(cid:10)(cid:1)(cid:3)(cid:6)(cid:6)(cid:8)(cid:4)(cid:1)(cid:25)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:1)(cid:6)(cid:3)(cid:4)(cid:5)(cid:1)(cid:7)(cid:3)(cid:4)(cid:8)(cid:1)(cid:9)(cid:3)(cid:4)(cid:8)(cid:1)(cid:10)(cid:3)(cid:4)(cid:8)(cid:1)(cid:11)(cid:3)(cid:4)(cid:8)(cid:1)(cid:12)(cid:3)(cid:4)(cid:13)(cid:33)(cid:10)(cid:4)(cid:3)(cid:19)(cid:24)(cid:8)(cid:6)(cid:10)(cid:2)(cid:17)(cid:1)(cid:14)(cid:3)(cid:4)(cid:8)nodelen.concepts(1):e6{themagazine,magazinequoted,chiefwilm,wilmdisenberg}(2):e7{countriesare,planningto,tohold,holdthe,theeuro}(3):s0{}(3a)1{ecb}(3b)3{europeancentral,centralbank}(4):r2{assaying}(5):r3{anumber,numberof}(6):r1{}(7):r5{aspart,partof,reserves}(8):r2{foreigncurrency}   original:anumberofcountriesarealreadyplanningtoholdtheeuroaspartoftheirforeigncur-rencyreserves,themagazinequotedeuropeancentralbankchiefwimduisenbergassaying.   [1,2,5,3a]:anumberofcountriesareplanningtoholdtheeuro,themaga-zinequotedecbchiefwimduisen-berg.   [2,5,6,7,8]:anumberofcountriesarealreadyplanningtoholdtheeuroaspartoftheirforeigncurrencyre-serves.   [2,7,8]:countriesareplanningtoholdtheeuroaspartoftheirforeigncurrencyreserves.   [2]:countriesareplanningtoholdtheeuro.figure3:acompressiontreeforanexamplesentence.e-nodes(diamonds)canbeextractedandusedasanindepen-dentsentences,r-nodes(circles)canberemoved,ands-nodes(squares)containsubstitutionalternatives.thetableshowsthewordbigramconceptscoveredbyeachnodeandthelengthitcontributestothesummary.examplesofresultingcompressioncandidatesaregivenontherightside,withthelistofnodesactivatedintheirderivations.inid8-2score(seetable3),butareductioninpyramidscore.ananalysisoftheresultingsum-mariesshowedthattherulesusedforimplementingsentencecompressionfailtoensurethatallcom-pressioncandidatesarevalidsentences,andabout60%ofthesummariescontainungrammaticalsen-tences.thisiscon   rmedbythelinguisticqual-ity4scoredropforthissystem.thepoorqualityofthecompressedsentencesexplainsthereductioninpyramidscores:humanjudgestendtonotgivecredittoungrammaticalsentencesbecausetheyob-scurethescus.wehaveshowninthissectionhowsentencecom-pressioncanbeimplementedinamorescalablewayundertheconcept-basedmodel,butitremainstobeshownthatsuchatechniquecanimprovesummaryquality.7relatedworkinadditiontoproposinganilpforthesentence-levelmodel,mcdonald(2007)discussesakindofsummary-levelmodel:thescoreofasummaryis4asmeasuredaccordingtothetac   08guidelines.determinedbyitscosinesimilaritytothecollectionofinputdocuments.thoughthisideaisonlyimple-mentedwithapproximatemethods,itissimilarinspirittoourconcept-basedmodelsinceitreliesonweightsforindividualsummarywordsratherthansentences.usingamaximumcoveragemodelforsumma-rizationisnotnew.filatova(2004)formalizestheidea,discussingitssimilaritytotheclassicalnp-hardproblem,butintheendusesagreedyapproxi-mationtogeneratesummaries.morerecently,yihetal.(2007)employasimilarmodelandusesastackdecodertoimproveonagreedysearch.globallyoptimalsummariesarealsodiscussedbyliu(2006)andjaouakallel(2004)whoapplygeneticalgo-rithmsfor   ndingselectionsofsentencesthatmaxi-mizesummary-levelmetrics.hassel(2006)useshillclimbingtobuildsummariesthatmaximizeaglobalinformationcriterionbasedonrandomindexing.thegeneralideaofconcept-levelscoringforsummarizationisemployedinthesumbasicsys-tem(nenkovaandvanderwende,2005),whichchoosessentencesgreedilyaccordingtothesumoftheirwordvalues(valuesarederivedfromfre-chapter 5. the maximum coverage objective

dates for the compression operations fail to ensure that all compression candidates
are valid sentences; about 60% of the summaries contained ungrammatical sentences.
still, the ilp gave exact solutions quickly (1-3) seconds per problem; the model
represents an e   cient way to perform joint extraction and compression. my more
recent work on joint modeling [berg-kirkpatrick et al., 2011] adapts this framework
for learning concept weights and weights for deleting parse tree nodes at the same
time.

46

chapter 6

ordering sentences

all the methods i   ve discussed so far involve choosing a set of sentences, neglecting
the order in which they are presented. since a great deal of meaning would be lost, or
at least convoluted, if a document   s sentences were presented in random order, we can
expect the order of sentences in a summary to have some bearing on that summary   s
quality. for example, when barzilay et al. asked people to order 8 sentences into a
summary, 50 people created only 21 unique orderings, with signi   cant overlap. while
there are a variety of coherent orderings, this result suggests that the set of coherent
orderings is a small subset of all possible orderings [barzilay et al., 2002].

6.1 ordering is hard
a variety of work on sentence ordering has produced only relatively small improve-
ment over a chronological baseline (sentences are ordered by source document publi-
cation date    rst, and second by their order of appearance in the source). the primary
competing feature is some form of lexical cohesion, as studies have shown that poor
readability tends to arise from jumps between topics.

conroy et al., for example, compute the sentence similarity (some variant of the
usual cosine distance) between all pairs of selected sentences, and then choose the
ordering that minimizes the sum of all pairwise distances [conroy et al., 2006]. the
resulting traveling salesman problem [lin, 1965] is np-hard and thus exponential in
the number of sentences, but many good approximations exist, and for short sum-
maries, exact solutions are feasible. while such a straightforward optimization is
appealing, the connection between sentence similarity and actual semantic cohesion
is tenuous.

bollegala et al., attempt to combine such lexical cohesion metrics with chrono-
logical features in a classi   er with some success [bollegala et al., 2010]. still, the

47

chapter 6. ordering sentences

chronological information proved by far the strongest individual feature, and the
classi   er gave signi   cant but small improvement.

certainly, current methods could stand to improve on the summary re-ordering
problem. but perhaps a more serious issue, especially when the desired summary
length is only a few sentences (as in the 2008 and 2009 datasets), is choosing a set of
sentences that have the potential to be ordered in a reasonable fashion. one possibility
is to try solving the ordering problem jointly with the selection problem. a simpler,
practical solution is instead to try selecting independent sentences, sentences that can
stand alone without relying on other context. if we can select independent sentences,
perhaps we can bypass the pesky ordering problem.

6.2 independent sentences
the focus on chronology and lexical cohesion obscures the biggest obstacle to co-
herence in an extractive summary: coreference. a person or organization might be
introduced with identifying detail in one sentence, but subsequent references are of-
ten pronominal. "he refused to comment for the story" has little meaning without
sentences that give context for "he" and "the story".

one way to    x incoherent summaries is to model the coreference structure in the
source documents, requiring, for example, that a sentence can only be included in a
summary if its dependencies are also included. certainly, cross-document coreference
resolution is an active area of research [haghighi and klein, 2010; rahman and ng,
2011], but even the best systems give 65%-75% accuracy on sets of news documents
like ours.

before going down this road, though, it would be prudent to see if the set of sen-
tences that has dependencies are really important. here, i   ll describe a classi   cation
approach that takes a pronoun and features of its containing sentence, and outputs
a decision: resolved or unresolved. as it turns out, the classi   er performs at over
90% accuracy, so we can tell fairly reliably if a sentence contains pronouns that re-
quire other sentences for context. removing these non-independent sentences ought
to improve overall coherence, and thus overall quality so long as content is not lost.
the classi   er is trained from the coreference resolution annotations in ontonotes
2.9 [hovy et al., 2006]. a pronoun is considered to be resolved if a non-pronominal
reference to the same entity is present in the sentence. the processing pipeline for
removing sentences with unresolved pronouns is as follows:

1. parse an input sentence using the berkeley constituency parser [petrov and

klein, 2007].

2. locate potential pronouns with the "prp" and "prp$" part-of-speech tags.

48

chapter 6. ordering sentences

3. extract features from the parse tree: pronoun position, relative position of nps,

other pronouns in the sentence, etc.

4. prune the sentence if at least one unresolved pronoun is identi   ed.

we train a linear classi   er, in this case adaboost [schapire, 1999] to help make
the learned weights interpretable1. 6,400 pronoun instances, of which 50% are pos-
itive examples (unresolved), are used as training data. table 6.1 shows the    rst
few (most important) features selected by the classi   er along with performance on a
1000-example test set.

the most useful piece of information is whether there is an noun phrase appearing
before the pronoun.
if there is, as in "president obama said he condemned the
attacks", usually this np directly resolves the pronoun. in fact, all of the cases (in
the test set) of unresolved pronouns have no preceding np; of course, some of these
   nd resolution later in the sentence, so more features are needed. top performance
on a held-out set of 9% error is reached with around 50 features.

feature (relative to pronoun)
no np before
closest preceding np starts with nnp
closest preceding np starts with prp
previous word is <comma>
previous word is <quotes>

error recall precision
34%
22%
19%
17%
15%

100%
90%
75%
79%
82%

50%
68%
88%
87%
87%

table 6.1: top    ve features selected by adaboost for classi   cation of pronouns as resolved
or unresolved.

table 6.2 shows the e   ect of pruning sentences with a variety of decision thresh-
olds. the results suggest that 30% of sentences can be pruned safely, perhaps even
improving id8 scores. also note that this pruning is fairly independent of prun-
ing derived from a content classi   er, as described in 4.5.1. together, the two classi   ers
can safely prune over 50% of the input sentences, giving a marginal improvement in
id8. while i have not conducted an evaluation of the linguistic quality of the
resulting summaries, they certainly contain fewer unresolved pronouns. and linguis-
tic quality scores in the 2009 evaluation were comparable to the best (non-baseline)
system (see figure 4.3).

this classi   cation approach adequately handles pronoun coreference cases, but
still many context-dependent sentences remain. the problem is that coreference is
more complex than names and pronouns: "the explosion", "obama   s decision", and

1we use an open-source implementation of adaboost [favre et al., ].

49

chapter 6. ordering sentences

2008

2009

threshold pruned id8-2 pruned id8-2
1
0.1
0.01
0.001
0

11.54
11.66
11.59
11.64
11.39

11.92
12.04
11.80
11.53
11.31

0%
30%
37%
45%
53%

0%
29%
37%
44%
54%

table 6.2: id8 results for di   erent sentence-pruning thresholds with the unresolved
pronoun classi   er.

"consequences", for example, all require elaboration. further improvements in linguis-
tic quality will likely involve more sophisticated identi   cation of context-dependent
phrases. whether it will be important to map the dependency structure remains to
be seen.

50

chapter 7

evaluating summaries

7.1 id165 recall
id8, as reported throughout this thesis, has been the object of considerable
criticism. as i understand the calculation reported in lin   s original paper, id8-
1 is computed by:

(cid:88)s   ref(cid:88)w   s

(cid:88)s   ref(cid:88)w   s

max(ccandidate(w), cref (w))

cref (w)

(7.1)

where ref is the set of reference summaries (there are four of these for each problem
in our datasets), and w is a word in the set of words in s. ccandidate(w) is the number
of times that word w appears in the candidate summary and cref (w) is the number of
times that word w appears in the references. id8-n replaces w with an id165.
this is an odd calculation for a number of reasons, but one obvious complaint,
especially if you subscribe the maximum coverage model for summarization, is that
repeated id165s in the candidate summary are rewarded. to    x this issue, and to
simplify the algorithm for automatic evaluation, i propose id165 recall as a new
metric. with g(y) the set of id165s in the candidate summary and g(y   ) the set
of id165s in the reference summaries, id165 recall is de   ned as:

(7.2)

table 7.1 compares correlations to show that nothing is lost by simplifying id8.
alternate correlation metrics show similar results.

|g(y )     g(y    )|

|g(y    )|

51

chapter 7. evaluating summaries

metric
id8-1
recall-1
id8-2
recall-2
id8-3
recall-3
id8-4
recall-4

2008 correlation 2009 correlation
oq
0.85
0.87
0.89
0.90
0.91
0.92
0.91
0.92

pyr
0.91
0.92
0.94
0.94
0.94
0.94
0.92
0.92

pyr
0.89
0.91
0.97
0.97
0.97
0.97
0.94
0.94

oq
0.81
0.84
0.88
0.89
0.88
0.89
0.86
0.87

table 7.1: pearson correlations between systems    average manual scores (overall quality
and pyramid) and average automatic scores (classic id8 and the new simple recall
metric).

note that unlike id8, id165 recall throws all the reference id165s together
in a single set, ignoring the fact that some of those id165s appear in all the references
and others appear in only one, for example. intuitively, as we saw with weighting n-
grams in the source documents, the more references a word appears in, the important
it is likely to be. pyramid takes this point of view, giving weight to a fact equal to
the number of references that include it. but experiments show that applying this
intuition to id165s is not so straightforward.

rather than count the total number of overlapping id165s, let   s subdivide these
so we count the number of overlapping id165s that appear in 1 reference, 2 refer-
ences, 3, and 4. table 7.2 shows pearson correlations for the counts in these subdivi-
sions. unigrams show the expected behavior: the number of overlaps you have with
words appearing in all the references is more indicative of quality than the number
of overlaps you have with words appearing in a single reference. but higher order
id165s have the opposite behavior. since 4-grams are the most extreme in this re-
gard, it seems reasonable to assume that part of the problem is sparsity. almost no
4-grams appear in all 4 reference summaries, so using this statistic is unreliable.

there is still another hidden source of variability here.

included in the set of
reference words are content words like "president" and "earthquake", but also function
words like "the" and "of", which are presumably much less important. id165 recall
makes no distinction between di   erent kinds of words; in fact, removing stopwords
(with a list) hurts correlation both with id8 and id165 recall. so another
reason that id165s appearing many times are potentially less useful is because these
are more likely to have more function words and fewer content words.

thus one way to pursue improved automatic id74 is to    nd a way

52

chapter 7. evaluating summaries

n
1
2
3
4

number of references
1
4
0.87
0.77
0.86
0.89
0.89
0.80
0.46
0.87

2
0.81
0.86
0.83
0.78

3
0.84
0.87
0.83
0.79

table 7.2: pearson correlations between systems    average overall quality scores and
average automatic scores broken down by order n and number of references; 2009 data.

to weight id165s in the references according to their predictive potential.

7.2 maximum recall
another reason to prefer id165 recall to id8 is that it lends itself more naturally
to optimization. in particular,    nding the maximum-id8 summary given a set
of references is di   cult because of the non-linearity in the max; but    nding the
maximum recall summary is a straightforward application of the maximum coverage
problem described earlier. in place of the values estimated from the input documents,
we can substitute an indicator for whether the id165 appears in the references. the
ilp gives e   cient solutions.

figure 7.1 is the result of a manual evaluation of a variety of summaries: two
baseline systems (b1 and b2), our state-of-the-art (soa) system, the maximum recall
oracle (mro) summaries, and the human-written abstracts (h). it appears that even
though the oracle   s id8 scores are high, they are still roughly correlated with
quality; human summaries, on the other hand, which tend to use di   erent language,
occupy a separate space in the plot. it seems reasonable to conclude that (1) human
summaries and extractive summaries exhibit di   erent relationships between id8
and quality, and (2) even at high values, id8 continues to correlate with quality,
though the relationship is non-linear:
improvements in id8 score mean less at
larger values.

the evaluation showed that the maximum recall extracts were of signi   cantly
better quality than the best automatic extractive system. further reinforcing the
point that systems have not achieved the best possible results in extraction, genest
et al. [genest et al., 2010] conducted an evaluation of manual extractive summaries
using the 2009 dataset. they found that the manual extracts were comparable in
pyramid score to the best automatic system, but signi   cantly worse in linguistic
quality and overall quality.

53

chapter 7. evaluating summaries

figure 7.1: id8-2 vs. manually evaluated quality is plotted for each summary. the
automatic systems b1, b2, and state of the art (soa) are grouped together and the trend
line is a linear    t for this data. the maximum recall oracle (mro) results all appear
above the trend line, while the human (h) results seem uncorrelated.

7.3 id104
manual evaluation is time-consuming. ideally, a judge would read the original set
of documents before deciding how well the important aspects are conveyed by a
summary. a typical 10-document problem could reasonably involve 25 minutes of
reading or skimming and 5 more minutes for assessing a 100-word summary. since
summary output can be quite variable, at least 30 topics should be evaluated to get
a robust estimate of performance. assuming a single judge evaluates all summaries
for a topic (more redundancy would be better), we get a rough time estimate: 17.5
hours to evaluate two systems.

thus it is of great interest to    nd ways of speeding up evaluation while minimizing
subjectivity. amazon   s mechanical turk (mturk) system has been used for a variety
of labeling and annotation tasks [snow et al., 2008], but such crowd-sourcing has not
been tested for summarization.

here i describe an experiment to test whether mturk is able to reproduce system-
level rankings that match expert opinion [gillick and liu, 2010]. unlike the results of

54

11.522.533.544.5500.050.10.150.20.250.3content responsivenessid8   2  b1, b2, soamrohchapter 7. evaluating summaries

other crowd-sourcing annotations for natural language tasks, here it seems that non-
expert judges are unable to provide expert-like scores and tend to disagree signi   cantly
with each other.

7.3.1 agreement and consistency
in the o   cial 2009 evaluation, each summary was judged by one of eight experts for
   overall quality    and    linguistic quality    on a 1 (   very poor   ) to 10 (   very good   )
scale. unfortunately, the lack of redundant judgments means we cannot estimate
inter-annotator agreement. however, we note that out of all 4576 submitted sum-
maries, there are 226 pairs that are identical, which allows us to estimate annotator
consistency. table 7.3 shows that an expert annotator will give the same summary
the same score just over half the time.

0
oq 119
lq 117

score di   erence

1
92
82

2
15
20

3 mean
0.54
0
7
0.63

table 7.3:
identical summaries often were given di   erent scores by the same expert
human judge (2009 data). counts of absolute score di   erences are shown for overall
quality (oq) and linguistic quality (lq).

7.3.2 designing a task for non-experts
one way to dramatically speed up evaluation is to use the experts    reference sum-
maries as a gold standard, leaving the source documents out entirely. this is the idea
behind automatic evaluation with id8, which measures id165 overlap with the
references, and assisted evaluation with pyramid, which measures overlap of facts
or "semantic content units" with the references. the same idea has also been em-
ployed in various manual evaluations, for example by haghighi and vanderwende
[2009], to directly compare the summaries of two di   erent systems. the potential
bias introduced by such abbreviated evaluation has not been explored.

the overall structure of the human intelligence task (hit) we designed for sum-
mary evaluation is as follows: the worker is asked to read the topic and description,
and then two reference summaries (there is no mention of the source documents). the
candidate summary appears next, followed by instructions to provide scores between

55

chapter 7. evaluating summaries

1 (very poor) and 10 (very good) in each category1. mouse-over on the category
names provides extra details, copied with slight modi   cations from dang [2007].

the initial hit design asked workers to perform a head-to-head comparison of
two candidate summaries, but we found this unsatisfactory for a number of reasons.
first, many of the resulting scores did not obey the transitive property: given sum-
maries x, y, and z, a single worker showed a preference for y > x and z > y, but
also x > z. second, while this kind of head-to-head evaluation may be useful for sys-
tem development, we are speci   cally interested here in comparing non-expert mturk
evaluation with expert tac evaluation.

we went through a few rounds of revisions to the language in the hit after
observing worker feedback. speci   cally, we found it was important to emphasize that
a good summary not only responds to the topic and description, but also conveys the
information in the references.

only workers with at least a 96% hit approval rating2 were allowed access to
this task. we monitored results manually and blocked workers (rejecting their work)
if they completed a hit in under 25 seconds. such suspect work typically showed
uniform scores (usually all 10s). nearly 30% of hits were rejected for this reason.

to encourage careful work, we included this note in our hits: "high annotator
consistency is important. if the scores you provide deviate from the average scores of
other annotators on the same hit, your work will be rejected. we will award bonuses
for particularly good work." we gave a few small bonuses ($0.50) to workers who left
thoughtful comments.

we experimented with a few di   erent compensation levels and observed a some-
what counter-intuitive result. higher compensation ($.10 per hit) yielded lower
quality work than lower compensation ($.07 per hit), judging by the number of
hits we rejected. it seems that lower compensation attracts workers who are less
interested in making money, and thus willing to spend more time and e   ort. there is
a trade-o   , though, as there are fewer workers willing to do the task for less money.

7.3.3 experiments and analysis
to assess how well mturk workers are able to emulate the work of expert judges
employed in the o   cial evaluation, we chose a subset of systems and analyze the
results of the two evaluations. the systems were chosen to represent the entire range of
average overall quality scores. system f is a simple lead baseline, which generates a
summary by selecting the    rst sentences up to 100 words of the most recent document.
the rest of the systems were submitted by various track participants. the mturk

1besides overall quality and linguistic quality, we include information content, to encourage

judges to distinguish between content and readability.

2mturk approval ratings calculated as the fraction of hits approved by requesters.

56

chapter 7. evaluating summaries

evaluation included two-times redundancy. that is, each summary was evaluated by
two di   erent people. the cost for the full evaluation, including 44 topics, 8 systems,
and 2x redundancy, at $.07 per hit, plus 10% commission for amazon, was $55.

table 7.4 shows average scores for the two evaluations. the data suggest that the
mturk judges are better at evaluating linguistic quality than content or overall
quality.
in particular, the mturk judges appear to have di   culty distinguishing
linguistic quality from content. we will defend these claims with more analysis,
below.

system

a
b
c
d
e
f
g
h

o   cial

oq
5.16
4.84
4.50
4.20
3.91
3.64
3.57
3.20

lq
5.64
5.27
4.93
4.09
4.70
6.70
3.43
5.23

mturk

lq
7.27
6.97
6.85
6.59
6.54
7.78
6.33
6.06

oq
7.03
6.78
6.51
6.15
6.19
7.06
5.82
5.75

c
7.27
6.78
6.49
6.50
6.58
6.56
6.28
5.62

table 7.4: comparison of overall quality (oq) and linguistic quality (lq) scores
between the o   cial and mturk evaluations. content (c) is evaluated by mturk workers
as well. note that system f is the lead baseline.

7.3.4 worker variability
the    rst important question to address involves the consistency of the workers. we
cannot compare agreement between the evaluations, but the mturk agreement statis-
tics suggest considerable variability.
in overall quality, the mean score di   erence
between two workers for the same hit is 2.4 (the standard deviation is 2.0). the
mean is 2.2 for linguistic quality (the standard deviation is 1.5).

in addition, the expert judges show more similarity with each other   as if they are
roughly in agreement about what makes a good summary. we compute each judge   s
average score and look at the standard deviation of these averages for the two groups.
the expert standard deviation is 1.0 (ranging from 3.0 to 6.1), whereas the mturk
standard deviation is 2.3 (ranging from 1.0 to 9.5). note that the average number of
hits performed by each mturk worker was just over 5.

finally, we can use regression analysis to show what fraction of the total score
variance is captured by judges, topics, and systems. we    t linear models in r using

57

chapter 7. evaluating summaries

binary indicators for each judge, topic, and system. redundant evaluations in the
mturk set are removed for unbiased comparison with the o   cial set. table 7.5 shows
that the di   erences between the evaluations are quite striking: taking the o   cial
data alone, the topics are the major source of variance, whereas the judges are the
major source of variance in the mturk data. the systems account for only a small
fraction of the variance in the mturk evaluation, which makes system ranking more
di   cult.

eval
o   cial
mturk

judges topics systems

0.28
0.44

0.40
0.13

0.13
0.05

table 7.5: id75 is used to model overall quality scores as a function of
judges, topics, and systems, respectively, for each data set. the r2 values, which give
the fraction of variance explained by each of the six models, are shown.

7.3.5 ranking comparisons
the o   cial evaluation, while lacking redundant judgments, was a balanced experi-
ment. that is, each judge scored every system for a single topic. the same is not true
for the mturk evaluation, and as a result, the average per-system scores shown in ta-
ble 7.4 may be biased. as a result, and because we need to test multiple system-level
di   erences simultaneously, a simple t-test is not quite su   cient. we use tukey   s hon-
estly signi   cant di   erences (hsd), explained in detail by yandell [1997], to assess
statistical signi   cance.

tukey   s hsd test computes signi   cance intervals based on the range of the sam-
ple means rather than individual di   erences, and includes an adjustment to correct
for imbalanced experimental designs. the r implementation takes as input a lin-
ear model, so we model scores using binary indicators for (j)udges, (t)opics, and
(s)ystems (see equation 7.3), and measure signi   cance in the di   erences between
system coe   cients (  k).

score =    +(cid:88)i

  iji +(cid:88)j

  jtj +(cid:88)k

  ksk

(7.3)

table 7.6 shows system rankings for the two evaluations. the most obvious dis-
crepancy between the o   cial and mturk rankings is system f, the baseline. both
expert and mturk judges gave f the highest scores for linguistic quality, a reason-
able result given its construction, whereas the other summaries tend to pull sentences

58

chapter 7. evaluating summaries

eval

o   cial (oq) a b
mturk (oq) f a
tac (lq)
mturk (lq) f a
mturk (c)
a b

ranking
da eb fc gc hd
c
ef gf db hb
f af bf hf cf ea db ge
bf cf df ef hc gc
e
ga hd

c
b

d

c

f

table 7.6:
systems are shown in rank order from highest (left) to lowest (right) for
each scoring metric: overall quality (oq), linguistic quality (lq), and content (c).
the superscripts indicate the rightmost system that is signi   cantly di   erent (at 95%
con   dence) according to tukey   s hsd test.

out of context. but the mturk judges also gave f the highest scores in overall qual-
ity, suggesting that readability is more important to amateur judges than experts, or
at least easier to identify. content appears the most di   cult category for the mturk
judges, as few signi   cant score di   erences emerge. even with more redundancy, it
seems unlikely that mturk judges could produce a ranking resembling the o   cial
overall quality ranking using this evaluation framework.

what does this mean for future evaluations? if we want to assess overall summary
quality   that is, balancing content and linguistic quality like expert judges do   we
will need to redesign the task for non-experts. perhaps mturk workers will be better
able to understand pyramid evaluation, which is designed to isolate content. extrinsic
evaluation, where judges use the summary to answer questions derived from the source
documents or the references, as done by callison-burch for evaluation of machine
translation systems [2009], is another possibility.

finally, these results suggest that anyone conducting an evaluation of summariza-
tion systems using non-experts should calibrate their results by asking their judges
to score summaries that have already been evaluated by experts.

7.4 towards extrinsic evaluation
so far, all the evaluation methods discussed in this chapter have been intrinsic. that
is, they are based on the idea of quality without regard to any particular task. while
some might consider an elegant summary an end in of itself, most potential summary
users are more pragmatic, so the value of a summary should be a measure of how
useful it is for performing some task.

towards this end, a few research e   orts have attempted to perform this kind

59

chapter 7. evaluating summaries

of task-based extrinsic evaluation. early results have been somewhat encouraging.
[mckeown et al., 2005] compared the performance of people trying
mckeown et al.
to gather facts about a set of documents. di   erent subsets of people were given
the original documents, headlines only, automatic summaries, and human-generated
abstracts. the study showed good evidence that people equipped with summaries
gave better answers than people without summaries. further, the higher the quality
[dorr et al.,
of the summary, the more satisfying the user experience. dorr et al.
2005] conducted a related study,    nding for example, that a summary allowed users
to complete a task 65% faster than a (single) original document.

while these studies help validate summarization as a useful area of research and
suggest that non-trivial progress has been made, i am not convinced that a shift
towards more extrinsic evaluation is necessary. by nature, extrinsic evaluation is
certainly more practical, but by necessity, less general. since extrinsic metrics have
tended to correlate well with intrinsic metrics, validating academic work on summa-
rization with intrinsic evaluation seems reasonable. occasional extrinsic evaluations
of the best performing systems are a good way to validate progress, and useful for
companies trying to benchmark performance on more speci   c tasks.

60

chapter 8

in summary

8.1 review
the previous chapters have addressed the range of problems facing the designer of an
id54 system.

input documents need to be cleaned and segmented into useful units. toward
this end, i described a set of features that are able to distinguish sentence-ending
periods from non-sentence-ending periods that follow abbreviations. feature weights
are learned from data and can be used for discrimination at very low error rates
(<0.5%) on a variety of corpora.

there are many methods for assigning value to sentences and then assembling
a summary. i described maximum marginal relevance and sumbasic because they
begin to chart a research trajectory that moved away from information retrieval
and towards functional criteria for scoring a summary based on statistics from the
set of input documents. the particular objective function i used, called maximum
coverage, implements the idea that a summary is worth the sum the of the values of
the concepts it covers. even a very rough approximation of the notion of a concept
with a word bigram gives state-of-the-art results.

maximum coverage also has some nice theoretical properties. it lends itself readily
to an integer linear program formulation that allows for fast exact solutions; it is also
submodular, implying that greedy approximations are guaranteed to come reasonably
close to optimal solutions. extending the framework to include syntactic sentence
compression is fairly straightforward.

one way to approach the tricky issue of sentence ordering is to prune sentences
that make ordering di   cult. the primary culprit here is pronoun reference. while
coreference resolution is di   cult, i show that identifying pronouns that are not re-
solved locally (in the current sentence) is much easier. using such a classi   er to prune
20-40% of sentences before selection can even improve id8.

61

chapter 8. in summary

lastly, i addressed a few issues in system evaluation.

i showed that a simpler
version of id8, called id165 recall, works at least as well (in correlation with
manual metrics).
i also showed directly how subjective evaluation can be: non-
expert readers tended not to be able to reproduce expert judgments of content as
they appeared distracted by linguistic quality.

8.2 data
the data used in my thesis work comes from the tac and duc workshops. but
there are a variety of other data sets available that may be worth pursuing, especially
because the tac/duc sets are quite small.
i have found that machine learning,
especially id170, tends to be unstable with such small training and
test sets. for example, feature engineering is di   cult when changing the feature set
has a less signi   cant e   ect than simply adding minimal random noise during training.
fortunately, the ldc recently released a large corpus of new york times articles,
spanning nearly 20 years (1988-2007). about 650,000 of these have accompanying
summaries (mostly abstractive). even after fairly aggressive pruning to remove short
documents, or documents with short abstracts, as well as corrections and editorials,
nearly 20% of these, over 100,000 pairs remain. a few example abstracts are shown
in table 8.1. each document also comes labeled with a variety of topic tags and
identifying keywords. though the corpus consists of only single-document summaries,
its strength is its size, which begs the question: what can be learned from a large
collection of documents and summaries?

the feature values we learned with regression (see table 4.4) can be estimated
from just a few documents; with such a simple model, thousands of training examples
are not really necessary. perhaps we can learn feature weights conditional on the
document type (sports, business, etc.). perhaps we can estimate the id203 of
each input word appearing in the summary (see table 2.1), the    rst step towards
a kind of summary model, akin to the acoustic model in id103 or the
translation model in machine translation.

in addition to news articles and their various summaries, there are collections of
research papers and abstracts, including the pubmed database and the acl anthol-
ogy. legal documents often include hand-written summaries. and, all sorts of movie,
music, book, and product reviews are often summarized by a star rating, perhaps a
more appealing unit of prediction than an actual summary.

62

chapter 8. in summary

rising costs put m.t.a.
projects at risk of delay

metropolitan transportation authority faces
rising costs that could imperil or delay sub-
way renovations, maintenance of signal systems
and purchase of new york city buses and sub-
way cars; is halfway through    ve-year, $21 bil-
lion program to expand and improve transit
system one estimate has program $1.4 billion
over budget; major problem has been rapid in-
crease in new york city construction costs, as
result of increasing materials costs and large
number of new projects, which gives contrac-
tors leverage to charge more; some transit ex-
perts are concerned that smaller repair and
maintenance projects may be sacri   ced in or-
der to go ahead with high-pro   le projects; au-
thority now faces two-pronged problem of its
long-term spending plan and its day-to-day op-
erating budget, which is expected to have large
operating de   cits beginning in 2008.

stigma aside, wall st. finds a

lot to like about tobacco

investors react exuberantly to news that altria
group, company once known as philip morris,
will spin o    its kraft foods division and be-
come primarily tobacco company; kraft foods
has stagnated in rapidly changing marketplace;
cigarettes have advantages over other consumer
products: are addictive, inexpensive to make
and there is global market for them; future
prospects for cigarette makers are very attrac-
tive in developing countries, where smoking has
not declined; recent court decisions favorable
to altria have caused its stock to soar; some
health advocates say that dwindling social ac-
ceptability of cigarettes will hurt long-term
prospects of companies like altria; analysts
question future of independent kraft foods,
which faces formidable challenges; lawyer has
threatened to    le injunction to stop divestiture
but several analysts dismiss chances of injunc-
tion   s success; charts of altria group   s stock
prices over decade.

figure 8.1:
sample abstracts (with titles) from the nyt corpus. these have been
cleaned up somewhat, and selected from a subset of the data with very short abstracts
and documents removed.

8.3 methods
maximum coverage appears to be a good general objective for summarization. that
is, if we only had a reasonable representation of facts and good estimates of their
relative values, this objective would yield meaningful summaries.

i have found that limiting the representation of facts to word id165s makes
the estimation of their values quite di   cult.
it is hard to beat a frequency-based
approach, even with fairly advanced techniques. this is because id165s provide only
noisy fragments of facts: "he wrote" or "decided not", for example, are faint echoes of
actual facts in the documents and their repetition across documents vaguely suggest
the importance of those underlying facts. frequency-based valuation also struggles
in cases like "george w. bush   : bigrams "george w.    and "w. bush" both get the
same counts, e   ectively multiplying the value of the full name by two. under these
circumstances, it is hard to expect any learning system to tease apart relative weights
of counts, order, and so on.

so while it is conceivable that better estimation of id165 values could improve

63

chapter 8. in summary

summaries, improving the representation of facts is likely to be more productive. for
example, we know that human-written abstract sentences tend to have more verbs
than the original documents. perhaps verb-based templates, like subject-verb-object,
would be more meaningful. of course, one bene   t of id165s is that counting them
is easy; i used word-id30 to limit diversity. counting the number of occurrences
of a particular subject-verb-object would be more di   cult, probably requiring cross-
document coreference resolution of nouns and pronouns.

thus my suggestion for future research is to focus on the extraction of facts from
documents, ideally across very large document collections like the new york times
corpus. to start, applying a few rules to parser output can give subject-verb-object
triples, and approximate string matching can help with robustness. note that we
cannot expect a summary built from non-id165 facts to outperform one built from
id165 facts as measured by id8 or id165 recall. in fact, when working on the
problems of identifying and matching facts, full summarization is a bit of a distraction.
these problems are useful in of themselves and deserve their own treatment. better
summarization systems will follow eventually.

64

bibliography

[aberdeen et al., 1995] j. aberdeen, j. burger, d. day, l. hirschman, p. robinson,
and m. vilain. mitre: description of the alembic system used for muc-6. in
proceedings of the 6th conference on message understanding, pages 141   155. asso-
ciation for computational linguistics morristown, nj, usa, 1995.

[barzilay et al., 2002] r. barzilay, n. elhadad, and k.r. mckeown. inferring strate-
gies for sentence ordering in multidocument news summarization. journal of arti-
   cial intelligence research, 17:35   55, 2002.

[berg-kirkpatrick et al., 2011] t. berg-kirkpatrick, d. gillick, and d. klein. learn-
ing to jointly extract and compress. association for computation linguistics, 2011.
[blei et al., 2003] d.m. blei, a.y. ng, and m.i. jordan. id44.

the journal of machine learning research, 3:993   1022, 2003.

[bollegala et al., 2010] d. bollegala, n. okazaki, and m. ishizuka. a bottom-up ap-
proach to sentence ordering for id57. information pro-
cessing & management, 46(1):89   109, 2010.

[callison-burch and baltimore, 2009] c. callison-burch and m. baltimore. fast,
cheap, and creative: evaluating translation quality using amazon    s mechan-
ical turk. in proceedings of the 2009 conference on empirical methods in natural
language processing, pages 286   295, 2009.

[carbonell and goldstein, 1998] j. carbonell and j. goldstein. the use of mmr,
diversity-based reranking for reordering documents and producing summaries.
research and development in information retrieval, pages 335   336, 1998.

[celikyilmaz and hakkani-tur, 2010] a. celikyilmaz and d. hakkani-tur. a hybrid
hierarchical model for id57. in proceedings of the 48th
annual meeting of the association for computational linguistics, pages 815   824,
2010.

65

bibliography

[conroy and o   leary, 2001] j.m. conroy and d.p. o   leary. text summarization
via id48. in proceedings of the 24th annual international acm
sigir conference on research and development in information retrieval, page 407,
2001.

[conroy et al., 2006] j.m. conroy, j.d. schlesinger, d.p. o   leary, and j. goldstein.

back to basics: classy 2006. in proceedings of duc, volume 6, 2006.

[dang, 2007] hoa trang dang. overview of duc 2007. in proceedings of duc   07

workshop, pages 1   10, 2007.

[daum   iii and marcu, 2005] h. daum   iii and d. marcu. induction of word and
phrase alignments for automatic document summarization. computational lin-
guistics, 31(4):505   530, 2005.

[dorr et al., 2005] b.j. dorr, c. monz, r. schwartz, and d. zajic. a methodology for
extrinsic evaluation of text summarization: does id8 correlate? intrinsic and
extrinsic evaluation measures for machine translation and/or summarization,
page 1, 2005.

[edmundson, 1964] hp edmundson. problems in automatic abstracting. communi-

cations of the acm, 7(4):263, 1964.

[erkan and radev, 2004] g. erkan and d.r. radev. lexrank: graph-based lexi-
cal centrality as salience in text summarization. journal of arti   cial intelligence
research, 22(2004):457   479, 2004.

[favre et al., ] benoit favre, dilek hakkani-t  r, and sebastien cuendet. icsiboost.

http://code.google.come/p/icsiboost.

[filatova and hatzivassiloglou, 2004] e. filatova and v. hatzivassiloglou. event-
in proceedings of acl workshop on summa-

based extractive summarization.
rization, volume 111, 2004.

[gallo et al., 1980] g. gallo, pl hammer, and b. simeone. quadratic knapsack

problems. mathematical programming study, 12:132   149, 1980.

[genest et al., 2010] p.e. genest, g. lapalme, and m. yous   -monod. hextac: the
creation of a manual extractive run. in proceedings of the second text analysis
conference, gaithersburg, maryland, usa: national institute of standards and
technology, 2010.

66

bibliography

[gillick and favre, 2009] d. gillick and b. favre. a scalable global model for sum-
marization. in proceedings of naacl workshop on integer id135
for natural language processing, 2009.

[gillick and liu, 2010] d. gillick and y. liu. non-expert evaluation of summariza-
tion systems is risky. in proceedings of the naacl hlt 2010 workshop on cre-
ating speech and language data with amazon   s mechanical turk, pages 148   151.
association for computational linguistics, 2010.

[gillick et al., 2010] d. gillick, b. favre, d. hakkani-tur, b. bohnet, y. liu, and
s. xie. the icsi/utd summarization system at tac 2009.
in proceedings of the
second text analysis conference, gaithersburg, maryland, usa: national institute
of standards and technology, 2010.

[gillick, 2009] d. gillick. sentence boundary detection and the problem with the

u.s. in proceedings of naacl: short papers, 2009.

[goldstein et al., 1999] j. goldstein, m. kantrowitz, v. mittal, and j. carbonell.
summarizing text documents: sentence selection and id74. in pro-
ceedings of the 22nd annual international acm sigir conference on research and
development in information retrieval, pages 121   128. acm, 1999.

[goldstein et al., 2000] jade goldstein, vibhu mittal, jaime carbonell, and mark
kantrowitz. id57 by sentence extraction. proceedings
of the anlp/naacl workshop on id54, pages 40   48, 2000.
[gong and liu, 2001] y. gong and x. liu. generic text summarization using rel-
evance measure and latent semantic analysis. in proceedings of the 24th annual
international acm sigir conference on research and development in informa-
tion retrieval, pages 19   25. acm new york, ny, usa, 2001.

[gra    et al., 2007] d. gra   , j. kong, k. chen, and k. maeda. english gigaword

third edition. linguistic data consortium, 2007.

[haghighi and klein, 2010] a. haghighi and d. klein. coreference resolution in a
in human language technologies: the 2010
modular, entity-centered model.
annual conference of the north american chapter of the association for com-
putational linguistics, pages 385   393. association for computational linguistics,
2010.

[haghighi and vanderwende, 2009] a. haghighi and l. vanderwende. exploring
content models for id57.
in proceedings of human
language technologies: the 2009 annual conference of the north american chap-
ter of the association for computational linguistics, pages 362   370, 2009.

67

bibliography

[hennig and labor, 2009] l. hennig and dai labor. topic-based multi-document
summarization with probabilistic latent semantic analysis. in international con-
ference on recent advances in natural language processing (ranlp), 2009.

[hochbaum, 1996] d.s. hochbaum. approximating covering and packing problems:
set cover, vertex cover, independent set, and related problems. pws publishing
co. boston, ma, usa, pages 94   143, 1996.

[hofmann, 1999] t. hofmann. probabilistic id45.

in proceed-
ings of the 22nd annual international acm sigir conference on research and
development in information retrieval, pages 50   57, 1999.

[hovy et al., 2006] e. hovy, m. marcus, m. palmer, l. ramshaw, and r. weischedel.
ontonotes: the 90% solution. in proceedings of the human language technology
conference of the naacl, companion volume: short papers on xx, pages 57   60.
association for computational linguistics, 2006.

[jing and mckeown, 1999] h. jing and k.r. mckeown. the decomposition of
human-written summary sentences.
in proceedings of the 22nd annual interna-
tional acm sigir conference on research and development in information re-
trieval, pages 129   136. acm, 1999.

[kiss and strunk, 2006] t. kiss and j. strunk. unsupervised multilingual sentence

boundary detection. computational linguistics, 32(4):485   525, 2006.

[knight and marcu, 2000] k. knight and d. marcu. statistics-based summarization-
step one: sentence compression. in proceedings of the national conference on
arti   cial intelligence, pages 703   710. menlo park, ca; cambridge, ma; london;
aaai press; mit press; 1999, 2000.

[knight, 1999] k. knight. a statistical mt tutorial workbook. in the jhu summer

workshop, 1999.

[krause and guestrin, 2005] a. krause and c. guestrin. a note on the budgeted
maximization of submodular functions. technical report cmu-cald-05, 103,
carnegie mellon university, 2005.

[kupiec et al., 1995] j. kupiec, j. pedersen, and f. chen. a trainable document sum-
marizer. in proceedings of the 18th annual international acm sigir conference
on research and development in information retrieval, pages 68   73, 1995.

[li et al., 2007] s. li, y. ouyang, w. wang, and b. sun. multi-document summa-

rization using support vector regression. in proceedings of duc, 2007.

68

bibliography

[lin and bilmes, 2010] h. lin and j. bilmes. id57 via
budgeted maximization of submodular functions. in human language technolo-
gies: the 2010 annual conference of the north american chapter of the associa-
tion for computational linguistics, pages 912   920. association for computational
linguistics, 2010.

[lin, 1965] s. lin. computer solutions of the traveling salesman problem. bell system

technical journal, 44(10):2245   2269, 1965.

[lin, 2003] c.y. lin. improving summarization performance by sentence compres-
sion: a pilot study.
in proceedings of the sixth international workshop on in-
formation retrieval with asian languages-volume 11, pages 1   8. association for
computational linguistics, 2003.

[lin, 2004] chin-yew lin. id8: a package for automatic evaluation of sum-
in proceedings of the workshop on text summarization branches out,

maries.
pages 25   26, 2004.

[loper and bird, 2002] e. loper and s. bird. nltk: the natural language toolkit. in
proceedings of the acl-02 workshop on e   ective tools and methodologies for teach-
ing natural language processing and computational linguistics-volume 1, page 70.
association for computational linguistics, 2002.

[lov  sz, 1983] l lov  sz. submodular functions and convexity. mathematical pro-

gramming: the state of the art, pages 235   257, 1983.

[luhn, 1958] h.p. luhn. the automatic creation of literature abstracts. ibm journal

of research and development, 2(2):159   165, 1958.

[madnani et al., 2007] n. madnani, d. zajic, b. dorr, n.f. ayan, and j. lin. multi-
ple alternative sentence compressions for automatic text summarization. in pro-
ceedings of the 2007 document understanding conference (duc-2007) at nlt/-
naacl, 2007.

[marcus et al., 1993] m.p. marcus, m.a. marcinkiewicz, and b. santorini. building a
large annotated corpus of english: the id32. computational linguistics,
19(2):330, 1993.

[martins and smith, 2009] a.f.t. martins and n.a. smith. summarization with a
joint model for sentence extraction and compression. in proceedings of the work-
shop on integer id135 for natural langauge processing, pages 1   9.
association for computational linguistics, 2009.

69

bibliography

[mcdonald, 2006] r. mcdonald. discriminative sentence compression with soft syn-

tactic constraints. in proceedings of the 11th eacl, pages 297   304, 2006.

[mcdonald, 2007] r. mcdonald. a study of global id136 algorithms in multi-

document summarization. lecture notes in computer science, 4425:557, 2007.

[mckeown et al., 2005] k. mckeown, r.j. passonneau, d.k. elson, a. nenkova, and
j. hirschberg. do summaries help? a task-based evaluation of multi-document sum-
marization. in proceedings of the 28th annual acm sigir conference on research
and development in information retrieval, salvador, brazil, page 6. citeseer, 2005.
[mihalcea and tarau, 2004] r. mihalcea and p. tarau. textrank   bringing order into

texts. in proceedings of emnlp, pages 404   411, 2004.

[nemhauser and wolsey, 1988] g.l. nemhauser and l.a. wolsey. integer and com-

binatorial optimization, volume 18. wiley new york, 1988.

[nemhauser et al., 1978] g.l. nemhauser, l.a. wolsey, and m.l. fisher. an analysis
of approximations for maximizing submodular set functions      i. mathematical
programming, 14(1):265   294, 1978.

[nenkova and passonneau, 2004] ani nenkova and rebecca passonneau. evaluating
content selection in summarization: the pyramid method. in proceedings of hlt-
naacl, 2004.

[nenkova and vanderwende, 2005] a. nenkova and l. vanderwende. the impact
of frequency on summarization. technical report msr-tr-2005-101, microsoft
research, redmond, washington, 2005.

[nenkova et al., 2006] ani nenkova, lucy vanderwende, and kathleen mckeown. a
compositional context sensitive multi-document summarizer: exploring the factors
that in   uence summarization. in proceedings of sigir, 2006.

[nenkova, 2008] a. nenkova. entity-driven rewrite for multidocument summariza-

tion. in proceedings of ijcnlp, 2008.

[page et al., 1998] l. page, s. brin, r. motwani, and t. winograd. the id95
citation ranking: bringing order to the web. technical report, stanford digital
library technologies project, 1998.

[palmer and hearst, 1997] d.d. palmer and m.a. hearst. adaptive multilingual sen-
tence boundary disambiguation. computational linguistics, 23(2):241   267, 1997.

70

bibliography

[papineni et al., 2002] k. papineni, s. roukos, t. ward, and w.j. zhu. id7: a
method for automatic evaluation of machine translation.
in proceedings of the
40th annual meeting on association for computational linguistics, pages 311   318.
association for computational linguistics, 2002.

[petrov and klein, 2007] slav petrov and dan klein. learning and id136 for hi-

erarchically split pid18s. in aaai 2007 (nectar track), 2007.

[rahman and ng, 2011] a. rahman and v. ng. narrowing the modeling gap: a
cluster-ranking approach to coreference resolution. journal of arti   cial intelli-
gence research, 40:469   521, 2011.

[reynar and ratnaparkhi, 1997] j.c. reynar and a. ratnaparkhi. a maximum en-
in proceedings of the fifth

tropy approach to identifying sentence boundaries.
conference on applied natural language processing, pages 16   19, 1997.

[schapire, 1999] r.e. schapire. a brief introduction to boosting. in international

joint conference on arti   cial intelligence, volume 16, pages 1401   1406, 1999.

[schilder and kondadadi, 2008] frank schilder and ravikumar kondadadi. fastsum:
fast and accurate query-based id57. in proceedings of
acl-08: hlt, short papers, 2008.

[shen et al., 2007] d. shen, j.t. sun, h. li, q. yang, and z. chen. document sum-
marization using conditional random    elds. in the proceedings of ijcai, volume 7,
pages 2862   2867, 2007.

[snow et al., 2008] r. snow, b. o   connor, d. jurafsky, and a.y. ng. cheap and
fast, but is it good? evaluating non-expert annotations for natural language tasks.
in proceedings of the conference on empirical methods in natural language pro-
cessing (emnlp 2008), 2008.

[steyvers and gri   ths, 2007] m. steyvers and t. gri   ths. probabilistic topic mod-

els. handbook of latent semantic analysis, page 427, 2007.

[turner and charniak, 2005] j. turner and e. charniak. supervised and unsuper-

vised learning for sentence compression. ann arbor, 100, 2005.

[yandell, 1997] b.s. yandell. practical data analysis for designed experiments. chap-

man & hall/crc, 1997.

[yih et al., 2007] w. yih, j. goodman, l. vanderwende, and h. suzuki. multi-
document summarization by maximizing informative content-words. in interna-
tional joint conference on arti   cial intelligence (ijcai 2007), 2007.

71

