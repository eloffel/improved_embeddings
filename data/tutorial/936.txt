c.c. aggarwal and c.x. zhai (eds.), mining text data, doi 10.1007/978-1-4614-3223-4_3,

 

 

chapter3asurveyoftextsummarizationtechniquesaninenkovauniversityofpennsylvanianenkova@seas.upenn.edukathleenmckeowncolumbiauniversitykathy@cs.columbia.eduabstractnumerousapproachesforidentifyingimportantcontentforautomatictextsummarizationhavebeendevelopedtodate.topicrepresentationapproaches   rstderiveanintermediaterepresentationofthetextthatcapturesthetopicsdiscussedintheinput.basedontheserepresenta-tionsoftopics,sentencesintheinputdocumentarescoredforimpor-tance.incontrast,inindicatorrepresentationapproaches,thetextisrepresentedbyadiversesetofpossibleindicatorsofimportancewhichdonotaimatdiscoveringtopicality.theseindicatorsarecombined,veryoftenusingmachinelearningtechniques,toscoretheimportanceofeachsentence.finally,asummaryisproducedbyselectingsentencesinagreedyapproach,choosingthesentencesthatwillgointhesummaryonebyone,orgloballyoptimizingtheselection,choosingthebestsetofsentencestoformasummary.inthischapterwegiveabroadoverviewofexistingapproachesbasedonthesedistinctions,withparticularat-tentiononhowrepresentation,sentencescoringorsummaryselectionstrategiesaltertheoverallperformanceofthesummarizer.wealsopointoutsomeofthepeculiaritiesofthetaskofsummarizationwhichhaveposedchallengestomachinelearningapproachesfortheproblem,andsomeofthesuggestedsolutions1.1portionsofthischapterhavealreadyappearedinourmoredetailedoverviewofsumma-rizationresearch[67].thelargermanuscriptincludessectionsongenerationtechniquesfor   springer science+business media, llc 2012 4344miningtextdatakeywords:extractivetextsummarization,topicrepresentation,machinelearningforsummarization1.howdoextractivesummarizerswork?summarizationsystemsneedtoproduceaconciseand   uentsummaryconveyingthekeyinformationintheinput.inthischapterweconstrainourdiscussiontoextractivesummarizationsystemsforshort,paragraph-lengthsummariesandexplainhowthesesystemsperformsummariza-tion.thesesummarizersidentifythemostimportantsentencesintheinput,whichcanbeeitherasingledocumentoraclusterofrelateddocuments,andstringthemtogethertoformasummary.thedecisionaboutwhatcontentisimportantisdrivenprimarilybytheinputtothesummarizer.thechoicetofocusonextractivetechniquesleavesoutthelargebodyoftext-to-textgenerationapproachesdevelopedforabstractivesumma-rization,butallowsustofocusonsomeofthemostdominantapproacheswhichareeasilyadaptedtotakeusers   informationneedintoaccountandworkforbothsingle-andmulti-documentinputs.moreover,byex-aminingthestagesintheoperationofextractivesummarizersweareabletopointoutcommonalitiesanddi   erencesinsummarizationapproacheswhichrelatetocriticalcomponentsofasystemandcouldexplaintheadvantagesofcertaintechniquesoverothers.inordertobetterunderstandtheoperationofsummarizationsystemsandtoemphasizethedesignchoicessystemdevelopersneedtomake,wedistinguishthreerelativelyindependenttasksperformedbyvirtuallyallsummarizers:creatinganintermediaterepresentationoftheinputwhichcapturesonlythekeyaspectsofthetext,scoringsentencesbasedonthatrepresentationandselectingasummaryconsistingofseveralsentences.intermediaterepresentationeventhesimplestsystemsderivesomeintermediaterepresentationofthetexttheyhavetosummarizeandidentifyimportantcontentbasedonthisrepresentation.topicrep-resentationapproachesconvertthetexttoanintermediaterepresenta-tioninterpretedasthetopic(s)discussedinthetext.someofthemostpopularsummarizationmethodsrelyontopicrepresentationsandthisclassofapproachesexhibitsanimpressivevariationinsophisticationandrepresentationpower.theyincludefrequency,tf.idfandtopicwordapproachesinwhichthetopicrepresentationconsistsofasimpletablesummarization,evaluationissuesandgenrespeci   csummarizationwhichwedonotaddressinthischapter.http://dx.doi.org/10.1561/1500000015asurveyoftextsummarizationtechniques45ofwordsandtheircorrespondingweights,withmorehighlyweightedwordsbeingmoreindicativeofthetopic;lexicalchainapproachesinwhichathesaurussuchasid138isusedto   ndtopicsorconceptsofsemanticallyrelatedwordsandthengiveweighttotheconcepts;latentsemanticanalysisinwhichpatternsofwordco-occurrenceareidenti   edandroughlyconstruedastopics,aswellasweightsforeachpattern;fullblownbayesiantopicmodelsinwhichtheinputisrepresentedasamixtureoftopicsandeachtopicisgivenasatableofwordprob-abilities(weights)forthattopic.indicatorrepresentationapproachesrepresenteachsentenceintheinputasalistofindicatorsofimportancesuchassentencelength,locationinthedocument,presenceofcertainphrases,etc.ingraphmodels,suchaslexrank,theentiredocumentisrepresentedasanetworkofinter-relatedsentences.scoresentencesonceanintermediaterepresentationhasbeende-rived,eachsentenceisassignedascorewhichindicatesitsimportance.fortopicrepresentationapproaches,thescoreiscommonlyrelatedtohowwellasentenceexpressessomeofthemostimportanttopicsinthedocumentortowhatextentitcombinesinformationaboutdi   erenttop-ics.forthemajorityofindicatorrepresentationmethods,theweightofeachsentenceisdeterminedbycombiningtheevidencefromthedi   er-entindicators,mostcommonlybyusingmachinelearningtechniquestodiscoverindicatorweights.inlexrank,theweightofeachsentenceisderivedbyapplyingstochastictechniquestothegraphrepresentationofthetext.selectsummarysentencesfinally,thesummarizerhastoselectthebestcombinationofimportantsentencestoformaparagraphlengthsummary.inthebestnapproaches,thetopnmostimportantsentenceswhichcombinedhavethedesiredsummarylengthareselectedtoformthesummary.inmaximalmarginalrelevanceapproaches,sentencesareselectedinaniterativegreedyprocedure.ateachstepoftheprocedurethesentenceimportancescoreisrecomputedasalinearcombinationbetweentheoriginalimportanceweightofthesentenceanditssimilaritywithalreadychosensentences.sentencesthataresimilartoalreadychosensentencesaredispreferred.inglobalselectionapproaches,theoptimalcollectionofsentencesisselectedsubjecttoconstraintsthattrytomaximizeoverallimportance,minimizeredundancy,and,forsomeapproaches,maximizecoherence.thereareveryfewinherentdependenciesbetweenthethreeprocess-ingstepsdescribedaboveandasummarizercanincorporateanycom-binationofspeci   cchoicesonhowtoperformthesteps.changesinthewayaspeci   cstepisperformedcanmarkedlychangetheperformance46miningtextdataofthesummarizer,andwewilldiscusssomeoftheknowndi   erencesasweintroducethetraditionalmethods.inrankingtheimportanceofsentencesforsummaries,otherfactorsalsocomeintoplay.ifwehaveinformationaboutthecontextinwhichthesummaryisgenerated,thiscanhelpindeterminingimportance.contextcantaketheformofinformationaboutuserneeds,oftenpre-sentedthroughaquery.contextcanincludetheenvironmentinwhichaninputdocumentissituated,suchasthelinkswhichpointtoawebpage.anotherfactorwhicha   ectssentencerankingisthegenreofadocument.whethertheinputdocumentisanewsarticle,anemailthread,awebpageorajournalarticlein   uencesthestrategiesusedtoselectsentences.webeginwithadiscussionoftopicrepresentationapproachesinsec-tion2.intheseapproachestheindependencebetweenthemethodsforderivingtheintermediaterepresentationandthoseforscoringsentencesismostclearandweemphasizetherangeofchoicesforeachaswedis-cussindividualapproaches.insection3wediscussapproachesthatfocusattentiononthecontextualinformationnecessaryfordeterminingsentenceimportanceratherthanthetopicrepresentationitself.wefol-lowwithapresentationofindicatorrepresentationapproachesinsection4.wethendiscussapproachestoselectingthesentencesofasummaryinsection5beforeconcluding.2.topicrepresentationapproachestopicrepresentationapproachesvarytremendouslyinsophisticationandencompassafamilyofmethodsforsummarization.herewepresentsomeofthemostwidelyappliedtopicrepresentationapproaches,aswellasthosethathavebeengainingpopularitybecauseoftheirrecentsuccesses.2.1topicwordsinremarkablyearlyworkontextsummarization[53],luhnproposedtheuseoffrequencythresholdstoidentifydescriptivewordsinadocu-menttobesummarized,asimplerepresentationofthedocument   stopic.thedescriptivewordsinhisapproachexcludethemostfrequentwordsinthedocument,whicharelikelytobedeterminers,prepositions,ordomainspeci   cwords,aswellasthoseoccurringonlyafewtimesamodernstatisticalversionofluhn   sideaappliesthelog-likelihoodra-tiotest[22]foridenti   cationofwordsthatarehighlydescriptiveoftheinput.suchwordshavebeentraditionallycalled   topicsignatures   inthesummarizationliterature[46].theuseoftopicsignatureswordsasurveyoftextsummarizationtechniques47asrepresentationoftheinputhasledtohighperformanceinselectingimportantcontentformulti-documentsummarizationofnews[15,38].topicsignaturesarewordsthatoccuroftenintheinputbutarerareinothertexts,sotheircomputationrequirescountsfromalargecol-lectionofdocumentsinadditiontotheinputforsummarization.oneofthekeystrengthsofthelog-likelihoodratiotestapproachisthatitprovidesawayofsettingathresholdtodivideallwordsintheinputintoeitherdescriptiveornot.thedecisionismadebasedonatestforsta-tisticalsigni   cance,tolargeextentremovingtheneedforthearbitrarythresholdsintheoriginalapproach.informationaboutthefrequencyofoccurrenceofwordsinalargebackgroundcorpusisnecessarytocomputethestatisticonthebasisofwhichtopicsignaturewordsaredetermined.thelikelihoodoftheinputiandthebackgroundcorpusiscomputedundertwoassumptions:(h1)thattheid203ofawordintheinputisthesameasinthebackgroundbor(h2)thatthewordhasadi   erent,higherid203,intheinputthaninthebackground.h1:p(w|i)=p(w|b)=p(wisnotdescriptive)h2:p(w|i)=piandp(w|b)=pbandpi>pb(wisdescriptive)thelikelihoodofatextwithrespecttoagivenwordofinterest,w,iscomputedviathebinomialdistributionformula.theinputandthebackgroundcorpusaretreatedasasequenceofwordswi:w1w2...wn.theoccurrenceofeachwordisabernoullitrialwithid203pofsuccess,whichoccurswhenwi=w.theoverallid203ofobservingthewordwappearingktimesinthentrialsisgivenbythebinomialdistributionb(k,n,p)=(cid:12)nk(cid:13)pk(1   p)n   k(3.1)forh1,theid203piscomputedfromtheinputandtheback-groundcollectiontakentogether.forh2,p1iscomputedfromtheinput,p2fromthebackground,andthelikelihoodoftheentiredataisequaltotheproductofthebinomialfortheinputandthatforthebackground.morespeci   cally,thelikelihoodratioisde   nedas  =b(k,n,p)b(ki,ni,pi).b(kb,nb,pb)(3.2)wherethecountswithsubscriptiarecomputedonlyfromtheinputtothesummarizerandthosewithindexbarecomputedovertheback-groundcorpus.thestatisticequalto   2log  hasaknownstatisticaldistribution(  2),whichcanbeusedtodeterminewhichwordsaretopicsignatures.48miningtextdatatopicsignaturewordsarethosethathavealikelihoodstatisticgreaterthanwhatonewouldexpectbychance.theid203ofobtainingagivenvalueofthestatisticpurelybychancecanbelookedupina  2distributiontable;forinstanceavalueof10.83canbeobtainedbychancewithid203of0.001.theimportanceofasentenceiscomputedasthenumberoftopicsignaturesitcontainsorastheproportionoftopicsignaturesinthesentence.bothofthesesentencescoringfunctionsarebasedonthesametopicrepresentation,thescorestheyassigntosentencesmayberatherdi   erent.the   rstapproachislikelytoscorelongersentenceshigher,simplybecausetheycontainmorewords.thesecondapproachfavorsdensityoftopicwords.2.2frequency-drivenapproachestherearetwopotentialmodi   cationsthatnaturallycometomindwhenconsideringthetopicwordsapproach.theweightsofwordsintopicrepresentationsneednotbebinary(either1or0)asinthetopicwordapproaches.inprincipleitwouldevenbebene   cialtobeabletocomparethecontinuousweightsofwordsanddeterminewhichonesaremorerelatedtothetopic.theapproacheswepresentinthissection   wordid203andtf.idf   indeedassignnon-binaryweightsrelatedonthenumberofoccurrencesofawordorconcept.researchhasalreadyshownthatthebinaryweightsgivemorestableindicatorsofsentenceimportancethanwordid203andtf.idf[34].nonethelessweoverviewtheseapproachesbecauseoftheirconceptualsimplicityandreasonableperformance.wealsodescribethelexicalchainsapproachtodeterminingsentenceimportance.incontrasttomostotherapproaches,itmakesuseofid138,alexicaldatabasewhichrecordssemanticrela-tionsbetweenwords.basedontheinformationderivedfromid138,lexicalchainapproachesareabletotracktheprominence,indicatedbyfrequency,ofdi   erenttopicsdiscussedintheinput.wordid203isthesimplestformofusingfrequencyintheinputasanindicatorofimportance2.theid203ofawordw,p(w)iscomputedfromtheinput,whichcanbeaclusterofrelateddocumentsorasingledocument.itiscalculatedasthenumberofoccurrencesofaword,c(w)dividedbythenumberofallwordsintheinput,n:2rawfrequencywouldbeevensimpler,butthismeasureistoostronglyin   uencedbydoc-umentlength.awordappearingtwiceina10worddocumentmaybeimportant,butnotnecessarilysoina1000worddocument.computingwordid203makesanadjustmentfordocumentlength.asurveyoftextsummarizationtechniques49p(w)=c(w)n(3.3)sumbasicisonesystemdevelopedtooperationalizetheideaofusingfrequencyforsentenceselection.itreliesonlyonwordid203tocalculateimportance[94].foreachsentencesjintheinputitassignsaweightequaltotheaverageid203p(wi)ofthecontentwordsinthesentence3,estimatedfromtheinputforsummarization:weight(sj)=(cid:4)wi   sjp(wi)|{wi|wi   sj}|(3.4)then,inagreedyfashion,sumbasicpicksthebestscoringsentencethatcontainsthewordthatcurrentlyhasthehighestid203.thisselectionstrategyassumesthatateachpointwhenasentenceisselected,asingleword   thatwithhighestid203   representsthemostim-portanttopicinthedocumentandthegoalistoselectthebestsentencethatcoversthisword.afterthebestsentenceisselected,theid203ofeachwordthatappearsinthechosensentenceisadjusted.itissettoasmallervalue,equaltothesquareoftheid203ofthewordatthebeginningofthecurrentselectionstep,tore   ectthefactthattheid203ofawordoccurringtwiceinasummaryislowerthantheid203ofthewordoccurringonlyonce.thisselectionloopisrepeateduntilthedesiredsummarylengthisachieved.withcontinuousweights,thereareevengreaternumberofpossibil-itiesforde   ningthesentencescoringfunctioncomparedtothetopicwordsmethod:theweightscanbesummed,multiplied,averaged,etc.ineachcasethescoringisderivedbythesamerepresentationbutthere-sultingsummarizerperformancecanvaryconsiderablydependingonthechoice[68].thesentenceselectionstrategyofsumbasicisavariationofthemaximalmarginalrelevancestrategy,butanapproachthatopti-mizestheoccurrenceofimportantwordsgloballyovertheentiresum-maryinsteadofgreedyselectionperformbetter[89].wordprobabilitiescanserveasthebasisforincreasinglycomplexviewsofsummarization[50].tf*idfweighting(termfrequency*inversedocumentfrequency)thewordid203approachreliesonastopwordlisttoeliminatetoocommonwordsfromconsideration.decidingwhichwordstoin-cludeinastoplist,however,isnotatrivialtaskandassigningtf*idfweightstowords[79,87]providesabetteralternative.thisweighting3sentencesthathavefewerthan15contentwordsareassignedweightzeroandastopwordlistisusedtoeliminateverycommonwordsfromconsideration.50miningtextdataexploitscountsfromabackgroundcorpus,whichisalargecollectionofdocuments,normallyfromthesamegenreasthedocumentthatistobesummarized;thebackgroundcorpusservesasindicationofhowoftenawordmaybeexpectedtoappearinanarbitrarytext.theonlyadditionalinformationbesidesthetermfrequencyc(w)thatweneedinordertocomputetheweightofawordwwhichappearsc(w)timesintheinputforsummarizationisthenumberofdocuments,d(w),inabackgroundcorpusofddocumentsthatcontaintheword.thisallowsustocomputetheinversedocumentfrequency:tf   idfw=c(w).logdd(w)(3.5)inmanycasesc(w)isdividedbythemaximumnumberofoccurrencesofanywordinthedocument,whichnormalizesfordocumentlength.descriptivetopicwordsarethosethatappearofteninadocument,butarenotverycommoninotherdocuments.wordsthatappearinmostdocumentswillhaveanidfclosetozero.thetf*idfweightsofwordsaregoodindicatorsofimportance,andtheyareeasyandfasttocompute.thesepropertiesexplainwhytf*idfisincorporatedinoneformoranotherinmostcurrentsystems[25,26,28   30,40].centroidsummarization[73],whichhasbecomeapopularbaselinesystem,isalsobuiltontf.idftopicrepresentation.inthisapproach,anempiricallydeterminedthresholdisset,andallwordswithtf.idfbelowthatthresholdareconsideredtohaveaweightofzero.inthiswaythecentroidapproachissimilartothetopicwordapproachbe-causewordswithlowweightaretreatedasnoiseandcompletelyignoredwhencomputingsentenceimportance.italsoresemblesthewordprob-abilityapproachbecauseitkeepsdi   erentialweights(tf.idf)forallwordabovethethreshold.thesentencescoringfunctioninthecentroidmethodisthesumofweightsofthewordsinit.lexicalchains[3,86,31]andsomerelatedapproachesrepresenttopicsthatarediscussedthroughoutatextbyexploitingrelationsbetweenwords.theycapturesemanticsimilaritybetweennounstodeterminetheimportanceofsentences.thelexicalchainsapproachcapturestheintuitionthattopicsareexpressedusingnotasinglewordbutinsteaddi   erentrelatedwords.forexample,theoccurrenceofthewords   car   ,   wheel   ,   seat   ,   passenger   indicatesacleartopic,evenifeachofthewordsisnotbyitselfveryfrequent.theapproachheavilyreliesonid138[63],amanuallycompiledthesauruswhichliststhedi   erentsensesofeachword,aswellaswordrelationshipssuchassynonymy,antonymy,part-wholeandgeneral-speci   c.asurveyoftextsummarizationtechniques51alargepartofbarzilayandelhadad   soriginalworkonapplyinglexicalchainsforsummarization[3]isonnewmethodsforconstruct-inggoodlexicalchains,withemphasisonwordsensedisambiguationofwordswithmultiplemeanings(i.e.theword   bank   canmeana   nancialinstitutionorthelandnearariverorlake).theydevelopanalgorithmthatimprovesonpreviousworkbywaitingtodisambiguatepolysemouswordsuntilallpossiblechainsforatexthavebeenconstructed;wordsensesaredisambiguatedbyselectingtheinterpretationswiththemostconnectionsinthetext.laterresearchfurtherimprovedboththerun-timeofthealgorithmsforbuildingoflexicalchains,andtheaccuracyofwordsensedisambiguation[86,31].barzilayandelhadadclaimthatthemostprevalentdiscoursetopicwillplayanimportantroleinthesummaryandarguethatlexicalchainsprovideabetterindicationofdiscoursetopicthandoeswordfrequencysimplybecausedi   erentwordsmayrefertothesametopic.theyde   nethestrengthofalexicalchainbyitslength,whichisequaltothenumberofwordsfoundtobemembersofthesamechain,anditshomogeneity,wherehomogeneitycapturesthenumberofdistinctlexicalitemsinthechaindividedbyitslength.theybuildthesummarybyextractingonesentenceforeachhighlyscoredchain,choosingthe   rstsentenceinthedocumentcontainingarepresentativewordforthechain.thisstrategyforsummaryselection   onesentenceperimportanttopic   iseasytoimplementbutpossiblytoorestrictive.thequestionthatstandsout,andwhichbarzilayandelhadadraisebutdonotad-dress,isthatmaybeforsometopicsmorethanonesentenceshouldbeincludedinthesummary.othersentencescoringtechniquesforlexicalchainsummarizationhavenotbeenexplored,i.e.sentencesthatincludeseveralofthehighlyscoringchainsmaybeevenmoreinformativeabouttheconnectionbetweenthediscussedtopics.inlaterwork,researcherschosetoavoidtheproblemofwordsensedis-ambiguationaltogetherbutstillusedid138totrackthefrequencyofallmembersofaconceptset[82,102].evenwithoutsensedisambigua-tion,theseapproacheswereabletoderiveconceptslike{war,campaign,warfare,e   ort,cause,operation,con   ict},{concern,carrier,worry,fear,scare}or{home,base,source,support,backing}.eachoftheindividualwordsintheconceptcouldappearonlyonceortwiceintheinput,buttheconceptitselfappearedinthedocumentfrequently.theheavyrelianceonid138isclearlyabottleneckfortheap-proachesabove,becausesuccessisconstrainedbythecoverageofword-net.becauseofthis,robustmethodssuchaslatentsemanticanalysisthatdonotuseaspeci   cstatichand-craftedresourcehavemuchappeal.52miningtextdata2.3latentsemanticanalysislatentsemanticanalysis(lsa)[19]isarobustunsupervisedtech-niqueforderivinganimplicitrepresentationoftextsemanticsbasedonobservedco-occurrenceofwords.gongandliu[33]proposedtheuseoflsaforsingleandmulti-documentgenericsummarizationofnews,asawayofidentifyingimportanttopicsindocumentswithouttheuseoflexicalresourcessuchasid138.buildingthetopicrepresentationstartsby   llinginanbymmatrixa:eachrowcorrespondstoawordfromtheinput(nwords)andeachcolumncorrespondstoasentenceintheinput(msentences).entryaijofthematrixcorrespondstotheweightofwordiinsentencej.ifthesentencedoesnotcontaintheword,theweightiszero,otherwisetheweightisequaltothetf*idfweightoftheword.standardtech-niquesforsingularvaluedecomposition(svd)fromlinearalgebraareappliedtothematrixa,torepresentitastheproductofthreematrices:a=u  vt.everymatrixhasarepresentationofthiskindandmanystandardlibrariesprovideabuilt-inimplementationofthedecomposi-tion.matrixuisanbymmatrixofrealnumbers.eachcolumncanbeinterpretedasatopic,i.e.aspeci   ccombinationofwordsfromtheinputwiththeweightofeachwordinthetopicgivenbytherealnumber.matrix  isdiagonalmbymmatrix.thesingleentryinrowiofthematrixcorrespondstotheweightofthe   topic   ,whichistheithcolumnofu.topicswithlowweightcanbeignored,bydeletingthelastkrowsofu,thelastkrowsandcolumnsof  andthelastkrowsofvt.thisprocedureiscalleddimensionalityreduction.itcorrespondstothethresholdsemployedinthecentroidandtopicwordsapproaches,andtopicswithlowweightaretreatedasnoise.matrixvtisanewrepresentationofthesentences,onesentenceperrow,eachofwhichisexpressednotintermsofwordsthatoccurinthesentencebutratherintermsofthetopiid19iveninu.thematrixd=  vtcombinesthetopicweightsandthesentencerepresentationtoindicatetowhatextentthesentenceconveysthetopic,withdijindicatingtheweightfortopiciinsentencej.theoriginalproposalofgongandliuwastoselectonesentenceforeachofthemostimportanttopics.theyperformdimensionalityreduc-tion,retainingonlyasmanytopicsasthenumberofsentencestheywanttoincludeinthesummary.thesentencewiththehighestweightforeachoftheretainedtopicsisselectedtoformthesummary.thisstrategysuf-fersfromthesamedrawbackasthelexicalchainsapproachbecausemorethanonesentencemayberequiredtoconveyallinformationpertinentasurveyoftextsummarizationtechniques53tothattopic.laterresearchershaveproposedalternativeprocedureswhichhaveledtoimprovedperformanceofthesummarizerincontentselection.oneimprovementistousetheweightofeachtopicinordertodeterminetherelativeproportionofthesummarythatshouldcoverthetopic,thusallowingforavariablenumberofsentencespertopic.anotherimprovementwastonoticethatoftensentencesthatdiscussseveraloftheimportanttopicsaregoodcandidatesforsummaries[88].toidentifysuchsentences,theweightofsentencesiissettoequalweight(si)=(cid:14)(cid:15)(cid:15)(cid:16)m(cid:9)j=1d2i,j(3.6)furthervariationsofthelsaapproachhavealsobeenexplored[72,35].thesystemsthatrelyonlsabestexemplifythesigni   canceoftheprocedureforsentencescoring.inthemanyvariantsofthealgorithm,thetopicrepresentationremainsthesamewhilethewaysentencesarescoredandchosenvaries,directlyin   uencingtheperformanceofthesummarizerwhenselectingimportantcontent.2.4bayesiantopicmodelsbayesianmodelsarethemostsophisticatedapproachfortopicrepre-sentationproposedforsummarizationwhichhasbeensteadilygainingpopularity[18,36,97,11].theoriginalbayesianmodelformulti-documentsummarization[18,36],derivesseveraldistinctprobabilisticdistributionsofwordsthatap-pearintheinput.onedistributionisforgeneralenglish(g),onefortheentireclustertobesummarized(c)andoneforeachindividualdoc-umentiinthatcluster(di).eachofg,canddconsistoftablesofwordsandtheirprobabilities,orweights,muchlikethewordid203approach,buttheweightsareverydi   erenting,candd:awordwithhighid203ingeneralenglishislikelytohave(almost)zeroweightintheclustertablec.thetables(id203distributions)arederivedasapartofahierarchicaltopicmodel[8].itisanunsupervisedmodelandtheonlydataitrequiresareseveralmulti-documentclusters;thegeneralenglishweightsre   ectoccurrenceofwordsacrossmostoftheinputclusters.thetopicmodelrepresentationsarequiteappealingbecausetheycap-tureinformationthatislostinmostoftheotherapproaches.they,forexample,haveanexplicitrepresentationoftheindividualdocumentsthatmakeuptheclusterthatistobesummarized,whileitiscustomaryinotherapproachestotreattheinputtoamulti-documentsummarizer54miningtextdataasonelongtext,withoutdistinguishingdocumentboundaries.thedetailedrepresentationwouldlikelyenablethedevelopmentofbettersummarizerswhichconveysthesimilaritiesanddi   erencesamongthedi   erentdocumentsthatmakeuptheinputformulti-documentsum-marization[55,24,54].itisalso   exibleinthemannerinwhichitderivesthegeneralenglishweightsofwords,withouttheneedforapre-determinedstopwordlist,oridfvaluesfromabackgroundcorpus.inadditiontotheimprovedrepresentation,thetopicmodelshighlighttheuseofadi   erentsentencescoringprocedure:kullback-lieber(kl)divergence.thekldivergencebetweentwoid203distributionscapturesthemismatchinprobabilitiesassignedtothesameeventsbythetwodistributions.insummarization,theeventsaretheoccurrenceofwords.theid203ofwordsinthesummarycanbecomputeddirectly,asthenumberoftimesthewordoccursdividedbythetotalnumberofwords.ingeneralthekldivergenceofid203distributionqwithrespecttodistributionpoverwordswisde   nedaskl(p||q)=(cid:9)wp(w)logp(w)q(w)(3.7)p(w)andq(w)aretheprobabilitiesofwinpandqrespectively.sentencesarescoredandselectedinagreedyiterativeprocedure[36].ineachiterationthebestsentenceitobeselectedinthesummaryisdeterminedastheoneforwhichthekldivergencebetweenc,theprobabilitiesofwordsintheclustertobesummarized,andthesummarysofar,includingi,issmallest.kldivergenceisappealingasawayofscoringandselectingsentenceinsummarizationbecauseittrulycapturesanintuitivenotionthatgoodsummariesaresimilartotheinput.thinkingaboutagoodsummaryinthiswayisnotnewinsummarization[21,74]butklprovidesawayofmeasuringhowtheimportanceofwords,givenbytheirprobabilities,changesinthesummarycomparedtotheinput.agoodsummarywouldre   ecttheimportanceofwordsaccordingtotheinput,sothedivergencebetweenthetwowillbelow.thisintuitionhasbeenstudiedextensivelyinworkonautomaticevaluationofcontentselectioninsummarization,whereanotherindicatorofdivergence   jensenshannondivergence   hasprovensuperiortokl[45,52].givenallthis,informationtheoreticmeasuresforscoringsentencesarelikelytogainpopularityevenoutsidethedomainonbayesiantopicmodelrepresentations.allthatisnecessaryinordertoapplyadiver-gencetoscorethesummaryisatablewithwordprobabilities.thewordid203approachesinthespiritofsumbasic[68]candirectlyap-asurveyoftextsummarizationtechniques55plydivergencemeasurestoscoresentencesratherthansum,multiplyoraveragetheprobabilitiesofwords;othermethodsthatassignweightstowordscannormalizetheweightstogetaid203distributionofwords.inthenextsectionwewillalsodiscussanapproachforsumma-rizingacademicarticleswhichuseskldivergencetoscoresentences.2.5sentenceid91anddomain-dependenttopicsinmulti-documentsummarizationofnews,theinputbyde   nitionconsistsofseveralarticles,possiblyfromdi   erentsources,onthesametopic.acrossthedi   erentarticlestherewillbesentencesthatcontainsimilarinformation.informationthatoccursinmanyoftheinputdocu-mentsislikelyimportantandworthselectinginasummary.ofcourse,verbatimrepetitiononthesentencelevelisnotthatcommonacrosssources.rather,similarsentencescanbeclusteredtogether[59,39,85].insummarization,cosinesimilarityisstandardlyusedtomeasurethesimilaritybetweenthevectorrepresentationsofsentences[78].inthisapproach,clustersofsimilarsentencesaretreatedasproxiesfortopics;clusterswithmanysentencesrepresentimportanttopicthemesintheinput.selectingonerepresentativesentencefromeachmainclusterisonewaytoproduceanextractivesummary,whileminimizingpossibleredundancyinthesummary.thesentenceid91approachtomulti-documentsummarizationexploitsrepetitionatthesentencelevel.themoresentencesthereareinacluster,themoreimportanttheinformationintheclusterisconsidered.belowisanexampleofasentenceclusterfromdi   erentdocumentsintheinputtoamulti-documentsummarizer.allfoursentencessharecommoncontentthatshouldbeconveyedinthesummary.s1palwasdevastatedbyapilots   strikeinjuneandbytheregion   scurrencycrisis.s2injune,palwasembroiledinacripplingthree-weekpilots   strike.s3tanwantstoretainthe200pilotsbecausetheystoodbyhimwhenthemajorityofpal   spilotsstagedadevastatingstrikeinjune.s4injune,palwasembroiledinacripplingthree-weekpilots   strike.constrainingeachsentencetobelongtoonlyoneclusterisadistinctdisadvantageofthesentenceid91approach,andgraphmethodsforsummarizationwhichwediscussinthenextsection,haveproventoexploitthesameideasinamore   exibleway.fordomain-speci   csummarization,however,id91ofsentencesfrommanysamplesfromthedomaincangiveagoodindicationaboutthetopicsthatareusuallydiscussedinthedomain,andthetypeof56miningtextdatainformationthatasummarywouldneedtoconvey.inthiscase,hid-denmarkovmodels(id48)thatcapture   story   ow      whattopicsarediscussedinwhatorderinthedomain   canbetrained[5,28].thesemodelscapitalizeonthefactthatwithinaspeci   cdomain,informationindi   erenttextsispresentedfollowingacommonpresentation   ow.forexample,newsarticlesaboutearthquakesoften   rsttalkaboutwheretheearthquakehappened,whatitsmagnitudewas,thenmentionhumanca-sualtiesordamage,and   nallydiscussrescuee   orts.such   story   ow   canbelearnedfrommultiplearticlesfromthesamedomain.statesintheid48correspondtotopicsinthedomain,whicharediscoveredviaiterativeid91ofsimilarsentencesfrommanyarticlesfromthedomainofinterest.eachstate(topic)ischaracterizedbyaid203distributionwhichindicateshowlikelyagivenwordistoappearinasen-tencethatdiscussesthetopic.transitionsbetweenstatesinthemodelcorrespondtotopictransitionsintypicaltexts.theseid48modelsdonotrequireanylabelleddatafortrainingandallowforbothcontentse-lectionandorderinginsummarization.thesentencesthathavehighestid203ofconveyingimportanttopicsareselectedinthesummary.evensimplerapproachtodiscoveringthetopicsinaspeci   cdomaincanbeappliedwhenthereareavailablesamplesfromthedomainthataremorestructuredandcontainhuman-writtenheadings.forexam-ple,thereareplentyofwikipediaarticlesaboutactorsanddiseases.id91similarsectionheadings,wheresimilarityisde   nedbycosinesimilarityforexample,willidentifythetopicsdiscussedineachtypeofarticle[80].theclusterswithmostheadingsrepresentthemostcom-montopics,andthemostcommonstringintheclusterisusedtolabelit.thisprocedurediscoversforexamplethatwhentalkingaboutactors,writersmostoftenincludeinformationabouttheirbiography,earlylife,careerandpersonallife.thentosummarizewebpagesreturnedbyasearchforaspeci   cactor,thesystemcancreateawikipedia-likewebpageonthe   y,selectingsentencesfromthereturnedresultsthatconveythesetopics.3.in   uenceofcontextinmanycases,thesummarizerhasavailableadditionalmaterialsthatcanhelpdeterminethemostimportanttopicsinthedocumenttobesummarized.forexampleinwebpagesummarization,theaugmentedinputconsistsofotherwebpagesthathavelinkstothepagesthatwewanttosummarize.inblogsummarization,thediscussionfollowingtheblogpostiseasilyavailableandhighlyindicativeofwhatpartsoftheblogpostareinterestingandimportant.insummarizationofscholarlyasurveyoftextsummarizationtechniques57papers,laterpapersthatcitethepapertobesummarizedandtheci-tationsentencesinparticular,providearichcontextthatindicatewhatsentencesintheoriginalpaperareimportant.userinterestsareoftentakenintoaccountinquery-focusedsummarization,wherethequerypro-videsadditionalcontext.alloftheseapproachesrelyingonaugmentedinputhavebeenexploitedforsummarization.3.1websummarizationonetypeofwebpagecontexttoconsideristhetextinpagesthatlinktotheonethathastobesummarized,inparticularthetextsurroundedbythehyperlinktagpointingtothepage.thistextoftenprovidesadescriptivesummaryofawebpage(e.g.,   accesstopaperspublishedwithinthelastyearbymembersofthenlpgroup   ).proponentsofus-ingcontexttoprovidesummarysentencesarguethatawebsiteincludesmultimedia,maycoverdiversetopics,anditmaybehardforasumma-rizertodistinguishgoodsummarycontentfrombad[20].theearliestworkonthisapproachwascarriedouttoprovidesnippetsforeachresultfromasearchengine[2].todetermineasummary,theirsystemissuedasearchforaurl,selectedallsentencescontainingalinktothaturlandthebestsentencewasidenti   edusingheuristics.laterworkhasextendedthisapproachthroughanalgorithmthatallowsselectionofasentencethatcoversasmanyaspectsofthewebpageaspossibleandthatisonthesametopic[20].forcoverage,delortetal.usedwordoverlap,normalizedbysentencelength,todeterminewhichsentencesareentirelycoveredbyothersandthuscanberemovedfromconsider-ationforthesummary.toensuretopicality,delort   ssystemselectsasentencethatisareferencetothepage(e.g.,   id98isanewssite   )asopposedtocontent(e.g.,   thetopstoryfortoday...   ).hecomputestopicalitybymeasuringoverlapbetweeneachcontextsentenceandthetextwithinthewebpage,normalizingbythenumberofwordsinthewebpage.whenthewebpagedoesnothavemanywords,insteadheclustersallsentencesinthecontextandchoosesthesentencethatismostsimilartoallothersusingcosinedistance.insummarizationofblogposts,importantsentencesareidenti   edbasedonwordfrequency[41].thecriticaldi   erencefromotherap-proachesisthatherefrequencyiscomputedoverthecommentsonthepostratherthentheoriginalblogentry.theextractedsentencesarethosethateliciteddiscussion.58miningtextdata3.2summarizationofscienti   carticlesimpactsummarization[60]isde   nedasthetaskofextractingsen-tencesfromapaperthatrepresentthemostin   uentialcontentofthatpaper.languagemodelsprovideanaturalwayforsolvingthetask.foreachpapertobesummarized,impactsummarizationmethods   ndotherpapersinalargecollectionthatcitethatpaperandextracttheareasinwhichthereferencesoccur.alanguagemodelisbuiltusingthecollectionofallreferenceareastoapaper,givingtheid203ofeachwordtooccurinareferencearea.thislanguagemodelgivesawayofscoringtheimportanceofsentencesintheoriginalarticle:importantsentencesarethosethatconveyinformationsimilartothatwhichlaterpapersdiscussedwhenreferringtotheoriginalpaper.themeasureofsimilaritybetweenasentenceandthelanguagemodelismeasuredbykldivergence.inordertoaccountfortheimportanceofeachsentencewithinthesummarizedarticlealone,theapproachuseswordprobabil-itiesestimatedfromthearticle.the   nalscoreofasentenceisalin-earcombinationofimpactimportancecomingfromkldivergenceandintrinsicimportancecomingfromthewordprobabilitiesintheinputarticle.3.3query-focusedsummarizationinquery-focusedsummarization,theimportanceofeachsentencewillbedeterminedbyacombinationoftwofactors:howrelevantisthatsen-tencetotheuserquestionandhowimportantisthesentenceinthecon-textoftheinputinwhichitappears.therearetwoclassesofapproachestothisproblem.the   rstadaptstechniquesforgenericsummarizationofnews.forexample,anapproachusingtopicsignaturewords[15]isextendedforquery-focusedsummarizationbyassumingthatthewordsthatshouldappearinasummaryhavethefollowingid203:awordhasid203zeroofappearinginasummaryforauserde   nedtopicifitneitherappearsintheuserquerynorisatopicsignaturewordfortheinput;theid203ofthewordtoappearinthesummaryis0.5ifiteitherappearsintheuserqueryorisatopicsignature,butnotboth;andtheid203ofawordtoappearinasummaryis1ifitisbothintheuserqueryandinthelistoftopicsignaturewordsfortheinput.theseprobabilitiesarearbitrarilychosen,butinfactworkwellwhenusedtoassignweightstosentencesequaltotheaverageid203ofwordsinthesentence.graph-basedapproaches[71]havealsobeenadaptedforquery-focusedsummarizationwithminormodi   cations.otherapproacheshavebeendevelopedthatusenewmethodsforiden-tifyingrelevantandsalientsentences.theseapproacheshaveusuallyasurveyoftextsummarizationtechniques59beendevelopedforspeci   ctypesofqueries.forexample,manypeoplehaveworkedongenerationofbiographicalsummaries,wherethequeryisthenameofthepersonforwhomabiographyshouldbegenerated.mostpeopleusesomebalanceoftop-downdrivenapproachesthatsearchforpatternsofinformationthatmightbefoundinabiography,oftenusingmachinelearningtoidentifythepatterns,combinedwithbottom-upapproachesthatsiftthroughallavailablematerialto   ndsentencesthatarebiographicalinnature[7,98,81,105,6].themostrecentoftheseapproachesuseslanguagemodelingofbiographicaltextsfoundonwikipediaandnon-biographicaltextsinanewscorpustoidentifybiographicalsentencesininputdocuments.producingsnippetsforsearchenginesisaparticularlyusefulqueryfocusedapplication[92,95].3.4emailsummarizationsummarizationmustbesensitivetotheuniquecharacteristicsofemail,adistinctlinguisticgenrethatexhibitscharacteristicsofbothwrittentextandspokenconversation.athreadoramailboxcontainsoneormoreconversationsbetweentwoormoreparticipantsovertime.asinsummarizationofspokendialog,therefore,summarizationneedstotaketheinteractivenatureofdialogintoaccount;aresponseisoftenonlymeaningfulinrelationtotheutteranceitaddresses.unlikespo-kendialog,however,thesummarizerneednotconcernitselfwithspeechrecognitionerrors,theimpactofpronunciation,ortheavailabilityofspeechfeaturessuchasid144.furthermore,responsesandreactionsarenotimmediateandduetotheasynchronousnatureofemail,theymayexplicitlymarkthepreviousemailpassagestowhichtheyarerele-vant.inearlyresearchonsummarizationofemailthreads,[66]usedanex-tractivesummarizertogenerateasummaryforthe   rsttwolevelsofthediscussionthreadtree,producingrelativelyshort   overviewsum-maries.   theyextractedasentenceforeachofthetwolevels,usingoverlapwithprecedingcontext.laterworkonsummarizationofemailthreads[75]zeroedinonthedialogicnatureofemail.theirsummarizerusedmachinelearningandreliedonemailspeci   cfeaturesinadditiontotraditionalfeatures,includingfeaturesrelatedtothethreadandfea-turesrelatedtoemailstructuresuchasthenumberofresponderstoamessage,similarityofasentencewiththesubject,etc.emailconversa-tionsareanaturalmeansofgettinganswerstoone   squestionsandtheasynchronousnatureofemailmakesitpossibleforonetopursuesev-eralquestionsinparallel.asaconsequence,question-answerexchanges60miningtextdata   gureasoneofthedominantusesofemailconversations.theseobser-vationsledtoresearchonidenti   cationofquestionandanswerpairsinemail[84,64]andtheintegrationofsuchpairsinextractivesummariesofemail[58].emailsummarizershavealsobeendevelopedforafullmailboxorarchiveinsteadofjustathread.[69]presentasystemthatcanbeusedforbrowsinganemailmailboxandthatbuildsuponmulti-documentsummarizationtechniques.they   rstclusterallemailintopicallyre-latedthreads.bothanoverviewandafull-lengthsummaryarethengeneratedforeachcluster.amorerecentapproachtosummarizationofemailwithinafolderusesanovelgraph-basedanalysisofquotationswithinemail[10].usingthisanalysis,careninietal.   ssystemcomputesagraphrepresentinghoweachindividualemaildirectlymentionsotheremails,atthegranularityoffragmentsandsentences.4.indicatorrepresentationsandmachinelearningforsummarizationindicatorrepresentationapproachesdonotattempttointerpretorrepresentthetopicsdiscussedintheinput.insteadtheycomeupwitharepresentationofthetextthatcanbeusedtodirectlyranksentencesbyimportance.graphmethodsareuniquebecauseintheirmostpopularformulationstheybasesummarizationonasingleindicatorofimpor-tance,derivedfromthecentralityofsentencesinagraphrepresentationoftheinput.incontrastotherapproachesemployavarietyofindica-torsandcombinethemeitherheuristicallyorusingmachinelearningtodecidewhichsentencesareworthytobeincludedinthesummary.4.1graphmethodsforsentenceimportanceinthegraphmodelsinspiredbytheid95algorithm[25,61],theinputisrepresentedasahighlyconnectedgraph.verticesrepresentsentencesandedgesbetweensentencesareassignedweightsequaltothesimilaritybetweenthetwosentences.themethodmostoftenusedtocomputesimilarityiscosinesimilaritywithtf*idfweightsforwords.sometimes,insteadofassigningweightstoedges,theconnectionsbe-tweenverticescanbedeterminedinabinaryfashion:theverticesareconnectedonlyifthesimilaritybetweenthetwosentencesexceedsapre-de   nedthreshold.sentencesthatarerelatedtomanyothersentencesarelikelytobecentralandwouldhavehighweightforselectioninthesummary.whentheweightsoftheedgesarenormalizedtoformaid203distributionsothattheweightofalloutgoingedgesfromagivenvertexasurveyoftextsummarizationtechniques61sumuptoone,thegraphbecomesamarkovchainandtheedgeweightscorrespondtotheid203oftransitioningfromonestatetoanother.standardalgorithmsforstochasticprocessescanbeusedtocomputetheid203ofbeingineachvertexofthegraphattimetwhilemakingconsecutivetransitionsfromonevertextonext.asmoreandmoretransitionsaremade,theid203ofeachvertexconverges,givingthestationarydistributionofthechain.thestationarydistributiongivestheid203of(beingat)agivenvertexandcanbecomputedusingiterativeapproximation.verticeswithhigherprobabilitiescorrespondtomoreimportantsentencesthatshouldbeincludedinthesummary.graph-basedapproacheshavebeenshowntoworkwellforbothsingle-documentandmulti-documentsummarization[25,61].sincetheap-proachdoesnotrequirelanguage-speci   clinguisticprocessingbeyondidentifyingsentenceandwordboundaries,itcanalsobeappliedtootherlanguages,forexample,brazilianportuguese[62].atthesametime,in-corporatingsyntacticandsemanticroleinformationinthebuildingofthetextgraphleadstosuperiorresultsoverplaintf*idfcosinesimi-larity[13].usingdi   erentweightingschemesforlinksbetweensentencesthatbelongtothesamearticleandsentencesfromdi   erentarticlescanhelpseparatethenotionsoftopicalitywithinadocumentandrecurrenttopicsacrossdocuments.thisdistinctioncanbeeasilyintegratedinthegraph-basedmodelsforsummarization[96].graphrepresentationsforsummarizationhadbeenexploredevenbe-foretheid95modelsbecamepopular.forexample,thepurposeofanoldergraph-basedsystemformulti-documentsummarization[55]istoidentifysalientregionsofeachstoryrelatedtoatopicgivenbyauser,andcomparethestoriesbysummarizingsimilaritiesanddi   er-ences.theverticesinthegrapharewords,phrasesandnamedentitiesratherthansentencesandtheirinitialweightisassignedusingtf*idf.edgesbetweenverticesarede   nedusingsynonymandhypernymlinksinid138,aswellascoreferencelinks.spreadingactivationisusedtoassignweightstonon-querytermsasafunctionoftheweightoftheirneighborsinthegraphandthetypeofrelationconnectingthenodes.inordertoavoidproblemswithcoherencethatmayarisewiththeselectionofsinglesentences,theauthorsofanotherapproach[78]arguethatasummarizershouldselectfullparagraphstoprovideadequatecontext.theiralgorithmconstructsatextgraphforadocumentusingcosinesimilaritybetweeneachpairofparagraphsinthedocument.theshapeofthetextgraphdetermineswhichparagraphstoextract.intheirexperiments,theyshowthattwostrategies,selectingparagraphs62miningtextdatathatarewellconnectedtootherparagraphsor   rstparagraphsoftopicaltextsegmentswithinthegraph,bothproducegoodsummaries.acombinationofthesubsententialgranularityofanalysiswherenodesarewordsandphrasesratherthansentencesandedgesaresyntacticdependencieshasalsobeenexplored[44].usingmachinelearningtech-niques,theauthorsattempttolearnwhatportionsoftheinputgraphwouldbeincludedinasummary.intheirexperimentsonsingledoc-umentsummarizationofnewsarticles,propertiesofthegraphsuchasincomingandoutgoinglinks,connectivityandid95weightsareidenti   edasthebestclassoffeaturesthatcanbeusedforcontentselec-tion.thisworkprovidesanexcellentexampleofhowmachinelearningcanbeusedtocombinearangeofindicatorsofimportanceratherthancommittingtoasingleone.4.2machinelearningforsummarizationedmundson   searlywork[23]setthedirectionforlaterinvestigationofapplyingmachinelearningtechniquesforsummarization[43].heproposedthatratherthanrelyingonasinglerepresentationoftopicsintheinput,manydi   erentindicatorsofimportancecanbecombined.thenacorpusofinputsandsummarieswrittenbypeoplecanbeusedtodeterminetheweightofeachindicator.insupervisedmethodsforsummarization,thetaskofselectingim-portantsentencesisrepresentedasabinaryclassi   cationproblem,par-titioningallsentencesintheinputintosummaryandnon-summarysen-tences.acorpuswithhumanannotationsofsentencesthatshouldbeincludedinthesummaryisusedtotrainastatisticalclassi   erforthedis-tinction,witheachsentencesrepresentedasalistofpotentialindicatorsofimportance.thelikelihoodofasentencetobelongtothesummaryclass,orthecon   denceoftheclassi   erthatthesentenceshouldbeinthesummary,isthescoreofthesentence.thechosenclassi   erplaystheroleofasentencescoringfunction,takingasaninputtheintermediaterepresentationofthesentenceandoutputtingthescoreofthesentence.themosthighlyscoringsentencesareselectedtoformthesummary,possiblyafterskippingsomebecauseofhighsimilaritytoalreadycho-sensentences.machinelearningapproachestosummarizationo   ergreatfreedombecausethenumberofindicatorsofimportanceispracticallyendless[40,70,104,44,27,37,99,51].anyofthetopicrepresentationap-proachesdiscussedabovecanserveasthebasisofindicators.somecommonfeaturesincludethepositionofthesentenceinthedocument(   rstsentencesofnewsarealmostalwaysinformative),positionintheasurveyoftextsummarizationtechniques63paragraph(   rstandlastsentencesareoftenimportant),sentencelength,similarityofthesentencewiththedocumenttitleorheadings,weightsofthewordsinasentencedeterminedbyanytopicrepresentationap-proach,presenceofnamedentitiesorcuephrasesfromapredeterminedlist,etc.itishardlyanexaggerationtosaythateveryexistingmachinelearn-ingmethodhasbeenappliedforsummarization.oneimportantdi   er-enceiswhethertheclassi   erassumesthatthedecisionaboutinclusioninthesummaryisindependentlydoneforeachsentence.thisassumptionisapparentlynotrealistic,andmethodsthatexplicitlyencodedependen-ciesbetweensentencessuchashiddenmarkovmodelsandconditionalrandomfieldsoutperformotherlearningmethods[14,30,83].aprobleminherentinthesupervisedlearningparadigmistheneces-sityoflabeleddataonwhichclassi   erscanbetrained.askingannota-torstoselectsummary-worthysentencesisareasonablesolution[93]butitistimeconsumingandevenmoreimportantly,annotatoragreementislowanddi   erentpeopletendtochoosedi   erentsentenceswhenaskedtoconstructanextractivesummaryofatext[76].partlymotivatedbythisissueandpartlybecauseoftheirinterestinultimatelydevelopingabstractivemethodsforsummarizationmanyresearchershaveinsteadworkedwithabstractswrittenbypeople(oftenprofessionalwriters).researchersconcentratedtheire   ortsondevelopingmethodsforauto-maticalignmentofthehumanabstractsandtheinput[56,42,104,4,17]inordertoprovidelabeleddataofsummaryandnon-summarysen-tencesformachinelearning.someresearchershavealsoproposedwaystoleveragetheinformationfrommanualevaluationofcontentselectioninsummarizationinwhichmultiplesentencescanbemarkedasexpress-ingthesamefactthatshouldbeinthesummary[16,27].alternatively,onecouldcomputesimilaritybetweensentencesinhumanabstractsandthoseintheinputinorderto   ndverysimilarsentences,notnecessarilydoingfullalignment[12].anotheroptionfortrainingaclassi   eristoemployasemi-supervisedapproach.inthisparadigm,asmallnumberofexamplesofsummaryandnon-summarysentencesareannotatedbypeople.thentwoclassi-   ersaretrainedonthatdata,usingdi   erentsetsoffeatureswhichareindependentgiventheclass[100]ortwodi   erentclassi   cationmethods[99].afterthatoneoftheclassi   ersisrunonunannotateddata,anditsmostcon   dentpredictionsareaddedtotheannotatedexamplestotraintheotherclassi   er,repeatingtheprocessuntilsomeprede   nedhaltingconditionismet.severalmodi   cationstostandardmachinelearningapproachesareappropriateforsummarization.ine   ectformulatingsummarizationas64miningtextdataabinaryclassi   cationproblem,whichscoresindividualsentences,isnotequivalentto   ndingthebestsummary,whichconsistsofseveralsen-tences.thisisexactlytheissueofselectingasummarythatwediscussinthenextsection.intrainingasupervisedmodel,theparametersmaybeoptimizedtoleadtoasummarythathasthebestscoreagainstahumanmodel[1,49].forgenericmulti-documentsummarizationofnews,supervisedmeth-odshavenotbeenshowntooutperformcompetitiveunsupervisedmeth-odsbasedonasinglefeaturesuchasthepresenceoftopicwordsandgraphmethods.machinelearningapproacheshaveprovedtobemuchmoresuccessfulinsingledocumentordomainorgenrespeci   csum-marization,whereclassi   erscanbetrainedtoidentifyspeci   ctypesofinformationsuchassentencesdescribingliteraturebackgroundinsci-enti   carticlesummarization[90],utterancesexpressingagreementordisagreementinmeetings[30],biographicalinformation[105,6,80],etc.5.selectingsummarysentencesmostsummarizationapproacheschoosecontentsentencebysentence:they   rstincludethemostinformativesentence,andthenifspacecon-straintspermit,thenextmostinformativesentenceisincludedinthesummaryandsoon.someprocessofcheckingforsimilaritybetweenthechosensentencesisalsousuallyemployedinordertoavoidtheinclusionofrepetitivesentences.5.1greedyapproaches:maximalmarginalrelevanceindexgreedyapproachtosummarizationoneoftheearlysumma-rizationapproachesforbothgenericandqueryfocusedsummarizationthathasbeenwidelyadoptedismaximalmarginalrelevance(mmr)[9].inthisapproach,summariesarecreatedusinggreedy,sentence-by-sentenceselection.ateachselectionstep,thegreedyalgorithmisconstrainedtoselectthesentencethatismaximallyrelevanttotheuserquery(orhashighestimportancescorewhenaqueryisnotavailable)andminimallyredundantwithsentencesalreadyincludedinthesummary.mmrmeasuresrelevanceandnoveltyseparatelyandthenusesalinearcombinationofthetwotoproduceasinglescorefortheimportanceofasentenceinagivenstageoftheselectionprocess.toquantifybothpropertiesofasentence,carbonellandgoldsteinusecosinesimilarity.forrelevance,similarityismeasuredtothequery,whilefornovelty,similarityismeasuredagainstsentencesselectedsofar.themmrap-proachwasoriginallyproposedforquery-focusedsummarizationintheasurveyoftextsummarizationtechniques65contextofinformationretrieval,butcouldeasilybeadaptedforgenericsummarization,forexamplebyusingtheentireinputasauser[33].infactanyofthepreviouslydiscussedapproachesforsentencescoringcanbeusedtocalculatetheimportanceofasentence.manyhaveadoptedthisseminalapproach,mostlyinitsgenericversion,sometimesusingdi   erentmeasuresofnoveltytoselectnewsentences[91,101,65].thisgreedyapproachofsequentialsentenceselectionmightnotbethate   ectiveforoptimalcontentselectionoftheentiresummary.onetypicalproblematicscenarioforgreedysentenceselection(discussedin[57])iswhenaverylongandhighlyrelevantsentencehappenstobeevaluatedasthemostinformativeearlyon.suchasentencemaycontainseveralpiecesofrelevantinformation,alongsidesomenotsorelevantfactswhichcouldbeconsiderednoise.includingsuchasentenceinthesummarywillhelpmaximizecontentrelevanceatthetimeofselection,butatthecostoflimitingtheamountofspaceinthesummaryremainingforothersentences.insuchcasesitisoftenmoredesirabletoincludeseveralshortersentences,whichareindividuallylessinformativethanthelongone,butwhichtakentogetherdonotexpressanyunnecessaryinformation.5.2globalsummaryselectionglobaloptimizationalgorithmscanbeusedtosolvethenewformu-lationofthesummarizationtask,inwhichthebestoverallsummaryisselected.givensomeconstraintsimposedonthesummary,suchasmaximizinginformativeness,minimizingrepetition,andconformingtorequiredsummarylength,thetaskwouldbetoselectthebestsum-mary.findinganexactsolutiontothisproblemisnp-hard[26],butapproximatesolutionscanbefoundusingadynamicprogrammingal-gorithm[57,103,102].exactsolutionscanbefoundquicklyviasearchtechniqueswhenthesentencescoringfunctionislocal,computableonlyfromthegivensentence[1].eveninglobaloptimizationmethods,informativenessisstillde   nedandmeasuredusingfeatureswell-exploredinthesentenceselectionlit-erature.theseincludewordfrequencyandpositioninthedocument[103],tf*idf[26],similaritywiththeinput[57],andconceptfrequency[102,32].globaloptimizationapproachestocontentselectionhavebeenshowntooutperformgreedyselectionalgorithmsinseveralevaluationsusingnewsdataasinput,andhaveprovedtobeespeciallye   ectiveforextractivesummarizationofmeetings[77,32].inadetailedstudyofglobalid136algorithms[57],ithasbeendemonstratedthatitispossibleto   ndanexactsolutionfortheop-66miningtextdatatimizationproblemforcontentselectionusingintegerlinearprogram-ming.theperformanceoftheapproximatealgorithmbasedondynamicprogrammingwaslower,butcomparabletothatoftheexactsolutions.intermsofrunningtime,thegreedyalgorithmisverye   cient,almostconstantinthesizeoftheinput.theapproximatealgorithmscaleslinearlywiththesizeoftheinputandisthusindeedpracticaltouse.therunningtimefortheexactalgorithmgrowssteeplywiththesizeoftheinputandisunlikelytobeusefulinpractice[57].however,whenamonotonesubmodularfunctionisusedtoevaluatetheinformativenessofthesummary,optimalornearoptimalsolutioncanbefoundquickly[48,47].6.conclusioninthischapterwehaveattemptedtogiveacomprehensiveoverviewofthemostprominentrecentmethodsforautomatictextsummariza-tion.wehaveoutlinedtheconnectiontoearlyapproachesandhavecontrastedapproachesintermsofhowtheyrepresenttheinput,scoresentencesandselectthesummary.wehavehighlightedthesuccessofkldivergenceasamethodforscoringsentenceswhichdirectlyincorpo-ratesanintuitionaboutthecharacteristicsofagoodsummary,aswellasthegrowinginterestinthedevelopmentofmethodsthatgloballyop-timizetheselectionofthesummary.wehaveshownhowsummarizationstrategiesmustbeadaptedtodi   erentgenres,suchaswebpagesandjournalarticles,takingintoaccountcontextualinformationthatguidessentenceselection.thesethreerecentdevelopmentsinsummarizationcomplementtraditionaltopicsinthe   eldthatconcernintermediaterep-resentationsandtheapplicationofappropriatemachinelearningmeth-odsforsummarization.references[1]a.aker,t.cohn,andr.gaizauskas.multi-documentsumma-rizationusinga*searchanddiscriminativetraining.inproceedingsofthe2010conferenceonempiricalmethodsinnaturallanguageprocessing,emnlp   10,pages482   491,2010.[2]e.amitayandc.paris.automaticallysummarizingwebsites-isthereawayaroundit?inproceedingsoftheacmconferenceoninformationandknowledgemanagement,pages173   179,2000.[3]r.barzilayandm.elhadad.textsummarizationswithlexicalchains.ininderjeetmaniandmarkmaybury,editors,advancesinautomatictextsummarization,pages111   121.mitpress,1999.asurveyoftextsummarizationtechniques67[4]r.barzilayandn.elhadad.sentencealignmentformonolingualcomparablecorpora.inproceedingsoftheconferenceonempiri-calmethodsinnaturallanguageprocessing,pages25   32,2003.[5]r.barzilayandl.lee.catchingthedrift:probabilisticcontentmodels,withapplicationstogenerationandsummarization.inhumanlanguagetechnologyconferenceofthenorthamericanchapteroftheassociationforcomputationallinguistics,pages113   120,2004.[6]f.biadsy,j.hirschberg,ande.filatova.anunsupervisedap-proachtobiographyproductionusingwikipedia.inproceedingsoftheannualmeetingoftheassociationforcomputationallin-guistics,pages807   815,2008.[7]s.blair-goldensohn,k.mckeown,anda.schlaikjer.defscriber:ahybridsystemforde   nitionalqa.inproceedingsoftheannualinternationalacmsigirconferenceonresearchanddevelop-mentininformationretrieval,pages462   462,2003.[8]d.blei,t.gri   ths,m.jordan,andj.tenenbaum.hierarchi-caltopicmodelsandthenestedchineserestaurantprocess.inadvancesinneuralinformationprocessingsystems,page2003,2004.[9]j.carbonellandj.goldstein.theuseofmmr,diversity-basedrerunningforreorderingdocumentsandproducingsummaries.inproceedingsoftheannualinternationalacmsigirconfer-enceonresearchanddevelopmentininformationretrieval,pages335   336,1998.[10]g.carenini,r.ng,andx.zhou.summarizingemailconversationswithcluewords.inproceedingsoftheinternationalconferenceonworldwideweb,pages91   100,2007.[11]a.celikyilmazandd.hakkani-tur.ahybridhierarchicalmodelformulti-documentsummarization.inproceedingsofthe48thannualmeetingoftheassociationforcomputationallinguistics,pages815   824,2010.[12]y.chali,s.hasan,ands.joty.doautomaticannotationtech-niqueshaveanyimpactonsupervisedcomplexquestionanswer-ing?inproceedingsofthejointconferenceoftheannualmeetingoftheaclandtheinternationaljointconferenceonnaturallanguageprocessingoftheafnlp,pages329   332,2009.[13]y.chaliands.joty.improvingtheperformanceoftherandomwalkmodelforansweringcomplexquestions.inproceedingsofthe68miningtextdataannualmeetingoftheassociationforcomputationallinguistics,shortpapers,pages9   12,2008.[14]j.conroyandd.o   leary.textsummarizationviahiddenmarkovmodels.inproceedingsoftheannualinternationalacmsi-girconferenceonresearchanddevelopmentininformationre-trieval,pages406   407,2001.[15]j.conroy,j.schlesinger,andd.o   leary.topic-focusedmulti-documentsummarizationusinganapproximateoraclescore.inproceedingsoftheinternationalconferenceoncomputationallinguisticsandtheannualmeetingoftheassociationforcom-putationallinguistics,pages152   159,2006.[16]t.copeckands.szpakowicz.leveragingpyramids.inproceedingsofthedocumentunderstandingconference,2005.[17]h.daum  eiiiandd.marcu.aphrase-basedid48approachtodocument/abstractalignment.inproceedingsoftheconferenceonempiricalmethodsinnaturallanguageprocessing,pages119   126,2004.[18]h.daum  eiiiandd.marcu.bayesianquery-focusedsummariza-tion.inproceedingsoftheinternationalconferenceoncompu-tationallinguisticsandtheannualmeetingoftheassociationforcomputationallinguistics,pages305   312,2006.[19]s.deerwester,s.dumais,g.furnas,t.landauer,andr.harsh-man.indexingbylatentsemanticanalysis.journaloftheameri-cansocietyforinformationscience,pages391   407,1990.[20]j.-y.delort,b.bouchon-meunier,andm.rifqi.enhancedwebdocumentsummarizationusinghyperlinks.inproceedingsoftheacmconferenceonhypertextandhypermedia,pages208   215,2003.[21]r.donaway,k.drummey,andl.mather.acomparisonofrank-ingsproducedbysummarizationevaluationmeasures.inproceed-ingsofthe2000naacl-anlpworkshoponautomaticsumma-rization-volume4,pages69   78,2000.[22]t.dunning.accuratemethodsforthestatisticsofsurpriseandcoincidence.computationallinguistics,19(1):61   74,1994.[23]h.edmundson.newmethodsinautomaticextracting.journaloftheacm,16(2):264   285,1969.[24]n.elhadad,m.-y.kan,j.klavans,andk.mckeown.customiza-tioninauni   edframeworkforsummarizingmedicalliterature.journalofarti   cialintelligenceinmedicine,33:179   198,2005.asurveyoftextsummarizationtechniques69[25]g.erkanandd.radev.lexrank:graph-basedcentralityassalienceintextsummarization.journalofarti   cialintelligenceresearch,2004.[26]e.filatovaandv.hatzivassiloglou.aformalmodelforinforma-tionselectioninmulti-sentencetextextraction.inproceedingsoftheinternationalconferenceoncomputationallinguistic,pages397   403,2004.[27]m.fuentes,e.alfonseca,andh.rodr    guez.supportvectorma-chinesforquery-focusedsummarizationtrainedandevaluatedonpyramiddata.inproceedingsoftheannualmeetingoftheasso-ciationforcomputationallinguistics,companionvolume:pro-ceedingsofthedemoandpostersessions,pages57   60,2007.[28]p.fungandg.ngai.onestory,one   ow:hiddenmarkovstorymodelsformultilingualmultidocumentsummarization.acmtransactionsonspeechandlanguageprocessing,3(2):1   16,2006.[29]s.furui,m.hirohata,y.shinnaka,andk.iwano.sentenceextraction-basedautomaticspeechsummarizationandevalua-tiontechniques.inproceedingsofthesymposiumonlarge-scaleknowledgeresources,pages33   38,2005.[30]m.galley.askip-chainconditionalrandom   eldforrankingmeet-ingutterancesbyimportance.inproceedingsoftheconferenceonempiricalmethodsinnaturallanguageprocessing,pages364   372,2006.[31]m.galleyandk.mckeown.improvingwordsensedisambigua-tioninlexicalchaining.inproceedingsoftheinternationaljointconferenceonarti   cialintelligence,pages1486   1488,2003.[32]d.gillick,k.riedhammer,b.favre,andd.hakkani-tur.aglobaloptimizationframeworkformeetingsummarization.inproceedingsoftheieeeinternationalconferenceonacoustics,speechandsignalprocessing,pages4769   4772,2009.[33]y.gongandx.liu.generictextsummarizationusingrelevancemeasureandlatentsemanticanalysis.inproceedingsoftheannualinternationalacmsigirconferenceonresearchanddevelop-mentininformationretrieval,pages19   25,2001.[34]s.gupta,a.nenkova,andd.jurafsky.measuringimportanceandqueryrelevanceintopic-focusedmulti-documentsummarization.inproceedingsoftheannualmeetingoftheassociationforcom-putationallinguistics,demoandpostersessions,pages193   196,2007.70miningtextdata[35]b.hachey,g.murray,andd.reitter.dimensionalityreductionaidstermco-occurrencebasedmulti-documentsummarization.insumqa   06:proceedingsoftheworkshopontask-focusedsum-marizationandquestionanswering,pages1   7,2006.[36]a.haghighiandl.vanderwende.exploringcontentmodelsformulti-documentsummarization.inproceedingsofhumanlan-guagetechnologies:the2009annualconferenceofthenorthamericanchapteroftheassociationforcomputationallinguis-tics,pages362   370,2009.[37]d.hakkani-turandg.tur.statisticalsentenceextractionforinformationdistillation.inproceedingsoftheieeeinternationalconferenceonacoustics,speechandsignalprocessing,volume4,pagesiv   1   iv   4,2007.[38]s.harabagiuandf.lacatusu.topicthemesformulti-documentsummarization.inproceedingsofthe28thannualinternationalacmsigirconferenceonresearchanddevelopmentininforma-tionretrieval,sigir   05,pages202   209,2005.[39]v.hatzivassiloglou,j.klavans,m.holcombe,r.barzilay,m.kan,andk.mckeown.sim   nder:a   exibleid91toolforsummarization.inproceedingsofthenaaclworkshoponau-tomaticsummarization,pages41   49,2001.[40]e.hovyandc.-y.lin.automatedtextsummarizationinsum-marist.inadvancesinautomatictextsummarization,pages82   94,1999.[41]m.hu,a.sun,ande.-p.lim.comments-orientedblogsumma-rizationbysentenceextraction.inproceedingsoftheacmconfer-enceoninformationandknowledgemanagement,pages901   904,2007.[42]h.jing.usinghiddenmarkovmodelingtodecomposehuman-writtensummaries.computationallinguistics,28(4):527   543,2002.[43]j.kupiec,j.pedersen,andf.chen.atrainabledocumentsum-marizer.inproceedingsoftheannualinternationalacmsi-girconferenceonresearchanddevelopmentininformationre-trieval,pages68   73,1995.[44]j.leskovec,n.milic-frayling,andm.grobelnik.impactoflin-guisticanalysisonthesemanticgraphcoverageandlearningofdocumentextracts.inproceedingsofthenationalconferenceonarti   cialintelligence,pages1069   1074,2005.asurveyoftextsummarizationtechniques71[45]c.-y.lin,g.cao,j.gao,andj.-y.nie.aninformation-theoreticapproachtoautomaticevaluationofsummaries.inproceedingsofthemainconferenceonhumanlanguagetechnologyconferenceofthenorthamericanchapteroftheassociationofcomputa-tionallinguistics(hlt-naacl   06),pages463   470,2006.[46]c.-y.linande.hovy.theautomatedacquisitionoftopicsigna-turesfortextsummarization.inproceedingsoftheinternationalconferenceoncomputationallinguistic,pages495   501,2000.[47]h.linandj.bilmes.multi-documentsummarizationviabud-getedmaximizationofsubmodularfunctions.innorthamericanchapteroftheassociationforcomputationallinguistics/humanlanguagetechnologyconference(naacl/hlt-2010),2010.[48]h.lin,j.bilmes,ands.xie.graph-basedsubmodularselectionforextractivesummarization.inproc.ieeeautomaticspeechrecognitionandunderstanding(asru),2009.[49]s.-h.lin,y.-m.chang,j.-w.liu,andb.chen.leveragingevalu-ationmetric-relatedtrainingcriteriaforspeechsummarization.inproceedingsoftheieeeinternationalconferenceonacoustics,speech,andsignalprocessing,icassp2010,pages5314   5317,2010.[50]s.-h.linandb.chen.ariskminimizationframeworkforex-tractivespeechsummarization.inproceedingsofthe48thannualmeetingoftheassociationforcomputationallinguistics,pages79   87,2010.[51]a.louis,a.joshi,anda.nenkova.discourseindicatorsforcon-tentselectioninsummarization.inproceedingsoftheannualmeetingofthespecialinterestgroupondiscourseanddialogue,pages147   156,2010.[52]a.louisanda.nenkova.automaticallyevaluatingcontentselec-tioninsummarizationwithouthumanmodels.inproceedingsofthe2009conferenceonempiricalmethodsinnaturallanguageprocessing(emnlp),pages306   314,2009.[53]h.p.luhn.theautomaticcreationofliteratureabstracts.ibmjournalofresearchanddevelopment,2(2):159   165,1958.[54]m.mana-l  opez,m.debuenaga,andj.g  omez-hidalgo.mul-tidocumentsummarization:anaddedvaluetoid91inin-teractiveretrieval.acmtransactionsoninformationssystems,22(2):215   241,2004.72miningtextdata[55]i.maniande.bloedorn.summarizingsimilaritiesanddi   erencesamongrelateddocuments.informationretrieval,1(1-2):35   67,april1999.[56]d.marcu.theautomaticconstructionoflarge-scalecorporaforsummarizationresearch.inproceedingsoftheannualinterna-tionalacmsigirconferenceonresearchanddevelopmentininformationretrieval,pages137   144,1999.[57]r.mcdonald.astudyofglobalid136algorithmsinmulti-documentsummarization.inproceedingsoftheeuropeanconfer-enceonirresearch,pages557   564,2007.[58]k.mckeown,l.shrestha,ando.rambow.usingquestion-answerpairsinextractivesummarizationofemailconversations.inpro-ceedingsoftheinternationalconferenceoncomputationallin-guisticsandintelligenttextprocessing,pages542   550,2007.[59]k.mckeown,j.klavans,v.hatzivassiloglou,r.barzilay,ande.eskin.towardsmultidocumentsummarizationbyreformulation:progressandprospects.inproceedingsofthenationalconferenceonarti   cialintelligence,pages453   460,1999.[60]q.meiandc.zhai.generatingimpact-basedsummariesforsci-enti   cliterature.inproceedingsoftheannualmeetingoftheassociationforcomputationallinguistics,pages816   824,2008.[61]r.mihalceaandp.tarau.textrank:bringingorderintotexts.inproceedingsoftheconferenceonempiricalmethodsinnaturallanguageprocessing,pages404   411,2004.[62]r.mihalceaandp.tarau.analgorithmforlanguageindependentsingleandmultipledocumentsummarization.inproceedingsoftheinternationaljointconferenceonnaturallanguageprocessing,pages19   24,2005.[63]g.a.miller,r.beckwith,c.fellbaum,d.gross,andk.j.miller.introductiontoid138:anon-linelexicaldatabase.internationaljournalofid69(specialissue),3(4):235   312,1990.[64]h.murakoshi,a.shimazu,andk.ochimizu.constructionofde-liberationstructureinemailconversation.inproceedingsoftheconferenceofthepaci   cassociationforcomputationallinguis-tics,pages570   577,2004.[65]g.murray,s.renals,andj.carletta.extractivesummarizationofmeetingrecordings.inproc.9theuropeanconferenceonspeechcommunicationandtechnology,pages593   596,2005.asurveyoftextsummarizationtechniques73[66]a.nenkovaanda.bagga.facilitatingemailthreadaccessbyextractivesummarygeneration.inproceedingsoftherecentad-vancesinnaturallanguageprocessingconference,2003.[67]a.nenkovaandk.mckeown.automaticsummarization.infoundationsandtrendsininformationretrieval5(2   3),pages103   233,2011.[68]a.nenkova,l.vanderwende,andk.mckeown.acompositionalcontextsensitivemulti-documentsummarizer:exploringthefac-torsthatin   uencesummarization.inproceedingsoftheannualinternationalacmsigirconferenceonresearchanddevelop-mentininformationretrieval,pages573   580,2006.[69]p.newmanandj.blitzer.summarizingarchiveddiscussions:abeginning.inproceedingsoftheinternationalconferenceonintel-ligentuserinterfaces,pages273   276,2003.[70]m.osborne.usingmaximumid178forsentenceextraction.inproceedingsoftheaclworkshoponautomaticsummarization,pages1   8,2002.[71]j.otterbacher,g.erkan,andd.radev.biasedlexrank:passageretrievalusingrandomwalkswithquestion-basedpriors.informa-tionprocessingandmanagement,45:42   54,january2009.[72]m.ozsoy,i.cicekli,andf.alpaslan.textsummarizationofturkishtextsusinglatentsemanticanalysis.inproceedingsofthe23rdinternationalconferenceoncomputationallinguistics(coling2010),pages869   876,august2010.[73]d.radev,h.jing,m.sty,andd.tam.centroid-basedsum-marizationofmultipledocuments.informationprocessingandmanagement,40:919   938,2004.[74]d.radev,s.teufel,h.saggion,w.lam,j.blitzer,h.qi,a.c  elebi,d.liu,ande.drabek.evaluationchallengesinlarge-scaledocumentsummarization.inproceedingsofthe41stannualmeetingonassociationforcomputationallinguistics(acl   03),pages375   382,2003.[75]o.rambow,l.shrestha,j.chen,andc.lauridsen.summariz-ingemailthreads.inhumanlanguagetechnologyconferenceofthenorthamericanchapteroftheassociationforcomputationallinguistics,2004.[76]g.rath,a.resnick,andr.savage.theformationofabstractsbytheselectionofsentences:part1:sentenceselectionbymanandmachines.americandocumentation,2(12):139   208,1961.74miningtextdata[77]k.riedhammer,d.gillick,b.favre,andd.hakkani-tur.packingthemeetingsummarizationknapsack.inproceedingsofthean-nualconferenceoftheinternationalspeechcommunicationas-sociation,pages2434   2437,2008.[78]g.salton,a.singhal,m.mitra,andc.buckley.automatictextstructuringandsummarization.informationprocessingandman-agement,33(2):193   208,1997.[79]g.saltonandc.buckley.term-weightingapproachesinauto-matictextretrieval.informationprocessingandmanagement,24:513   523,1988.[80]c.sauperandr.barzilay.automaticallygeneratingwikipediaarticles:astructure-awareapproach.inproceedingsofthejointconferenceofthe47thannualmeetingoftheaclandthe4thinternationaljointconferenceonnaturallanguageprocessingoftheafnlp,pages208   216,2009.[81]b.schi   man,i.mani,andk.concepcion.producingbiographicalsummaries:combininglinguisticknowledgewithcorpusstatistics.inproceedingsoftheannualmeetingoftheassociationforcom-putationallinguistics,pages458   465,2001.[82]b.schi   man,a.nenkova,andk.mckeown.experimentsinmul-tidocumentsummarization.inproceedingsoftheinternationalconferenceonhumanlanguagetechnologyresearch,pages52   58,2002.[83]d.shen,j.-t.sun,h.li,q.yang,andz.chen.documentsummarizationusingconditionalrandom   elds.inproceedingsofthe20thinternationaljointconferenceonarti   calintelligence,pages2862   2867,2007.[84]l.shresthaandk.mckeown.detectionofquestion-answerpairsinemailconversations.inproceedingsoftheinternationalcon-ferenceoncomputationallinguistic,2004.[85]a.siddharthan,a.nenkova,andk.mckeown.syntacticsimpli   -cationforimprovingcontentselectioninmulti-documentsumma-rization.inproceedingsoftheinternationalconferenceoncom-putationallinguistic,pages896   902,2004.[86]h.silberandk.mccoy.e   cientlycomputedlexicalchainsasanintermediaterepresentationforautomatictextsummarization.computationallinguistics,28(4):487   496,2002.[87]k.sparckjones.astatisticalinterpretationoftermspeci   cityanditsapplicationinretrieval.journalofdocumentation,28:11   21,1972.asurveyoftextsummarizationtechniques75[88]j.steinberger,m.poesio,m.a.kabadjov,andk.jeek.twousesofanaphoraresolutioninsummarization.informationprocessingandmanagement,43(6):1663   1680,2007.[89]w.yih,j.goodman,l.vanderwende,andh.suzuki.multi-documentsummarizationbymaximizinginformativecontent-words.inproceedingsoftheinternationaljointconferenceonarti   cialintelligence,pages1776   1782,2007.[90]s.teufelandm.moens.summarizingscienti   carticles:exper-imentswithrelevanceandrhetoricalstatus.computationallin-guisics.,28(4):409   445,2002.[91]d.radev,t.allison,s.blair-goldensohn,j.blitzer,a.celebi,s.dimitrov,e.drabek,a.hakim,w.lam,d.liu,j.otterbacher,h.qi,h.saggion,s.teufel,a.winkel,andz.zhang.mead-aplatformformultidocumentmultilingualtextsummarization.inproceedingsoftheinternationalconferenceonlanguagere-sourcesandevaluation,2004.[92]a.turpin,y.tsegay,d.hawking,andh.williams.fastgenera-tionofresultsnippetsinwebsearch.inproceedingsoftheannualinternationalacmsigirconferenceonresearchanddevelop-mentininformationretrieval,pages127   134,2007.[93]j.ulrich,g.murray,andg.carenini.apubliclyavailableanno-tatedcorpusforsupervisedemailsummarization.inproceedingsoftheaaaiemailworkshop,pages77   87,2008.[94]l.vanderwende,h.suzuki,c.brockett,anda.nenkova.be-yondsumbasic:task-focusedsummarizationwithsentencesimpli-   cationandlexicalexpansion.informationprocessingandman-agment,43:1606   1618,2007.[95]r.varadarajanandv.hristidis.asystemforquery-speci   cdoc-umentsummarization.inproceedingsoftheacmconferenceoninformationandknowledgemanagement,2006.[96]x.wanandj.yang.improveda   nitygraphbasedmulti-documentsummarization.inhumanlanguagetechnologycon-ferenceofthenorthamericanchapteroftheassociationforcom-putationallinguistics,companionvolume:shortpapers,pages181   184,2006.[97]d.wang,s.zhu,t.li,andy.gong.multi-documentsumma-rizationusingsentence-basedtopicmodels.inproceedingsoftheacl-ijcnlp2009conferenceshortpapers,pages297   300,2009.76miningtextdata[98]r.weischedel,j.xu,anda.licuanan.ahybridapproachtoansweringbiographicalquestions.inmarkmaybury,editor,newdirectionsinquestionanswering,pages59   70,2004.[99]k.wong,m.wu,andw.li.extractivesummarizationusingsupervisedandsemi-supervisedlearning.inproceedingsofthe22ndinternationalconferenceoncomputationallinguistics(col-ing2008),pages985   992,2008.[100]s.xie,h.lin,andy.liu.semi-supervisedextractivespeechsummarizationviaco-trainingalgorithm.ininterspeech,the11thannualconferenceoftheinternationalspeechcommunica-tionassociation,pages2522   2525,2010.[101]s.xieandy.liu.usingcorpusandknowledge-basedsimilar-itymeasureinmaximummarginalrelevanceformeetingsumma-rization.inproceedingsoftheieeeinternationalconferenceonacoustics,speechandsignalprocessing,pages4985   4988,2008.[102]s.ye,t.-s.chua,m.-y.kan,andl.qiu.documentconceptlatticefortextunderstandingandsummarization.informationprocessingandmanagement,43(6):1643   1662,2007.[103]w.yih,j.goodman,l.vanderwende,andh.suzuki.multi-documentsummarizationbymaximizinginformativecontent-words.inproceedingsoftheinternationaljointconferenceonarti   cialintelligence,pages1776   1782,2007.[104]l.zhouande.hovy.aweb-trainedextractionsummarizationsystem.inproceedingsoftheconferenceofthenorthamericanchapteroftheassociationforcomputationallinguisticsonhu-manlanguagetechnology,pages205   211,2003.[105]l.zhou,m.ticrea,ande.hovy.multi-documentbiographysum-marization.inproceedingsoftheconferenceonempiricalmethodsinnaturallanguageprocessing,pages434   441,2004.