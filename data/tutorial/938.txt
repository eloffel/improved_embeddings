a survey on automatic text summarization

dipanjan das

andr  e f.t. martins

language technologies institute
carnegie mellon university
{dipanjan, afm}@cs.cmu.edu

november 21, 2007

abstract

the increasing availability of online information has necessitated intensive
research in the area of automatic text summarization within the natural lan-
guage processing (nlp) community. over the past half a century, the prob-
lem has been addressed from many di   erent perspectives, in varying domains
and using various paradigms. this survey intends to investigate some of the
most relevant approaches both in the areas of single-document and multiple-
document summarization, giving special emphasis to empirical methods and
extractive techniques. some promising approaches that concentrate on speci   c
details of the summarization problem are also discussed. special attention is
devoted to automatic evaluation of summarization systems, as future research
on summarization is strongly dependent on progress in this area.

1

introduction

the sub   eld of summarization has been investigated by the nlp community for
nearly the last half century. radev et al. (2002) de   ne a summary as    a text that
is produced from one or more texts, that conveys important information in the
original text(s), and that is no longer than half of the original text(s) and usually
signi   cantly less than that   . this simple de   nition captures three important aspects
that characterize research on id54:

    summaries may be produced from a single document or multiple documents,
    summaries should preserve important information,
    summaries should be short.
even if we agree unanimously on these points, it seems from the literature that
any attempt to provide a more elaborate de   nition for the task would result in
disagreement within the community. in fact, many approaches di   er on the manner
of their problem formulations. we start by introducing some common terms in the

1

summarization dialect: extraction is the procedure of identifying important sections
of the text and producing them verbatim; abstraction aims to produce important
material in a new way; fusion combines extracted parts coherently; and compression
aims to throw out unimportant sections of the text (radev et al., 2002).

earliest instances of research on summarizing scienti   c documents proposed
paradigms for extracting salient sentences from text using features like word and
phrase frequency (luhn, 1958), position in the text (baxendale, 1958) and key
phrases (edmundson, 1969). various work published since then has concentrated on
other domains, mostly on newswire data. many approaches addressed the problem
by building systems depending of the type of the required summary. while extractive
summarization is mainly concerned with what the summary content should be, usu-
ally relying solely on extraction of sentences, abstractive summarization puts strong
emphasis on the form, aiming to produce a grammatical summary, which usually
requires advanced language generation techniques. in a paradigm more tuned to
information retrieval (ir), one can also consider topic-driven summarization, that
assumes that the summary content depends on the preference of the user and can
be assessed via a query, making the    nal summary focused on a particular topic.

a crucial issue that will certainly drive future research on summarization is
evaluation. during the last    fteen years, many system evaluation competitions like
trec,1 duc2 and muc3 have created sets of training material and have estab-
lished baselines for performance levels. however, a universal strategy to evaluate
summarization systems is still absent.

in this survey, we primarily aim to investigate how empirical methods have been
used to build summarization systems. the rest of the paper is organized as fol-
lows: section 2 describes single-document summarization, focusing on extractive
techniques. section 3 progresses to discuss the area of multi-document summariza-
tion, where a few abstractive approaches that pioneered the    eld are also considered.
section 4 brie   y discusses some unconventional approaches that we believe can be
useful in the future of summarization research. section 5 elaborates a few eval-
uation techniques and describes some of the standards for evaluating summaries
automatically. finally, section 6 concludes the survey.

2 single-document summarization

usually, the    ow of information in a given document is not uniform, which means
that some parts are more important than others. the major challenge in summa-
rization lies in distinguishing the more informative parts of a document from the
less ones. though there have been instances of research describing the automatic
creation of abstracts, most work presented in the literature relies on verbatim ex-
traction of sentences to address the problem of single-document summarization. in

1see http://trec.nist.gov/.
2see http://duc.nist.gov/.
3see http://www.itl.nist.gov/iad/894.02/related projects/muc/proceedings/.

muc 7 toc.html

2

this section, we describe some eminent extractive techniques. first, we look at early
work from the 1950s and 60s that kicked o    research on summarization. second,
we concentrate on approaches involving machine learning techniques published in
the 1990s to today. finally, we brie   y describe some techniques that use a more
complex natural language analysis to tackle the problem.

2.1 early work

most early work on single-document summarization focused on technical documents.
perhaps the most cited paper on summarization is that of (luhn, 1958), that de-
scribes research done at ibm in the 1950s. in his work, luhn proposed that the
frequency of a particular word in an article provides an useful measure of its sig-
ni   cance. there are several key ideas put forward in this paper that have assumed
importance in later work on summarization. as a    rst step, words were stemmed to
their root forms, and stop words were deleted. luhn then compiled a list of content
words sorted by decreasing frequency, the index providing a signi   cance measure of
the word. on a sentence level, a signi   cance factor was derived that re   ects the
number of occurrences of signi   cant words within a sentence, and the linear distance
between them due to the intervention of non-signi   cant words. all sentences are
ranked in order of their signi   cance factor, and the top ranking sentences are    nally
selected to form the auto-abstract.

related work (baxendale, 1958), also done at ibm and published in the same
journal, provides early insight on a particular feature helpful in    nding salient parts
of documents: the sentence position. towards this goal, the author examined 200
paragraphs to    nd that in 85% of the paragraphs the topic sentence came as the    rst
one and in 7% of the time it was the last sentence. thus, a naive but fairly accurate
way to select a topic sentence would be to choose one of these two. this positional
feature has since been used in many complex machine learning based systems.

edmundson (1969) describes a system that produces document extracts. his
primary contribution was the development of a typical structure for an extractive
summarization experiment. at    rst, the author developed a protocol for creating
manual extracts, that was applied in a set of 400 technical documents. the two
features of word frequency and positional importance were incorporated from the
previous two works. two other features were used: the presence of cue words
(presence of words like signi   cant, or hardly), and the skeleton of the document
(whether the sentence is a title or heading). weights were attached to each of these
features manually to score each sentence. during evaluation, it was found that about
44% of the auto-extracts matched the manual extracts.

2.2 machine learning methods

in the 1990s, with the advent of machine learning techniques in nlp, a series of semi-
nal publications appeared that employed statistical techniques to produce document
extracts. while initially most systems assumed feature independence and relied on
naive-bayes methods, others have focused on the choice of appropriate features and

3

on learning algorithms that make no independence assumptions. other signi   cant
approaches involved id48 and id148 to improve ex-
tractive summarization. a very recent paper, in contrast, used neural networks and
third party features (like common words in search engine queries) to improve purely
extractive single document summarization. we next describe all these approaches
in more detail.

2.2.1 naive-bayes methods

kupiec et al. (1995) describe a method derived from edmundson (1969) that is able
to learn from data. the classi   cation function categorizes each sentence as worthy
of extraction or not, using a naive-bayes classi   er. let s be a particular sentence,
s the set of sentences that make up the summary, and f1, . . . , fk the features.
assuming independence of the features:

p (s     s | f1, f2, ..fk) =

(1)

(cid:81)k
i=1 p (fi | s     s)    p (s     s)

(cid:81)k

i=1 p (fi)

the features were compliant to (edmundson, 1969), but additionally included the
sentence length and the presence of uppercase words. each sentence was given a
score according to (1), and only the n top sentences were extracted. to evaluate
the system, a corpus of technical documents with manual abstracts was used in
the following way: for each sentence in the manual abstract, the authors manually
analyzed its match with the actual document sentences and created a mapping
(e.g. exact match with a sentence, matching a join of two sentences, not matchable,
etc.). the auto-extracts were then evaluated against this mapping. feature analysis
revealed that a system using only the position and the cue features, along with the
sentence length sentence feature, performed best.

aone et al. (1999) also incorporated a naive-bayes classi   er, but with richer
features. they describe a system called dimsum that made use of features like
term frequency (tf ) and inverse document frequency (idf) to derive signature words.4
the idf was computed from a large corpus of the same domain as the concerned
documents. statistically derived two-noun word collocations were used as units for
counting, along with single words. a named-entity tagger was used and each entity
was considered as a single token. they also employed some shallow discourse analysis
like reference to same entities in the text, maintaining cohesion. the references
were resolved at a very shallow level by linking name aliases within a document
like    u.s.    to    united states   , or    ibm    for    international business machines   .
synonyms and morphological variants were also merged while considering lexical
terms, the former being identi   ed by using id138 (miller, 1995). the corpora
used in the experiments were from newswire, some of which belonged to the trec
evaluations.

4words that indicate key concepts in a document.

4

2.2.2 rich features and id90

lin and hovy (1997) studied the importance of a single feature, sentence position.
just weighing a sentence by its position in text, which the authors term as the
   position method   , arises from the idea that texts generally follow a predictable
discourse structure, and that the sentences of greater topic centrality tend to occur in
certain speci   able locations (e.g. title, abstracts, etc). however, since the discourse
structure signi   cantly varies over domains, the position method cannot be de   ned
as naively as in (baxendale, 1958). the paper makes an important contribution by
investigating techniques of tailoring the position method towards optimality over a
genre and how it can be evaluated for e   ectiveness. a newswire corpus was used, the
collection of zi   -davis texts produced from the tipster5 program; it consists of
text about computer and related hardware, accompanied by a set of key topic words
and a small abstract of six sentences. for each document in the corpus, the authors
measured the yield of each sentence position against the topic keywords. they then
ranked the sentence positions by their average yield to produce the optimal position
policy (opp) for topic positions for the genre.

two kinds of evaluation were performed. previously unseen text was used for
testing whether the same procedure would work in a di   erent domain. the    rst
evaluation showed contours exactly like the training documents. in the second eval-
uation, word overlap of manual abstracts with the extracted sentences was measured.
windows in abstracts were compared with windows on the selected sentences and
corresponding precision and recall values were measured. a high degree of coverage
indicated the e   ectiveness of the position method.

in later work, lin (1999) broke away from the assumption that features are
independent of each other and tried to model the problem of sentence extraction
using id90, instead of a naive-bayes classi   er. he examined a lot of fea-
tures and their e   ect on sentence extraction. the data used in this work is a
publicly available collection of texts, classi   ed into various topics, provided by the
tipster-summac6 evaluations, targeted towards information retrieval systems.
the dataset contains essential text fragments (phrases, clauses, and sentences) which
must be included in summaries to answer some trec topics. these fragments were
each evaluated by a human judge. the experiments described in the paper are with
the summarist system developed at the university of southern california. the
system extracted sentences from the documents and those were matched against
human extracts, like most early work on extractive summarization.

some novel features were the query signature (normalized score given to sen-
tences depending on number of query words that they contain), ir signature (the
m most salient words in the corpus, similar to the signature words of (aone et al.,
1999)), numerical data (boolean value 1 given to sentences that contained a num-
ber in them), proper name (boolean value 1 given to sentences that contained a
proper name in them), pronoun or adjective (boolean value 1 given to sentences

5see http://www.itl.nist.gov/iaui/894.02/related_projects/tipster/.
6see http://www-nlpir.nist.gov/related_projects/tipster_summac/index.html.

5

that contained a pronoun or adjective in them), weekday or month (similar as pre-
vious feature) and quotation (similar as previous feature). it is worth noting that
some features like the query signature are question-oriented because of the setting
of the evaluation, unlike a generalized summarization framework.

the author experimented with various baselines, like using only the positional
feature, or using a simple combination of all features by adding their values. when
evaluated by matching machine extracted and human extracted sentences, the deci-
sion tree classi   er was clearly the winner for the whole dataset, but for three topics,
a naive combination of features beat it. lin conjectured that this happened because
some of the features were independent of each other. feature analysis suggested
that the ir signature was a valuable feature, corroborating the early    ndings of
luhn (1958).

2.2.3 id48

in contrast with previous approaches, that were mostly feature-based and non-
sequential, conroy and o   leary (2001) modeled the problem of extracting a sentence
from a document using a hidden markov model (id48). the basic motivation for
using a sequential model is to account for local dependencies between sentences.
only three features were used: position of the sentence in the document (built into
the state structure of the id48), number of terms in the sentence, and likeliness of
the sentence terms given the document terms.

figure 1: markov model to extract to three summary sentences from a document
(conroy and o   leary, 2001).

the id48 was structured as follows: it contained 2s + 1 states, alternating be-
tween s summary states and s+1 nonsummary states. the authors allowed    hesita-
tion    only in nonsummary states and    skipping next state    only in summary states.
figure 1 shows an example id48 with 7 nodes, corresponding to s = 3. using the
trec dataset as training corpus, the authors obtained the maximum-likelihood
estimate for each transition id203, forming the transition matrix estimate   m,
whose element (i, j) is the empirical id203 of transitioning from state i to j.
associated with each state i was an output function, bi(o) = pr(o | state i) where
o is an observed vector of features. they made a simplifying assumption that the
features are multivariate normal. the output function for each state was thus esti-
mated by using the training data to compute the maximum likelihood estimate of
its mean and covariance matrix. they estimated 2s + 1 means, but assumed that all
of the output functions shared a common covariance matrix. evaluation was done

6

no321nononoby comparing with human generated extracts.

2.2.4 id148

(cid:32)
log p (c) +(cid:88)

i

(cid:33)

osborne (2002) claims that existing approaches to summarization have always as-
sumed feature independence. the author used id148 to obviate this
assumption and showed empirically that the system produced better extracts than
a naive-bayes model, with a prior appended to both models. let c be a label, s
the item we are interested in labeling, fi the i-th feature, and   i the corresponding
feature weight. the conditional log-linear model used by osborne (2002) can be
stated as follows:

(cid:33)

  ifi(c, s)

,

(2)

where z(s) = (cid:80)

c exp ((cid:80)

p (c | s) =

1

z(s)

exp

i   ifi(c, s)). in this domain, there are only two possible
labels: either the sentence is to be extracted or it is not. the weights were trained
by conjugate id119. the authors added a non-uniform prior to the model,
claiming that a log-linear model tends to reject too many sentences for inclusion in
a summary. the same prior was also added to a naive-bayes model for comparison.
the classi   cation took place as follows:

(cid:32)(cid:88)

i

label(s) = arg max
c   c

p (c)    p (s, c) = arg max
c   c

  ifi(c, s)

.

(3)

the authors optimized the prior using the f2 score of the classi   er as an objective
function on a part of the dataset (in the technical domain). the summaries were
evaluated using the standard f2 score where f2 = 2pr
p+r , where the precision and recall
measures were measured against human generated extracts. the features included
word pairs (pairs of words with all words truncated to ten characters), sentence
length, sentence position, and naive discourse features like inside introduction or
inside conclusion. with respect to f2 score, the log-linear model outperformed the
naive-bayes classi   er with the prior, exhibiting the former   s e   ectiveness.

2.2.5 neural networks and third party features

in 2001-02, duc issued a task of creating a 100-word summary of a single news
article. however, the best performing systems in the evaluations could not outper-
form the baseline with statistical signi   cance. this extremely strong baseline has
been analyzed by nenkova (2005) and corresponds to the selection of the    rst n
sentences of a newswire article. this surprising result has been attributed to the
journalistic convention of putting the most important part of an article in the initial
paragraphs. after 2002, the task of single-document summarization for newswire
was dropped from duc. svore et al. (2007) propose an algorithm based on neu-
ral nets and the use of third party datasets to tackle the problem of extractive
summarization, outperforming the baseline with statistical signi   cance.

7

the authors used a dataset containing 1365 documents gathered from id98.com,
each consisting of the title, timestamp, three or four human generated story high-
lights and the article text. they considered the task of creating three machine
highlights. the human generated highlights were not verbatim extractions from the
article itself. the authors evaluated their system using two metrics: the    rst one
concatenated the three highlights produced by the system, concatenated the three
human generated highlights, and compared these two blocks; the second metric con-
sidered the ordering and compared the sentences on an individual level.

svore et al. (2007) trained a model from the labels and the features for each
sentence of an article, that could infer the proper ranking of sentences in a test
document. the ranking was accomplished using ranknet (burges et al., 2005), a
pair-based neural network algorithm designed to rank a set of inputs that uses the
id119 method for training. for the training set, they used id8-1
(lin, 2004) to score the similarity of a human written highlight and a sentence
in the document. these similarity scores were used as soft labels during training,
contrasting with other approaches where sentences are    hard-labeled   , as selected
or not.

some of the used features based on position or id165s frequencies have been
observed in previous work. however, the novelty of the framework lay in the use
of features that derived information from query logs from microsoft   s news search
engine7 and wikipedia8 entries. the authors conjecture that if a document sentence
contained keywords used in the news search engine, or entities found in wikipedia
articles, then there is a greater chance of having that sentence in the highlight. the
extracts were evaluated using id8-1 and id8-2, and showed statistically
signi   cant improvements over the baseline of selecting the    rst three sentences in a
document.

2.3 deep natural language analysis methods

in this subsection, we describe a set of papers that detail approaches towards single-
document summarization involving complex natural language analysis techniques.
none of these papers solve the problem using machine learning, but rather use a set
of heuristics to create document extracts. most of these techniques try to model the
text   s discourse structure.

barzilay and elhadad (1997) describe a work that used considerable amount of
linguistic analysis for performing the task of summarization. for a better under-
standing of their method, we need to de   ne a lexical chain: it is a sequence of related
words in a text, spanning short (adjacent words or sentences) or long distances (en-
tire text). the authors    method progressed with the following steps: segmentation
of the text, identi   cation of lexical chains, and using strong lexical chains to identify
the sentences worthy of extraction. they tried to reach a middle ground between
(mckeown and radev, 1995) and (luhn, 1958) where the former relied on deep

7see http://search.live.com/news.
8see http://en.wikipedia.org.

8

semantic structure of the text, while the latter relied on word statistics of the doc-
uments. the authors describe the notion of cohesion in text as a means of sticking
together di   erent parts of the text. lexical cohesion is a notable example where
semantically related words are used. for example, let us take a look at the following
sentence.9

john bought a jag. he loves the car.

(4)
here, the word car refers to the word jag in the previous sentence, and exempli   es
lexical cohesion. the phenomenon of cohesion occurs not only at the word level,
but at word sequences too, resulting in lexical chains, which the authors used as
a source representation for summarization. semantically related words and word
sequences were identi   ed in the document, and several chains were extracted, that
form a representation of the document. to    nd out lexical chains, the authors used
id138 (miller, 1995), applying three generic steps:

1. selecting a set of candidate words.

2. for each candidate word,    nding an appropriate chain relying on a relatedness

criterion among members of the chains,

3. if it is found, inserting the word in the chain and updating it accordingly.

the relatedness was measured in terms of id138 distance. simple nouns and
noun compounds were used as starting point to    nd the set of candidates. in the
   nal steps, strong lexical chains were used to create the summaries. the chains were
scored by their length and homogeneity. then the authors used a few heuristics to
select the signi   cant sentences.

in another paper, ono et al. (1994) put forward a computational model of dis-
course for japanese expository writings, where they elaborate a practical procedure
for extracting the discourse rhetorical structure, a binary tree representing relations
between chunks of sentences (rhetorical structure trees are used more intensively in
(marcu, 1998a), as we will see below). this structure was extracted using a series
of nlp steps: sentence analysis, rhetorical id36, segmentation, can-
didate generation and preference judgement. evaluation was based on the relative
importance of rhetorical relations. in the following step, the nodes of the rhetori-
cal structure tree were pruned to reduce the sentence, keeping its important parts.
same was done for paragraphs to    nally produce the summary. evaluation was done
with respect to sentence coverage and 30 editorial articles of a japanese newspaper
were used as the dataset. the articles had corresponding sets of key sentences and
most important key sentences judged by human subjects. the key sentence coverage
was about 51% and the most important key sentence coverage was 74%, indicating
encouraging results.

marcu (1998a) describes a unique approach towards summarization that, unlike
most other previous work, does not assume that the sentences in a document form
a    at sequence. this paper used discourse based heuristics with the traditional

9example from http://www.cs.ucd.ie/staff/jcarthy/home/lex.html.

9

features that have been used in the summarization literature. the discourse theory
used in this paper is the rhetorical structure theory (rst) that holds between
two non-overlapping pieces of text spans: the nucleus and the satellite. the author
mentions that the distinction between nuclei and satellites comes from the empir-
ical observation that the nucleus expresses what is more essential to the writer   s
purpose than the satellite; and that the nucleus of a rhetorical relation is compre-
hensible independent of the satellite, but not vice versa. marcu (1998b) describes
the details of a rhetorical parser producing a discourse tree. figure 2 shows an
example discourse tree for a text example detailed in the paper. once such a dis-

figure 2: example of a discourse tree from marcu (1998a). the numbers in the
nodes denote sentence numbers from the text example. the text below the number
in selected nodes are rhetorical relations. the dotted nodes are satellites and
the normals ones are the nuclei.

course structure is created, a partial ordering of important units can be developed
from the tree. each equivalence class in the partial ordering is derived from the
new sentences at a particular level of the discourse tree. in figure 2, we observe
that sentence 2 is at the root, followed by sentence 8 in the second level. in the
third level, sentence 3 and 10 are observed, and so forth. the equivalence classes
are 2 > 8 > 3, 10 > 1, 4, 5, 7, 9 > 6.

if it is speci   ed that the summary should contain the top k% of the text, the    rst
k% of the units in the partial ordering can be selected to produce the summary. the
author talks about a summarization system based just on this method in (marcu,
1998b) and in one of his earlier papers.
in this paper, he merged the discourse
based heuristics with traditional heuristics. the metrics used were id91 based

10

antithesis2elaborationelaboration22elaboration3justification8exemplification12345784581091056contrastevidenceconcessionmetric (each node in the discourse tree was assigned a cluster score; for leaves the
score was 0, for the internal nodes it was given by the similarity of the immediate
children; discourse tree a was chosen to be better than b if its id91 score
was higher), marker based metric (a discourse structure a was chosen to be better
than a discourse structure b if a used more rhetorical relations than b), rhetorical
id91 based technique (measured the similarity between salient units of two text
spans), shape based metric (preferred a discourse tree a over b if a was more skewed
towards the right than b), title based metric, position based metric, connectedness
based metric (cosine similarity of an unit to all other text units, a discourse structure
a was chosen to be better than b if its connectedness measure was more than b).
a weighted linear combination of all these scores gave the score of a discourse
structure. to    nd the best combination of heuristics, the author computed the
weights that maximized the f-score on the training dataset, which was constituted
by newswire articles. to do this, he used a gsat-like algorithm (selman et al.,
1992) that performed a greedy search in a seven dimensional space of the metrics.
for a part of his corpus (the trec dataset), a best f-score of 75.42% was achieved
for the 10% summaries which was 3.5% higher than a baseline lead based algorithm,
which was very encouraging.

3 id57

extraction of a single summary from multiple documents has gained interest since
mid 1990s, most applications being in the domain of news articles. several web-
based news id91 systems were inspired by research on multi-document summa-
rization, for example google news,10 columbia newsblaster,11 or news in essence.12
this departs from single-document summarization since the problem involves mul-
tiple sources of information that overlap and supplement each other, being contra-
dictory at occasions. so the key tasks are not only identifying and coping with
redundancy across documents, but also recognizing novelty and ensuring that the
   nal summary is both coherent and complete.

the    eld seems to have been pioneered by the nlp group at columbia university
(mckeown and radev, 1995), where a summarization system called summons13
was developed by extending already existing technology for template-driven message
understanding systems. although in that early stage multi-document summariza-
tion was mainly seen as a task requiring substantial capabilities of both language
interpretation and generation, it later gained autonomy, as people coming from dif-
ferent communities added new perspectives to the problem. extractive techniques
have been applied, making use of similarity measures between pairs of sentences.
approaches vary on how these similarities are used: some identify common themes
through id91 and then select one sentence to represent each cluster (mckeown

10see http://news.google.com.
11see http://newsblaster.cs.columbia.edu.
12see http://newsinessence.com.
13summarizing online news articles.

11

et al., 1999; radev et al., 2000), others generate a composite sentence from each
cluster (barzilay et al., 1999), while some approaches work dynamically by includ-
ing each candidate passage only if it is considered novel with respect to the previous
included passages, via maximal marginal relevance (carbonell and goldstein, 1998).
some recent work extends id57 to multilingual environ-
ments (evans, 2005).

the way the problem is posed has also varied over time. while in some pub-
lications it is claimed that extractive techniques would not be e   ective for multi-
document summarization (mckeown and radev, 1995; mckeown et al., 1999), some
years later that claim was overturned, as extractive systems like mead14 (radev
et al., 2000) achieved good performance in large scale summarization of news arti-
cles. this can be explained by the fact that summarization systems often distinguish
among themselves about what their goal actually is. while some systems, like sum-
mons, are designed to work in strict domains, aiming to build a sort of brie   ng
that highlights di   erences and updates accross di   erent news reports, putting much
emphasis on how information is presented to the user, others, like mead, are large
scale systems that intend to work in general domains, being more concerned with
information content rather than form. consequently, systems of the former kind re-
quire a strong e   ort on language generation to produce a grammatical and coherent
summary, while latter systems are probably more close to the information retrieval
paradigm. abstractive systems like summons are di   cult to replicate, as they
heavily rely on the adaptation of internal tools to perform information extraction
and language generation. on the other hand, extractive systems are generally easy
to implement from scratch, and this makes them appealing when sophisticated nlp
tools are not available.

3.1 abstraction and information fusion

as far as we know, summons (mckeown and radev, 1995; radev and mckeown,
1998) is the    rst historical example of a id57 system. it
tackles single events about a narrow domain (news articles about terrorism) and
produces a brie   ng merging relevant information about each event and how reports
by di   erent news agencies have evolved over time. the whole thread of reports is
then presented, as illustrated in the following example of a    good    summary:

   in the afternoon of february 26, 1993, reuters reported that a suspect
bomb killed at least    ve people in the world trade center. however,
associated press announced that exactly    ve people were killed in the
blast. finally, associated press announced that arab terrorists were
possibly responsible for the terrorist act.   

rather than working with raw text, summons reads a database previously
built by a template-based message understanding system. a full multi-document

14available for download at http://www.summarization.com/mead/.

12

summarizer is built by concatenating the two systems,    rst processing full text as
input and    lling template slots, and then synthesizing a summary from the extracted
information. the architecture of summons consists of two major components: a
content planner that selects the information to include in the summary through
combination of the input templates, and a linguistic generator that selects the right
words to express the information in grammatical and coherent text. the latter
component was devised by adapting existing language generation tools, namely the
fuf/surge system15. content planning, on the other hand, is made through
summary operators, a set of heuristic rules that perform operations like    change of
perspective   ,    contradiction   ,    re   nement   , etc. some of these operations require
resolving con   icts, i.e., contradictory information among di   erent sources or time
instants; others complete pieces of information that are included in some articles
and not in others, combining them into a single template. at the end, the linguis-
tic generator gathers all the combined information and uses connective phrases to
synthesize a summary.

while this framework seems promising when the domain is narrow enough so that
the templates can be designed by hand, a generalization for broader domains would
be problematic. this was improved later by mckeown et al. (1999) and barzilay
et al. (1999), where the input is now a set of related documents in raw text, like
those retrieved by a standard search engine in response to a query. the system starts
by identifying themes, i.e., sets of similar text units (usually paragraphs). this is
formulated as a id91 problem. to compute a similarity measure between text
units, these are mapped to vectors of features, that include single words weighted
by their tf-idf scores, noun phrases, proper nouns, synsets from the id138
database and a database of semantic classes of verbs. for each pair of paragraphs, a
vector is computed that represents matches on the di   erent features. decision rules
that were learned from data are then used to classify each pair of text units either
as similar or dissimilar; this in turn feeds a subsequent algorithm that places the
most related paragraphs in the same theme.

once themes are identi   ed, the system enters its second stage: information fu-
sion. the goal is to decide which sentences of a theme should be included in the
summary. rather than just picking a sentence that is a group representative, the
authors propose an algorithm which compares and intersects predicate argument
structures of the phrases within each theme to determine which are repeated often
enough to be included in the summary. this is done as follows:    rst, sentences are
parsed through collins    statistical parser (collins, 1999) and converted into depen-
dency trees, which allows capturing the predicate-argument structure and identify
functional roles. determiners and auxiliaries are dropped; fig. 3 shows a sentence
representation.

the comparison algorithm then traverses these dependency trees recursively,
adding identical nodes to the output tree. once full phrases (a verb with at least
two constituents) are found, they are marked to be included in the summary. if two

15fuf, surge, and other tools developed by the columbia nlp group are available at

http://www1.cs.columbia.edu/nlp/tools.cgi.

13

figure 3: dependency tree representing the sentence    mcveigh, 27, was charged
with the bombing    (extracted from (mckeown et al., 1999)).

phrases, rooted at some node, are not identical but yet similar, the hypothesis that
they are paraphrases of each other is considered; to take this into account, corpus-
driven id141 rules are written to allow paraphrase intersection.16 once the
summary content (represented as predicate-argument structures) is decided, a gram-
matical text is generated by translating those structures into the arguments expected
by the fuf/surge language generation system.

3.2 topic-driven summarization and mmr

carbonell and goldstein (1998) made a major contribution to topic-driven sum-
marization by introducing the maximal marginal relevance (mmr) measure. the
idea is to combine query relevance with information novelty; it may be applicable
in several tasks ranging from text retrieval to topic-driven summarization. mmr
simultaneously rewards relevant sentences and penalizes redundant ones by consid-
ering a linear combination of two similarity measures.

let q be a query or user pro   le and r a ranked list of documents retrieved by
a search engine. consider an incremental procedure that selects documents, one at
a time, and adds them to a set s. so let s be the set of already selected documents
in a particular step, and r \ s the set of yet unselected documents in r. for each
candidate document di     r \ s, its marginal relevance mr(di) is computed as:

mr(di) :=   sim1(di, q)     (1       ) max
dj   s

sim2(di, dj)

(5)

where    is a parameter lying in [0, 1] that controls the relative importance given
to relevance versus redundancy. sim1 and sim2 are two similarity measures; in the

16a full description of the kind of id141 rules used can be found in (barzilay et al.,
1999). examples are: ordering of sentence components, main clause vs. relative clause, realization
in di   erent syntactic categories (e.g. classi   er vs. apposition), change in grammatical features
(active/passive, time, number, etc.), head omission, transformation from one pos to another,
using semantically related words (e.g. synonyms), etc.

14

andkan1998].wematchtwoverbsthatsharethesamesemanticclassinthisclassi   cation.inadditiontotheaboveprimitivefeaturesthatallcom-paresingleitemsfromeachtextunit,weusecompositefea-turesthatcombinepairsofprimitivefeatures.ourcompos-itefeaturesimposeparticularconstraintsontheorderofthetwoelementsinthepair,onthemaximumdistancebetweenthetwoelements,andonthesyntacticclassesthatthetwoelementscomefrom.theycanvaryfromasimplecom-bination(e.g.,   twotextunitsmustsharetwowordstobesimilar   )tocomplexcaseswithmanyconditions(e.g.,   twotextunitsmusthavematchingnounphrasesthatappearinthesameorderandwithrelativedifferenceinpositionnomorethan   ve   ).inthismanner,wecaptureinformationonhowsimilarlyrelatedelementsarespacedoutinthetwotextunits,aswellassyntacticinformationonwordcombi-nations.matchesoncompositefeaturesindicatecombinedevidenceforthesimilarityofthetwounits.todeterminewhethertheunitsmatchoverall,weemployamachinelearningalgorithm[cohen1996]thatinducesde-cisionrulesusingthefeaturesthatreallymakeadifference.asetofpairsofunitsalreadymarkedassimilarornotbyahumanisusedfortrainingtheclassi   er.wehavemanuallymarkedasetof8,225paragraphcomparisonsfromthetdtcorpusfortrainingandevaluatingoursimilarityclassi   er.forcomparison,wealsouseanimplementationofthetf*idfmethodwhichisstandardformatchingtextsinin-formationretrieval.wecomputethetotalfrequency(tf)ofwordsineachtextunitandthenumberofunitsinourtrain-ingseteachwordappearsin(df,ordocumentfrequency).theneachtextunitisrepresentedasavectoroftf*idfscores,calculatedastf(wordi)  logtotalnumberofunitsdf(wordi)similaritybetweentextunitsismeasuredbythecosineoftheanglebetweenthecorrespondingtwovectors(i.e.,thenormalizedinnerproductofthetwovectors),andtheopti-malvalueofathresholdforjudgingtwounitsassimilariscomputedfromthetrainingset.afterallpairwisesimilaritiesbetweentextunitshavebeencalculated,weutilizeaid91algorithmtoiden-tifythemes.asaparagraphmaybelongtomultiplethemes,moststandardid91algorithms,whichpartitiontheirinputset,arenotsuitableforourtask.weuseagreedy,one-passalgorithmthat   rstconstructsgroupsfromthemostsimilarparagraphs,seedingthegroupswiththefullycon-nectedsubcomponentsofthegraphthatthesimilarityrela-tionshipinducesoverthesetofparagraphs,andthenplacesadditionalparagraphswithinagroupifthefractionofthemembersofthegrouptheyaresimilartoexceedsapresetthreshold.languagegenerationgivenagroupofsimilarparagraphs   atheme   theprob-lemistocreateaconciseand   uentfusionofinformationinthistheme,re   ectingfactscommontoallparagraphs.astraightforwardmethodwouldbetopickarepresentativesubjectclass: noun27class: cardinalbombing class: nounmcveighwithclass: prepositiondefinite: yeschargeclass: verbvoice :passivepolarity: +tense: pastfigure4:dependencygrammarrepresentationofthesen-tence   mcveigh,27,waschargedwiththebombing   .sentencethatmeetssomecriteria(e.g.,athresholdnumberofcommoncontentwords).inpractice,however,anyrepre-sentativesentencewillusuallyincludeembeddedphrase(s)containinginformationthatisnotcommontoallsentencesinthetheme.furthermore,othersentencesinthethemeof-tencontainadditionalinformationnotpresentedintherep-resentativesentence.ourapproach,therefore,usesinter-sectionamongthemesentencestoidentifyphrasescommontomostparagraphsandthengeneratesanewsentencefromidenti   edphrases.intersectionamongthemesentencesintersectioniscarriedoutinthecontentplanner,whichusesaparserforinterpretingtheinputsentences,withournewworkfocusingonthecomparisonofphrases.themesen-tencesare   rstrunthroughastatisticalparser[collins1996]andthen,inordertoidentifyfunctionalroles(e.g.,subject,object),areconvertedtoadependencygrammarrepresenta-tion[kittredgeandmel     cuk1983],whichmakespredicate-argumentstructureexplicit.wedevelopedarule-basedcomponenttoproducefunc-tionalroles,whichtransformsthephrase-structureoutputofcollins   parsertodependencygrammar;functionwords(de-terminersandauxiliaries)areeliminatedfromthetreeandcorrespondingsyntacticfeaturesareupdated.anexampleofathemesentenceanditsdependencygrammarrepresen-tationareshowninfigure4.eachnon-auxiliarywordinthesentencehasanodeintherepresentation,andthisnodeisconnectedtoitsdirectdependents.thecomparisonalgorithmstartswithallsubtreesrootedatverbsfromtheinputdependencystructure,andtraversesthemrecursively:iftwonodesareidentical,theyareaddedtotheoutputtree,andtheirchildrenarecompared.onceafullphrase(verbwithatleasttwoconstituents)hasbeenfound,itiscon   rmedforinclusioninthesummary.dif   cultiesarisewhentwonodesarenotidentical,butaresimilar.suchphrasesmaybeparaphrasesofeachotherandstillconveyessentiallythesameinformation.sincethemesentencesareaprioriclosesemantically,thissigni   cantlyexperiments both were set to the standard cosine similarity traditionally used in the
vector space model, sim1(x, y) = sim2(x, y) = (cid:104)x,y(cid:105)
(cid:107)x(cid:107)  (cid:107)y(cid:107). the document achieving the
highest marginal relevance, dmmr = arg maxdi   r\s mr(di), is then selected, i.e.,
added to s, and the procedure continues until a maximum number of documents
are selected or a minimum relevance threshold is attained. carbonell and goldstein
(1998) found experimentally that choosing dynamically the value of    turns out to be
more e   ective than keeping it    xed, namely starting with small values (       0.3) to
give more emphasis to novelty, and then increasing it (       0.7) to focus on the most
relevant documents. to perform summarization, documents can be    rst segmented
into sentences or paragraphs, and after a query is submitted, the mmr algorithm
can be applied followed by a selection of the top ranking passages, reordering them as
they appeared in the original documents, and presenting the result as the summary.
one of the attractive points in using mmr for summarization is its topic-oriented
feature, through its dependency on the query q, which makes it particularly ap-
pealing to generate summaries according to a user pro   le: as the authors claim,    a
di   erent user with di   erent information needs may require a totally di   erent sum-
mary of the same document.    this assertion was not being taken into account by
previous id57 systems.

3.3 graph spreading activation

mani and bloedorn (1997) describe an information extraction framework for sum-
marization, a graph-based method to    nd similarities and dissimilarities in pairs
of documents. albeit no textual summary is generated, the summary content is
represented via entities (concepts) and relations that are displayed respectively as
nodes and edges of a graph. rather than extracting sentences, they detect salient
regions of the graph via a spreading activation technique.17

this approach shares with the method described in section 3.2 the property
of being topic-driven; there is an additional input that stands for the topic with
respect to which the summary is to be generated. the topic is represented through
a set of entry nodes in the graph. a document is represented as a graph as follows:
each node represents the occurrence of a single word (i.e., one word together with
its position in the text). each node can have several kinds of links: adjacency
links (adj) to adjacent words in the text, same links to other occurrences of the
same word, and alpha links encoding semantic relationships captured through
id138 and netowl18. besides these, phrase links tie together sequences of
adjacent nodes which belong to the same phrase, and name and coref links
stand for co-referential name occurrences; fig. 4 shows some of these links.

once the graph is built, topic nodes are identi   ed by stem comparison and be-
come the entry nodes. a search for semantically related text is then propagated from
these to the other nodes of the graph, in a process called spreading activation. salient

17the name    spreading activation    is borrowed from a method used in information retrieval

(salton and buckley, 1988) to expand the search vocabulary.

18see http://www.netowl.com.

15

figure 4: examples of nodes and links in the graph for a particular sentence (detail
extracted from from a    gure in (mani and bloedorn, 1997)).

words and phrases are initialized according to their tf-idf score. the weight of
neighboring nodes depends on the node link traveled and is an exponentially decay-
ing function of the distance of the traversed path. traveling within a sentence is
made cheaper than across sentence boundaries, which in turn is cheaper than across
paragraph boundaries. given a pair of document graphs, common nodes are identi-
   ed either by sharing the same stem or by being synonyms. analogously, di   erence
nodes are those that are not common. for each sentence in both documents, two
scores are computed: one score that re   ects the presence of common nodes, which
is computed as the average weight of these nodes; and another score that computes
instead the average weights of di   erence nodes. both scores are computed after
spreading activation. in the end, the sentences that have higher common and dif-
ferent scores are highlighted, the user being able to specify the maximal number of
common and di   erent sentences to control the output. in the future, the authors
expect to use these structure to actually compose abstractive summaries, rather
than just highlighting pieces of text.

3.4 centroid-based summarization

although id91 techniques were already being employed by mckeown et al.
(1999) and barzilay et al. (1999) for identi   cation of themes, radev et al. (2000)
pioneered the use of cluster centroids to play a central role in summarization. a full
description of the centroid-based approach that underlies the mead system can
be found in (radev et al., 2004); here we sketch brie   y the main points. perhaps
the most appealing feature is the fact that it does not make use of any language
generation module, unlike most previous systems. all documents are modeled as
bags-of-words. the system is also easily scalable and domain-independent.

the    rst stage consists of topic detection, whose goal is to group together news
articles that describe the same event. to accomplish this task, an agglomerative
id91 algorithm is used that operates over the tf-idf vector representations
of the documents, successively adding documents to clusters and recomputing the

16

1.39:aoki, thejapanese ambassador,said intelephone callstofujimori.japanesebroadcaster nhk that therebelswanted to talk directly to1.43:according to some estimates, only acouple hundredarmedfollowersremain.2.19they arefreeingus toshownot doing us anyharm,"said onewoman.1.12:policesaid theyslippedthroughsecuritydrivinginto thecompoundwithchampagneandbyposingaswaiters,hors d   oeuvres.associated pressreuters...2.27:although the mrta gained support in itsearly days in the mid-1980s as a robingive to the poor, it lost public sympathy afterturning increasingly to kidnapping, bombingbillion in damage to the country   s infrastructuresince 1980.and drug activities.2.28:guerillaconflicts inperuhave cost at least 30,000 lives and $25...close ties with japan.1.33: among the hostages were japanese ambassador morihisa aoki andthe ambassadors of brazil, bolivia, cuba, canada, south korea,.........2.26:the mrta called tuesday   s"breakingthesilence."1.32: president alberto fujimori, who is of japanese ancestry, has hadgermany, austria and venezuela.operationhood-style movement that robbed the rich to2.1: peru rebels hold 200 in japanese2.3:lima - heavily armed guerrillas threatenedfrom within the embassy residence.2.13:the rebels said they had 400 to 500ambassador   s homerebels.peruvian government freed imprisoned fellow2.2:by andrew cawthorneon wednesday to kill at least 200 hostages,japanese ambassador   s residence unless themany of them high-ranking officials, held at thewas imprisoned in 1992. 2:14 they also calledfor a review of peru   s judicial system and directnegotiations with the government beginning atdawn on wednesday.......2.22:the attack was a major blow tofujimori   s government, which had claimedvirtual victory in a 16-year war on communistrebels belonging to the mrta and the largerand better-known maoist shining path.1.2: copyright nando.net copyright the associated press1.3: *u.s. ambassador not among hostages in peru1.4:*peru embassy attackers thought defeated in 19921.5:lima, peru(dec 18, 1996 05:54 a.m. est) well-armed guerillasposing as waiters and carrying bottles of champagne sneaked into aglittering reception and seized hundreds of diplomats and other guests.1.6:as police ringed the building early wednesday, an excited rebel...compound at the start of the reception, which was in honor of japaneseemperor akihito   s birthday.......1.28:many leaders of the tupac amaru which is smaller than peru   swas captured in june 1992 and is serving a life sentence, as is his2.4:"if they do not release our prisoners, wewill all die in here," a guerrilla from thecomrades in jail and said their highest prioritywas release of victor polay, their leader whomovement (mrta) told a local radio stationcuban-inspired tupac amaru revolutionarysoon after her release that she had been eating and drinking in an elegantus:    don   t lift your heads up or you will be shot."1.19:adjhostages," a rebel who did not give his name told  a local radio station ina telephone call from inside the compound."theguerillasstalked around theresidencegroundsthreateninglieutenant, peter cardenas. that they aretopic: tupac amaru1.1:rebels in peru hold hundreds of hostages inside japanese diplomaticresidencethreatened to start killing the hostages.1.11:the group of 23 rebels, including three women entered the1.17:another guest, bbc correspondant sally bowen said in a reportmarquee on the lawn when the explosions occurred....1.25: "we are clear: the liberation of all our comrades, or we die with all the1.30:othertop commanders conceded defeatjuly1993.and surrendered in...corefcorefsamemaoist shining path movement are in jail. 1.29:its chief, victor polay,adj1.38:fujimoriwhose sister was among theanemergency cabinet meetingtoday.hostages released, calledalphaadj, the rebels threatened to kill the remaining captives.1.24:early wednesdayfigure5:textsoftworelatedarticles.thetop5salientsentencescontainingcommonwordshavethesecommonwordsinboldface;likewise,thetop5salientsentencescontaininguniquewordshavetheseuniquewordsinitalics.centroids according to

(cid:80)

d   cj
|cj|

  d

cj =

(6)

where cj is the centroid of the j-th cluster, cj is the set of documents that belong
to that cluster, its cardinality being |cj|, and   d is a    truncated version    of d that
vanishes on those words whose tf-idf scores are below a threshold. centroids
can thus be regarded as pseudo-documents that include those words whose tf-
idf scores are above a threshold in the documents that constitute the cluster. each
event cluster is a collection of (typically 2 to 10) news articles from multiple sources,
chronologically ordered, describing an event as it develops over time.

the second stage uses the centroids to identify sentences in each cluster that
are central to the topic of the entire cluster. in (radev et al., 2000), two metrics
are de   ned that resemble the two summands in the mmr (see section 3.2): cluster-
based relative utility (cbru) and cross-sentence informational subsumption (csis).
the    rst accounts for how relevant a particular sentence is to the general topic of
the entire cluster; the second is a measure of redundancy among sentences. unlike
mmr, these metrics are not query-dependent. given one cluster c of documents
segmented into n sentences, and a compression rate r, a sequence of nr sentences
are extracted in the same order as they appear in the original documents, which in
turn are ordered chronologically. the selection of the sentences is made by approx-
imating their cbru and csis.19 for each sentence si, three di   erent features are
used:

    its centroid value (ci), de   ned as the sum of the centroid values of all the

words in the sentence,

    a positional value (pi), that is used to make leading sentences more important.
let cmax be the centroid value of the highest ranked sentence in the document.
then pi = n   i+1

n cmax.

    the    rst-sentence overlap (fi), de   ned as the inner product between the word
occurrence vector of sentence i and that of the    rst sentence of the document.

the    nal score of each sentence is a combination of the three scores above minus a
redundancy penalty (rs) for each sentence that overlaps highly ranked sentences.

3.5 multilingual id57

evans (2005) addresses the task of summarizing documents written in multiple
languages; this had already been sketched by hovy and lin (1999). multilingual
summarization is still at an early stage, but this framework looks quite useful for
newswire applications that need to combine information from foreign news agen-
cies. evans (2005) considered the scenario where there is a preferred language in
which the summary is to be written, and multiple documents in the preferred and

19the two metrics are used directly for evaluation (see (radev et al., 2004) for more details).

17

in foreign languages are available. in their experiments, the preferred language was
english and the documents are news articles in english and arabic. the rationale is
to summarize the english articles without discarding the information contained in
the arabic documents. the ibm   s id151 system is    rst ap-
plied to translate the arabic documents to english. then a search is made, for each
translated text unit, to see whether there is a similar sentence or not in the english
documents. if so, and if the sentence is found relevant enough to be included in the
summary, the similar english sentence is included instead of the arabic-to-english
translation. this way, the    nal summary is more likely to be grammatical, since
machine translation is known to be far from perfect. on the other hand, the result
is also expected to have higher coverage than using just the english documents,
since the information contained in the arabic documents can help to decide about
the relevance of each sentence. in order to measure similarity between sentences, a
tool named simfinder 20 was employed: this is a tool for id91 text based on
similarity over a variety of lexical and syntactic features using a log-id75
model.

4 other approaches to summarization

this section describes brie   y some unconventional approaches that, rather than
aiming to build full summarization systems, investigate some details that underlie
the summarization process, and that we conjecture to have a role to play in future
research on this    eld.

4.1 short summaries

witbrock and mittal (1999) claim that extractive summarization is not very pow-
erful in that the extracts are not concise enough when very short summaries are
required. they present a system that generated headline style summaries. the cor-
pus used in this work was newswire articles from reuters and the associated press,
publicly available at the ldc21. the system learned statistical models of the rela-
tionship between source text units and headline units. it attempted to model both
the order and the likelihood of the appearance of tokens in the target documents.
both the models, one for content selection and the other for surface realization were
used to co-constrain each other during the search in the summary generation task.
for content selection, the model learned a translation model between a docu-
ment and its summary (brown et al., 1993). this model in the simplest case can be
thought as a mapping between a word in the document and the likelihood of some
word appearing in the summary. to simplify the model, the authors assumed that
the id203 of a word appearing in a summary is independent of its structure.
this mapping boils down to the fact that the id203 of a particular summary

20see http://www1.cs.columbia.edu/nlp/tools.cgi#simfinder.
21see http://ldc.upenn.edu.

18

candidate is the product of the probabilities of the summary content and that con-
tent being expressed using a particular structure.

the surface realization model used was a bigram model. viterbi id125
was used to e   ciently    nd a near-optimal summary. the markov assumption was
violated by using backtracking at every state to strongly discourage paths that
repeated terms, since bigrams that start repeating often seem to pathologically
overwhelm the search otherwise. to evaluate the system, the authors compared
its output against the actual headlines for a set of input newswire stories. since
phrasing could not be compared, they compared the generated headlines against
the actual headlines, as well as the top ranked summary sentence of the story. since
the system did not have a mechanism to determine the optimal length of a headline,
six headlines for each story were generated, ranging in length from 4 to 10 words
and they measured the term-overlap between each of the generated headlines and
the test. for headline length 4, there was 0.89 overlap in the headline and there was
0.91 overlap amongst the top scored sentence, indicating useful results.

4.2 sentence compression

knight and marcu (2000) introduced a statistical approach to sentence compression.
the authors believe that understanding the simpler task of compressing a sentence
may be a fruitful    rst step to later tackle the problems of single and multi-document
summarization.

sentence compression is de   ned as follows: given a sequence of words w =
w1w2 . . . wn that constitute a sentence,    nd a subsequence wi1wi2 . . . wik, with
1     i1 < i2 < . . . ik     n, that is a compressed version of w . note that there
are 2n possibilities of output. knight and marcu (2000) considered two di   erent
approaches: one that is inspired by the noisy-channel model, and another one based
on id90. due to its simplicity and elegance, we describe the    rst approach
here.

the noisy-channel model considers that one starts with a short summary s,
drawn according to the source model p (s), which is then subject to channel noise to
become the full sentence t, in a process guided by the channel model p (t|s). when
the string t is observed, one wants to recover the original summary according to:

  s = arg max

s

p (s|t) = arg max

s

p (s)p (t|s).

(7)

this model has the advantage of decoupling the goals of producing a short text that
looks grammatical (incorporated in the source model) and of preserving important
information (which is done through the channel model).
in (knight and marcu,
2000), the source and channel models are simple models inspired by probabilistic
context-free grammars (pid18s). the following id203 mass functions are de-
   ned over parse trees rather than strings: ptree(s), the id203 of a parse tree
that generates s, and pexpand tree(t|s), the id203 that a small parse tree that
generates s is expanded to a longer one that generates t.

19

the sentence t is    rst parsed by using collins    parser (collins, 1999). then,
rather than computing ptree(s) over all the 2n hypotheses for s, which would be
exponential in the sentence length, a shaded-forest structure is used: the parse
tree of t is traversed and the grammar (learned from the id3222) is used
to check recursively which nodes may be removed from each production in order
to achieve another valid production. this algorithm allows to compute e   ciently
ptree(s) and pexpand tree(t|s) for all possible grammatical summaries s. conceptually,
the id87 works the other way around: summaries are the original
strings that are expanded via expansion templates. expansion operations have the
e   ect of decreasing the id203 pexpand tree(t|s). the probabilities ptree(s) and
pexpand tree(t|s) consist in the usual factorized expression for pid18s times a bigram
distribution over the leaves of the tree (i.e. the words). in the end, the log id203
is (heuristically) divided by the length of the sentence s in order not to penalize
excessively longer sentences (this is done commonly in id103).

more recently, daum  e iii and marcu (2002) extended this approach to document
compression by using rhetorical structure theory as in marcu (1998a), where the
entire document is represented as a tree, hence allowing not only to compress relevant
sentences, but also to drop irrelevant ones. in this framework, daum  e iii and marcu
(2004) employed kernel methods to decide for each node in the tree whether or not
it should be kept.

4.3 sequential id194

we conclude this section by mentioning some recent work that concerns document
representation, with applications in summarization. in the bag-of-words representa-
tion (salton et al., 1975) each document is represented as a sparse vector in a very
large euclidean space, indexed by words in the vocabulary v . a well-known tech-
nique in information retrieval to capture word correlation is id45
(lsi), that aims to    nd a linear subspace of dimension k     |v | where documents
may be approximately represented by their projections.

these classical approaches assume by convenience that euclidean geometry is
a proper model for text documents. as an alternative, gous (1999) and hall and
hofmann (2000) used the framework of information geometry (amari and nagaoka,
2001) to generalize lsi to the multinomial manifold, which can be identi   ed with
the id203 simplex

xi = 1, xi     0 for i = 1, . . . , n

.

(8)

i=1

instead of    nding a linear subspace, as in the euclidean case, they learn a subman-
ifold of pn   1. to illustrate this idea, gous (1999) split a book (machiavelli   s the
prince) into several text blocks (its numbered pages), considered each page as a
point in p|v |   1, and projected data into a 2-dimensional submanifold. the result is
22see http://www.cis.upenn.edu/~treebank/.

20

(cid:40)
x     rn | n(cid:88)

pn   1 =

(cid:41)

the representation of the book as a sequential path in r2, tracking the evolution of
the subject matter of the book over the course of its pages (see fig. 5). inspired by

figure 5: the 113 pages of the prince projected onto a 2-dimensional space (ex-
tracted from (gous, 1999)). the in   ection around page 85 re   ects a real change in
the subject matter, where the book shifts from political theory to a more biograph-
ical discourse.

this framework, lebanon et al. (2007) suggested representing a document as a sim-
plicial curve (i.e. a curve in the id203 simplex), yielding the locally weighted
bag-of-words (lowbow) model. according to this representation, a length-normalized
document is a function x : [0, 1]    v     r+ such that

x(t, wj) = 1,

for any t     [0, 1].

(9)

(cid:88)

wj   v

we can regard the document as a continuous signal, and x(t, wj) as expressing
the relevance of word wj at instant t. this generalizes both the pure sequential
representation and the (global) bag-of-words model. let y = (y1, . . . , yn)     v n be
a n-length document. the pure sequential representation of y arises by de   ning
x = xseq with:

(10)
where (cid:100)a(cid:101) denotes the smallest integer greater than a. the global bag-of-words
representation of x corresponds to de   ning x = xbow, where

xseq(t, wj) =

0,

if wj = y(cid:100)tn(cid:101)
if wj (cid:54)= y(cid:100)tn(cid:101),

(cid:26) 1,

xbow(  , wj) =

xseq(t, wj)dt,        [0, 1], j = 1, . . . ,|v |.

(11)

in this case, the curve degenerates into a single point in the simplex, which is
the maximum likelihood estimate of the multinomial parameters. an intermediate

0

21

(cid:90) 1

(cid:90) 1

representation arises by smoothing (10) via a function f  ,   : [0, 1]     r++, where
       [0, 1] and        r++ are respectively a location and a scale parameter. an
example of such a smoothing function is the truncated gaussian de   ned in [0, 1]
and normalized. this allows de   ning the lowbow representation at    of the n-lenght
document (y1, . . . , yn)     v n as the function x : [0, 1]    v     r+ such that:

x(  , wj) =

xseq(t, wj)f  ,  (t)dt.

(12)

0

the scale of the smoothing function controls the amount of locality/globality in
the id194 (see fig. 6): when            we recover the global bow
representation (11); when        0, we approach the pure sequential representation
(10).

figure 6: the lowbow representation of a document with |v | = 3, for several values
of the scale parameter    (extracted from (lebanon, 2006)).

representing a document as a simplicial curve allows us to characterize geomet-
rically several properties of the document. for example, the tangent vector    eld
along the curve describes sequential    topic trends    and their change; the curvature
measures the amount of wigglyness or deviation from a geodesic path. this prop-
erties can be useful for tasks like text segmentation or summarization; for example
plotting the velocity of the curve ||   x(  )|| along time o   ers a visualization of the doc-
ument where local maxima tend to correspond to topic boundaries (see (lebanon
et al., 2007) for more information).

22

5 evaluation

evaluating a summary is a di   cult task because there does not exist an ideal sum-
mary for a given document or set of documents. from papers surveyed in the previ-
ous sections and elsewhere in literature, it has been found that agreement between
human summarizers is quite low, both for evaluating and generating summaries.
more than the form of the summary, it is di   cult to evaluate the summary con-
tent. another important problem in summary evaluation is the widespread use of
disparate metrics. the absence of a standard human or automatic evaluation met-
ric makes it very hard to compare di   erent systems and establish a baseline. this
problem is not present in other nlp problems, like parsing. besides this, manual
evaluation is too expensive: as stated by lin (2004), large scale manual evaluation
of summaries as in the duc conferences would require over 3000 hours of human ef-
forts. hence, an evaluation metric having high correlation with human scores would
obviate the process of manual evaluation. in this section, we would look at some im-
portant recent papers that have been able to create standards in the summarization
community.

5.1 human and automatic evaluation

lin and hovy (2002) describe and compare various human and automatic metrics to
evaluate summaries. they focus on the evaluation procedure used in the document
understanding conference 2001 (duc-2001), where the summary evaluation en-
vironment (see) interface was used to support the human evaluation part. nist
assessors in duc-2001 compared manually written ideal summaries with summaries
generated automatically by summarization systems and baseline summaries. each
text was decomposed into a list of units (sentences) and displayed in separate win-
dows in see. to measure the content of summaries, assessors stepped through each
model unit (mu) from the ideal summaries and marked all system units (su) shar-
ing content with the current model unit, rating them with scores in the range 1     4
to specify that the marked system units express all (4), most (3), some (2) or hardly
any (1) of the content of the current model unit. grammaticality, cohesion, and co-
herence were also rated similarly by the assessors. the weighted recall at threshold
t (where t range from 1 to 4) is then de   ned as

recallt =

number of mus marked at or above t
number of mus in the model summary .

(13)

an interesting study is presented that shows how unstable the human markings
for overlapping units are. for multiple systems, the coverage scores assigned to the
same units were di   erent by human assessors 18% of the time for the single document
task and 7.6% of the time for multi-document task. the authors also observe that
inter-human agreement is quite low in creating extracts from documents (    40% for
single-documents and     29% for multi-documents). to overcome the instability of
human evaluations, they proposed using automatic metrics for summary evaluation.

23

inspired by the machine translation evaluation metric id7 (papineni et al., 2001),
they outline an accumulative id165 matching score (which they call nams),

nams = a1    nam1 + a2    nam2 + a3    nam3 + a4    nam4,

where the namn id165 hit ratio is de   ned as:

# of matched id165s between mu and s

total # of id165s in mu

(14)

(15)

with s denoting here the whole system summary, and where only content words
were used in forming the id165s. di   erent con   gurations of ai were tried; the
best correlation with human judgement (using spearman   s rank order correlation
coe   cient) was achieved using a con   guration giving 2/3 weight to bigram matches
and 1/3 to unigrams matches with id30 done by the porter stemmer.

5.2 id8

lin (2004) introduced a set of metrics called recall-oriented understudy for gist-
ing evaluation (id8)23 that have become standards of automatic evaluation of
summaries.
in what follows, let r = {r1, . . . , rm} be a set of reference summaries, and let s be
a summary generated automatically by some system. let   n(d) be a binary vector
representing the id165s contained in a document d; the i-th component   i
n(d) is 1
if the i-th id165 is contained in d and 0 otherwise. the metric id8-n is an
id165 recall based statistic that can be computed as follows:

id8-n(s) =

(16)
where (cid:104)., .(cid:105) denotes the usual inner product of vectors. this measure is closely related
to id7 which is a precision related measure. unlike other measures previously
considered, id8-n can be used for multiple reference summaries, which is quite
useful in practical situations. an alternative is taking the most similar summary in
the reference set,

id8-nmulti(s) = max
r   r

(cid:104)  n(r),   n(s)(cid:105)
(cid:104)  n(r),   n(r)(cid:105) .

(17)

another metric in (lin, 2004) applies the concept of longest common subse-
quences 24 (lcs). the rationale is: the longer the lcs between two summary sen-
tences, the more similar they are. let r1, . . . , ru be the reference sentences of the
documents in r, and s a candidate summary (considered as a concatenation of
sentences). the id8-l is de   ned as an lcs based f-measure:

id8-l(s) =

(1 +   2)rlcsplcs
rlcs +   2plcs

(18)

23see http://openid8.com/default.aspx.
24a subsequence of a string s = s1 . . . sn is a string of the form si1 . . . sin where 1     i1 < . . . in     n.

24

(cid:80)
(cid:80)
r   r(cid:104)  n(r),   n(s)(cid:105)
r   r(cid:104)  n(r),   n(r)(cid:105) ,

pu
pu
i=1 |ri|

pu

i=1 lcs(ri,s)

, plcs(s) =

, |x| denotes the length of
where rlcs(s) =
sentence x, lcs(x, y) denotes the length of the lcs between sentences x and y,
and    is a (usually large) parameter to balance precision and recall. notice that
the lcs function may be computed by a simple id145 approach.
the metric (18) is further re   ned by including weights that penalize subsequence
matches that are not consecutive, yielding a new measure denoted id8-w.

i=1 lcs(ri,s)

|s|

yet another measure introduced by lin (2004) is id8-s, which can be seen
as a gappy version of id8-n for n = 2 and is aptly called skip bigram. let   2(d)
be a binary vector indexed by ordered pairs of words; the i-th component   i
2(d) is
1 if the i-th pair is a subsequence of d and 0 otherwise. the metric id8-s is
computed as follows:

where rs(s) =

id8-s(s) =

pu
pu
i=1(cid:104)  2(ri),  2(s)(cid:105)
i=1(cid:104)  2(ri),  2(ri)(cid:105) and ps(s) =

(1 +   2)rsps
rs +   2ps

pu
i=1(cid:104)  2(ri),  2(s)(cid:105)
(cid:104)  2(s),  2(s)(cid:105)

(19)

.

the various versions of id8 were evaluated by computing the correlation
coe   cient between id8 scores and human judgement scores. id8-2 per-
formed the best among the id8-n variants. id8-l, id8-w, and
id8-s all performed very well on the duc-2001 and duc-2002 datasets. how-
ever, correlation achieved with human judgement for id57
was not as high as single-document ones; improvement on this side of the paradigm
is an open research topic.

5.3 information-theoretic evaluation of summaries

a very recent approach (lin et al., 2006) proposes to use an information-theoretic
method to automatic evaluation of summaries. the central idea is to use a diver-
gence measure between a pair of id203 distributions, in this case the jensen-
shannon divergence, where the    rst distribution is derived from an automatic sum-
mary and the second from a set of reference summaries. this approach has the
advantage of suiting both the single-document and the multi-document summariza-
tion scenarios.
let d = {d1, . . . , dn} be the set of documents to summarize (which is a singleton
set in the case of single-document summarization). assume that a distribution
parameterized by   r generates reference summaries of the documents in d. the
task of summarization can be seen as that of estimating   r. analogously, assume
that every summarization system is governed by some distribution parameterized
by   a. then, we may de   ne a good summarizer as one for which   a is close to   r.
one information-theoretic measure between distributions that is adequate for this
is the kl divergence (cover and thomas, 1991),

kl(p  a||p  r) =

p  a
i

log p  a
i
p  r
i

.

(20)

m(cid:88)

i=1

however, the kl divergence is unbounded and goes to in   nity whenever p  a

i vanishes

25

and p  r
i does not, which requires using some kind of smoothing when estimating the
distributions. lin et al. (2006) claims that the measure used here should also be
symmetric,25 another thing that the kl divergence is not. hence, they propose to
use the jensen-shannon divergence which is bounded and symmetric:26

js(p  a||p  r) =

1
2 kl(p  a||r) +

1
2 kl(p  r||r) =
2 h(p  a),

2 h(p  a)     1

= h(r)     1

(21)

where r = 1

2 p  a + 1

2 p  r is the average distribution.

to evaluate a summary sa given a reference summary sr, the authors propose
to use the negative js divergence between the estimates of p  a and p  r given the
summaries,

score(sa|sr) =    js(p

    a||p

    r)

the parameters are estimated via a posteriori maximization assuming a multi-
nomial generation model for each summary (which means that they are modeled as
bags-of-words) and using dirichlet priors (the conjugate priors of the multinomial
family). so:

the summary, a0 =(cid:80)m

i=1 ai)

where (m being the number of distinct words, a1, . . . , am being the word counts in

    a = arg max
  a

p(sa|  a)p(  a),

p(sa|  a) =

  (a0 + 1)
i=1   (ai + 1)

ai

  a,i

(cid:81)m
(cid:81)m

m(cid:89)

i=1

m(cid:89)

  i   1

  a,i

and

  (  0)
i=1   (  i)

p(  a) =

where   i are hyper-parameters and   0 =(cid:80)m
    a,i = ai +   i     1
a0 +   0     m

i=1

(22)

(23)

(24)

(25)

(26)

i=1   i. after some algebra, we get

which is similar to id113 with smoothing.27     r is estimated analogously using the
reference summary sr. not surprisingly, if we have more than one reference sum-
mary, the map estimation given all summaries equals map estimation given their
concatenation into a single summary.

25however, the authors do not give much support for this claim. in our view, there is no reason

to require symmetry.

26although this is not mentioned in (lin et al., 2006), the jensen-shannon divergence also satis   es
the axioms to be a squared metric, as shown by endres and schindelin (2003). it has also a plethora
of properties that are presented elsewhere, but this is out of scope of this survey.

27in particular if   i = 1 it is just id113 (id113).

26

the authors experimented three automatic evaluation schemes (js with smooth-
ing, js without smoothing, and kl divergence) against manual evaluation; the best
performance was achieved by js without smoothing. this is not surprising since, as
seen above, the js divergence is bounded, unlike the kl divergence, and so it does
not require smoothing. smoothing has the e   ect of pulling the two distributions
more close to the uniform distribution.

6 conclusion

the rate of information growth due to the world wide web has called for a need
to develop e   cient and accurate summarization systems. although research on
summarization started about 50 years ago, there is still a long trail to walk in
this    eld. over time, attention has drifted from summarizing scienti   c articles to
news articles, electronic mail messages, advertisements, and blogs. both abstractive
and extractive approaches have been attempted, depending on the application at
hand. usually, abstractive summarization requires heavy machinery for language
generation and is di   cult to replicate or extend to broader domains. in contrast,
simple extraction of sentences have produced satisfactory results in large-scale ap-
plications, specially in id57. the recent popularity of
e   ective newswire summarization systems con   rms this claim.

this survey emphasizes extractive approaches to summarization using statisti-
cal methods. a distinction has been made between single document and multi-
document summarization. since a lot of interesting work is being done far from
the mainstream research in this    eld, we have chosen to include a brief discussion
on some methods that we found relevant to future research, even if they focus only
on small details related to a general summarization process and not on building an
entire summarization system.

finally, some recent trends in automatic evaluation of summarization systems
have been surveyed. the low inter-annotator agreement    gures observed during
manual evaluations suggest that the future of this research area heavily depends on
the ability to    nd e   cient ways of automatically evaluating these systems and on
the development of measures that are objective enough to be commonly accepted
by the research community.

acknowledgements

we would like to thank noah smith and einat minkov for valuable suggestions
during the course of the survey. we would also like to thank alex rudnicky and
mohit kumar for insightful discussions at various points during 2006-2007.

27

references

amari, s.-i. and nagaoka, h. (2001). methods of information geometry (transla-

tions of mathematical monographs). oxford university press. [20]

aone, c., okurowski, m. e., gorlinsky, j., and larsen, b. (1999). a trainable
summarizer with knowledge acquired from robust nlp techniques.
in mani, i.
and maybury, m. t., editors, advances in automatic text summarization, pages
71   80. mit press. [4, 5]

barzilay, r. and elhadad, m. (1997). using lexical chains for text summarization.

in proceedings ists   97. [8]

barzilay, r., mckeown, k., and elhadad, m. (1999).

context of id57. in proceedings of acl    99.
14, 16]

information fusion in the
[12, 13,

baxendale, p. (1958). machine-made index for technical literature - an experiment.

ibm journal of research development, 2(4):354   361. [2, 3, 5]

brown, f., pietra, v. j. d., pietra, s. a. d., and mercer, r. l. (1993). the
mathematics of id151: parameter estimation. comput.
linguist., 19(2):263   311. [18]

burges, c., shaked, t., renshaw, e., lazier, a., deeds, m., hamilton, n., and
hullender, g. (2005). learning to rank using id119.
in icml    05:
proceedings of the 22nd international conference on machine learning, pages 89   
96, new york, ny, usa. acm. [8]

carbonell, j. and goldstein, j. (1998). the use of mmr, diversity-based reranking
for reordering documents and producing summaries. in proceedings of sigir    98,
pages 335   336, new york, ny, usa. [12, 14, 15]

collins, m. (1999). head-driven statistical models for natural language parsing.

phd thesis, university of pennsylvania. [13, 20]

conroy, j. m. and o   leary, d. p. (2001). text summarization via hidden markov

models. in proceedings of sigir    01, pages 406   407, new york, ny, usa. [6]

cover, t. and thomas, j. (1991). elements of id205. wiley. [25]

daum  e iii, h. and marcu, d. (2002). a noisy-channel model for document com-
pression. in proceedings of the conference of the association of computational
linguistics (acl 2002). [20]

daum  e iii, h. and marcu, d. (2004). a tree-position kernel for document compres-

sion. in proceedings of duc2004. [20]

edmundson, h. p. (1969). new methods in automatic extracting. journal of the

acm, 16(2):264   285. [2, 3, 4]

28

endres, d. m. and schindelin, j. e. (2003). a new metric for id203 distribu-

tions. ieee transactions on id205, 49(7):1858   1860. [26]

evans, d. k. (2005). similarity-based multilingual id57.

technical report cucs-014-05, columbia university. [12, 17]

gous, a. (1999). spherical subfamily models. [20, 21]

hall, k. and hofmann, t. (2000). learning curved multinomial subfamilies for
natural language processing and information retrieval. in proc. 17th international
conf. on machine learning, pages 351   358. morgan kaufmann, san francisco,
ca. [20]

hovy, e. and lin, c. y. (1999). automated text summarization in summarist. in
mani, i. and maybury, m. t., editors, advances in automatic text summariza-
tion, pages 81   94. mit press. [17]

knight, k. and marcu, d. (2000). statistics-based summarization - step one: sen-

tence compression. in aaai/iaai, pages 703   710. [19]

kupiec, j., pedersen, j., and chen, f. (1995). a trainable document summarizer.

in proceedings sigir    95, pages 68   73, new york, ny, usa. [4]

lebanon, g. (2006). sequential id194s and simplicial curves. in
proceedings of the 22nd conference on uncertainty in arti   cial intelligence. [22]

lebanon, g., mao, y., and dillon, j. (2007). the locally weighted bag of words
framework for id194. j. mach. learn. res., 8:2405   2441. [21,
22]

lin, c.-y. (1999). training a selection function for extraction. in proceedings of

cikm    99, pages 55   62, new york, ny, usa. [5]

lin, c.-y. (2004). id8: a package for automatic evaluation of summaries. in
marie-francine moens, s. s., editor, text summarization branches out: pro-
ceedings of the acl-04 workshop, pages 74   81, barcelona, spain.
[8, 23, 24,
25]

lin, c.-y., cao, g., gao, j., and nie, j.-y. (2006). an information-theoretic ap-
in proceedings of hlt-naacl

proach to automatic evaluation of summaries.
   06, pages 463   470, morristown, nj, usa. [25, 26]

lin, c.-y. and hovy, e. (1997). identifying topics by position. in proceedings of
the fifth conference on applied natural language processing, pages 283   290, san
francisco, ca, usa. [5]

lin, c.-y. and hovy, e. (2002). manual and automatic evaluation of summaries. in
proceedings of the acl-02 workshop on id54, pages 45   51,
morristown, nj, usa. [23]

29

luhn, h. p. (1958). the automatic creation of literature abstracts. ibm journal of

research development, 2(2):159   165. [2, 3, 6, 8]

mani, i. and bloedorn, e. (1997). id57 by graph search

and matching. in aaai/iaai, pages 622   628. [15, 16]

marcu, d. (1998a). improving summarization through rhetorical parsing tuning. in
proceedings of the sixth workshop on very large corpora, pages 206-215, pages
206   215, montreal, canada. [9, 10, 20]

marcu, d. c. (1998b). the rhetorical parsing, summarization, and generation of
natural language texts. phd thesis, university of toronto. adviser-graeme hirst.
[10]

mckeown, k., klavans, j., hatzivassiloglou, v., barzilay, r., and eskin, e.
(1999). towards multidocument summarization by reformulation: progress and
prospects. in aaai/iaai, pages 453   460. [11, 12, 13, 14, 16]

mckeown, k. r. and radev, d. r. (1995). generating summaries of multiple news
articles. in proceedings of sigir    95, pages 74   82, seattle, washington. [8, 11,
12]

miller, g. a. (1995). id138: a lexical database for english. commun. acm,

38(11):39   41. [4, 9]

nenkova, a. (2005). automatic text summarization of newswire: lessons learned
in proceedings of aaai 2005,

from the document understanding conference.
pittsburgh, usa. [7]

ono, k., sumita, k., and miike, s. (1994). abstract generation based on rhetorical
structure extraction. in proceedings of coling    94, pages 344   348, morristown,
nj, usa. [9]

osborne, m. (2002). using maximum id178 for sentence extraction. in proceedings
of the acl   02 workshop on id54, pages 1   8, morristown,
nj, usa. [7]

papineni, k., roukos, s., ward, t., and zhu, w.-j. (2001). id7: a method for
automatic evaluation of machine translation. in proceedings of acl    02, pages
311   318, morristown, nj, usa. [24]

radev, d. r., hovy, e., and mckeown, k. (2002). introduction to the special issue

on summarization. computational linguistics., 28(4):399   408. [1, 2]

radev, d. r., jing, h., and budzikowska, m. (2000). centroid-based summarization
of multiple documents: sentence extraction, utility-based evaluation, and user
studies. in naacl-anlp 2000 workshop on id54, pages
21   30, morristown, nj, usa. [12, 16, 17]

30

radev, d. r., jing, h., stys, m., and tam, d. (2004). centroid-based summariza-
tion of multiple documents. information processing and management 40 (2004),
40:919   938. [16, 17]

radev, d. r. and mckeown, k. (1998). generating natural language summaries

from multiple on-line sources. computational linguistics, 24(3):469   500. [12]

salton, g. and buckley, c. (1988). on the use of spreading activation methods in
automatic information. in proceedings of sigir    88, pages 147   160, new york,
ny, usa. [15]

salton, g., wong, a., and yang, a. c. s. (1975). a vector space model for automatic

indexing. communications of the acm, 18:229   237. [20]

selman, b., levesque, h. j., and mitchell, d. g. (1992). a new method for solving

hard satis   ability problems. in aaai, pages 440   446. [11]

svore, k., vanderwende, l., and burges, c. (2007). enhancing single-document
summarization by combining ranknet and third-party sources. in proceedings of
the emnlp-conll, pages 448   457. [7, 8]

witbrock, m. j. and mittal, v. o. (1999). ultra-summarization (poster abstract):
a statistical approach to generating highly condensed non-extractive summaries.
in proceedings of sigir    99, pages 315   316, new york, ny, usa. [18]

31

