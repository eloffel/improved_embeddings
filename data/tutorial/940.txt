automatic text summarization

(the state of the art 2007 and new challenges)

karel je  ek, josef steinberger

katedra informatiky a v  po  etn   techniky, fav,

z  u - z  pado  esk   univerzita v plzni, univerzitn   22, 306 14 plze  

{jezek ka, jstein}@kiv.zcu.cz

c(cid:13) v  clav sn    el (ed.): znalosti 2008, pp. 1   12, isbn 978-80-227-2827-0.

fiit stu bratislava,   stav informatiky a softv  rov  ho in  inierstva, 2008.

automatic text summarization (the state of the art 2007 and new challenges)   karel je  ek1, josef steinberger11katedra informatiky a v  po  etn   techniky, fav, z  u     z  pado  esk   univerzita v plzni, univerzitn   22, 306 14 plze   {jstein, jezek_ka}@kiv.zcu.cz abstract. the headline of this paper names a research area originating from the late 50   s but not loosing its popularity until the present time. moreover, one of the most relevant today   s problems caused by the rapid growth of the web, which is called information overloading, has increased the necessity of more sophisticated and powerful summarizers. this paper shortly introduces a taxonomy of summarization methods and an overview of their principles from classical ones, over corpus based, to knowledge rich approaches. we consider various aspects which can affect their classification. a special attention is devoted to application of recent information reduction methods, based on algebraic transformations. further, we introduce experiences with the development of our own summarizing method. finally, some new ideas and a conception for the future of this field are mentioned. 1   introduction enormous increasing and easy availability of information on the world wide web have recently resulted in brushing up the classical linguistics problem - the condensation of information from text documents. this task is essentially a data reduction process. it was manually exerted from time out of mind and firstly computerized in late 50th . resulted summary has to inform by selection and or by generalization on important content and conclusions in the original text. recent scientific knowledge and more efficient computers form a new challenge giving the chance to solve the information overload problem or at least to postpone its decision and decrease its negative impact.   there are plenty of various definitions what actually text summarization means. except this mentioned a few lines above e.g.:        a brief but accurate representation of the contents of a document   ,        a distilling the most important information from a source to produce an abridged version for a particular user/users and task/tasks   ,    the quantitative features, which can characterize the summary include: 2

karel je  ek, josef steinberger

    semantic informativeness (can be viewed as the measure of ability to reconstruct from the summary the original text),     coherence (express the way how the parts of the summary create together an integrated sequence)     compression ratio.  the history of automatic i.e. computerized summarization began 50 years ago. as the oldest publication, describing an implementation of an automatic summarizer is often cited [1]. luhn   s method uses term frequencies to appraise eligibility of sentences for the summary. its main idea is based on knowledge that significant words carrying most information are not too frequent nor too seldom in the text. establishing boundaries of words significance by the help of their frequency would be a matter of experience. the next step is ranking of sentences, reflecting the number of significant words and their distance in a sentence. after it remains only to choose one or several highly ranked as a result. it should be mentioned (it seems nowadays funny) that luhn   s motivation was as well information overload.  the next remarkable progress was done ten years later [2]. edmundson   s work introduced hypothesis concerning high information value of title phrases, sentences from the beginning and from the conclusion of the article, sentences containing cue words and phrases as    important   ,    results are   ,    paper introduces   , etc.  even if next years brought further results, the renaissance of this field and remarkable progress came in 90th. we should take notice [3] or [4]. it is the time of broader use of artificial intelligence methods in this area and combination of various methods in hybrid systems. new millennium due to www expansion shifted the interest of researches to summarization of groups of documents, multimedia documents and application of new algebraic method for data reduction.   this paper is organized following way. 2. chapter describes the basic notions and typology of summarizers. chapter 3 is devoted to a short overview of classical methods. chapter 4 is on new approaches with impact on algebraic reduction methods, including our own lsa-based approach. the last chapter concludes the paper and the further research is proposed. 2   taxonomy of summarizing methods there are several, often orthogonal views which can be used to characterize summarizers. the list and description of the most often cited follows.  comparing the form of summary we recognize:     extracts, they are summaries completely consisting of word sequences copied from the original document. as the word sequences can be used phrases, sentences or paragraphs. as expected, extracts suffer from automatic text summarization

3

inconsistencies, lack of balance, and lack of cohesion. sentences may be extracted out of context, anaphoric reference may be broken.     abstracts, they are summaries containing word sequences not present in the original. up to now it is too hard task for computer research to solve it successfully.   the view coming from the level of processing distinguishes:     surface-level approaches, in which case information is represented in notions of shallow features and their combination. shallow features include e.g. statistically salient terms, positionally salient terms, terms from cue phrases, domain-specific or a user   s query terms. results have the form of extracts.     deeper-level approaches may produce extracts or abstracts. the later case uses synthesis involving id86. they need some semantic analysis e.g. can use entity approaches and build a representation of text entities (text units) and their relationships to determine salient parts. relationships of entities include thesaural relations, syntactic relations, meaning relations and others. they can as well use discourse approaches and model the text structure on the base of e.g. hypertext markup or rhetorical structure.  another typology comes from the purpose the summary serves:     indicative summaries give abbreviated information on the main topics of a document. they should preserve its most important passages and often are used as the end part of ir systems, being returned by search system instead of full document. their aim should be to help a user to decide whether the original document is worth reading. the typical lengths of indicative summaries range between 5 till 10% of the complete text.     informative summaries provide a substitute (   surrogate   ,    digest   ) for full document, retaining important details, while reducing information volume. informative summary is typically 20-30 % of the original text.     critical or evaluative summaries capture the point of view of the summary author on a given subject. reviews are typical example, but they are little bit out of scope of nowadays automatic summarizers. it should be noted, that all three mentioned groups are not mutually exclusive and they are common summaries serving both indicative and informative function. it is quite usual to hold informative summarizers as a subset of indicative ones.  when distinguished by the audience we can recognize:     generic summaries, when the result is aimed at a broad community of readers, all major topics are equally important,     query-based summaries, when the result is based on a question e.g.    what are the causes of the high inflation?        user focused or topic focused summaries, which are tailored to the interest of particular user or emphasize only particular topics.   4

karel je  ek, josef steinberger

there are some other views we can use for taxonomy of summarizers e.g.: span of processed text:     single document or id57. language:     monolingual versus multilingual. genre:     scientific article or report or news     3   overview of methods based on classical principles 3.1 pioneering works the first approaches of the automatic text summarization used only simple (surface level) indicators to decide what parts of a text include into the summary. the oldest sentence extraction algorithm was developed in 1958 [1]. it used frequencies of terms as the sentence relevance criterion. the basic idea was that a writer will repeat certain words when writing about a given topic. the importance of terms is considered proportional to their frequency in summarized documents. the frequencies are used in the next step to score and select sentences for the extract. other indicators of relevance used in [5] are the position of a sentence within the document and the presence of certain cue-words (i.e., words like    important    or    relevant   ) or words contained in the title. the combination of cue-words, title words and the position of a sentence was used in [2] to produce extracts and was demonstrated their similarity with human written abstracts. 3.2 statistical methods in [4] was proved that the relevance of document terms is inversely proportional to the number of documents in the corpus containing the term. the formula for term relevance evaluation is given by tfi  x idfi, where tfi is the frequency of term i in the document and idfi is the inverted frequency of documents containing this term. sentences can be subsequently scored for instance by summing relevance of terms in the sentence.  an implementation of a more ingenious statistical method was described in [3]. it uses a bayesian classifier to compute the id203 that a sentence in a source document should be included in a summary. to train the classifier the authors used a corpus of 188 pairs of full documents/summaries. the characteristic features used in bayesian formula include except of word frequency also uppercase words, sentence length, phrase structure, in-paragraph position.  an alternative way how to measure term relevance was proposed in [6]. instead of rough term counting the authors used concept relevance which can be determined automatic text summarization

5

using id138. e.g. the occurrence of the concept    car    is counted when the word    auto    is found as well as when, for instance,    autocar   ,    tires   , or    brake    are found.  3.3 methods based on text connectivity anaphoric expressions1 that refer to previously mentioned parts of the text need to know their antecedents in order to be understood. extractive methods can fail to capture the relations between concepts in a text. if a sentence containing an anaphoric link is extracted without the previous context the summary can become difficult to understand. cohesive properties comprise relations between expressions of the text. they have been explored by different summarization approaches.  let us mention a method called lexical chains, which was introduced in [7]. it uses the id138 thesaurus for determining cohesive relations between terms (i.e., repetition, synonymy, antonymy, hypernymy, and holonymy) and composes the chains by related terms. their scores are determined on the basis of the number and type of relations in the chain. only those sentences where the strongest chains are highly concentrated are selected for the summary. a similar method where sentences are scored according to the objects they mention was presented in [8]. the objects are identified by a co-reference resolution system. co-reference resolution is the process of determining whether two expressions in natural language refer to the same entity. the sentences where the occurrence of frequently mentioned objects overcomes the given limit are included into the summary.  into the group of methods based on text connectivity we can include the methods utilizing rhetorical structure theory (rst). rst is a theory about text organization. it consists of a number of rhetorical relations that connect together text units. the relations tie together a nucleus     which is central to the writer   s goal, and a satellite - less central or marginal parts. from relations is composed a tree-like representation which is used for extraction of text unit into the summary. in [9] sentences are penalized according to their rhetorical role in the tree. in the concrete a weight of 1 is given to satellite units and a weight of 0 is given to nuclei units. the final score of a sentence is given by the sum of weights from the root of the tree to the sentence. in [10], each parent node identifies its nuclear children as salient. the children are promoted to the parent level. the process is recursive down the tree. the score of a unit is given by the level it obtained after promotion. 3.4 iterative graph methods iterative graph algorithms, such as hits [11] or google   s id95 [12] have been originally developed as exploring tools of the link-structure to rank web pages. later                                                            1 anaphoric expression is a word or phrase which refers back to some previously expressed word or phrase or meaning (typically, pronouns such as herself, himself, he, she).  6

karel je  ek, josef steinberger

on they were successfully used in other areas e.g. citation analysis, social networks etc. in graph ranking algorithms, the importance of a vertex within the graph is iteratively computed from the entire graph. in [13] the graph-based model was applied to natural language processing, resulting in an algorithm named textrank.  the same graph-based ranking principles were applied in summarization [14]. a graph is constructed by adding a vertex for each sentence in the text. edges between vertices are established using sentence inter-connections. these connections are defined using a similarity relation, where similarity is measured as a function of content overlap. the overlap of two sentences can be determined as the number of common tokens between lexical representations of two sentences. the iterative part of algorithm is consequently applied on the graph of sentences. when its processing is finished, vertices (sentences) are sorted by their scores. the top ranked sentences are included in the result. 3.5 coming close to human abstracts there is a qualitative difference between the summaries produced by current automatic summarizers and the abstracts written by human abstractors. computer systems can identify the important topics of an article with only a limited accuracy. another factor is that most summarizers rely on extracting key sentences or paragraphs. however, if the extracted sentences are disconnected in the original article and they are strung together in the extract, the result can be incoherent and sometimes even misleading.   lately, some non-sentence-extractive summarization methods have started to appear. instead of reproducing full sentences from the summarized text, these methods either compress the sentences [15, 16, 17, 18], or re-generate new sentences from scratch [19]. in [20] a cut-and-paste strategy was proposed. the authors have identified six editing operations in human abstracting: (i) sentence reduction; (ii) sentence combination; (iii) syntactic transformation; (iv) lexical id141; (v) generalization and specification; and (vi) reordering. summaries produced this way resemble the human summarization process more than extraction does. however, if large quantities of text need to be summarized, sentence extraction is a more efficient method.  extraction is robust towards all irregularities of input text. it is failure-proof and less language dependent.  4   new approaches based on algebraic reduction several approaches based on algebraic reduction methods have appeared in the last couple of years. the most widely used is latent semantic analysis (lsa) [21], however other methods, like non-negative id105 (nmf) [22] or semi-discrete matrix decomposition (sdd) [23] look promising as well. automatic text summarization

7

4.1 lsa in summarization background lsa is a fully automatic algebraic-statistical technique for extracting and representing the contextual usage of words    meanings in passages of discourse. the basic idea is that the aggregate of all the word contexts in which a given word does and does not appear provides mutual constraints that determine the similarity of meanings of words and sets of words to each other. lsa has been used in a variety of applications (e.g., information retrieval, document categorization, information filtering, and text summarization).  the heart of the analysis in summarization background is a id194 developed in two steps. the first step is the creation of a term by sentences matrix a = [a1, a2, . . ., an], where each column ai represents the weighted term-frequency vector of sentence i in the document under consideration5. if there are m terms and sentences in the document, then we will obtain an m  n matrix a. the next step is to apply singular value decomposition (svd) to matrix a. the svd of an m  n matrix a is defined as:         a = u  vt ,             (1)  where u = [uij] is an m  n column-orthonormal matrix whose columns are called left singular vectors.   =diag(  1,   2, . . . ,   n) is an n  n diagonal matrix, whose diagonal elements are non-negative singular values sorted in descending order. v = [vij] is an n  n orthonormal matrix, whose columns are called right singular vectors. the dimensionality of the matrices is reduced to r most important dimensions and thus, u    is m  r,       is r  r and vt    is r  n matrix2.   from a mathematical point of view, svd derives a mapping between the m-dimensional space specified by the weighted term-frequency vectors and the r-dimensional singular vector space. from an nlp perspective, what svd does is to derive the latent semantic structure of the document represented by matrix a: i.e. a breakdown of the original document into r linearly-independent base vectors which express the main    topics    of the document. svd can capture interrelationships among terms, so that terms and sentences can be clustered on a    semantic    basis rather than on the basis of words only. furthermore, as demonstrated in [24], if a word combination pattern is salient and recurring in a document, this pattern will be captured and represented by one of the left singular vectors. the magnitude of the corresponding singular value indicates the importance degree of this pattern within the document. any sentences containing this word combination pattern will be projected along this singular vector, and the sentence that best represents this pattern will have the largest value with this vector. assuming that each particular word combination pattern describes a certain topic in the document, each left singular                                                            2 u   , resp.      , v    t, denotes matrix u, resp.   , vt, reduced to r dimensions. 8

karel je  ek, josef steinberger

vector can be viewed as representing such a topic [25], the magnitude of its singular value representing the importance degree of this topic3. 4.2 lsa-based single-document approaches the summarization method proposed in [26] uses the representation of a document thus obtained to choose the sentences to go in the summary on the basis of the relative importance of the    topics    they mention, described by the matrix vt. the summarization algorithm simply chooses for each    topic    the most important sentence for that topic: i.e., the kth sentence chosen is the one with the largest index value in the kth right singular vector in matrix vt.  the main drawback of gong and liu   s method is that when l sentences are extracted the top l topics are treated as equally important. as a result, a summary may include sentences about    topics    which are not particularly important. in order to fix the problem, we changed the selection criterion to include in the summary the sentences whose vectorial representation in the matrix   2  v has the greatest    length   , instead of the sentences containing the highest index value for each    topic   . intuitively, the idea is to choose the sentences with greatest combined weight across all important topics, possibly including more than one sentence about an important topic, rather than one sentence for each topic. more formally: after computing the svd of a term by sentences matrix, we compute the length of each sentence vector in   2  vt , which represents its summarization score as well (for details see [27]).  in [28] an lsa-based summarization of meeting recordings was presented. the authors followed the gong and liu approach, but rather than extracting the best sentence for each topic, n best sentences were extracted, with n determined by the corresponding singular values from matrix   . the number of sentences in the summary that will come from the first topic is determined by the percentage that the largest singular value represents out of the sum of all singular values, and so on for each topic. thus, id84 is no longer tied to summary length and more than one sentence per topic can be chosen.  another summarization method that uses lsa was proposed in [29]. it is a mixture of graph-based and lsa-based approaches. after performing svd on the word-by-sentence matrix and reducing the dimensionality of the latent space, they reconstruct the corresponding matrix a   =u        v   t. each column of a    denotes the semantic sentence representation. these sentence representations are then used, instead of a keyword-based frequency vector, for the creation of a text relationship map to represent the structure of a document. a ranking algorithm is then applied in the resulting map (see section 3.4).                                                            3 in [25] it was shown that the dependency of the significance of each particular topic on the magnitude of its corresponding singular value is quadratical. automatic text summarization

9

4.3 lsa-based multi-document approaches in [30] we proposed the extension of the method to process a cluster of documents written about the same topic. id57 is one step more complex task than single-document summarization. it brings into new problems we have to deal with. the first step is again to create a term by sentence matrix. in this case we include in the matrix all sentences from the cluster of documents. (in the case of single-document summarization we included the sentences from the one document.) then we run sentence ranking. each sentence gets the score, which is computed in the same way as when we summarize a single document     vector length in the matrix   2  vt. now, we are ready to select the best sentences (the ones with the greatest score) for the summary.  however, two documents written about the same topic/event can contain similar sentences and thus we need to solve redundancy. we propose the following process: before adding a sentence into the summary, look if there is a similar sentence already in the summary. the similarity is measured by the cosine similarity in the original term space. we determine a threshold here. extracted sentence should be close to the user query. to satisfy this, query terms get a higher weight in the input matrix.   another problem of this approach is that it favours long sentences. it is natural because a longer sentence probably contains more significant terms than a shorter one. we solve this by dividing the sentence score by number   of   termslk, where lk is the length coefficient.  experiments showed good results with a low dimensionality. it is enough to use up to 10 dimensions (topics). however, the topics are not equally important. the magnitude of each singular value holds the topic importance. to make it more general we experimented with different power functions in the computation of the final matrix used for determination of sentence score:   power  vt.  in [31], an interesting id57 approach based on lsa and maximal marginal relevance (mmr) was proposed. a common approach for determining relevance and redundancy in id57 is to use mmr, in which candidate sentences are represented as weighted term-frequency vectors which can thus be compared to query vectors to gauge similarity and alreadyextracted sentence vectors to gauge redundancy, via the cosine of the vector pairs. while this has proved successful to a degree, the sentences are represented merely according to weighted term frequency in the document, and so two similar sentences stand a chance of not being considered similar if they don   t share the same terms. one way to rectify this is to do lsa on the matrix first before proceeding to implement mmr, but this still only exploits term co-occurrence within the documents at hand. in contrast, the system described in [31] attempts to derive more robust representations of sentences by building a large semantic space using lsa on a very large corpus. 10

karel je  ek, josef steinberger

5   conclusion we presented the history and the state of the art in the automatic text summarization research area. we paid the most attention to the approaches based on algebraic reduction methods. their strong property is that they work only with the context of terms and thus they do not depend on a particular language. the evaluation of summarization methods has the same importance as the own summarizing. the annual summarization evaluation conference duc (document understanding conference) set the direction in the evaluation processes. however, still the only fully automatic method for the comparison of summarizers    quality is id8 [32], which compares human-written abstracts and system summaries by matches of id165s. we plan to participate at duc   08 with our new summarizer, whose core will be based on tensor lsa. three dimensions, instead of two, will be used     terms, sentences and documents. the idea of the method is that two sentences will be projected close to each other if they contain the same terms. similarly, documents will be projected close to each other if they contain the same terms. terms will be closer if they are contained in the same sentence/document. this way, the topics should be created more accurately when compared with matrix lsa. in the resulting space either mmr or our previous vector length approach can be used. acknowledgement this work was supported by grant no. 2c06009 cot-sewing. references 1. luhn, h.p.: the automatic creation of literature abstracts. in ibm journal of research development 2(2). (1958) 159   165. 2. edmundson, h.p.: new methods in automatic extracting. in journal of the association for computing machinery 16(2). (1969) 264   285. 3. kupiec, j., pedersen, j.o., chen, f.: a trainable document summarizer. in research and development in information retrieval. (1995) 68   73. 4. salton, g.: automatic text processing. addison-wesley publishing company, (1988). 5. baxendale, p.b.: man-made index for technical literature - an experiment. in ibm journal of research development, 2(4), 1958, pp. 354   361. 6. hovy, e., lin, c-y.: automated text summarization in summarist. in i. mani and m.t. maybury, eds., advances in automatic text summarization, 1999, the mit press,  pp. 81   94. 7. barzilay, r., elhadad, m.: using lexical chains for text summarization. in proceedings of the acl/eacl   97 workshop on intelligent scalable text summarization, madrid, spain, 1997, pp. 10   17. 8.  boguraev, b., kennedy, c.: salience-based content characterization of text documents. in i. mani and m.t. maybury, eds., advances in automatic text summarization, 1999, the mit press. automatic text summarization

11

9. ono, k., sumita, k., miike, s.: abstract generation based on rhetorical structure extraction. in proceedings of the international conference on computational linguistics, kyoto, japan, 1994, pp. 344-348. 10. marcu, d.: from discourse structures to text summaries. in proceedings of the acl97/eacl97 workshop on intelligent scalable text summarization, madrid, spain, 1997, pp. 82   88. 11. kleinberg, j.m.: authoritative sources in a hyper-linked environment. in journal of the acm, 46(5), 1999, pp. 604-632. 12. brin, s., page, l.: the anatomy of a large-scale hypertextual web search engine. in computer networks and isdn systems, 30, 1998, pp. 1   7. 13. mihalcea, r., tarau, p.: text-rank - bringing order into texts. in proceeding of the conference on empirical methods in natural language processing, barcelona, spain, 2004. 14. mihalcea, r., tarau, p.: an algorithm for language independent single and multiple document summarization. in proceedings of the international joint konference on natural language processing, korea, 2005. 15. jing, h.: sentence reduction for automatic text summarization. in proceedings of the 6th applied natural language processing conference, seattle, usa, 2000, pp. 310   315. 16. knight, k., marcu, d.: statistics-based summarization step one: sentence compression. in proceeding of the 17th national conference of the american association for artificial intelligence, 2000, pp. 703   710. 17. sporleder, c., lapata, m.: discourse chunking and its application to sentence compression. in proceedings of hlt/emnlp, vancouver, canada, 2005, pp. 257   264. 18. steinberger, j., je  ek, k.: sentence compression for the lsa-based summarizer. in proceedings of the 7th international conference on information systems implementation and modelling, p  erov, czech republic, 2006, pp. 141   148. 19. mckeown, k., klavans, j., hatzivassiloglou, v., barzilay, r., eskin, e.: from discourse structures to text summaries. in towards multidocument summarization by reformulation: progress and prospects, aaai/iaai, 1999, pp. 453   460. 20. jing, h., mckeown, k.: cut and paste based text summarization. in proceedings of the 1st meeting of the north american chapter of the association for computational linguistics, seattle, usa, 2000, pp. 178   185. 21. landauer, t.k., dumais, s.t.: a solution to platos problem: the latent semantic analysis theory of the acquisition, induction, and representation of knowledge. in psychological review, 104, 1997, pp. 211   240. 22. lee, d.d., seung, h.s.:. learning the parts of objects by non-negative id105. in nature, 401 (6755), 1999 pp. 788   791. 23. kolda, t.g., o   leary, d.p.: a semidiscrete matrix decomposition for id45 information retrieval. in acm transactions on information systems 16(4), 1998, pp. 322   346. 24. berry, m.w., dumais, s.t., o   brien., g.w.: using id202 for intelligent ir. in siam review, 37(4), 1995. 25. ding, ch.: a probabilistic model for id45. in journal of the american society for information science and technology, 56(6), 2005, pp. 597   608. 26. yihong gong, xin liu: generic text summarization using relevance measure and latent semantic analysis. in proceedings of acm sigir. new orleans, usa, 2002. 27. steinberger, j., je  ek, k.: text summarization and singular value decomposition. in lecture notes for computer science vol. 2457, springer-verlag, 2004, pp. 245-254. 28. murray, g., renals, s., carletta j.: extractive summarization of meeting recordings. in proceedings of interspeech, lisboa, portugal, 2005. 12

karel je  ek, josef steinberger

29. yeh, j.-y., ke, h.-r., yang, w.-p, meng, i-h.: text summarization using a trainable summarizer and latent semantic analysis. in special issue of information processing and management on an asian digital libraries perspective, 41(1), 2005, pp. 75   95. 30. steinberger, j., k  i    an, m.: lsa-based id57. proceedings of 8th international phd workshop on systems and control, balatonfured, hungary, 2007. 31. b. hachey, g. murray, and d. reitter. the embra system at duc 2005: query-oriented id57 with a very large latent semantic space. in proceedings of the document understanding conference (duc) 2005, vancouver, canada, 2005. 32. lin, ch.: id8: a package for automatic evaluation of summaries. in proceedings of the workshop on text summarization branches out, barcelona, spain, 2004.                  