school of computer science

topic models, latent space 
models, sparse coding,  

and all that 

acknowledgement: amr ahmed, qirong ho, and jun zhu  

   eric xing @ cmu, acl tutorial 2012 

1 

we are inundated with data    (cid:1)

       humans cannot afford to deal with (e.g., search, browse, or 

measure similarity) a huge number of text and media documents 

(from images.google.cn)(cid:1)

       we need computers to help out     

   eric xing @ cmu, acl tutorial 2012 

2 

1 

to get started on intelligent systems for 
automated processing and management of 
large text or media corpora     

       here are some important elements to consider before you start: 

       task: 

       embedding (visualization)? classification? id91? topic extraction?     

       data representation: 

      

input and output (e.g., continuous, binary, counts,    )  

       model: 

      

       id45? id110? markov random fields? regression? id166?  
id136: 
       mcmc? variational? spectrum analysis? 

       learning: 

       id113? mcle? max margin?  

       computation: 

       desktop? hadoop? mpi? 

       evaluation: 

       visualization? human interpretability? perperlexity? predictive accuracy?  

      

it is better to consider one element at a time! 

   eric xing @ cmu, acl tutorial 2012 

3 

tasks: 

       say, we want to have a mapping    , so that  

       compare similarity  
       classify contents 
       cluster/group/categorize docs 
       distill semantics and perspectives  
      

..  

   eric xing @ cmu, acl tutorial 2012 

4 

2 

representation:(cid:1)

       data: 

bag of words representation 

as for the arabian and palestinean voices that are against 
the current negotiations and the so-called peace process, 
they are not against peace per se, but rather for their well-
founded predictions that israel would not give an inch of 
the west bank (and most probably the same for golan 
heights) back to the arabs. an 18 months of "negotiations" 
in madrid, and washington proved these predictions. now 
many will jump on me saying why are you blaming israelis 
for no-result negotiations. i would say why would the 
arabs stall the negotiations, what do they have to loose ?(cid:1)

       each document is a vector in the word space 
      

ignore the order of words in a document. only count matters! 

arabian(cid:1)
negotiations(cid:1)

against(cid:1)
israel(cid:1)

peace(cid:1)
arabs(cid:1) blaming(cid:1)

       a high-dimensional and sparse representation 
       not efficient text processing tasks, e.g., search, document  

 classification, or similarity measure 

       not effective for browsing 

   eric xing @ cmu, acl tutorial 2012 

5 

how to model semantic? 
       q: what is it about? 
       a: mainly mt, with syntax, some learning 

0.6                          0.3                   0.1    

mt                    syntax              learning 

mixing 
proportion 

source 
target 
smt 

alignment 

score 
id7 

parse 
tree 
noun 
phrase 
grammar 

id18 

likelihood 

em 

hidden 

parameters 
estimation 

argmax 

i

 
s
c
p
o
t

a hierarchical phrase-based model  
for id151 

we present a statistical phrase-based  
translation model that uses hierarchical  
phrases   phrases that contain sub-phrases.  
the model is formally a synchronous  
context-free grammar but is learned  
from a bitext without any syntactic  
information. thus it can be seen as a  
shift to the formal machinery of syntax 
based translation systems without any  
linguistic commitment. in our experiments 
 using id7 as a metric, the hierarchical  
phrase based model achieves a relative  
improvement of 7.5% over pharaoh,  
a state-of-the-art phrase-based system. 

unigram over vocabulary 

topic models 

   eric xing @ cmu, acl tutorial 2012 

6 

3 

topic model: the big picture 

unstructured collection 

 structured topic network 

topic discovery 

w1 
x 
x 

x 

wn 

w2 

x 
word simplex 

dimensionality   
reduction 

tk 

t1 
x 
x  x x 
topic simplex 

t2 

   eric xing @ cmu, acl tutorial 2012 

7 

topic model as a graphical model 

generating a document 

prior 

    

z  

w  

nd  

n  

     

k  

we can go beyond this by 
adding more variables and 
structures to the graph! 

   eric xing @ cmu, acl tutorial 2012 

8 

4 

learning and id136 

  n  

  i  

 advanced issues: 
       objective function: likelihood? margin? rss?     
       data: iid docs, streaming text, multimodal media     
       algorithm: direct optimization, monte carlo, variational methods     
system: single machine, multi-core machine, distributed system     
      

   eric xing @ cmu, acl tutorial 2012 

9 

deliverables: 

 we want: 

topics and categorization of documents 
semantic-based ranking of docs 

      
      
       multimedia id136 
       automatic translation  
      
           

predict how topics evolve 

   eric xing @ cmu, acl tutorial 2012 

10 

5 

questions: 

       what is the mathematical and computational basis of all 

these?  

       how to do it right, modular, fast, and real time? 

       how to build other related applications on topic models? 

       how to scale up?   

   eric xing @ cmu, acl tutorial 2012 

11 

plan of this tutorial 

       1. overview of basic topic models 
       2. computational challenges and two classical algorithmic 

paths  

       3. scenario i: multimodal data 
       4. scenario ii: when supervision is available  
       5. scenario iii: what if i don't know the total number of topics  
       6. scenario iv: topic evolution in streaming corpus.  
       7: advanced subject i: sparsity in id96 (see 

emnlp talk)  

       8: advanced subject ii: scalability, complexity, and fast 

algorithms (optional) 

       9: other applications (optional) 

   eric xing @ cmu, acl tutorial 2012 

12 

6 

1. overview of topic models 

   eric xing @ cmu, acl tutorial 2012 

13 

understanding document corpora 

       a document collection is a dataset where each data point is 

itself a collection of simpler data. 

       text documents are collections of words. 
       segmented images are collections of regions. 
       user histories are collections of purchased items. 

       many modern problems ask questions on such data. 

is this text document relevant to my query? 

       what topics do these documents    span   ? 
      
       which category is this text/image in? 
       how have topics changed over time? 
       who wrote this specific document? 
       what will author x write about? 
       and so on   .. 

   eric xing @ cmu, acl tutorial 2012 

14 

7 

the vector space model 

       represent each document by a high-dimensional vector in the 

space of words 

   eric xing @ cmu, acl tutorial 2012 

15 

id45 

document 

m 
r
e
t

= 

... 

* 

* 

x  

(m x n) 

t  

(m x k) 

    

(k x k) 

dt  

(k x n) 

       lsi does not define a properly normalized id203 distribution of 

observed and latent entities  
       does not support probabilistic reasoning under uncertainty and data fusion 

   eric xing @ cmu, acl tutorial 2012 

16 

8 

how our brain might work     

apoptosis + medicine 

   eric xing @ cmu, acl tutorial 2012 

17 

how our brain might work     

apoptosis + medicine 

probabilistic 
generative 

model 

   eric xing @ cmu, acl tutorial 2012 

18 

9 

how our brain might work     

apoptosis + medicine 

statistical 
id136 

   eric xing @ cmu, acl tutorial 2012 

19 

what is learning 

learning is about seeking a predictive and/or executable understanding of natural/artificial 
subjects, phenomena, or activities from     

apoptosis + medicine 

grammatical rules 
manufacturing procedures 
natural laws 
    

id136 

   eric xing @ cmu, acl tutorial 2012 

20 

10 

connecting id203 models to 
data 

(generative model) 
p(data | parameters) 

probabilistic 

model 

real world 

data 

p(parameters | data) 

(id136) 

   eric xing @ cmu, acl tutorial 2012 

21 

what is a graphical model?  
--- example from a signal transduction pathway 

       a possible world for cellular signal transduction:  

receptor a 

x1 

receptor b 

x2 

kinase c 

x3 

kinase d 

x4 

kinase e 

x5 

tf f 

x6 

gene g 

x7 

gene h 

x8 

       a total of  28 joint state 

configurations 

       no "structured insight" of  

the domain  

   eric xing @ cmu, acl tutorial 2012 

22 

11 

recap of basic prob. concepts 

       representation: what is the joint id203 dist. on multiple 

variables? 

       how many state configurations in total? --- 28 
       are they all needed to be represented? 
       do we get any scientific/medical insight? 

       learning: where do we get all this probabilities?  

       maximal-likelihood estimation? but how much data do we need? 
       where do we put domain knowledge in terms of plausible relationships between 

variables, and plausible values of the probabilities? 

      

id136: if not all variables are observable, how to compute 
the conditional distribution of latent variables given evidence? 

   eric xing @ cmu, acl tutorial 2012 

23 

gm: structure simplifies 
representation 

       dependencies among variables 

receptor a 

x1 

receptor b 

x2 

membrane 

kinase c 

x3 

    kinase d 

x4 

kinase e 

x5 

tf f 

x6 

gene g 

x7 

gene h 

x8 

   eric xing @ cmu, acl tutorial 2012 

cytosol 

p(x8| x5, x6) 

f 
f 

e 
e 

nucleus 

24 

12 

probabilistic id114 

       represent dependency structure with a graph 

       node <-> random variable 
       edges encode dependencies 

       absence of edge -> conditional independence 

       directed and undirected versions 

       why is this useful? 

       a language for communication 
       a language for computation 
       a language for development 

       origins:  

       wright 1920   s 
      

independently developed by spiegelhalter and lauritzen in statistics and pearl in 
computer science in the late 1980   s 

   eric xing @ cmu, acl tutorial 2012 

25 

probabilistic id114, 
con'd 

      

if xi's are conditionally independent (as described by a pgm), the joint 
can be factored to a product of simpler terms, e.g.,  

 p(x1, x2, x3, x4, x5, x6, x7, x8)  
=  p(x1) p(x2) p(x3| x1) p(x4| x2) p(x5| x2) 

 p(x6| x3, x4) p(x7| x6) p(x8| x5, x6) 

       why we may favor a pgm? 

       representation cost: how many id203 statements are needed?  

2+2+4+4+4+8+4+8=36, an 8-fold reduction from 28!  

       algorithms for systematic and efficient id136/learning computation 

       exploring the graph structure and probabilistic (e.g., bayesian, markovian) 

semantics 

       incorporation of domain knowledge and causal (logical) structures 

   eric xing @ cmu, acl tutorial 2012 

26 

13 

probabilistic id136 

       computing statistical queries regarding the network, e.g.: 

is node x independent on node y given nodes z,w ? 

      
       what is the id203 of x=true if (y=false and z=true)? 
       what is the joint distribution of (x,y) if z=false? 
       what is the likelihood of some full assignment? 
       what is the most likely assignment of values to all or a subset the nodes of the 

network? 

       general purpose algorithms exist to fully automate such computation  

       computational cost depends on the topology of the network 
       exact id136:  

       the junction tree algorithm 

       approximate id136;  

       loopy belief propagation, variational id136, monte carlo sampling  

   eric xing @ cmu, acl tutorial 2012 

27 

an 
(incomplete) 
genealogy 
of graphical 
models 

(picture by zoubin 
ghahramani and 
sam roweis) 

   eric xing @ cmu, acl tutorial 2012 

28 

14 

latent semantic structure in gm 

latent structure   

distribution over words 

inferring latent structure 

words  

   eric xing @ cmu, acl tutorial 2012 

29 

how to model semantics? 
       q: what is it about? 
       a: mainly mt, with syntax, some learning 

0.6                          0.3                   0.1    

mt                    syntax              learning 

admixing 
proportion 

source 
target 
smt 

alignment 

score 
id7 

parse 
tree 
noun 
phrase 
grammar 

id18 

likelihood 

em 

hidden 

parameters 
estimation 
argmax 

i

 
s
c
p
o
t

a hierarchical phrase-based model  
for id151 

we present a statistical phrase-based  
translation model that uses hierarchical  
phrases   phrases that contain sub-phrases.  
the model is formally a synchronous  
context-free grammar but is learned  
from a bitext without any syntactic  
information. thus it can be seen as a  
shift to the formal machinery of syntax 
based translation systems without any  
linguistic commitment. in our experiments 
 using id7 as a metric, the hierarchical  
phrase based model achieves a relative  
improvement of 7.5% over pharaoh,  
a state-of-the-art phrase-based system. 

unigram over vocabulary 

topic models 

   eric xing @ cmu, acl tutorial 2012 

30 

15 

why this is useful? 
       q: what is it about? 
       a: mainly mt, with syntax, some learning 

0.6                          0.3                   0.1    

mt                    syntax              learning 

admixing 
proportion 

       q: give me similar document? 

       structured way of browsing the collection 

       other tasks 

       id84  

       tf-idf vs. topic mixing proportion 
       classification, id91, and more     

a hierarchical phrase-based model  
for id151 

we present a statistical phrase-based  
translation model that uses hierarchical  
phrases   phrases that contain sub-phrases.  
the model is formally a synchronous  
context-free grammar but is learned  
from a bitext without any syntactic  
information. thus it can be seen as a  
shift to the formal machinery of syntax 
based translation systems without any  
linguistic commitment. in our experiments 
 using id7 as a metric, the hierarchical  
phrase based model achieves a relative  
improvement of 7.5% over pharaoh,  
a state-of-the-art phrase-based system. 

   eric xing @ cmu, acl tutorial 2012 

31 

words in contexts 

    it was a nice shot.     

      

   eric xing @ cmu, acl tutorial 2012 

32 

16 

words in contexts (con'd) 

      

the opposition labor party fared even worse,  with a 
predicted 35 seats,  seven less than last election. 

   eric xing @ cmu, acl tutorial 2012 

33 

"words" in contexts (con'd) 

   eric xing @ cmu, acl tutorial 2012 

sivic et al. iccv 2005 
34 

17 

a possible generative process of 
a  document 

o

a

n

 

.8 

m oney 

l

money 
b a n k  
loan 

 
k
n
a
b

loan 
topic 1 

.3 

.2 

.7 

ri v

r

 

r

i

v

e

bank 
r e a m  
s t
r 
e
bank 
stream 
topic 2 
mixture  

components 

(distributions over 

elements) 

document  1:  money1  bank1  bank1  loan1  river2  stream2 
bank1  money1  river2  bank1  money1  bank1    loan1      money1 
stream2 bank1  money1 bank1 bank1 loan1 river2 stream2 bank1 
money1  river2 bank1 money1  bank1    loan1      bank1   money1 
stream2  

document 2: river2   stream2 bank2 stream2 bank2 money1  
loan1 river2  stream2 loan1 bank2 river2 bank2 bank1    stream2 
river2  loan1 bank2 stream2 bank2 money1  loan1 river2  stream2 
bank2  stream2  bank2  money1    river2    stream2    loan1  bank2 
river2  bank2  money1    bank1    stream2  river2  bank2  stream2 
bank2 money1   

admixing weight 

vector   	


(represents all 
components    
contributions) 

bayesian approach:    use priors    
admixture weights
 ~ dirichlet(    )  
mixture components ~ dirichlet(    )  

   eric xing @ cmu, acl tutorial 2012 

35 

method one: 

   eric xing @ cmu, acl tutorial 2012 

36 

18 

probabilistic lsi 

hoffman (1999) 

  	

d 

  	


k 

zn 

wn 

n 

m 

p(d,wn) = p(d)
  

   
z

n

          

       
       

n =1

       
p(wn | zn)p(zn | d)
       
       

   eric xing @ cmu, acl tutorial 2012 

37 

    

probabilistic lsi 

       a "generative" model 
       models each word in a document as a sample from a mixture 

model. 

       each word is generated from a single topic, different words in 

the document may be generated from different topics. 

       a topic is characterized by a distribution over words. 
       each document is represented as a list of admixing 
proportions for the components (i.e. topic vector    ). 

   eric xing @ cmu, acl tutorial 2012 

38 

19 

id44 

blei, ng and jordan (2003) 

essentially a bayesian plsi: 

  	


  k 

k 

  	


  	


zn 

wn 

n 

m 

n
       
p(  )p(  )
   
       
       
n =1

   eric xing @ cmu, acl tutorial 2012 

p(zn   )p(wn   zn)

       
       d   d  
       

39 

p(w) =

   

   
z

    

lda 

       generative model 
       models each word in a document as a sample from a mixture 

model. 

       each word is generated from a single topic, different words in 

the document may be generated from different topics. 

       a topic is characterized by a distribution over words. 
       each document is represented as a list of admixing 

proportions for the components (i.e. topic vector). 

       the topic vectors and the word rates each follows a dirichlet 

prior --- essentially a bayesian plsi  

   eric xing @ cmu, acl tutorial 2012 

40 

20 

lda was first invented by 
geneticist     

        how to present population structure? 

       structure 

ancestral 
proportion  

profiles 

id136 of population structure using multilocus genotype data. j.k. pritcha
rd, m. stephens and p. j. donnelly, 2000. genetics 155: 945-959. 
genetic structure of human populations (rosenberg et al. science, 2002)!
41 

topic models = mixed 
membership models = admixture 

generating a document 

prior 

    

z  

w  

nd  

n  

which prior to use? 

     

k  

   eric xing @ cmu, acl tutorial 2012 

42 

21 

choices of priors 

       dirichlet (lda) (blei et al. 2003) 

       conjugate prior means efficient id136 
       can only capture variations in each topic   s 

intensity independently  

       logistic normal (ctm=lontam) 

(blei & lafferty 2005, ahmed & 
xing 2006) 
       capture the intuition that some topics are highly 
correlated and can rise up in intensity together  

       not a conjugate prior implies hard id136 

       nested crp (blei et al 2005) 

       defines hierarchy on topics 
            

   eric xing @ cmu, acl tutorial 2012 

43 

generative semantic of lontam 

generating a document 

    

     

k  

   ~ lnk   ,  (
)
   ~ nk    1   ,  (
)                k = 0
       
       
       
  i = exp   i    log 1+
       
       
       
       
       
       
k    1
   
i=1

       
c   ( ) = log 1+
       
       

k    1
   
i=1
       
           
       

       
       
       

e  i

e  i

- log partition function 
- id172 constant 

   eric xing @ cmu, acl tutorial 2012 

44 

    

     

z  

w  

nd  

n  

    

22 

outcomes from a topic model  
       the    topics       in a corpus: 

      
      
      

there is no name for each    topic   , you need to name it! 
there is no objective measure of good/bad 
the shown topics are the    good    ones, there are many many trivial ones, meaningless ones, 
redundant ones,     you need to manually prune the results 

       how many topics?        

   eric xing @ cmu, acl tutorial 2012 

45 

outcomes from a topic model  
       the    topic vector       of each doc 

       create an embedding of docs in a    topic space    
      
      

their no ground truth of    to measure quality of id136  
but on    it is possible to define an    objective    measure of goodness, such as classification 
error, retrieval of similar docs, id91, etc., of documents 
but there is no consensus on whether these tasks bear the true value of topic models      

      

   eric xing @ cmu, acl tutorial 2012 

46 

23 

outcomes from a topic model  

       the per-word topic indicator z: 

       not very useful under the bag of word representation,  

 because of loss of ordering 

       but it is possible to define simple probabilistic linguistic 

constraints (e.g, bi-grams) over z and get potentially 
interesting results [griffiths, steyvers, blei, & tenenbaum, 2004]   

   eric xing @ cmu, acl tutorial 2012 

47 

outcomes from a topic model  

       the topic graph s (when using ctm): 

       kind of interesting for understanding/visualizing large corpora  

[david blei, mlss09] 

   eric xing @ cmu, acl tutorial 2012 

48 

24 

outcomes from a topic model 

       topic change trends 

   eric xing @ cmu, acl tutorial 2012 

49 

[david blei, mlss09] 

method two: 

   eric xing @ cmu, acl tutorial 2012 

50 

25 

the harmonium 

hidden units 

visible units 

p(x,h |  ) = exp{   i  i(xi)

   
i

+   j  j(h j)
   
j

+   i, j  i, j(xi,h j)
   
i, j

    a(  ) }

   eric xing @ cmu, acl tutorial 2012 

51 

a binomial word-count model 

e.p. xing, r. yan and a. g. hauptmann,  uai 2006 

topics 

hj = 3: topic j has strength 3 

h j = wi, jxi
xi = n: word i has count n 

   
i

words counts 
p(h | x) =
normalh j
   
j
  
p(x |h) =
bixi [ n,  
   
i
       
       p(x)     exp
i    -log  (xi)-log  (n - xi)
(
  ixi
       
       

    
       
       
    w ij
    x i,1
   
       
       
       
       
i
exp(  j +
)
wij h j
j   
1+exp(  j +
wij h j
j   

) ]

) + 1

2

j   

i   (

wi, jxi

)2

       
       
       

   eric xing @ cmu, acl tutorial 2012 

52 

26 

    

    

    

    

the computational trade-off 

undirected model: learning is hard, id136 is easy. 

directed model: learning is "easier", id136 is hard. 

example: document retrieval. 

topics 

words 

retrieval is based on comparing (posterior) topic distributions of documents. 
- directed models:  id136 is slow. learning is relatively    easy   . 
- undirected model: id136 is fast. learning is slow but can be done offline. 

   eric xing @ cmu, acl tutorial 2012 

53 

method three: 

   eric xing @ cmu, acl tutorial 2012 

54 

27 

sparse coding	

= 

   

 

x 

   

 

    

       let x be a signal, e.g., speech, image, etc. 
       let    be a set of normalized    basis vectors    

x 

       we call it dictionary 

          is    adapted    to x if it can represent it with a few basis vectors 

       there exists a sparse vector    such that x            
       we call    the sparse code 

   eric xing @ cmu, acl tutorial 2012 

55 

primer on sparse coding 

       sparse coding with appropriate constraints: 

reconstruction loss 

sparsity-inducing regularizer 

 

  

       reconstruction loss can be: 

      

the general log-likelihood loss of an exponential family distribution (lee et al., 
2010) 

       sparisty-inducing regularizer can be: 

      

the l0    pseudo-norm   :  
the l1 norm:  

        1 :=   i

      
       structured regularizers, e.g., group lasso (bengio et al., 2009) 

|  i|

  (  i, 0)

        0 :=   i

       suggests an alternating optimization procedure 

np-hard 
convex 

        1/2 :=   g      ig   2

   eric xing @ cmu, acl tutorial 2012 

56 

28 

sparse topical coding 

       goal: design a non-probabilistic topic model that is amenable to 

      
      
      

direct control on the posterior sparsity of inferred representations 
avoid dealing with id172 constant when considering supervision or rich features 
seaid113ss integration with a convex id168 (e.g., id166 hinge loss) 

       we extend sparse coding to hierarchical sparse topical coding 

       word code    
      

document code s 

reconstruction loss 

sparse codes 

non-negative codes 

topical bases 

   eric xing @ cmu, acl tutorial 2012 

truncated aggregation 

j. zhu, & e.p. xing. uai, 2011 

57 

summary:  
latent sub-space models 

latent representation 
  

the model:  
p(w,  ;  )

    

words  

    

    

inferring latent representation: 

p(  | w) =

p(w,  )
p(w)

learning the subspace: 
  = argmin f  (w,  )

   eric xing @ cmu, acl tutorial 2012 

58 

    

29 

the big picture 

unstructured collection 

 structured topic network 

topic discovery 

w1 
x 
x 

x 

wn 

w2 

x 
word simplex 

t1 
x 
x  x x 
topic space  
(e.g, a simplex) 

t2 

dimensionality   
reduction 

tk 

   eric xing @ cmu, acl tutorial 2012 

59 

comparison of model semantics  

documents 

topic 

 
s
d
r
o
w

 
s
d
r
o
w

w       =      b                        	


 
c
i
p
o
t

 
c
i
p
o
t

 
s
d
r
o
w

topic 

documents 

    w  = b'     
   
  

documents 

topics 

p(w) 
           =     	


 
s
d
r
o
w

 
)
z
|
w
(
p

documents 

topic 

stc/nmf/lsi 

documents 

    

  =(  1,...,    ),   i=p(z) 

topic-mixing is via marginalizing 
over word labeling 

lda 

p(     w )    z   
  

    
   

 
s
c
i
p
o
t

 
s
d
r
o
w

p(w) 

documents 

 
s
d
r
o

=	

w

b 

 
c
i
p
o
t

  =(  1,...,    ) 

mixing is via determining 
    
individual word rate 

p(     w )    b'     
   
  

60 

   eric xing @ cmu, acl tutorial 2012 

harmonium 

    

30 

comparison of topic space 

topic 1 

topic 
simplex 

topic 1 

topic space 

word 
simplex 

topic 2 

topic 3 

topic 2 

word count 
quadrant 

topic 3 

   eric xing @ cmu, acl tutorial 2012 

61 

comparisons 

lda vs. harmonium 

[xing, yan, and hauptman, uai 2005] 

classification 

retrieval 

annotation 

       lda is actually doing very poor on several    objectively    

evaluatable predictive tasks 

   eric xing @ cmu, acl tutorial 2012 

62 

31 

comparisons 

lda vs. stc 
[zhu and xing, uai 2011] 

   eric xing @ cmu, acl tutorial 2012 

63 

sparse word codes 

       sparsity ratio: percentage of zeros 

       nmf: non-negative id105 
       medlda (zhu et al., 2009) 
       reglda: lda with entropic regularizer 
       gaussstc: use l2 rather than l1-norm 

   eric xing @ cmu, acl tutorial 2012 

64 

32 

2. computational challenges and 
three algorithmic paths  

   eric xing @ cmu, acl tutorial 2012 

65 

computation on lda 

       id136 

       given a document d 

       posterior: p(   |   ,  ,    ,d) 
       evaluation: p(d|   ,  ,    ) 

  n  

  i  

       learning 

       given a collection of documents {di} 

       parameter estimation 

   eric xing @ cmu, acl tutorial 2012 

66 

33 

exact bayesian id136 on lda 
is intractable 

       a possible query: 

   n  

       close form solution? 

   n  

  n  

    

  n  

  n  

  -n  

    

    

  n  

  n  

    

    

    

    

       sum in the denominator over tn terms, and integrate over n k-dimensional topic 

vectors 

   eric xing @ cmu, acl tutorial 2012 

67 

approximate id136 

       variational id136 

       mean field approximation (blei et al) 
       expectation propagation (minka et al) 
       variational 2nd-order taylor approximation (ahmed and xing) 

       id115 

       id150 (griffiths et al) 

   eric xing @ cmu, acl tutorial 2012 

68 

34 

collapsed id150 
(tom griffiths & mark steyvers) 

       collapsed id150 

      

integrate out   	


for variables z = z1, z2,    , zn 
(t+1) from p(zi|z-i, w) 
(t+1),   , zi-1

   draw zi
 

 z-i = z1

(t+1), z2

(t+1), zi+1

  i  

(t),    , zn

(t) 

{z(1), z(2), . . . , z(t )}

   =

z(t)

1

t    t

   eric xing @ cmu, acl tutorial 2012 

  n  

69 

id150  

       need full conditional distributions for variables 
       since we only sample z we need 

  n  

  i  

g 
g 

number of times word w assigned to topic j 

number of times topic j used in document d 

   eric xing @ cmu, acl tutorial 2012 

70 

35 

id150 

iteration 
1 

   eric xing @ cmu, acl tutorial 2012 

71 

id150 

iteration 
1             2 

   eric xing @ cmu, acl tutorial 2012 

72 

36 

id150 

iteration 
1             2 

   eric xing @ cmu, acl tutorial 2012 

73 

g 

g 

id150 

iteration 
1             2 

   eric xing @ cmu, acl tutorial 2012 

74 

g 

g 

37 

id150 

iteration 
1             2 

   eric xing @ cmu, acl tutorial 2012 

75 

g 

g 

id150 

iteration 
1             2 

   eric xing @ cmu, acl tutorial 2012 

76 

g 

g 

38 

id150 

iteration 
1             2 

   eric xing @ cmu, acl tutorial 2012 

77 

g 

g 

id150 

iteration 
1             2 

   eric xing @ cmu, acl tutorial 2012 

78 

g 

g 

39 

id150 

iteration 
1             2                                          1000 

   =

z(t)

1

t    t

   eric xing @ cmu, acl tutorial 2012 

79 

g 

g 

variational id136 
(e.g., mf, jordan et al 1999, gmf, xing et al 2004)  

       variational approximation 

   	


       data set: 

       15,000 documents  
       90,000 terms 
       2.1 million words 

       model: 

       100 factors 
       9 million parameters 

       on a single machine mcmc could converge too slowly for this 

problem, but     

   eric xing @ cmu, acl tutorial 2012 

80 

40 

learning a tm 

       id113: 

  1,  2,   ,  k
{
  

},   = argmax
(  ,  )

   

log p di   ,  
)
)

(

(

       need statistics on topic-specific word assignment (due to z), topic 

vector distribution (due to   ), etc. 
      
    

e.g,, this is the formula for topic k:  

  k =

  (zd,dn, k)wd,dn

d   d=1

nd   dn=1

1

   d nd

       these are hidden variables, therefore need an em algorithm (also 

known as data augmentation, or da, in monte carlo paradigm) 

       this is a    reduce    step in parallel implementation 

   eric xing @ cmu, acl tutorial 2012 

81 

how to evaluate id136/
learning algorithm? 

       empirical performance on, say, id91, classification, topic 

saliency, perplexity      ? 

       there is no ground truth, poor/good performance may come 

from model, data, algorithm, parameter tuning     

      

in simulation you know the ground, thus you can exclusively 
compare the difference caused by id136/learning 
algorithm! 

   eric xing @ cmu, acl tutorial 2012 

82 

41 

case study:  
correlated topic model 

two approaches to approximate it: 
- blei and lafferty use tangent 
- (xing 2005) uses second order truncated 
taylor approximation 

non-conjugacy comes 
here 

   eric xing @ cmu, acl tutorial 2012 

83 

variational id136 of ctm  

  

  

  

z
w

  

  * is full matrix 

multivariate 

quadratic approx. 

closed form 
solution for   *,   *  

ahmed&xing 05 

log partition function 

  *  

  *  

  

     

z

w

  

  * is assumed to be diagonal 

tangent approx. 

numerical 
optimization to 
fit   *, diag(  *) 
blei&lafferty 05 

   eric xing @ cmu, acl tutorial 2012 

84 

42 

variational id136 with no tears 

  

  

    

z
w

  

iterate until convergence 
       pretend you know e[z1:n] 

       p(  |e[z1:n],   ,   ) 

       now you know e[  ] 
       p(z1:n|e[  ], w1:n,   1:k) 

       more formally: 

message passing scheme  (gmf) 
equivalent to previous method (xing et. al.2003) 

   eric xing @ cmu, acl tutorial 2012 

85 

lontam variations id136 

       fully factored distribution 

       two clusters:    and z1:n 

       fixed point equations 

  

  

    
z
w

  

  

    

z
w

  

  

   eric xing @ cmu, acl tutorial 2012 

86 

43 

variational   	


now what is                ?  

  

  

  

qz 

  

  

    
z

    
z
w

   eric xing @ cmu, acl tutorial 2012 

87 

variational   	


    

  

    

q  	


z
w

  

  

    
z
w

   eric xing @ cmu, acl tutorial 2012 

88 

44 

tangent approximation 

   eric xing @ cmu, acl tutorial 2012 

89 

different learning/id136 
deliver different performance 

test on synthetic text  
(of    known    ground truth): 

  

  

  
z
w

     

   eric xing @ cmu, acl tutorial 2012 

90 

45 

comparison: accuracy and speed 

l2 error in topic vector est. 
and # of iterations 

      

varying num. of topics 

      

varying voc. size 

      

varying num. words per 
document 

   eric xing @ cmu, acl tutorial 2012 

91 

result on nips collection 

       nips proceeding from 1988-2003 
       14036 words  
       2484 docs 
       80% for training and 20% for testing 
       fit both models with 10,20,30,40 topics 
       compare perplexity on held out data 

       the perplexity of a language model with respect to text x is the reciprocal of the 

geometric average of the probabilities of the predictions in text x.  so, if text x has 
k words, then the perplexity of the language model with respect to that text is  
 

 pr(x) -1/k 

 

 

     

   eric xing @ cmu, acl tutorial 2012 

92 

46 

comparison: perplexity 

   eric xing @ cmu, acl tutorial 2012 

93 

classification result on pnas 
collection 

       pnas abstracts from 1997-2002 

      
      

2500  documents 
average of 170 words per document 

       fitted 40-topics model using both approaches 
       use low dimensional representation to predict the abstract category 

       use id166 classifier 
      

85% for training and 15% for testing 

classification accuracy 

-   notable difference 
-   examine the low dimensional 
representations below 

   eric xing @ cmu, acl tutorial 2012 

94 

47 

computation on undirected tm 

[welling et al nips 04, xing et al, uai 05] 

undirected model: learning is hard, id136 is easy. 

directed model: learning is "easier", id136 is hard. 

example: document retrieval. 

topics 

words 

retrieval is based on comparing (posterior) topic distributions of documents. 
- directed models:  id136 is slow. learning is relatively    easy   . 
- undirected model: id136 is fast. learning is slow but can be done offline. 

   eric xing @ cmu, acl tutorial 2012 

95 

properties of directed networks 

       factors are marginally 

independent. 

       factors are conditionally 

dependent given observations 
on the visible nodes.  

       easy ancestral sampling. 

       learning with (variational) em  

   eric xing @ cmu, acl tutorial 2012 

96 

48 

properties of harmoniums 

       factors are marginally dependent. 

       factors are conditionally 

independent given observations on 
the visible nodes.  

      

iterative id150. 

       learning with contrastive 

divergence  

   eric xing @ cmu, acl tutorial 2012 

97 

learning and id136 

       maximal likelihood learning based on gradient ascent. 

       gradient computation requires model distribution p(.) 
      

 p(.) is intractable 

p 

       contrastive divergence 

       approximate p(.) with id150 

       variational approximation 

       gmf approximation 
q(x,z,h) =

#
i

! 

q(xi |"i)

q(zk |  k,$k)

#
k

   eric xing @ cmu, acl tutorial 2012 

#
j

q(h j |%j)

98 

49 

performance 

classification 

retrieval 

annotation 

   eric xing @ cmu, acl tutorial 2012 

99 

computation on stc 

[zhu and xing, uai 11] 

       hierarchical sparse coding 

      

for each document 

       word code 

       document code (truncated averaging) 

       dictionary learning 

       projected id119 
       any faster alternative method can be used 

   eric xing @ cmu, acl tutorial 2012 

100 

50 

opt. algorithm for sparse coding 

       much research has been done for optimizing a convex, but 

non-smooth objective (may subject to some constraints, e.g., 
non-negativity) 

       greedy algorithm for the non-convex l0    pseudo-norm   :  

      
      

select the element with maximum correlation with the residual 
known as    matching pursuit    (mallat & zhang, 1993) 
       for the convex l1 norm, many algorithms: 

       soft-thresholding with coordinate descent (friedman et al., 2007; fu, 1998; zhu & 

xing, 2011) 

iterative re-weighted least squares (daubechies et al., 2008) 

       proximal methods (nesterov, 2007; jenatton et al., 2010) 
       active-set methods (roth & fischer, 2008) 
      
       lars (efron et al., 2004) solves for id173 path 
       online/stochastic variants  
           

   eric xing @ cmu, acl tutorial 2012 

101 

opt. algorithm for dictionary 
learning 

       optimize a convex and usually smooth objective w/o (convex) 

constraints 

       general optimization procedure can be applied, less research 

has been done for this step 
       projected id119 
       block-wise coordinate descent 
           

       a recent progress is made on online/stochastic optimization 

method (mairal et al., 2010) 

   eric xing @ cmu, acl tutorial 2012 

102 

51 

performance 

~ 10 times speed up in train &test 

   eric xing @ cmu, acl tutorial 2012 

103 

3. scenario i: multimodal data 

   eric xing @ cmu, acl tutorial 2012 

104 

52 

multi-view analysis 

       two-view webpage documents (blum & mitchell, 1998) 

       contents 
      

in-link anchor texts 

       multi-view images 

       local features, e.g., color histogram, sift bag-of-word, 

sparse sift codes, et al. 

       global features, e.g., gist (oliva & torralba, 2001), et al. 
       online tags 

       social media and social networks  

           

       problems with a flat model, e.g., id166 

       type difference (from different distributions) 
       scale, length difference, etc. 
      

incapable of doing view-level prediction 

       multi-view data analysis via latent sub-space discovery 

       provide a good joint distribution  
       provide good conditional distributions of the description type 

conditioned on the primary type 

athlete 
horse 
grass 
trees 
sky 
saddle 

   eric xing @ cmu, acl tutorial 2012 

105 

latent space models for images 

   beach    

id44 (lda) 

c 

   

d 

z 

n 

w 

   eric xing @ cmu, acl tutorial 2012 

fei-fei et al. iccv 2005 
106 

53 

image representation 

cat, grass, tiger, water 

representation vector 

(real, 1 per image segment) 

annotation vector 

(binary, same for each segment) 

   eric xing @ cmu, acl tutorial 2012 

107 

to generate an image     

   eric xing @ cmu, acl tutorial 2012 

108 

54 

annotated images 

       forsyth et. al. (2001): images as documents where region-

specific feature vectors are like visual words. 

       a captioned image can be thought of as annotated data: two 

documents, one of which describes the other. 

   eric xing @ cmu, acl tutorial 2012 

109 

gaussian-multinomial lda 

[bliei et al, jmlr 05] 

       a natural next step is to glue two lda models together. 
       bottom: a traditional lda model on captions 
       top: a gaussian-lda model on images  

       each region is a multivariate gaussian 
 does not work well 

   eric xing @ cmu, acl tutorial 2012 

      

110 

55 

exchangeability 

       like lda, gm-lda implicitly makes an exchangeability assumption about 

words and regions, and their corresponding topics. 

       the order in which words and regions are generated does not matter.  
       but this is goes against the way we   re thinking about the data!  
       the words are chosen to describe the image. 
       the implicit exchangeability assumptions in the model should reflect this. in 

other words, we want to model partial exchangeability 

   eric xing @ cmu, acl tutorial 2012 

111 

corr-lda 

[bliei et al, jmlr 05] 

       since, w is conditioned on z, the image must be generated first. 
       unlike gm-lda, the caption is guaranteed to be generated by a subset 

of the same hidden factors which generated the image. 

       the model enforces a correspondence between the latent space 

associated with images and the latent space associated with captions. 

   eric xing @ cmu, acl tutorial 2012 

112 

56 

automatic annotation 

   eric xing @ cmu, acl tutorial 2012 

113 

text-based id162 

   eric xing @ cmu, acl tutorial 2012 

114 

57 

multi-view social media data 

friendship network 

user 
text 

friends 

interest 

interest 

   eric xing @ cmu, acl tutorial 2012 

interest 
labels 

user 
image 

115 

latent space models for network 

   eric xing @ cmu, acl tutorial 2012 

116 

58 

example: 

   eric xing @ cmu, acl tutorial 2012 

117 

mixed membership stochastic 
blockmodel [airoldi, blei, fienberg and xing, 2008] 

   

   

   eric xing @ cmu, acl tutorial 2012 

118 

59 

in the mixed-membership 
simplex [airoldi, blei, fienberg and xing, 2008] 

    

    

   eric xing @ cmu, acl tutorial 2012 

119 

the    facebook    model 

  	
   

latent	
   space	
   

  i	
   

yi	
   

  ,  2	
   

interest	
   model	
   

p users 

di docs 

zik	
   

wik tokens 

  	
   

fikl	
   

wikl	
   

text	
   model	
   

  back	
   

k topics 

  a	
   

  	
   

qi neighbors 

sij	
   

network	
   model	
   

m positive edges 

k topics 

eij	
   

  ab	
   

  0,  1	
   

   eric xing @ cmu, acl tutorial 2012 

120 

60 

the    facebook    model 

topic vector prior 

  	
   

p users 

di docs 

user topic vectors 

latent	
   space	
   

  i	
   

zik	
   

document topic indicators 

qi friends 

sij	
   

yi	
   

user interest 
(real-valued, +1 or -1) 

  ,  2	
   

gaussian user 
interest prior 

interest	
   model	
   

network	
   model	
   

m friendships 

eij	
   

k topics 

  ab	
   

topic-topic 
friendship probabilities 

  0,  1	
   

friendship 
id203 prior 

   eric xing @ cmu, acl tutorial 2012 

121 

foreground/ 
background prior 

wik tokens 

  	
   

fikl	
   

wikl	
   

observed words 

word foreground/ 
background indicators 

text	
   model	
   

background 
vocabulary 

  back	
   

k topics 

topic 
vocabularies 

  a	
   

vocabulary prior 

  	
   

a peep of 
the 
facebook 
community 

camping 

ca (4.9%) +2.48 
country_music, farm, 

fish, george_strait, hunt, 

jason_aldean, mafia, 
park, texas_hold_em 

6.9% 

4.3% 

4.5% 

co (5.4%) +1.38 

beatles, call_of_duty, 
dexter, family_guy, 
harry_potter, hike, 
history, johnny_cash, 
pink_floyd, star_wars 

cooking 

co (2.2%) +0.99 
beach, center, 

city, club, 

garden, grill, 
magazine, 
music, travel, 

wine 

9.7% 

4.4% 

6.3% 

ca (0.8%) -1.15 

bath, body, 

elementary_school, 

jail, live, net, sir, 
st_louis, tire, work 

ca (1.3%) -0.44 
art, bar, center, 
design, farm, grill, 

llc, magazine, 

photography, studio 

ca (1.6%) -0.23 

conservative, facebook, 
funer, military, project, 
soldier, stop, support, 

walt_disney, 

walt_disney_world 

4.6% 

11.2% 

facebook fanpages 

ca (11.5%) +0.92, co (12.2%) +0.61 
mo (12.5%) +1.04, sp (14.7%) +1.25 

ca (1.7%) -0.93, co (1.4%) -0.91 
mo (1.3%) -1.02, sp (1.4%) -0.63 

adam_sandler, basketball, disney, drake, 

dr_pepper, family_guy, fresh_prince, 
hangover, harry_potter, movie, mtv, 

nicki_minaj, oreo, simpsons, starbucks, 

starburst, subway, skittles, twilight, youtube 

bet, click, cop, dear, fan, find, glug, head, 

italian, justin_bieber, law_order, math, 

million, movie, office, org, page, problem, 
reach, solve, sorry, sound, stay_up_late, 
strong, sure, the_big_bang_theory, twilight 

informal conversation in status updates 

co (1.8%) -1.06 

bet, children, fan, fanatic, 

glee, million, nation, 
pants_on_the_ground, 

pizza, usa 

co (1.1%) -0.65 

art, beauty, boutique, 

design, italian, 
olive_garden, 

photography, restaurant, 

salon, studio 

ca (1.9%) -0.59, co (2.0%) -0.88 
mo (1.4%) -1.01, sp (0.8%) -1.10 

cash, chicken_coop, farm, farmville, flower, 

free, girl, group, hip_hop, join, kelloggs, 

mafia_wars, pop_tart, progress, work, zynga 

ca (3.1%) +1.43, co (29.9%) 

mo (29.6%) +5.17, sp (0.9%) 

+6.24 

-0.72 

buddy, care, don   t, elf, find, friend, hear, 
homework, hurt, mad, mean, person, say, 
smile, sorry, stop, stupid, talk, text, truth 

5.2% 

ca (5.8%) -0.86, co (4.6%) -0.93 

mo (6.6%) -0.42, sp (12.4%)  +2.65 

ca (1.2%) -0.31, co (0.8%) -0.30 
mo (0.8%) -0.80, sp (1.1%) -0.49 

beauty, boy, boyfriend, cute, dude, friend, girl, 

girlfriend, guy, hot, mean, play, say, text, 

treat, ugly 

annoy, answer, ask, dad, food, found, friend, 

hey, house, look, mom, mommy, mother, 

nevermind, smell, sex, slut, ye, yeah 

4.3% 

21.9% 

mo (8.3%)  +1.73 

ac_dc, beatles, family_guy, 

gummy_worm, history, 

metallica, music, simpsons, 

south_park, sour 

mo (0.6%)  +1.64 

end, epic, find, listen, 
money, movie, music, 

pocket, sex, won 

sp (0.8%) +3.04 

basketball, call_of_duty, 
dance, footbal, listen, 

music, nike, play, 

soccer, show 

sp (0.9%)  +0.90 

beatles, family_guy, good_game, 
greys_anatomy, espn, sportscenter, 

the_secret_life_of_the_american_teenager 

13.3% 

12.6% 

mo (1.3%) -0.39 
art, design, music, 

photography, product, 
spell, studio, taught, 

toy, usa 

mo (1.9%) -0.62 
bag, bar, center, 
family, food, grill, 

red_hot_chili_peppers, 

restaurant, sport 

5.9% 

sp (0.7%) -0.06 
camp, farm, fish, 
grill, hunt, mafia, 

olive_garden, 
park, pizza, 
restaurant 

4.4% 

sp (1.2%) -0.94 

art, blue, center, design, 

kim_kardashian, llc, 

photography, 

soulja_boy_tell_em 

movies 

   eric xing @ cmu, acl tutorial 2012 

13.2% 

sports 

122 

61 

the harmonium counterpart 

[xing et al, uai 05] 

   	


just add one more wing: 
p(z |h) =
)
,  2 ]

normal[   2(  j + wij h j

   
k

   
j

, 

p(h | x,z) =

   
j

normal[ wij xi

   
i

    ,1
+ ukjzk

k

]

    

    

   eric xing @ cmu, acl tutorial 2012 

123 

multi-wing harmoniums 

   eric xing @ cmu, acl tutorial 2012 

124 

62 

multi-view markov networks 
       an simple undirected gm with conditional independence: 

       local conditional markov networks (crfs conditioned on latent h): 

       conditionally independent latent variables 

       very efficient for fully observed view input data; potentially scale up to 

large data (e.g., millions of images) (weston et al, 2010) 

   eric xing @ cmu, acl tutorial 2012 

125 

examples of latent topics 

   eric xing @ cmu, acl tutorial 2012 

126 

63 

are we done? 

       what was our task? 

       embedding (lower dimensional representation): yes, doc       	

       distillation of semantics: kind of, we   ve learned    topics        
       classification: is it good? 
       id91: is it reasonable?  
       other predictive tasks? 

   eric xing @ cmu, acl tutorial 2012 

127 

4. scenario ii: when supervision 
is available  

   eric xing @ cmu, acl tutorial 2012 

128 

64 

problem: discriminative topic models 
for text classification/scoring  

       democratic or republican?  

       movie review/scoring 

   eric xing @ cmu, acl tutorial 2012 

129 

we want to answer      

       are we satisfied with the conventional topic models and the 

id113 method for prediction? 

       can we learn a predictive model better? 

   eric xing @ cmu, acl tutorial 2012 

130 

65 

the shocking results on lda  

classification 

retrieval 

annotation 

       lda is actually doing very poor on several    objectively    

evaluatable predictive tasks 

   eric xing @ cmu, acl tutorial 2012 

131 

why? 

       lda is not designed, nor trained for such tasks, such as 

classification, there is not warrantee that the estimated topic 
vector    is good at discriminating documents 

   eric xing @ cmu, acl tutorial 2012 

132 

66 

unsupervised latent subspace 
discovery 
       finding latent subspace representations (an old topic) 

       mapping a high-dimensional representation into a latent low-dimensional representation, where each 

dimension can have some interpretable meaning, e.g., a semantic topic 

       examples: 

      

topic models (aka lda) [blei et al 2003] 

      

total scene latent space models [li et al 2009] 

       multi-view latent markov models [xing et al 2005] 

   	


athlet
e 
horse 
grass 
trees 
sky 
saddl
e 

   	


   	


      

pca, cca,     

   eric xing @ cmu, acl tutorial 2012 

133 

predictive subspace learning 
with supervision 

       unsupervised latent subspace representations are generic but 

can be sub-optimal for predictions 

       many datasets are available with supervised side information 

       tripadvisor hotel review (
http://www.tripadvisor.com) 

       labelme 

http://labelme.csail.mit.edu/ 

       can be noisy, but not random noise (ames & naaman, 2007) 

       many others 
flickr (http://www.flickr.com/) 

      
      

labels & rating scores are usually assigned based on some intrinsic property of the data 
helpful to suppress noise and capture the most useful aspects of the data 

       goals: 

       discover latent subspace representations that are both predictive and 

interpretable by exploring weak supervision information 

   eric xing @ cmu, acl tutorial 2012 

134 

67 

i. supervised topic model(cid:1)

(blei & mcauliffe, 2007)(cid:1)

       how to integrate the max-margin principle into a 

probabilistic latent variable model? 

max-likelihood 

estimation(cid:1)

slda(cid:1)

max-margin and max-

likelihood(cid:1)

medlda(cid:1)

(zhu et al, icml 2009)(cid:1)

   eric xing @ cmu, acl tutorial 2012 

135 

supervised topic model(cid:1)

       lda ignores documents    side information (e.g., categories or rating 
score), thus lead to suboptimal topic representation for supervised 
tasks 

       supervised topic models handle such problems, e.g., slda (blei & 

mcauliffe, 2007) and disclda(simon et al., 2008)(cid:1)

       generative procedure (slda): 

       for each document d: 

       sample a topic proportion 
       for each word: 

       sample a topic 
       sample a word  

       sample  

(blei & mcauliffe, 2007)(cid:1)

continuous  (regression)(cid:1)
discrete (classification)(cid:1)

   eric xing @ cmu, acl tutorial 2012 

136 

68 

how to train slda? 

       maximize 

       maximize 

       support vector machines 

   eric xing @ cmu, acl tutorial 2012 

137 

support vector machines 

class 2 

class 1 

   eric xing @ cmu, acl tutorial 2012 

138 

69 

id166 using vc-dimension 

   eric xing @ cmu, acl tutorial 2012 

139 

id166 using vc-dimension 

       thus large-margin     small vc-dim     better generalization 

bound 

       recall that d+1 is the upper bound for a linear classifier in d-

space 

   eric xing @ cmu, acl tutorial 2012 

140 

70 

id113 versus max-margin learning 

       likelihood-based estimation 

       probabilistic (joint/conditional likelihood 

model) 

       easy to perform bayesian learning, 

and incorporate prior knowledge, latent 
structures, missing data 
       bayesian id173!! 

       max-margin learning 

       non-probabilistic (concentrate on input-

output mapping) 

       not obvious how to perform bayesian 

learning or consider prior, and missing data 

       sound theoretical guarantee with limited 

samples 

       maximum id178 discrimination (med) (jaakkola, et al., 1999)   

       model averaging 
       the optimization problem (binary classification) 

   eric xing @ cmu, acl tutorial 2012 

141 

a road map for max-margin 
learning 

id166                       
id166                       

m3n                       
m3n                       
b

r 

e

a

c

med                       
med                       

 ?                        

med-mn 
= smed +    bayesian    m3n 
primal and dual sparse! 

   eric xing @ cmu, acl tutorial 2012 

142 

71 

maxent discrimination markov 
network 

       structured maxent discrimination (smed): 

       feasible subspace of weight distribution: 

       average from distribution of m3ns 

   eric xing @ cmu, acl tutorial 2012 

143 

medlda: a max-margin approach(cid:1)

       big picture of supervised topic models 

       slda: optimizes the joint likelihood for regression and classification 
       disclda: optimizes the conditional likelihood for classification only 

       medlda: based on max-margin learning for both regression and classification 

   eric xing @ cmu, acl tutorial 2012 

144 

72 

medlda regression model(cid:1)

(zhu et al, icml 2009)(cid:1)

       bayesian slda: 

       med estimation: 

model fitting(cid:1)

       variational bound 

       predictive rule:(cid:1)

predictive accuracy(cid:1)

   eric xing @ cmu, acl tutorial 2012 

145 

medlda classification model(cid:1)

(zhu et al, icml 2009)(cid:1)

       bayesian slda: 

       multiclass medlda classification model: 

       variational bound 

       predictive rule:(cid:1)

   eric xing @ cmu, acl tutorial 2012 

146 

73 

variational em alg.(cid:1)

       e-step: infer the posterior distribution of hidden r.v. 
       m-step: estimate unknown parameters 

      

independence assumption:  

       optimize l over     : 

       the first two terms are the same as in lda 
       the third and fourth terms are similar to those of slda, but in expected 

version. the variance matters! 

       the last term is a regularizer. only support vectors affect the topic proportions 
 optimize l over other variables. see the paper for details!(cid:1)

      

   eric xing @ cmu, acl tutorial 2012 

147 

medtm: a general framework(cid:1)

       medlda can be generalized to arbitrary topic models: 

       unsupervised or supervised 
       generative  or undirected random fields (e.g., harmoniums) 

       med topic model (medtm)    

model fitting(cid:1)

predictive accuracy(cid:1)

          : hidden r.v.s in the underlying topic model, e.g.,            in lda 
          : parameters in predictive model, e.g.,       in slda 
          : parameters of the topic model, e.g.,      in lda 
          : an variational upper bound of the log-likelihood 
          : a convex function over slack variables(cid:1)

   eric xing @ cmu, acl tutorial 2012 

148 

74 

experiments(cid:1)

       goal: 

       to qualitatively and quantitatively evaluate how the max-margin 

estimates of medlda affect its topic discovering procedure 

       data sets    

       20 newsgroups (classification) 

       documents from 20 categories 
       ~ 20,000 documents in each group 
       remove stop word as listed in umass mallet 

       movie review (regression) 
       5006 documents, and 1.6m words 
       dictionary: 5000 terms selected by tf-idf 
       preprocessing to make the response approximately normal (blei & mcauliffe, 2007) 

   eric xing @ cmu, acl tutorial 2012 

149 

document modeling(cid:1)

       data set: 20 newsgroups 
       110 topics + 2d embedding with id167 (var der maaten & hinton, 2008)(cid:1)

medlda(cid:1)

lda(cid:1)

   eric xing @ cmu, acl tutorial 2012 

150 

75 

document modeling (cont   )(cid:1)

comp.graphics(cid:1)
      comp.graphics:(cid:1)

politics.mideast(cid:1)

   eric xing @ cmu, acl tutorial 2012 

151 

classification(cid:1)

       data set: 20newsgroups 

       binary classification:     alt.atheism     and    talk.religion.misc    (simon et al., 2008) 
       multiclass classification: all the 20 categories 

       models:  disclda, slda (binary only! classification slda (wang et al., 

2009)), lda+id166 (baseline), medlda, medlda+id166 

       measure: relative improvement ratio(cid:1)

   eric xing @ cmu, acl tutorial 2012 

152 

76 

regression(cid:1)

       data set: movie review (blei & mcauliffe, 2007) 
       models: medlda(partial), medlda(full), slda, lda+svr 
       measure: predictive r2  and per-word log-likelihood 

sharp decrease in svs(cid:1)

   eric xing @ cmu, acl tutorial 2012 

153 

time efficiency(cid:1)

       binary classification 

       multiclass: 

      

 medlda is comparable with lda+id166 

       regression: 

       medlda is comparable with slda 

   eric xing @ cmu, acl tutorial 2012 

154 

77 

ii. supervised multi-view mns 

       a probabilistic method with an additional view of response 

variables y 

id172 factor 

       parameters can be learned with maximum likelihood 

estimation, e.g., special supervised harmonium (yang et al., 
2007) 
      

contrastive divergence is the commonly used approximation method in learning 
undirected latent variable models (welling et al., 2004; salakhutdinov & murray, 
2008). 

   eric xing @ cmu, acl tutorial 2012 

155 

max-margin learning of mns  

       expected discriminant function: 

       prediction rule: 

       hinge loss: 

       joint max-margin and max-likelihood estimation: 

      where                               is data likelihood 

       the rationale is: we want to find a latent representation   and a prediction model    , 

which on one hand tend to predict as accurate as possible on training data, while on the 
other hand tend to explain the data well. 

   eric xing @ cmu, acl tutorial 2012 

156 

78 

predictive latent representation 

      

id167 (van der maaten & hinton, 2008) 2d embedding of the 
discovered latent space representation on the trecvid 2003 
data 

mmh 

       avg-kl: average pair-wise divergence	

twh 

   eric xing @ cmu, acl tutorial 2012 

157 

predictive latent representation 

       example latent topics discovered by a 60-topic mmh on flickr animal data	

   eric xing @ cmu, acl tutorial 2012 

158 

79 

classification results 

       data sets: 

      
      

(left) trecvid 2003: (text + image features) 
(right) flickr 13 animal: (sift + image features) 

       models:   

       baseline(id166),dwh+id166, gm-mixture+id166, gm-lda+id166, twh, medlda

(sift only), mmh 

trecvid 

   eric xing @ cmu, acl tutorial 2012 

flickr 

159 

retrieval results 

       data set:  trecvid 2003 

       each test sample is treated as a query, training samples are ranked based on the 

cosine similarity between a training sample and the given query 

       similarity is computed based on the discovered latent topic representations 

       models:  dwh, gm-mixture, gm-lda, twh,  mmh 
       measure: (left) average precision on different topics and 

(right) precision-recall curve	

   eric xing @ cmu, acl tutorial 2012 

160 

80 

iii. supervised stc 

       joint loss minimization 

coordinate descent alg. applies with closed-form update rules 

      
       no sum-exp function; seaid113ss integration with non-probabilistic large-margin 

principle 

   eric xing @ cmu, acl tutorial 2012 

161 

classification accuracy 

       20 newsgroup data: 

   eric xing @ cmu, acl tutorial 2012 

162 

81 

time efficiency 

      

training & testing time 

       no calls of digamma function 
       converge faster with one additional dimension of freedom 

   eric xing @ cmu, acl tutorial 2012 

163 

summary 

       max-margin, instead of max-likelihood learning of supervised topic 

models (medlda, mmh, medstc) 
       explicit interpretation of effects by support vectors 
       medlda can discover discriminative topic representations that are more 

suitable for supervised tasks 

       the classification model is efficient and can avoid dealing with the 

id172 factor of a glm 

       the same principle can be applied to a wide variety of probabilistic 

(medtm) and non-probabilistic latent variable models(cid:1)

   eric xing @ cmu, acl tutorial 2012 

164 

82 

5. scenario iii: what if i don't 
know the total number of topics? 

   eric xing @ cmu, acl tutorial 2012 

165 

id91 

   eric xing @ cmu, acl tutorial 2012 

166 

83 

a classical approach 
       id91 as mixture modeling 

       then "model selection"  

   eric xing @ cmu, acl tutorial 2012 

167 

random partition of id203 
space 

.   (event, pevent)  

centroid :=  	


data point :=(x,  )	


    

   eric xing @ cmu, acl tutorial 2012 

168 

84 

dirichlet process 

       a cdf, g, on possible worlds 
of random partitions follows a 
dirichlet process if for any 
measurable finite partition 
(  1,  2, ..,   m): 

a distribution 

another 
distribution 

 (g(  1), g(  2),    , g(  m) ) ~ 
dirichlet(   g0(  1),    .,   g0(  m) )  

 where g0 is the base measure 
and    is the scale parameter 

   eric xing @ cmu, acl tutorial 2012 

169 

stick-breaking process 

location 

g =   k  (  k)
   
k =1
  k ~ g0
  k =1
   
k =1
k-1
   
j =1

  k =   k
  k ~ beta(1,  )

(1-   k)

mass 

0 

0.6 

0.3 

0.4 

0.5 

0.8 

0.4 

0.3 

0.24 

   eric xing @ cmu, acl tutorial 2012 

g0 

170 

    

85 

chinese restaurant process 

.... 

   eric xing @ cmu, acl tutorial 2012 

171 

mcmc for crp 

       id150 for exploring the posterior distribution under 

the proposed model 
       under the crp metaphor, due to exchangeability, every sample 

can be treated as the last sample! 

    

p(ci = k |c[   i],x,  )     p(ci = k |c[   i]) p(xi |  k,h[   i],c[   i])

posterior                           prior           x      likelihood"

crp"

       one can also integrate out the parameters such as    and perform 

collapse id150  

       id150 algorithm: draw samples of each random 

variable to be sampled given values of all the remaining variables 

   eric xing @ cmu, acl tutorial 2012 

172 

86 

convergence of ancestral 
id136 

   eric xing @ cmu, acl tutorial 2012 

173 

variational id136 [blei & 
jordan 2005, kurihara et al 2007] 

       on a single machine id150 solution is not efficient 

enough to scale up to the large scale problems. 

       truncated stick-breaking approximation can be formulated in 

the space of explicit, non-exchangeable cluster labels. 

       variational id136 can now be applied to such a finite-

dimensional distribution 

       variational id136:  

       for a complicated p(x1, x2,     xn), approximate it with q(x): 

   eric xing @ cmu, acl tutorial 2012 

174 

87 

approximations to dp 

       truncated stick-breaking 

representation 

       finite symmetric dirichlet 

approximation 

       the joint distribution can be 

expressed as: 

       the joint distribution can be 

expressed as: 

   eric xing @ cmu, acl tutorial 2012 

175 

vb id136 

       we can then apply the vb id136 on the four approximations 

the approximated posterior distribution for tsb and fsd are  

depending on marginalization or not, v and    may be integrated out. 

   eric xing @ cmu, acl tutorial 2012 

176 

88 

how to build an infinite lda? 

generakve	
   process	
   

-        for	
   each	
   document	
   d	
   
-        	
   sample	
     d	
   ~	
   dirichlet(  )	
   
-        	
   for	
   each	
   word	
   w	
   in	
   d	
   
-        	
   sample	
   z	
   ~	
   mulk(  d)	
   	
   
-        sample	
   w	
   ~	
   mulk(  z)	
   

  	


  	
   	
   	
   

z	
   	
   

w	
   	
   

n	
   

d	
   	
   

  	
   	
   	
   

k

topics   	
   trends	
   evolve	
   over	
   kme?	
   

topics   	
   distribukons	
   	
   evolve	
   over	
   kme?	
   

number	
   of	
   topics	
   grow	
   with	
   the	
   data?	
   

   eric xing @ cmu, acl tutorial 2012 

177 

the chinese restaurant 
franchise process 

       hierarchical dirichlet process mixture (hdpm) automatically 

determines number of topics in lda 

       we will focus on the chinese restaurant franchise process 

construction  
       a set of restaurants that share a global menu 

       metaphor 

       restaurant = documents 
       customer = word 
       dish =  topic 
       global menu = set of topics 

hdpm	
   

   eric xing @ cmu, acl tutorial 2012 

178 

89 

the chinese restaurant 
franchise process 

global menu 

  1 

  2 

  3 

  4 

m1:	
   number	
   of	
   	
   tables	
   
serving	
   this	
   dish	
   (topic) 

  4:	
   distribukon	
   for	
   

topic	
   4 

restaurant 1 

restaurant 2 

customers	
   
customers	
   

sharing	
   the	
   same	
   dish 
sharing	
   the	
   same	
   dish 

dish	
   served 

table 

   eric xing @ cmu, acl tutorial 2012 

179 

the chinese restaurant 
franchise process 

global menu 

  1 

  2 

  3 

  4 

restaurant 1 

restaurant 2 

restaurant 3 

? 

generakve	
   process	
   
-        for	
   customer	
   w	
   in restaurant 3	
   	
   
-        	
   choose	
   table	
   j	
      	
   nj	
   
-        	
   choose	
   a	
   new	
   table	
   	
   b	
      	
     	
   	
   

-        	
   sample	
   a	
   new	
   dish	
   for	
   this	
   table	
   

   eric xing @ cmu, acl tutorial 2012 

  	
   

180 

90 

the chinese restaurant 
franchise process	

global menu 

  1 

  2 

  3 

  4 

w	
   ~	
   mulk(l(	
     3)) 

restaurant 1 

restaurant 2 

restaurant 3 

generakve	
   process	
   
-        for	
   customer	
   w	
   in restaurant 3	
   	
   
-        	
   choose	
   table	
   j	
      	
   nj	
   
-        	
   choose	
   a	
   new	
   table	
   	
   b	
      	
     	
   	
   

-        	
   sample	
   a	
   new	
   dish	
   for	
   this	
   table	
   

   eric xing @ cmu, acl tutorial 2012 

? 

  	
   

181 

the chinese restaurant 
franchise process 

global menu 

  1 

  2 

  3 

  4 

restaurant 1 

restaurant 2 

restaurant 3 

generakve	
   process	
   
-        for	
   customer	
   w	
   in restaurant 3	
   	
   
-        	
   choose	
   table	
   j	
      	
   nj	
   
-        	
   choose	
   a	
   new	
   table	
   	
   b	
      	
     	
   	
   

-        	
   sample	
   a	
   new	
   dish	
   for	
   this	
   table	
   

   eric xing @ cmu, acl tutorial 2012 

? 

  	
   

182 

91 

the chinese restaurant 
franchise process	

global menu 

  1 

  2 

  3 

  4 

new 

  	
   

restaurant 1 

restaurant 2 

restaurant 3 

generakve	
   process	
   
-        for	
   customer	
   w	
   in restaurant 3	
   	
   
-        	
   choose	
   table	
   j	
      	
   nj	
   
-        	
   choose	
   a	
   new	
   table	
   	
   b	
      	
     	
   	
   
-        	
   sample	
   a	
   new	
   dish	
   for	
   this	
   table	
   
-        	
   	
   exiskng	
   dish	
   k	
       mk	
   	
   	
   
-        	
   	
   a	
   new	
   dish	
          	
   

   eric xing @ cmu, acl tutorial 2012 

? 

? 

  	
   

183 

the chinese restaurant 
franchise process	

global menu 

  1 

  2 

  3 

  4 

new 

? 

  	
   

w	
   ~	
   mulk(l(	
     3)) 

restaurant 1 

restaurant 2 

restaurant 3 

generakve	
   process	
   
-        for	
   customer	
   w	
   in restaurant 3	
   	
   
-        	
   choose	
   table	
   j	
      	
   nj	
   
-        	
   choose	
   a	
   new	
   table	
   	
   b	
      	
     	
   	
   
-        	
   sample	
   a	
   new	
   dish	
   for	
   this	
   table	
   
-        	
   	
   exiskng	
   dish	
   k	
       mk	
   	
   	
   
-        	
   	
   a	
   new	
   dish	
          	
   

   eric xing @ cmu, acl tutorial 2012 

  	
   

184 

92 

the chinese restaurant 
franchise process	

global menu 

  1 

  2 

  3 

  4 

  5 
new 

? 

  5~	
   h 

restaurant 1 

restaurant 2 

generakve	
   process	
   
-        for	
   customer	
   w	
   in restaurant 3	
   	
   
-        	
   choose	
   table	
   j	
      	
   nj	
   
-        	
   choose	
   a	
   new	
   table	
   	
   b	
      	
     	
   	
   
-        	
   sample	
   a	
   new	
   dish	
   for	
   this	
   table	
   
-        	
   	
   exiskng	
   dish	
   k	
       mk	
   	
   	
   
-        	
   	
   a	
   new	
   dish	
          	
   

   eric xing @ cmu, acl tutorial 2012 

? 

restaurant 3 

w	
   ~	
   mulk(l(	
     5)) 

  	
   

185 

the chinese restaurant 
franchise process	

global menu 

  1 

  2 

  3 

  4 

  5 

restaurant 1 

restaurant 2 

restaurant 3 

topics   	
   trends	
   evolve	
   over	
   kme?	
   

topics   	
   distribukons	
   	
   evolve	
   over	
   kme?	
   

number	
   of	
   topics	
   grow	
   with	
   the	
   data?	
   

   eric xing @ cmu, acl tutorial 2012 

186 

93 

summary: from lda to infinite 
topic models 

dirichlet 

stick  
breaking 

   
k 

  j	


zi 
wi 
n 

j 

a single image  

with k topic 

an lda 

   
   	


  j	


zi 
wi 
n 

a single image  
with inf-topic 

a dp 

   
   	


  	


  j	


zi 
wi 
n 

j 

j images  
with inf-topic 

an hdp 

   eric xing @ cmu, acl tutorial 2012 

187 

6. scenario iv: topic evolution in 
streaming corpus 

   eric xing @ cmu, acl tutorial 2012 

188 

94 

how to model topic evolution? 

research 
topics 

nature papers  
from 1900-2000 

1900 

2000  ? 

   eric xing @ cmu, acl tutorial 2012 

189 

problem statement 

topics	
   

research	
   	
   
papers	
   

1900	
   

given 

discover 

bio	
   

cs	
   

2009	
   

       potentially infinite number of topics 

       with time-varying trends 
       and time-varying distributions 
       and variable durations 

       topics can die 
       new topics can born 

   eric xing @ cmu, acl tutorial 2012 

190 

95 

dynamic lda 
dynamic	
   id91	
   

the big picture 

  	


     

z

w

n 
d

lda	
   

k 

  

hdpm	
   

infinite dynamic  
topic models 

   eric xing @ cmu, acl tutorial 2012 

191 

the big picture 

[blei and lafferty, 2006] 

dynamic lda 
dynamic	
   id91	
   

  	


     

z

w

n 
d

lda	
   

k 

  

hdpm	
   

infinite dynamic  
topic models 

   eric xing @ cmu, acl tutorial 2012 

192 

96 

text stream 

    

  

  

  

z

w

    

  

  

  

z

w

    

  

  

  

z

w

    

  

  

  

z

w

1990 

1991 

2004 

2005 

   eric xing @ cmu, acl tutorial 2012 

193 

text stream 

    

  

    

     

z

w

1990 

1991 

2004 

2005 

   eric xing @ cmu, acl tutorial 2012 

194 

97 

how to model topic evolution 

topic trends 

topic keywords 

topic correlations 

number of topics 

the dynamic correlated  

topic model 

1990 

1991 

2004 

2005 

   eric xing @ cmu, acl tutorial 2012 

195 

building blocks 

  	


  	


  	


ctms 

kalman filters 

     

z  

w  

n 

d  

     

     

x1 

y1 

k

x2 

y2 

x3 

y3 

x4 

y4 

  	


     

z  

w  

n 

d  

k

xt 

yt 

  	


  	


   eric xing @ cmu, acl tutorial 2012 

196 

98 

the dynamic ctm 

  1	


  2	


    	


  1	


  2	


     

z  

w  
n 

d1  

     

z  

w  
n 

d2  

     

z  

w  
n 

    	


dt  

k

   eric xing @ cmu, acl tutorial 2012 

197 

generalized mean field id136 

  1	


  2	


    	


     

z  

n 

d1  

     

z  

n 

d2  

generalized mean field id136: 

     

z  

n 

dt  

k

   eric xing @ cmu, acl tutorial 2012 

198 

99 

experimental results 

       nips data set 

       12 years 
       14036 words  
       2484 docs 
       90% for training and 10% for testing 

   eric xing @ cmu, acl tutorial 2012 

199 

topic trends 

   eric xing @ cmu, acl tutorial 2012 

200 

100 

topic words over time 

   eric xing @ cmu, acl tutorial 2012 

201 

topic correlations over time 

   eric xing @ cmu, acl tutorial 2012 

202 

101 

the big picture 

[teh et al, 2006] 

dynamic lda 
dynamic	
   id91	
   

  	


     

z

w

n 
d

lda	
   

k 

  

hdpm	
   

infinite dynamic  
topic models 

   eric xing @ cmu, acl tutorial 2012 

203 

the chinese restaurant 
franchise process 

       hdpm automatically determines number of topics in lda 
       we will focus on the chinese restaurant franchise process 

construction  
       a set of restaurants that share a global menu 

       metaphor 

       restaurant = documents 
       customer = word 
       dish =  topic 
       global menu = set of topics 

we have covered it already! 

hdpm	
   

   eric xing @ cmu, acl tutorial 2012 

204 

102 

the big picture 

[ahmed and xing, 2010] 

dynamic lda 
dynamic	
   id91	
   

  	


     

z

w

n 
d

lda	
   

k 

  

hdpm	
   

infinite dynamic  
topic models 

   eric xing @ cmu, acl tutorial 2012 

205 

the    evolving    chinese 
restaurant franchise process	

global menu t=1 

global menu t=2 

  1,1 

  2,1 

  3,1 

  4,1 

  5,1 

           =         *    

epoch 1 

documents in epoch 1 are 
generated as before 

pseudo	
   counts 

decay	
   factor 

topics at end of epoch 1 
-    height (mk,1) represent topic k   s popularity 
-      k,1 represents topic k   s word distribution  
observations 
-   popular topics at epoch 1 are likely to be popular at 
epoch 2 
-      k,2 is likely to smoothly evolve from     k,1  

   eric xing @ cmu, acl tutorial 2012 

206 

103 

the    evolving    chinese 
restaurant franchise process	

global menu t=1 

global menu t=2 

  1,1 

  2,1 

  3,1 

  4,1 

  5,1 

  2,2 

  3,2 

new	
   real	
   dish	
   served 

  3,2	
   ~	
   normal(.|   3,1,  ) 

epoch 1 

inherited	
   but	
   not	
   yet	
   used 

   eric xing @ cmu, acl tutorial 2012 

207 

the    evolving    chinese 
restaurant franchise process	

global menu t=1 

global menu t=2 

  1,1 

  2,1 

  3,1 

  4,1 

  5,1 

  2,2 

  3,2 

epoch 1 

generakve	
   process	
   
-        for	
   customer	
   w	
   in restaurant 1	
   	
   
-        [as	
   in	
   stakc	
   case]	
   choose	
   table	
   j	
      	
   nj	
   	
   
-        	
   choose	
   a	
   new	
   table	
   	
   b	
      	
     	
   	
   

-        	
   sample	
   a	
   new	
   dish	
   for	
   this	
   table	
   
-        	
   	
   exiskng	
   and	
   	
   inherited	
   dish	
   k	
       m`k,2  + mk,2	
   	
   	
   
-    existing but not inherited dish k     m`k,2  then   k,2 ~ normal(.|   k,1,  )	
   
-        	
   	
   a	
   new	
   dish	
           then   new ~ h	
   

   eric xing @ cmu, acl tutorial 2012 

208 

104 

the    evolving    chinese 
restaurant franchise process	

global menu t=1 

global menu t=2 

  1,1 

  2,1 

  3,1 

  4,1 

  5,1 

  2,2 

  3,2 

epoch 1 

generakve	
   process	
   
-        for	
   customer	
   w	
   in restaurant 1	
   	
   
-        [as	
   in	
   stakc	
   case]	
   choose	
   table	
   j	
      	
   nj	
   	
   
-        	
   choose	
   a	
   new	
   table	
   	
   b	
      	
     	
   	
   

-        	
   sample	
   a	
   new	
   dish	
   for	
   this	
   table	
   
-        	
   	
   exiskng	
   and	
   	
   inherited	
   dish	
   k	
       m`k,2  + mk,2	
   	
   	
   
-    existing but not inherited dish k     m`k,2  then   k,2 ~ normal(.|   k,1,  )	
   
-        	
   	
   a	
   new	
   dish	
           then   new ~ h	
   

   eric xing @ cmu, acl tutorial 2012 

209 

the    evolving    chinese 
restaurant franchise process	

global menu t=1 

global menu t=2 

  1,1 

  2,1 

  3,1 

  4,1 

  5,1 

  1,2 

  2,2 

  3,2 

  1,2	
   ~	
   normal(.|   1,1,  ) 

epoch 1 

generakve	
   process	
   
-        for	
   customer	
   w	
   in restaurant 1	
   	
   
-        [as	
   in	
   stakc	
   case]	
   choose	
   table	
   j	
      	
   nj	
   	
   
-        	
   choose	
   a	
   new	
   table	
   	
   b	
      	
     	
   	
   

-        	
   sample	
   a	
   new	
   dish	
   for	
   this	
   table	
   
-        	
   	
   exiskng	
   and	
   	
   inherited	
   dish	
   k	
       m`k,2  + mk,2	
   	
   	
   
-    existing but not inherited dish k     m`k,2  then   k,2 ~ normal(.|   k,1,  )	
   
-        	
   	
   a	
   new	
   dish	
           then   new ~ h	
   

   eric xing @ cmu, acl tutorial 2012 

210 

105 

the    evolving    chinese 
restaurant franchise process	

global menu t=1 

global menu t=2 

  1,1 

  2,1 

  3,1 

  4,1 

  5,1 

  2,2 

  3,2 

   6,2 

  6,2	
   ~	
   h 

epoch 1 

generakve	
   process	
   
-        for	
   customer	
   w	
   in restaurant 1	
   	
   
-        [as	
   in	
   stakc	
   case]	
   choose	
   table	
   j	
      	
   nj	
   	
   
-        	
   choose	
   a	
   new	
   table	
   	
   b	
      	
     	
   	
   

-        	
   sample	
   a	
   new	
   dish	
   for	
   this	
   table	
   
-        	
   	
   exiskng	
   and	
   	
   inherited	
   dish	
   k	
       m`k,2  + mk,2	
   	
   	
   
-    existing but not inherited dish k     m`k,2  then   k,2 ~ normal(.|   k,1,  )	
   
-        	
   	
   a	
   new	
   dish	
           then   new ~ h	
   

   eric xing @ cmu, acl tutorial 2012 

211 

the    evolving    chinese 
restaurant franchise process	

global menu t=1 

global menu t=2 

global menu t=3 

  1,1 

  2,1 

  3,1 

  4,1 

  5,1 

  1,2 

  2,2 

  3,2 

   6,2 

epoch 1 

epoch 2 

died	
   out	
   topics 

newly	
   born 

   eric xing @ cmu, acl tutorial 2012 

212 

106 

the    evolving    chinese 
restaurant franchise process	

global menu t=1 

global menu t=2 

global menu t=3 

  1,1 

  2,1 

  3,1 

  4,1 

  5,1 

  1,2 

  2,2 

  3,2 

   6,2 

epoch 1 

epoch 2 

topics   	
   trends	
   evolve	
   over	
   kme?	
   

topics   	
   distribukons	
   	
   evolve	
   over	
   kme?	
   

number	
   of	
   topics	
   grow	
   with	
   the	
   data?	
   

   eric xing @ cmu, acl tutorial 2012 

213 

the    evolving    chinese 
restaurant franchise process	

global menu t=1 

global menu t=2 

global menu t=3 

  1,1 

  2,1 

  3,1 

  4,1 

  5,1 

  1,2 

  2,2 

  3,2 

   6,2 

epoch 1 

epoch 2 

-   we just described a first order rcrf process 
-    for a general   -order process 

   eric xing @ cmu, acl tutorial 2012 

214 

107 

id136 

       id150 

       sample a table for each word 
       sample a topic for each table 
       sample the topic parameter over time  
       sample hyper-parameters 

       how to deal with non-conjugacy 

       algorithm 8 in neal   s 1998 + metropolis-hasting 

       efficiency 

       the markov blanket contains the previous and following     epochs  

   eric xing @ cmu, acl tutorial 2012 

215 

sampling a topic for a table 

global menu t=1 

global menu t=2 

global menu t=3 

  1,1 

  2,1 

  3,1 

  4,1 

  5,1 

  1,2 

  2,2 

  3,2 

   6,2 

past	
   

emission	
   

non-     conjugacy	
   

future	
   

e   ciency	
   

   eric xing @ cmu, acl tutorial 2012 

216 

108 

sampling a topic for a table 

global menu t=1 

global menu t=2 

global menu t=3 

  1,1 

  2,1 

  3,1 

  4,1 

  5,1 

  1,2 

  2,2 

  3,2 

   6,2 

  /3	
   

~	
   h=	
   n(0,    )	
   

past	
   

emission	
   

non-     conjugacy	
   

future	
   

e   ciency	
   

   eric xing @ cmu, acl tutorial 2012 

217 

sampling a topic for a table 

global menu t=1 

global menu t=2 

global menu t=3 

  1,1 

  2,1 

  3,1 

  4,1 

  5,1 

  1,2 

  2,2 

  3,2 

   6,2 

past	
   

emission	
   

future	
   

non-     conjugacy	
   

pre-     compute	
   
and	
   update	
   

   eric xing @ cmu, acl tutorial 2012 

218 

109 

sampling topic parameters 

  1	
   	
   	
   

v 

  2	
   	
   	
   

v 

    	
   	
   	
   

v 

       v|       mult( logistic(  )) 
       linear-state space model with non-gaussian emission 
       use laplace approximation inside the forward-backward 

algorithm 

       use the resulting distribution as a proposal 

   eric xing @ cmu, acl tutorial 2012 

219 

experiments  

       simulated data 

       simulated 20 epochs with 100 data points in each epoch 

       timeline of the nips conference 

       13 years 
       1740 documents 
       950 words per document 
       ~3500 vocabulary 

   eric xing @ cmu, acl tutorial 2012 

220 

110 

simulation experiment 

   eric xing @ cmu, acl tutorial 2012 

221 

sample documents: 

ground truth 

recovered 

   eric xing @ cmu, acl tutorial 2012 

222 

111 

analyzing the nips corpus 

start state 

(b) 

posterior sample 

(a) 

   eric xing @ cmu, acl tutorial 2012 

(c) 

223 

1990 

rl 

1991 

boosting 

bayesian 

mixtures 

generalizat

oin 

id91 

1994 

1995 

1996 

ica 

memory 

kernels 

classification 

control 

pm 

som 

1987 

speech 

neuro 
sience 

nn 

classificati

on 

methods 

control 

prob. 
models 

image 

speech 

mixtures 

ica 

kernels 

   eric xing @ cmu, acl tutorial 2012 

224 

112 

1987 
field  
 code  

temperature  

tree  

boltzmann  

energy  
annealing  

node  

id203 
1990 
em  

 expert  
mixture  
gating  
missing  
experts  
gaussian  
parameters  

density 

method  
solution  
energy  
 values  
gradient  

convergence  

equation  
algorithms 

1990 
field  
 tree  
 level  
 energy  
id203  

node  

annealing  
boltzmann  
variables   
1994 

mixtures 

mixture  
 em   

likelihood  
missing  
experts  
mixtures  
gaussian  
parameters 

gradient  
weight  
method  
methods   
local  rate  
optimal  
descent  
solution 

1993 
tree  

 variables  

node   
level  

id203  

field  

distribution  
structure  

graph  
energy 
1999 
mixture  
gaussian  

 em   

likelihood  
parameters  

analysis  
density  
factor  

variables  
distribution 

1996 
variables  

graph  
 tree  

id203  

field  

structure  

node  

distribution  

energy 

1995 

ica 

wavelet  
natural  

separation  
source   

ica  

coefficients  
independent  

basis 

1999 

id203  
variables  
tree  field  
distribution  
graph  nodes  
belief  node  
id136  
propagation 

pm 

1999 
source  
 ica   
blind  

separation  
coefficients  

natural  

independent  

basis  
wavelet 

gradient  
matrix   
weight  

algorithms  
local  rate  
problems  

point  
equation 

   eric xing @ cmu, acl tutorial 2012 

matrix  

algorithms  
gradient  

convergence  

equation  
optimal  
method  
parameter 

methods 

225 

1996 

1997 

1998 

1999 

kernels 

support  
kernel   
id166  

id173  

sv  

 vectors  
feature  
regression 

 kernel  
support  

sv  
 id166  

machines  
regression  

vapnik  
feature  
solution 

kernel  
support   

id166 

regression  

feature  
machines  
solution  
margin  pca 

kernel	
   	
   	
   id166	
   	
   

	
   support	
   	
   
regression	
   	
   
solukon	
   	
   
machines	
   	
   

matrix	
   	
   feature	
   	
   
regularizakon	
   

-   support vector method for function 
approximation, regression estimation, 
and signal processing,  
v.vapnik, s. e. golowich and a.smola 
-    support vector regression machines 
h. drucker, c. burges, l. kaufman, a. 
smola and v. vapnik 
-   improving the accuracy and speed of 
support vector machines,  
c. burges and b. scholkopf 

-    from id173 operators to 
support vector kernels,  
a. smola and b. schoelkopf 
-    prior knowledge in support vector 
kernels,  
b. schoelkopf, p. simard, a. smola 
and v.vapnik 

-    uniqueness of the id166 solution,  
c. burges and d.. crisp 
-    an improved decomposition 
algorithm for regression support 
vector machines,  
p. laskov 
..... many more 

   eric xing @ cmu, acl tutorial 2012 

226 

113 

dynamic lda 
dynamic	
   id91	
   

the big picture 

  	


     

z

w

n 
d

lda	
   

k 

  

hdpm	
   

infinite dynamic  
topic models 

   eric xing @ cmu, acl tutorial 2012 

227 

quantitative analysis 

   eric xing @ cmu, acl tutorial 2012 

228 

114 

hyper-parameter sensitivity 

  1   

v 

  2   

v 

       

v 

   eric xing @ cmu, acl tutorial 2012 

229 

hyper-parameter sensitivity 

  1   

v 

  2   

v 

       

v 

   eric xing @ cmu, acl tutorial 2012 

230 

115 

hyper-parameter sensitivity 

global menu t=3 

   eric xing @ cmu, acl tutorial 2012 

231 

8. algorithmic scalability and 
large-scale learning 

   eric xing @ cmu, acl tutorial 2012 

232 

116 

scaling topic models to large 
document collections 

       large-scale corpora have millions, even billions of documents, 

with vocabulary sizes in the millions 
       runtime and memory challenges! 

       scaling to such corpora requires techniques such as: 

1.    efficient data representations and algorithms 

2.    parallelization over multiple cpus 

3.    online id136, to handle incoming documents one-at-a-time 

   eric xing @ cmu, acl tutorial 2012 

233 

efficient data representations 
and algorithms 

       key observation: most documents contain just a small fraction 

of the words in the vocabulary 
       we say that the documents are sparse 

vocabulary space 

doc 1 

doc 2 

fox 
brown 
lazy 
dog 

lorem 
ipsum 
dolor 

latent 
dirichlet 
allocation 

doc 3 

   eric xing @ cmu, acl tutorial 2012 

234 

117 

efficient data representations 
and algorithms 

       collapsed id150 is a very popular id136 

algorithm for topic models 
       cgs samples just the word-topic indicators z, without having to sample document 

topic vectors    or topic vocabularies    

       cgs requires us to track of two types of counts: 

       topic-word: for each topic k, # of times it is assigned to vocabulary word v 

      
      

i.e. for all documents m and words n, # of zmn s.t. zmn = k and wmn = v 
represents topic vocabularies    

       document-topic: in each document m, # of words n assigned to topic k 

      
      

i.e. for all words n in document m, # of zmn s.t. zmn = k 
represents document topic vectors    

  	


  k 

k 

  	


  m 

zmn 

wmn 

n 

m 

   eric xing @ cmu, acl tutorial 2012 

235 

efficient data representations 
and algorithms 

       store both word-topic and document-topic counts using a 

dictionary (key-value) data structure 
       take advantage of sparsity to save memory! 
       savings can be very large: 

       e.g. you have 500 topics but each document uses only 5 on average 
       e.g. you have 1 million vocabulary words, but each topic uses only 10,000 on average 

document-topic counts 

for document m 

topic 1: 5 words 
topic 6: 3 words 
topic 8: 1 word 
    

topic-word counts 

for topic k 

dog: 10 occurrences 
cat: 15 occurrences 
mouse: 3 occurrences 
    

   eric xing @ cmu, acl tutorial 2012 

236 

118 

efficient data representations 
and algorithms 

it   s not just about saving memory 

      
       we can speed up collapsed id150 by exploiting 

sparsity in word-topic and document-topic counts 
       notice that each word-topic indicator z follows a discrete distribution over k topics 
       we sample z by drawing u ~ uniform(0,1), and then iterating through each of the 

k topic choices until the cumulative id203 mass exceeds u 

u 

1 

2 

3 

4 

      

if we consider the topic choices with the largest id203 mass first, we   ll stop 
after fewer topics 

u 

4 

1 

2 

3 

       see yao, mimno and mccallum (2009) for details 

   eric xing @ cmu, acl tutorial 2012 

237 

efficient data representations 
and algorithms 

efficient methods for topic model id136 on streaming 
document collections (yao, mimno and mccallum 2009) 

   eric xing @ cmu, acl tutorial 2012 

238 

119 

efficient data representations 
and algorithms 

       what about variational id136? 

       we can apply stochastic id119 to speed up 

variational updates 
instead of computing the full gradient, just subsample terms from the gradient! 
      
       specifically, we subsample random documents, and then keep gradient terms 

belonging to those documents 
       this works because the topic model log-likelihood decomposes as a sum over 

documents! 

       often, we can obtain good performance with just a small fraction of the terms 

   eric xing @ cmu, acl tutorial 2012 

239 

parallelization over multiple 
cpus 

       efficient data/algorithms can only get you so far on one cpu 

       the processor industry is no longer focused on single-cpu performance 

in 5 years, new processors will not be much faster than today   s processors 

      
       but they will have many more cpu cores for parallel programming! 

       on the other hand, text corpora are growing rapidly 

       english wikipedia has nearly 4 million articles 
       blogosphere generates 900 thousand posts per day in 2008, and almost certainly more 

today (source: technorati) 

       we must parallelize     both now, and in the future as well 

   eric xing @ cmu, acl tutorial 2012 

240 

120 

parallelization over multiple 
cpus 

       three common strategies for parallel id136: 

       apply    standard    variational id136 (vb) or id150, but distribute the 

documents over the cpus 
       advantages: easy extension to standard variational/mcmc algorithms 
       drawbacks: convergence no longer guaranteed under some situations, like collapsed 

id150 

       use id143ing (aka sequential monte carlo sampling) with p particles, and 

split the particles over the cpus 
       advantages: convergence is always guaranteed; can pick any number of particles p 
       drawbacks: na  ve sampler can lead to very poor results, thus care is needed in designing 

the sampler 

       use auxiliary variables to distribute id136  

       advantages: convergence guaranteed, collapsed id150 on individual cpus. 
       drawbacks: latency introduced by network if entire data cannot reside on a single 

machine. 

   eric xing @ cmu, acl tutorial 2012 

241 

distributing documents 

       by distributing documents over cpus, we can: 

      

infer the word-topic indicators z and document topic vectors in    in parallel 
(conditioned on the topic vocabularies   ) 
       easily implemented as a map operation 
       to infer the topic vocabularies   : 

       consolidate statistics from the word-topic indicators z into one cpu 
       have that cpu infer the      s (conditioned on z   s) 
       easily implemented as a map-reduce operation 

       works because of conditional independence in the model! 

  	


  k 

k 

  	


  m 

zmn 

wmn 

n 

m 

   eric xing @ cmu, acl tutorial 2012 

242 

121 

distributing documents 

      

      

in collapsed id150 however, the conditional 
independence assumptions break down 
      
       mcmc convergence guarantees no longer hold when parallel sampling the z   s 

integration of   ,    makes the z   s depend on each other 

in practice however, parallel cgs sampling does produce 
good results (asuncion, smyth and welling 2008) 
       though this may or may not generalize to more complex topic models 

  	


  k 

k 

  	


  m 

zmn 

wmn 

n 

m 

   eric xing @ cmu, acl tutorial 2012 

243 

distributing documents 

asynchronous distributed learning of topic models 

(asuncion, smyth and welling 2008) 

   eric xing @ cmu, acl tutorial 2012 

244 

122 

sequential monte carlo 

       under smc (id143ing), we    evolve    the posterior 

distribution one set of documents at a time 
       represent the posterior as a set of weighted samples (called    particles   ) 

       within a set of documents, particles are evolved according to some proposal distribution 
       each particle can be evolved independently in parallel 
in the illustration below, each circle is a particle. axis locations represent latent 
variable values, and sizes represent particle weights. 

      

documents 1   n 

documents (n+1)   2n  documents (2n+1)   3n 

z,  ,   

z,  ,   

add docs (n+1)   2n, 
and update particles 

add docs (2n+1)   3n 

   eric xing @ cmu, acl tutorial 2012 

z,  ,   

245 

sequential monte carlo 

smc id136 performs 
well on real world datasets 

increasing the number of 

particles improves 

performance 

online id136 for the infinite topic-cluster model: storylines from 

streaming text (ahmed, ho, teo, eisenstein, smola, xing 2011) 

   eric xing @ cmu, acl tutorial 2012 

246 

123 

online id136 

       often, we want to add new documents to the model 

incrementally 
       but we can   t afford to rerun id136 all documents, especially for huge corpora! 
       how can we insert the new documents in a statistically principled manner? 

       online id136 allows us to incorporate the influence of new 

documents 
       sequential monte carlo (already explained) 
       online variational id136 

   eric xing @ cmu, acl tutorial 2012 

247 

online variational id136 

       key idea: split the set of docs into smaller    minibatches   , 

similar to sequential monte carlo 

      

in each minibatch: 
       perform variational id136 on word-topic indicators z and document topic vectors   , for 

all docs in the minibatch 

       perform gradient steps on the topic vocabularies   , using only terms corresponding to 

docs in the minibatch 

       the use of minibatches is equivalent to stochastic gradient updates, which are 

guaranteed to converge (hoffman, blei and bach 2010) 

       we can process docs as they arrive, one minibatch at a time 

   eric xing @ cmu, acl tutorial 2012 

248 

124 

online variational id136 

online learning for id44 

(hoffman, blei and bach 2009) 

   eric xing @ cmu, acl tutorial 2012 

249 

auxiliary variable representation  

       key idea    dirichlet mixtures of dirichlet processes are 

dirichlet processes    

       we can re-write the generative process of dpmm as:- 

       by adding additional constrain on the concentration parameter 

of bottom level dp we can re-write hdp as:- 

   eric xing @ cmu, acl tutorial 2012 

250 

125 

auxiliary variable representation 

   eric xing @ cmu, acl tutorial 2012 

251 

auxiliary variable representation 

   eric xing @ cmu, acl tutorial 2012 

252 

126 

algorithmic scalability and 
large-scale learning 

       id206 matter 
       dictionaries to exploit vocabulary/topic sparsity 
       faster sampling by reordering topics 

       parallelization is great, but one needs to be careful 

       multiple parallel id136 approaches, with their own pros/cons 
       conditional independence allows us to divide documents among processors 
       auxiliary variables can provide conditional independence in collapsed samplers 

       online id136 is possible 

       for the same reasons that parallelization is possible 
       use incoming document minibatches to update topic vocabulary distributions 

   eric xing @ cmu, acl tutorial 2012 

253 

9: other apps (optional) 

   eric xing @ cmu, acl tutorial 2012 

254 

127 

i. machine translation 

smt 

   eric xing @ cmu, acl tutorial 2012 

b. zhao and e.p xing,   
acl 2006 

255 

word alignment 

                                                
      (cid:1)

the  economy  and  trade  relations  between russia and tianjin develop  steadily 

   eric xing @ cmu, acl tutorial 2012 

256 

128 

the statistical formulation 

contemporary 

comparable 

parallel 

monolingual 

translation model 

language model 

   eric xing @ cmu, acl tutorial 2012 

257 

bitam: from monolingual to bilingual 
topic models  

(zhao & xing, acl/coling 2006) 

       monolingual space,  a unigram lm 

       a topic corresponding to a point in the word simplex. 
       admixture of unigrams (blei, et al. 2003) 

       bilingual space,  a translation lexicon 

       given a topic z,  a word usually has limited translations. 
       topic-specific translation lexicons are sharper 
       each topic is a point in the conditional simplex 
       admixture of topic-specific translation lexicons  

(zhao & xing, acl/coling 2006) 

       example 

       a chinese word    club   ,  the translations can be: 

ogre  
0.4 
0.0 

war 
0.5 
0.1 
   eric xing @ cmu, acl tutorial 2012 

socialize 
0.0 
0.5 

interests 
0.1 
0.4 

258 

129 

bitam: a generative process 

       sample topic weights      from a dirichlet(   ) 
       sample a topic  z  from multinomial (   ) 
       for each word  f  in the sentence  

       sample an alignment       from an alignment model 
       generate f with word         from a topic-specific lexicon 

   eric xing @ cmu, acl tutorial 2012 

259 

bitam model-1 

       graphical model (a language to encode dependencies) 

         i 

         j 

         n 

         m 

   eric xing @ cmu, acl tutorial 2012 

260 

130 

gmf id136 

approximate 
the integral 

approximate 
the posterior 

optimization 

problem 

   eric xing @ cmu, acl tutorial 2012 

261 

an upgrade path for bitams 

sent-pair level topics 

id48 for alignment 

word-pair level topics 

   eric xing @ cmu, acl tutorial 2012 

word-pair & id48 

262 

131 

experiments 

       training data 

       small:  treebank 316 doc-pairs (133k english words) 
       large:  fbis-beijing, sinorama, xinhuanews, (15m english words). 

       word alignment accuracy & translation quality 

       f-measure 
       id7 

   eric xing @ cmu, acl tutorial 2012 

263 

model selection 

       choosing num-topics k 

       10-fold cross-validation  
       number of topics is set to be 50 for 23 million words corpus 

   eric xing @ cmu, acl tutorial 2012 

264 

132 

topics 

t1 

teams, sports, disabled, games 
members, people, cause, water, national, 
handicapped 

t2  shenzhen, singapore, hongkong, stock, 

national, investment,  yuan, options, 
million, dollar 

t3  chongqing, company, takeover, 
shenzhen, tianjin, city, national, 
government, project, companies 

t4  hongkong, trade, export, import, foreign, 

tech., high, 1998, year, technology 

t5  house, construction, government, 

employee, living, provinces, macau,  
anhui, yuan 

t6  gas, company, energy, usa, russia, 
france, chongqing, resource,  china, 
economy, oil 

t1     ,       ,       ,       ,    ,       ,    , 

         ,       ,        (cid:1)

t2        ,    ,    ,    ,    ,    ,       ,       , 

      ,           (cid:1)

t3        ,       ,    ,    ,     ,       ,       , 

      ,    ,       (cid:1)

t4        ,       ,       ,       ,       ,       , 

      ,       ,    ,        (cid:1)

t5        ,    ,       ,       ,       ,    ,    

   ,       ,       ,    ,     (cid:1)

t6        ,          ,    ,    ,       ,       , 

      ,    ,    ,       (cid:1)

   eric xing @ cmu, acl tutorial 2012 

265 

hm-bitam versus others 

   eric xing @ cmu, acl tutorial 2012 

266 

133 

translation evaluations 

   eric xing @ cmu, acl tutorial 2012 

267 

translation evaluations 

   eric xing @ cmu, acl tutorial 2012 

268 

134 

ii. exploring and deciphering social 
networks 

   eric xing @ cmu, acl tutorial 2012 

269 

dynamic network tomography 

       how to model dynamics in a simplex? 

project an individual/stock in  
network into a "tomographic" space 

trajectory of an individual/stock  
in the "tomographic" space 

   eric xing @ cmu, acl tutorial 2012 

270 

135 

evolving networks 

    

    

    

march 2005 

january 2006 

august 2006 

   eric xing @ cmu, acl tutorial 2012 

271 

dynamic mmsb (dmmsb) [xing, fu, and song, 

aoas 2009] 

c 

   eric xing @ cmu, acl tutorial 2012 

272 

136 

dynamic mixture of mmsb 
(dm3sb) 

[ho, le, and xing, submitted 2010] 

time-varying 
role prior 

     

c

(1) 

  h

  h 

   

(1) 

ci

(1) 

  i

cluster 

selection prior 

zi   j

(1)  zj   i

(1) 

n

n

     

    

    

(t) 

  h

(t) 

ci

(t) 

  i

zi   j

(t)  zj   i

(t) 

n

n

legend 
hidden role prior 
actor hidden roles 
observed interactions 
role compatibility matrix 

(1) 

ei,j
n  n 

time-varying 
network model 

(t) 

ei,j
n  n 

role compatibility 

matrix 

  k,l 
k  k 

   eric xing @ cmu, acl tutorial 2012 

273 

algorithm: generalized mean field  

(xing et al. 2004) 

2 

1 

3 

      

id136 via variational em  
      
      
      

generalized mean field 
laplace approximation 
kalman filter & rts smoother 

   eric xing @ cmu, acl tutorial 2012 

274 

137 

dmmsb vs. mmsb 

   eric xing @ cmu, acl tutorial 2012 

275 

dm3sb vs. dmmsb 

   eric xing @ cmu, acl tutorial 2012 

276 

138 

goodness of fit 

   eric xing @ cmu, acl tutorial 2012 

277 

case study 1: sampson   s monk 
network 
       dataset description 

       18 monks (junior members in a monastery) 
       liking relations recorded 
       3 time-points in one year period 
       timing: before a major conflict outbreak 

       recall static analysis: 

   eric xing @ cmu, acl tutorial 2012 

278 

139 

sampson   s monk network:  
role trajectories 

       the trajectories of the varying role-vectors over time 

   eric xing @ cmu, acl tutorial 2012 

279 

case study 2: the 109th congress 

march 2005 

january 2006 

august 2006 

us senator voting records 
100 senators, 109th congress (jan 2005     dec 2006) in 8 epochs 

   eric xing @ cmu, acl tutorial 2012 

280 

140 

senate network: role trajectories 

voting data preprocessed into a network graph using (kolar et al., 2008) 

colored bars: estimated latent space vector 
numbers under bars: estimated cluster 
letters beside actor index: political party and state 
   eric xing @ cmu, acl tutorial 2012 

role compatibility matrix b 

role 1 = passive, 2/4 = democratic clique, 

3 = republican clique 

281 

senate network: role trajectories 

jon	
   corzine   s	
   seat	
   (#28,	
   democrat,	
   

new	
   jersey)	
   was	
   taken	
   over	
   by	
   

bob	
   menendez	
   from	
   t=5	
   onwards.	
   

cluster legend 

corzine	
   was	
   especially	
   lep-     wing,	
   
so	
   much	
   that	
   his	
   views	
   did	
   not	
   

align	
   with	
   the	
   majority	
   of	
   

democrats	
   (t=1	
   to	
   4).	
   

once	
   menendez	
   took	
   over,	
   the	
   
latent	
   space	
   vector	
   for	
   senator	
   

#28	
   shiped	
   towards	
   role	
   4,	
   
corresponding	
   to	
   the	
   main	
   
democrakc	
   vokng	
   clique.	
   

ben	
   nelson	
   (#75)	
   is	
   a	
   right-     wing	
   democrat	
   

(nebraska),	
   whose	
   views	
   are	
   more	
   
consistent	
   with	
   the	
   republican	
   party.	
   

observe	
   that	
   as	
   the	
   109th	
   congress	
   

proceeds	
   into	
   2006,	
   nelson   s	
   latent	
   space	
   

vector	
   includes	
   more	
   of	
   role	
   3,	
   

corresponding	
   to	
   the	
   main	
   republican	
   

vokng	
   clique.	
   

this	
   coincides	
   with	
   nelson   s	
   re-     eleckon	
   as	
   
the	
   senator	
   from	
   nebraska	
   in	
   late	
   2006,	
   

during	
   which	
   a	
   high	
   proporkon	
   of	
   

republicans	
   voted	
   for	
   him.	
   

   eric xing @ cmu, acl tutorial 2012 

282 

141 

summary of this tutorial 

       1. overview of basic topic models 
       2. computational challenges and two classical algorithmic 

paths  

       3. scenario i: multimodal data 
       4. scenario ii: when supervision is available  
       5. scenario iii: what if i don't know the total number of topics  
       6. scenario iv: topic evolution in streaming corpus.  
       7: advanced subject i: sparsity in id96 (see 

emnlp talk)  

       8: advanced subject ii: scalability, complexity, and fast 

algorithms (optional) 

       9: other applications (optional) 

   eric xing @ cmu, acl tutorial 2012 

283 

conclusion 

       gm-based topic models are cool 

       flexible  
       modular 
      

interactive 

       there are many ways of implementing topic models 

       unsupervised 
      

supervised 

       efficient id136/learning algorithms 

       gmf, with laplace approx. for non-conjugate dist. 
       mcmc 

       many applications 

           
       word-sense disambiguation 
      
       network id136 

image understanding 

   eric xing @ cmu, acl tutorial 2012 

284 

142 

more research questions we ask: 

   eric xing @ cmu, acl tutorial 2012 

285 

143 

