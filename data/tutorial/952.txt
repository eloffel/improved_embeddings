variational id136 for 
structured nlp models

acl, august 4, 2013

david burkett and dan klein

tutorial outline

1. structured models and factor graphs

2. mean field

3. structured mean field

4. belief propagation

5. structured belief propagation

6. wrap-up

part 1: structured models and 

factor graphs

structured nlp models
example: hidden markov model

(sample application: id52)

outputs
(pos tags)

inputs
(words)

goal: queries from posterior

structured nlp models
example: hidden markov model

structured nlp models
example: hidden markov model

structured nlp models
example: hidden markov model

structured nlp models
example: hidden markov model

structured nlp models
example: hidden markov model

structured nlp models
example: hidden markov model

factor graph notation

variables yi

factors

cliques

binary factor

unary factor

factor graph notation

variables yi

factors

cliques

factor graph notation

variables yi

factors

cliques

variables have factor (clique) neighbors:

factors have variable neighbors:

(lafferty et al., 2001)

structured nlp models
example: conditional random field

(sample application: id39)

structured nlp models
example: conditional random field

structured nlp models
example: conditional random field

structured nlp models

example: edge-factored id33

o

o

o

l

o

l

o

l

o

l

the

cat

ate

the

rat

(mcdonald et al., 2005)

structured nlp models

example: edge-factored id33

l

o

l

l

o

l

o

o

o

o

the

cat

ate

the

rat

structured nlp models

example: edge-factored id33

o

o

o

r

r

o

o

l

l

o

the

cat

ate

the

rat

structured nlp models

example: edge-factored id33

o

o

o

r

o

l

o

l

o

l

the

cat

ate

the

rat

structured nlp models

example: edge-factored id33

l

l

l

l

l

l

l

l

l

l

id136

input: factor graph

output: marginals

id136

typical nlp approach: dynamic programs!

examples:

sequence models (forward/backward)

phrase structure parsing (cky, inside/outside)

id33 (eisner algorithm)

itg parsing (bitext inside/outside)

complex structured models

id52

joint

named entity 
recognition

(sutton et al., 2004)

complex structured models

id33

with second order features

(mcdonald & pereira, 2006)

(carreras, 2007)

complex structured models

word alignment

vi

el

gato

fr  o

(taskar et al., 2005)

i

saw the cold

cat

complex structured models

word alignment

vi

el

gato

fr  o

i

saw the cold

cat

variational id136

approximate id136 techniques that can be 
applied to any graphical model

this tutorial:

mean field: approximate the joint distribution 
with a product of marginals

belief propagation: apply tree id136 
algorithms even if your graph isn   t a tree

structure: what changes when your factor graph 
has tractable substructures

part 2: mean field

mean field warmup

wanted:

idea: coordinate ascent

key object: assignments

iterated conditional modes (besag, 1986)

mean field warmup

wanted:

mean field warmup

wanted:

mean field warmup

wanted:

mean field warmup

wanted:

mean field warmup

wanted:

approximate result: 

iterated conditional modes 

example

iterated conditional modes 

example

iterated conditional modes 

example

iterated conditional modes 

example

iterated conditional modes 

example

iterated conditional modes 

example

iterated conditional modes 

example

iterated conditional modes 

example

mean field intro

mean field is coordinate ascent, 

just like iterated conditional 

modes, but with soft 

assignments to each variable!

mean field intro

wanted:

idea: coordinate ascent

key object: (approx) marginals

mean field intro

mean field intro

mean field intro

wanted:

mean field intro

wanted:

mean field intro

wanted:

mean field procedure

wanted:

mean field procedure

wanted:

mean field procedure

wanted:

mean field procedure

wanted:

example results

mean field derivation

goal:

approximation: 

constraint:

objective:

procedure:  coordinate ascent on each

what   s the update?

mean field update

1) 

2) 

3-9)   lots of algebra

10) 

approximate expectations

f

yi

general:

general update *

exponential family:

generic:

mean field id136 example

1

2

2

1

5

1

1

1

.4

.6

.2

.2

.5

.1

.7

.3

.5

.5

.5

.5

mean field id136 example

1

2

2

1

5

1

1

1

.4

.6

.2

.2

.5

.1

.7

.3

.5

.5

.69

.31

mean field id136 example

1

2

2

1

5

1

1

1

.4

.6

.2

.2

.5

.1

.7

.3

.5

.5

.69

.31

mean field id136 example

1

2

2

1

5

1

1

1

.4

.6

.2

.2

.5

.1

.7

.3

.40 .60

.69

.31

mean field id136 example

1

2

2

1

5

1

1

1

.4

.6

.2

.2

.5

.1

.7

.3

.40 .60

.73

.27

mean field id136 example

1

2

2

1

5

1

1

1

.4

.6

.2

.2

.5

.1

.7

.3

.38 .62

.73

.27

mean field id136 example

1

2

2

1

5

1

1

1

.4

.6

.2

.2

.5

.1

.7

.3

.38 .62

.73

.27

.28 .45

.10 .17

mean field id136 example

2

1

1

1

1

1

2

1

.67 .33

.67

.33

.44 .22

.22 .11

.67 .33

.67

.33

.44 .22

.22 .11

mean field id136 example

1

9

1

1

1

5

1

1

.62 .38

.62

.38

.56 .06

.06 .31

.82 .18

.82

.18

.67 .15

.15 .03

mean field q&a

are the marginals guaranteed to converge to 
the right thing, like in sampling?

no

is the algorithm at least guaranteed to 
converge to something?

so it   s just like em?

yes

yes

why only local optima?!

variables:

discrete distributions:
e.g.  p(0,1,0,   0) = 1

all distributions

(all convex combos)

mean field approximable

(can represent all discrete ones, but not all)

part 3: structured mean field

mean field approximation

model:

approximate graph:

   

   

   

   

   

   

   

   

structured mean field 

approximation

model:

approximate graph:

   

   

   

   

   

   

   

   

(xing et al, 2003)

structured mean field 

approximation

structured mean field 

approximation

structured mean field 

approximation

computing structured updates

??

computing structured updates

updating         .

consists of computing 

all marginals

.

marginal id203 of

under    .

computed with

forward-backward

structured mean field notation

structured mean field notation

structured mean field notation

structured mean field notation

structured mean field notation

connected 
components

structured mean field notation

neighbors:

structured mean field updates

na  ve mean field:

structured mean field:

expected feature counts

component factorizability *

condition

example feature

generic condition

(pointwise product)

component factorizability *

(abridged)

use conjunctive indicator features

joint parsing and alignment

project       

and

product

   

      

of

levels

high

      

   

(burkett et al, 2010)

joint parsing and alignment

input:

sentences

project       

and

product

   

      

of

levels

high

      

   

joint parsing and alignment

output: trees contain

nodes

project       

and

product

   

      

of

levels

high

      

   

joint parsing and alignment

output:

alignments

project       

and

product

   

      

of

levels

high

      

   

joint parsing and alignment

output:

alignments

contain bispans

project       

and

product

   

      

of

levels

high

      

   

joint parsing and alignment

output:

project       

and

product

   

      

of

levels

high

      

   

joint parsing and alignment

variables

project       

and

product

   

      

of

levels

high

      

   

joint parsing and alignment

variables

project       

and

product

   

      

of

levels

high

      

   

joint parsing and alignment

variables

project       

and

product

   

      

of

levels

high

      

   

joint parsing and alignment

factors

project       

and

product

   

      

of

levels

high

      

   

joint parsing and alignment

factors

project       

and

product

   

      

of

levels

high

      

   

joint parsing and alignment

factors

project       

and

product

   

      

of

levels

high

      

   

joint parsing and alignment

factors

project       

and

product

   

      

of

levels

high

      

   

notational abuse

subscript omission:

shorthand:

skip nonexistent substructures:

structural factors 

are implicit

model form

training

expected feature 

counts

marginals

structured mean field 

approximation

approximate component scores

monolingual parser:

score for 

if we knew           : 

score for 

to compute         :

score for 

expected feature counts

for fixed            :

marginals computed

with bitext inside-outside

marginals computed
with inside-outside

id136 procedure

initialize:

id136 procedure

iterate marginal updates:

   until convergence!

approximate marginals

decoding

(minimum risk)

structured mean field summary

split the model into pieces you have dynamic 
programs for

substitute expected feature counts for actual 
counts in cross-component factors

iterate computing marginals until convergence

structured mean field tips

try to make sure cross-component features are 
products of indicators
you don   t have to run all the way to 
convergence; marginals are usually pretty good 
after just a few rounds
recompute marginals for fast components 
more frequently than for slow ones

e.g. for joint parsing and alignment, the two 
monolingual tree marginals (           ) were updated 
until convergence between each update of the itg 
marginals (           )

break time!

part 4: belief propagation

belief propagation

wanted:

idea: pretend graph is a tree

key objects:

beliefs (marginals)
messages

belief propagation intro

assume we 
have a tree

//

belief propagation intro

messages

variable to factor

factor to variable

//

both take form of    distribution    over

messages general form

messages from variables to factors:

messages general form

messages from factors to variables:

marginal beliefs

belief propagation on tree-

structured graphs

if the factor graph has no cycles, bp is exact

can always order message computations

after one pass, marginal beliefs are correct

   loopy    belief propagation

problem: we no longer have a tree

solution: ignore problem

   loopy    belief propagation

just start passing messages anyway!

belief propagation q&a

are the marginals guaranteed to converge to 
the right thing, like in sampling?

no

well, is the algorithm at least guaranteed to 
converge to something, like mean field?

no

will everything often work out more or less 
ok in practice?

maybe

belief propagation example

exact

.34 .66

.81 .19

1

1

1

8

.16

.84

1

8

9

1

7

1

1

3

.59

.41

6

2

1

3

7

1

1

3

.5
.67

.5
.33

bp

.5
.5
.67 .33

.5

.5

.5

.5

belief propagation example

exact

.34 .66

.59

.41

.16

.84

.67

.33

bp

.67 .33
.31 .69

1

1

1

8

.5
.23

.5
.77

.81 .19

.5

.5

belief propagation example

exact

.34 .66

.59

.41

.16

.84

.81 .19

bp

.31 .69

.5
.5
.74 .26

.67
.74

.33
.26

6

2

1

3

.23

.77

belief propagation example

exact

.34 .66

bp

.31 .69

.59

.41

.16

.84

.74

.26

.81 .19

.74 .26
.86 .14

.23
.13

.77
.87

1

8

9

1

belief propagation example

exact

.34 .66

.59

.41

.16

.84

bp

.35 .65

7

1

1

3

.53

.47

.13

.87

.81 .19

.86 .14

belief propagation example

exact

.34 .66

.59

.41

.16

.84

.53

.47

bp

.30 .70

1

1

1

8

.14

.86

.81 .19

.86 .14

belief propagation example

exact

.34 .66

.59

.41

.16

.84

.81 .19

bp

.30 .70

.80 .20

.61

.39

6

2

1

3

.14

.86

belief propagation example

exact

.34 .66

bp

.30 .70

.59

.41

.16

.84

.61

.39

.81 .19

.79 .21

.19

.81

1

8

9

1

belief propagation example

exact

.34 .66

bp

.37 .63

.59

.41

.16

.84

.57

.43

.21

.79

.81 .19

.77 .23

belief propagation example

exact

.34 .66

.81 .19

.59

.41

.16

.84

.85

.15

mean
field

.38 .62

.97 .03

.57

.43

.03

.97

bp

.37 .63

.77 .23

.21

.79

belief propagation example

exact

.24 .76

bp

.28 .72

.29

.71

.19

.81

.36

.64

.27

.73

.77 .23

.69 .31

playing telephone

part 5: belief propagation with 

structured factors

structured factors

problem:

computing factor messages is exponential in arity

many models we care about have high-arity factors

solution:

take advantage of nlp tricks for efficient sums

examples:

word alignment (at-most-one constraints)

id33 (tree constraint)

warm-up exercise

warm-up exercise

warm-up exercise

warm-up exercise

warm-up exercise

warm-up exercise

warm-up exercise

warm-up exercise

benefits:

cleans up notation

saves time 
multiplying

enables efficient 
summing

the shape of structured bp

isolate the combinatorial factors

figure out how to compute 
efficient sums

directly exploiting sparsity

id145

work out the bookkeeping

or, use a reference!

word alignment with bp

(cromi  res & kurohashi, 2009)

(burkett & klein, 2012)

computing messages from factors

exponential in arity of factor

(have to sum over all assignments)

arity 1

arity

arity

computing constraint factor 

messages

input:

goal:

computing constraint factor 

messages

: assignment to variables where 

computing constraint factor 

messages

: assignment to variables where 

: special case for all off

computing constraint factor 

messages

input:

goal:

only need to consider

for 

computing constraint factor 

messages

computing constraint factor 

messages

computing constraint factor 

messages

computing constraint factor 

messages

computing constraint factor 

messages

computing constraint factor 

messages

computing constraint factor 

messages

computing constraint factor 

messages

1. precompute:

2.

3. partition:

4. messages: 

using bp marginals

expected feature counts:

marginal decoding:

id33 with bp

(smith & eisner, 2008)

(martins et al., 2010)

id33 with bp

arity 1 

arity 2

arity

exponential in arity of factor

messages from the tree factor

input:                              for all variables

goal:                               for all variables

what do parsers do?

initial state:

value of an edge (   has parent   ): 

value of a tree:

run inside-outside to compute:

total score for all trees:

total score for an edge: 

(klein & manning, 2002)

initializing the parser

problem:

product over edges in    :

or

product over all edges,
including

solution: use odds ratios

running the parser

sums we want:

computing tree factor messages

1. precompute:

2.

initialize:

3. run inside-outside

4. messages:

using bp marginals

expected feature counts:

minimum risk decoding:

1. initialize:

2. run parser:

structured bp summary

tricky part is factors whose arity grows with 
input size

simplify the problem by focusing on sums of 
total scores

exploit problem-specific structure to compute 
sums efficiently

use odds ratios to eliminate    default    values 
that don   t appear in dynamic program sums

belief propagation tips

don   t compute unary messages multiple times

store variable beliefs to save time computing 
variable to factor messages (divide one out)

update the slowest messages less frequently

you don   t usually need to run to convergence; 
measure the speed/performance tradeoff

part 6: wrap-up

mean field vs belief propagation

when to use mean field:

models made up of weakly interacting structures 
that are individually tractable

joint models often have this flavor

when to use belief propagation:

models with intersecting factors that are tractable 
in isolation but interact badly

you often get models like this when adding non-
local features to an existing tractable model

mean field vs belief propagation

mean field advantages

for models where it applies, the coordinate ascent 
procedure converges quite quickly

belief propagation advantages

more broadly applicable

more freedom to focus on factor graph design 
when modeling

advantages of both

work pretty well when the real posterior is 
peaked (like in nlp models!)

other variational techniques

id58

mean field for models with parametric forms (e.g. 
liang et al., 2007; cohen et al., 2010)

expectation propagation

theoretical generalization of bp
works kind of like mean field in practice; good for 
product models (e.g. hall and klein, 2012)

convex relaxation

optimize a convex approximate objective

related techniques

id209

not probabilistic, but good for finding maxes in 
similar models (e.g. koo et al., 2010; denero & 
machery, 2011)

search approximations

e.g. pruning, id125, reranking

orthogonal to approximate id136 techniques 
(and often stackable!)

thank you

appendix a: bibliography

references

id49

john d. lafferty, andrew mccallum, and fernando c. 
n. pereira (2001). id49: 
probabilistic models for segmenting and labeling 
sequence data. in icml.

edge-factored id33

ryan mcdonald, koby crammer, and fernando  
pereira (2005). online large-margin training of 
dependency parsers. in acl.
ryan mcdonald, fernando  pereira, kiril ribarov, and 
jan haji   (2005). non-projective id33 
using spanning tree algorithms. in hlt/emnlp.

references

factorial chain crf

charles sutton, khashayar rohanimanesh, and  
andrew mccallum (2004). dynamic conditional 
random fields: factorized probabilistic models for 
labeling and segmenting sequence data. in icml.

second-order id33

ryan mcdonald and fernando pereira (2006). online 
learning of approximate id33 
algorithms. in eacl.
xavier carreras (2007). experiments with a higher-
order projective dependency parser. in conll shared 
task session.

references

max matching word alignment

ben taskar, simon, lacoste-julien, and dan klein 
(2005). a discriminative matching approach to word 
alignment. in hlt/emnlp.

iterated conditional modes

julian besag (1986). on the statistical analysis of dirty 
pictures. journal of the royal statistical society, series 
b. vol. 48, no. 3, pp. 259-302.

structured mean field

eric p. xing, michael i. jordan, and stuart russell 
(2003). a generalized mean field algorithm for 
variational id136 in exponential families. in uai.

references

joint parsing and alignment

david burkett, john blitzer, and dan klein (2010). joint 
parsing and alignment with weakly synchronized 
grammars. in naacl.

word alignment with belief propagation

jan niehues and stephan vogel (2008). discriminative 
word alignment via alignment matrix modelling. in 
acl:hlt.
fabien cromi  res and sadao kurohashi (2009). an 
alignment algorithm using belief propagation and a 
structure-based distortion model. in eacl.
david burkett and dan klein (2012). fast id136 in 
phrase extraction models with belief propagation. in 
naacl.

references

id33 with belief propagation

david a. smith and jason eisner (2008). id33 by 
belief propagation. in emnlp.
andr   f. t. martins, noah a. smith, eric p. xing, pedro m. q. 
aguiar, and m  rio a. t. figueiredo (2010). turbo parsers: 
id33 by approximate variational id136. in 
emnlp.

odds ratios

dan klein and chris manning (2002). a generative constituent-
context model for improved grammar induction. in acl.

id58

percy liang, slav petrov, michael i. jordan, and dan klein (2007). 
the infinite pid18 using hierarchical dirichlet processes. in 
emnlp/conll.
shay b. cohen, david m. blei, and noah a. smith (2010). 
variational id136 for adaptor grammars. in naacl.

references

expectation propagation

david hall and dan klein (2012). training factored pid18s 
with expectation propagation. in emnlp-conll.

id209

terry koo, alexander m. rush, michael collins, tommi
jaakkola, and david sontag (2010). id209 for 
parsing with non-projective head automata. in emnlp.
alexander m. rush, david sontag, michael collins, and 
tommi jaakkola (2010). on id209 and linear 
programming relaxations for natural language processing. 
in emnlp.
john denero and klaus macherey (2011). model-based 
aligner combination using id209. in acl.

further reading

theoretical background

martin j. wainwright and michael i. jordan (2008). 
id114, exponential families, and 
variational id136. foundations and trends in 
machine learning, vol. 1, no. 1-2, pp. 1-305.

gentle introductions

christopher m. bishop (2006). pattern recognition 
and machine learning. springer.
david j.c. mackay (2003). id205, 
id136, and learning algorithms. cambridge 
university press.

further reading

more variational id136 for structured nlp

zhifei li, jason eisner, and sanjeev khudanpur (2009). 
variational decoding for id151. in acl.
michael auli and adam lopez (2011). a comparison of loopy 
belief propagation and id209 for integrated id35 
id55 and parsing. in acl.
veselin stoyanov and jason eisner (2012). minimum-risk 
training of approximate crf-based nlp systems. in naacl.
jason naradowsky, sebastian riedel, and david a. smith (2012). 
improving nlp through marginalization of hidden syntactic 
structure. in emnlp-conll.
greg durrett, david hall, and dan klein (2013). decentralized 
entity-level modeling for coreference resolution. in acl.

appendix b: mean field update 

derivation

mean field update derivation

model:

approximate graph: 

goal:

mean field update derivation

mean field update derivation

mean field update derivation

mean field update derivation

mean field update derivation

mean field update derivation

mean field update derivation

mean field update derivation

mean field update derivation

mean field update derivation

mean field update derivation

mean field update derivation

mean field update derivation

mean field update derivation

mean field update derivation

appendix c: joint parsing and 

alignment component distributions

joint parsing and alignment 

component distributions

joint parsing and alignment 

component distributions

joint parsing and alignment 

component distributions

appendix d: forward-backward 

as belief propagation

forward-backward as belief 

propagation

forward-backward as belief 

propagation

forward-backward as belief 

propagation

forward-backward marginal beliefs

